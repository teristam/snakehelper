Directory structure:
└── snakemake-snakemake/
    ├── README.md
    ├── CODE_OF_CONDUCT.md
    ├── doc-environment.yml
    ├── Dockerfile
    ├── LICENSE.md
    ├── MANIFEST.in
    ├── pyproject.toml
    ├── setup.py
    ├── .dockerignore
    ├── .gitpod.yml
    ├── .readthedocs.yml
    ├── .sonarcloud.properties
    ├── .test_durations
    ├── .wci.yml
    ├── apidocs/
    │   ├── conf.py
    │   ├── index.rst
    │   ├── Makefile
    │   ├── requirements.txt
    │   ├── .readthedocs.yaml
    │   ├── _templates/
    │   │   ├── module_template.rst
    │   │   └── toc.html
    │   └── api_reference/
    │       ├── snakemake_api.rst
    │       ├── snakemake_utils.rst
    │       └── internal/
    │           ├── modules.rst
    │           ├── snakemake.assets.rst
    │           ├── snakemake.caching.rst
    │           ├── snakemake.common.rst
    │           ├── snakemake.common.tests.rst
    │           ├── snakemake.common.tests.testcases.groups.rst
    │           ├── snakemake.common.tests.testcases.rst
    │           ├── snakemake.common.tests.testcases.simple.rst
    │           ├── snakemake.deployment.rst
    │           ├── snakemake.executors.rst
    │           ├── snakemake.ioutils.rst
    │           ├── snakemake.linting.rst
    │           ├── snakemake.remote.rst
    │           ├── snakemake.report.html_reporter.data.rst
    │           ├── snakemake.report.html_reporter.rst
    │           ├── snakemake.report.html_reporter.template.components.rst
    │           ├── snakemake.report.html_reporter.template.rst
    │           ├── snakemake.report.rst
    │           ├── snakemake.rst
    │           ├── snakemake.script.rst
    │           ├── snakemake.settings.rst
    │           ├── snakemake.template_rendering.rst
    │           ├── snakemake.unit_tests.rst
    │           └── snakemake.unit_tests.templates.rst
    ├── docs/
    │   ├── conf.py
    │   ├── index.rst
    │   ├── Makefile
    │   ├── requirements.txt
    │   ├── _static/
    │   │   ├── custom.css
    │   │   ├── gurubase-widget.js
    │   │   └── sphinx-argparse.css
    │   ├── _templates/
    │   │   └── toc.html
    │   ├── executing/
    │   │   ├── caching.rst
    │   │   ├── cli.rst
    │   │   ├── grouping.rst
    │   │   ├── interoperability.rst
    │   │   └── monitoring.rst
    │   ├── getting_started/
    │   │   ├── installation.rst
    │   │   └── migration.rst
    │   ├── project_info/
    │   │   ├── authors.rst
    │   │   ├── citations.rst
    │   │   ├── codebase.rst
    │   │   ├── contributing.rst
    │   │   ├── faq.rst
    │   │   ├── history.md
    │   │   ├── license.rst
    │   │   └── more_resources.rst
    │   ├── snakefiles/
    │   │   ├── best_practices.rst
    │   │   ├── configuration.rst
    │   │   ├── deployment.rst
    │   │   ├── foreign_wms.rst
    │   │   ├── modularization.rst
    │   │   ├── reporting.rst
    │   │   ├── storage.rst
    │   │   ├── testing.rst
    │   │   ├── utils.rst
    │   │   └── writing_snakefiles.rst
    │   └── tutorial/
    │       ├── additional_features.rst
    │       ├── advanced.rst
    │       ├── basics.rst
    │       ├── setup.rst
    │       ├── tutorial.rst
    │       └── interaction_visualization_reporting/
    │           ├── tutorial.rst
    │           └── workdir/
    │               └── workflow/
    │                   ├── Snakefile
    │                   ├── envs/
    │                   │   ├── download.yaml
    │                   │   ├── pystats.yaml
    │                   │   └── rstats.yaml
    │                   ├── notebooks/
    │                   │   ├── get_data.py.ipynb
    │                   │   ├── plot_horsepower_vs_mpg.py.ipynb
    │                   │   └── plot_horsepower_vs_mpg.r.ipynb
    │                   ├── report/
    │                   │   ├── cars.rst
    │                   │   ├── horsepower_vs_mpg.rst
    │                   │   └── workflow.rst
    │                   └── resources/
    │                       └── datavzrd/
    │                           └── cars.yaml
    ├── examples/
    │   ├── c/
    │   │   ├── README.txt
    │   │   ├── include/
    │   │   │   └── hello.h
    │   │   └── src/
    │   │       ├── hello.c
    │   │       ├── hellofunc.c
    │   │       ├── Makefile
    │   │       └── Snakefile
    │   ├── cufflinks/
    │   │   ├── hg19.fa
    │   │   ├── hg19.gtf
    │   │   ├── Snakefile
    │   │   └── mapped/
    │   │       ├── 101.bam
    │   │       ├── 102.bam
    │   │       ├── 103.bam
    │   │       └── 104.bam
    │   ├── flux/
    │   │   ├── Dockerfile
    │   │   └── Snakefile
    │   ├── hello-world/
    │   │   ├── config.yaml
    │   │   ├── Snakefile
    │   │   ├── envs/
    │   │   │   ├── matplotlib.yaml
    │   │   │   └── xsv.yaml
    │   │   └── scripts/
    │   │       └── plot-hist.py
    │   ├── latex/
    │   │   ├── document.tex
    │   │   ├── response-to-editor.tex
    │   │   ├── Snakefile
    │   │   └── tex.rules
    │   └── mirna/
    │       └── dag.dot
    ├── misc/
    │   ├── nano/
    │   │   ├── README.md
    │   │   └── syntax/
    │   │       └── snakemake.nanorc
    │   └── vim/
    │       ├── README.md
    │       ├── ftdetect/
    │       │   └── snakemake.vim
    │       ├── ftplugin/
    │       │   └── snakemake/
    │       │       ├── comment.vim
    │       │       ├── folding.vim
    │       │       └── sections.vim
    │       └── syntax/
    │           └── snakemake.vim
    ├── playground/
    │   └── .gitkeep
    ├── src/
    │   └── snakemake/
    │       ├── __init__.py
    │       ├── __main__.py
    │       ├── api.py
    │       ├── benchmark.py
    │       ├── checkpoints.py
    │       ├── cwl.py
    │       ├── decorators.py
    │       ├── exceptions.py
    │       ├── gui.html
    │       ├── gui.py
    │       ├── ioflags.py
    │       ├── logging.py
    │       ├── modules.py
    │       ├── notebook.py
    │       ├── output_index.py
    │       ├── parser.py
    │       ├── path_modifier.py
    │       ├── persistence.py
    │       ├── profiles.py
    │       ├── report.css
    │       ├── resources.py
    │       ├── ruleinfo.py
    │       ├── rules.py
    │       ├── shell.py
    │       ├── snakemake.code-workspace
    │       ├── sourcecache.py
    │       ├── spawn_jobs.py
    │       ├── storage.py
    │       ├── target_jobs.py
    │       ├── utils.py
    │       ├── wrapper.py
    │       ├── assets/
    │       │   └── __init__.py
    │       ├── caching/
    │       │   ├── __init__.py
    │       │   ├── hash.py
    │       │   ├── local.py
    │       │   └── storage.py
    │       ├── common/
    │       │   ├── __init__.py
    │       │   ├── argparse.py
    │       │   ├── configfile.py
    │       │   ├── git.py
    │       │   ├── prefix_lookup.py
    │       │   ├── tbdstring.py
    │       │   ├── typing.py
    │       │   ├── workdir_handler.py
    │       │   └── tests/
    │       │       ├── __init__.py
    │       │       └── testcases/
    │       │           ├── __init__.py
    │       │           ├── groups/
    │       │           │   ├── __init__.py
    │       │           │   ├── caption.rst
    │       │           │   └── Snakefile
    │       │           └── simple/
    │       │               ├── __init__.py
    │       │               ├── caption.rst
    │       │               └── Snakefile
    │       ├── deployment/
    │       │   ├── __init__.py
    │       │   ├── conda.py
    │       │   ├── containerize.py
    │       │   ├── env_modules.py
    │       │   └── singularity.py
    │       ├── executors/
    │       │   ├── __init__.py
    │       │   ├── dryrun.py
    │       │   ├── google_lifesciences_helper.py
    │       │   ├── jobscript.sh
    │       │   ├── local.py
    │       │   └── touch.py
    │       ├── io/
    │       │   ├── fmt.py
    │       │   └── flags/
    │       │       ├── __init__.py
    │       │       └── access_patterns.py
    │       ├── ioutils/
    │       │   ├── __init__.py
    │       │   ├── branch.py
    │       │   ├── collect.py
    │       │   ├── evaluate.py
    │       │   ├── exists.py
    │       │   ├── input.py
    │       │   ├── lookup.py
    │       │   ├── rule_items_proxy.py
    │       │   └── subpath.py
    │       ├── linting/
    │       │   ├── __init__.py
    │       │   ├── links.py
    │       │   ├── rules.py
    │       │   └── snakefiles.py
    │       ├── remote/
    │       │   ├── __init__.py
    │       │   ├── AzBlob.py
    │       │   ├── dropbox.py
    │       │   ├── EGA.py
    │       │   ├── FTP.py
    │       │   ├── gfal.py
    │       │   ├── gridftp.py
    │       │   ├── GS.py
    │       │   ├── HTTP.py
    │       │   ├── iRODS.py
    │       │   ├── NCBI.py
    │       │   ├── S3.py
    │       │   ├── S3Mocked.py
    │       │   ├── SFTP.py
    │       │   ├── webdav.py
    │       │   ├── XRootD.py
    │       │   └── zenodo.py
    │       ├── report/
    │       │   ├── __init__.py
    │       │   ├── common.py
    │       │   ├── rulegraph_spec.py
    │       │   └── html_reporter/
    │       │       ├── __init__.py
    │       │       ├── common.py
    │       │       ├── data/
    │       │       │   ├── __init__.py
    │       │       │   ├── categories.py
    │       │       │   ├── common.py
    │       │       │   ├── configfiles.py
    │       │       │   ├── metadata.py
    │       │       │   ├── packages.py
    │       │       │   ├── results.py
    │       │       │   ├── rulegraph.py
    │       │       │   ├── rules.py
    │       │       │   ├── runtimes.py
    │       │       │   └── timeline.py
    │       │       └── template/
    │       │           ├── __init__.py
    │       │           ├── index.html.jinja2
    │       │           ├── style.css
    │       │           └── components/
    │       │               ├── __init__.py
    │       │               ├── abstract_menu.js
    │       │               ├── abstract_results.js
    │       │               ├── abstract_view_manager.js
    │       │               ├── app.js
    │       │               ├── breadcrumbs.js
    │       │               ├── button.js
    │       │               ├── category.js
    │       │               ├── common.js
    │       │               ├── content.js
    │       │               ├── icon.js
    │       │               ├── list_heading.js
    │       │               ├── list_item.js
    │       │               ├── menu.js
    │       │               ├── metadata.js
    │       │               ├── navbar.js
    │       │               ├── report_info.js
    │       │               ├── result_info.js
    │       │               ├── result_view_button.js
    │       │               ├── rule_graph.js
    │       │               ├── rule_info.js
    │       │               ├── search_results.js
    │       │               ├── stats.js
    │       │               ├── subcategory.js
    │       │               └── toggle.js
    │       ├── scheduling/
    │       │   ├── __init__.py
    │       │   ├── greedy.py
    │       │   ├── job_scheduler.py
    │       │   └── milp.py
    │       ├── settings/
    │       │   ├── __init__.py
    │       │   ├── enums.py
    │       │   └── types.py
    │       ├── template_rendering/
    │       │   ├── __init__.py
    │       │   ├── jinja2.py
    │       │   └── yte.py
    │       └── unit_tests/
    │           ├── __init__.py
    │           └── templates/
    │               ├── __init__.py
    │               ├── common.py.jinja2
    │               ├── conftest.py.jinja2
    │               └── ruletest.py.jinja2
    ├── tests/
    │   ├── README.md
    │   ├── __init__.py
    │   ├── common.py
    │   ├── conftest.py
    │   ├── test_api.py
    │   ├── test_args.py
    │   ├── test_executor_test_suite.py
    │   ├── test_expand.py
    │   ├── test_internals.py
    │   ├── test_io.py
    │   ├── test_linting.py
    │   ├── test_output_index.py
    │   ├── test_path_modifier.py
    │   ├── test_persistence.py
    │   ├── test_prefix_lookup.py
    │   ├── test_schema.py
    │   ├── test_script.py
    │   ├── test_sourcecache.py
    │   ├── tests_using_conda.py
    │   ├── knapsack/
    │   │   ├── 1.txt
    │   │   ├── 2.txt
    │   │   ├── 3.txt
    │   │   └── Snakefile
    │   ├── linting/
    │   │   ├── absolute_paths/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── envvars/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── iofile_by_index/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── log_directive/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── long_run/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── missing_software_definition/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── mixed_func_and_rules/
    │   │   │   ├── common.smk
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── not_used_params/
    │   │   │   ├── config.yaml
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── params_prefix/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── path_add/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   ├── singularity/
    │   │   │   ├── negative.smk
    │   │   │   └── positive.smk
    │   │   └── tab_usage/
    │   │       ├── negative.smk
    │   │       └── positive.smk
    │   ├── profile/
    │   │   └── Snakefile
    │   ├── resources/
    │   │   └── slurmcluster/
    │   │       ├── Makefile
    │   │       └── Vagrantfile
    │   ├── test (with parenthese's)/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test01/
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── expected-results/
    │   │       └── test.inter
    │   ├── test02/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test03/
    │   │   ├── params
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test04/
    │   │   ├── params
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test05/
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── expected-results/
    │   │       ├── test.1.inter
    │   │       ├── test.1.inter2
    │   │       ├── test.2.inter
    │   │       ├── test.2.inter2
    │   │       ├── test.3.inter
    │   │       ├── test.3.inter2
    │   │       └── test.predictions
    │   ├── test06/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test07/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test08/
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── test2.in
    │   ├── test09/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test10/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test11/
    │   │   ├── import.snakefile
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── expected-results/
    │   │       └── test.inter
    │   ├── test12/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test13/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test15/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_access_patterns/
    │   │   ├── sbatch
    │   │   ├── Snakefile
    │   │   └── fs-storage/
    │   │       └── .gitkeep
    │   ├── test_all_temp/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_ambiguousruleexception/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_ancient/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── A
    │   │       ├── B
    │   │       ├── C
    │   │       ├── D
    │   │       └── old_file
    │   ├── test_ancient_cli/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── 1
    │   │       ├── 2
    │   │       ├── 3
    │   │       └── 4
    │   ├── test_ancient_dag/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_archive/
    │   │   ├── Snakefile
    │   │   └── test-env.yaml
    │   ├── test_azure_batch/
    │   │   └── Snakefile
    │   ├── test_bash/
    │   │   └── Snakefile
    │   ├── test_batch/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── a.inter.txt
    │   │       └── bar.txt
    │   ├── test_batch_final/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── bar.txt
    │   │       └── foo.txt
    │   ├── test_benchmark/
    │   │   ├── script.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test.benchmark_run.txt
    │   │       ├── test.benchmark_run_shell.txt
    │   │       ├── test.benchmark_script.txt
    │   │       └── test.benchmark_shell.txt
    │   ├── test_benchmark_jsonl/
    │   │   ├── input1.zero
    │   │   ├── input2.zero
    │   │   ├── script.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test.benchmark_run.jsonl
    │   │       ├── test.benchmark_run_shell.jsonl
    │   │       ├── test.benchmark_script.jsonl
    │   │       └── test.benchmark_shell.jsonl
    │   ├── test_cache_multioutput/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_censored_path/
    │   │   └── Snakefile
    │   ├── test_checkpoint_allowed_rules/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── b.txt
    │   ├── test_checkpoint_missout/
    │   │   ├── problem.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── final.txt
    │   ├── test_checkpoint_open/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── storage/
    │   │   │       └── test2.txt
    │   │   └── storage/
    │   │       └── test.txt
    │   ├── test_checkpoints/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── aggregated/
    │   │   │       └── a.txt
    │   │   └── samples/
    │   │       ├── a.txt
    │   │       └── b.txt
    │   ├── test_checkpoints_dir/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── aggregated/
    │   │   │       ├── a.txt
    │   │   │       └── b.txt
    │   │   └── samples/
    │   │       ├── a.txt
    │   │       └── b.txt
    │   ├── test_checkpoints_many/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── collect/
    │   │           └── s1/
    │   │               └── all_done.txt
    │   ├── test_cloud_checkpoints_issue574/
    │   │   ├── config.json
    │   │   ├── env.yml
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_cluster_cancelscript/
    │   │   ├── sbatch
    │   │   ├── scancel.sh
    │   │   ├── Snakefile.nonstandard
    │   │   ├── status.sh
    │   │   ├── test.in
    │   │   └── expected-results/
    │   │       └── scancel.txt
    │   ├── test_conda/
    │   │   ├── Snakefile
    │   │   └── test-env.yaml
    │   ├── test_conda_cmd_exe/
    │   │   ├── Snakefile
    │   │   └── test-env.yaml
    │   ├── test_conda_custom_prefix/
    │   │   ├── Snakefile
    │   │   └── test-env.yaml
    │   ├── test_conda_function/
    │   │   └── Snakefile
    │   ├── test_conda_global/
    │   │   ├── env.yaml
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_conda_named/
    │   │   └── Snakefile
    │   ├── test_conda_pin_file/
    │   │   ├── Snakefile
    │   │   ├── test-env.linux-64.pin.txt
    │   │   ├── test-env.yaml
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_conda_python_3_7_script/
    │   │   ├── Snakefile
    │   │   ├── test_python_env.yaml
    │   │   ├── test_script_python_3_7.py
    │   │   └── expected-results/
    │   │       └── version.txt
    │   ├── test_conda_python_script/
    │   │   ├── Snakefile
    │   │   ├── test_python_env.yaml
    │   │   ├── test_script.py
    │   │   └── expected-results/
    │   │       └── version.txt
    │   ├── test_conda_run/
    │   │   ├── Snakefile
    │   │   ├── test_python_env.yaml
    │   │   ├── test_script_run.py
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_conditional/
    │   │   └── Snakefile
    │   ├── test_config/
    │   │   ├── Snakefile
    │   │   ├── test.json
    │   │   ├── test.rules
    │   │   ├── test2.json
    │   │   └── test3.json
    │   ├── test_config_merging/
    │   │   ├── config_cmdline_01.yaml
    │   │   ├── config_cmdline_02.yaml
    │   │   ├── config_snakefile.yaml
    │   │   └── Snakefile
    │   ├── test_config_replacing/
    │   │   ├── cli-config.yaml
    │   │   ├── Snakefile
    │   │   ├── workflow-config.yaml
    │   │   └── expected-results/
    │   │       └── result.txt
    │   ├── test_config_replacing_nocli/
    │   │   ├── Snakefile
    │   │   ├── workflow-config.yaml
    │   │   └── expected-results/
    │   │       └── result.txt
    │   ├── test_config_yte/
    │   │   ├── config.yaml
    │   │   └── Snakefile
    │   ├── test_container/
    │   │   ├── Snakefile
    │   │   └── test.txt
    │   ├── test_containerized/
    │   │   ├── Dockerfile
    │   │   ├── env.yaml
    │   │   └── Snakefile
    │   ├── test_convert_to_cwl/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_converting_path_for_r_script/
    │   │   ├── env.yaml
    │   │   ├── r-script.R
    │   │   ├── Snakefile
    │   │   ├── text-file.txt
    │   │   └── expected-results/
    │   │       └── .github_touch
    │   ├── test_core_dependent_threads/
    │   │   └── Snakefile
    │   ├── test_cwl/
    │   │   ├── Snakefile
    │   │   ├── test.txt
    │   │   └── expected-results/
    │   │       └── sorted.txt
    │   ├── test_default_flags/
    │   │   └── Snakefile
    │   ├── test_default_remote/
    │   │   └── Snakefile
    │   ├── test_default_resources/
    │   │   ├── Snakefile
    │   │   └── test.txt
    │   ├── test_default_storage_local_job/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_default_target/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── 1.txt
    │   │       └── 2.txt
    │   ├── test_deferred_func_eval/
    │   │   ├── Snakefile
    │   │   ├── test1.in
    │   │   └── test2.in
    │   ├── test_delete_all_output/
    │   │   ├── infile
    │   │   ├── Snakefile
    │   │   ├── Snakefile_inner
    │   │   └── expected-results/
    │   │       └── all_ok
    │   ├── test_delete_output/
    │   │   ├── nosuchfile
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── output.file
    │   │       └── foo/
    │   │           └── output.foo.file
    │   ├── test_deploy_hashing/
    │   │   ├── a.post-deploy.sh
    │   │   ├── a.yaml
    │   │   ├── b.yaml
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── a.txt
    │   │       └── b.txt
    │   ├── test_deploy_script/
    │   │   ├── Snakefile
    │   │   ├── test-env.post-deploy.sh
    │   │   └── test-env.yaml
    │   ├── test_deploy_sources/
    │   │   ├── Snakefile
    │   │   └── scripts/
    │   │       └── test.py
    │   ├── test_directory/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   ├── child_to_input
    │   │   │   ├── dir_to_link_to/
    │   │   │   │   └── .snakemake_timestamp
    │   │   │   ├── some/
    │   │   │   │   ├── dir-child
    │   │   │   │   └── shadow/
    │   │   │   │       └── .snakemake_timestamp
    │   │   │   └── some_other_dir/
    │   │   │       └── .snakemake_timestamp
    │   │   └── input_dir/
    │   │       └── child
    │   ├── test_dup_out_patterns/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_dynamic_container/
    │   │   └── Snakefile
    │   ├── test_empty_include/
    │   │   ├── include.rules
    │   │   └── Snakefile
    │   ├── test_ensure/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test2.txt
    │   ├── test_env_modules/
    │   │   ├── Snakefile
    │   │   └── test-env-modules.sh
    │   ├── test_envvars/
    │   │   └── Snakefile
    │   ├── test_exists/
    │   │   ├── Snakefile
    │   │   └── test.txt
    │   ├── test_expand_flag/
    │   │   └── Snakefile
    │   ├── test_expand_list_of_functions/
    │   │   └── Snakefile
    │   ├── test_failed_intermediate/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test3.txt
    │   ├── test_filegraph/
    │   │   ├── Snakefile
    │   │   └── scripts/
    │   │       └── vis.py
    │   ├── test_filesep_windows/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── subfolder/
    │   │   │       └── test2.out2
    │   │   └── subfolder/
    │   │       └── .gitkeep
    │   ├── test_format_params/
    │   │   └── Snakefile
    │   ├── test_format_wildcards/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── foo.txt
    │   ├── test_fstring/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── SID23454678.txt
    │   ├── test_ftp_immediate_close/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── size.txt
    │   ├── test_generate_unit_tests/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── .tests/
    │   │   │       └── units/
    │   │   │           ├── common.py
    │   │   │           ├── conftest.py
    │   │   │           ├── test_a.py
    │   │   │           ├── test_b.py
    │   │   │           ├── a/
    │   │   │           │   ├── config/
    │   │   │           │   │   └── config/
    │   │   │           │   │       └── config.json
    │   │   │           │   ├── data/
    │   │   │           │   │   └── input.csv
    │   │   │           │   └── expected/
    │   │   │           │       └── test/
    │   │   │           │           └── 0.txt
    │   │   │           └── b/
    │   │   │               ├── config/
    │   │   │               │   └── config/
    │   │   │               │       └── config.json
    │   │   │               ├── data/
    │   │   │               │   └── test/
    │   │   │               │       └── 0.txt
    │   │   │               └── expected/
    │   │   │                   └── test/
    │   │   │                       └── 0.tsv
    │   │   └── .tests/
    │   │       └── integration/
    │   │           ├── input.csv
    │   │           ├── config/
    │   │           │   └── config.json
    │   │           └── test/
    │   │               ├── 0.tsv
    │   │               ├── 0.txt
    │   │               ├── 1.tsv
    │   │               ├── 1.txt
    │   │               ├── 2.tsv
    │   │               └── 2.txt
    │   ├── test_get_log_both/
    │   │   ├── environment.yaml
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── wrapper.py
    │   ├── test_get_log_complex/
    │   │   ├── environment.yaml
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── wrapper.py
    │   ├── test_get_log_none/
    │   │   ├── environment.yaml
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── wrapper.py
    │   ├── test_get_log_stderr/
    │   │   ├── environment.yaml
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── wrapper.py
    │   ├── test_get_log_stdout/
    │   │   ├── environment.yaml
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── wrapper.py
    │   ├── test_github_issue105/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── a.txt
    │   │       └── b.txt
    │   ├── test_github_issue1062/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_github_issue1069/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_github_issue1158/
    │   │   ├── qsub.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── output
    │   ├── test_github_issue1261/
    │   │   └── Snakefile
    │   ├── test_github_issue1384/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitempty
    │   ├── test_github_issue1389/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test1.txt
    │   │       └── test2.txt
    │   ├── test_github_issue1396/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_github_issue1460/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_github_issue1469/
    │   │   └── Snakefile
    │   ├── test_github_issue1498/
    │   │   ├── module.smk
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── results/
    │   │           ├── all.done
    │   │           └── other/
    │   │               └── results/
    │   │                   ├── a.txt
    │   │                   └── b.txt
    │   ├── test_github_issue1500/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   ├── expected-results/
    │   │   │   └── .gitkeep
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_github_issue1542/
    │   │   ├── a.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── a.txt
    │   ├── test_github_issue1550/
    │   │   └── Snakefile
    │   ├── test_github_issue1618/
    │   │   └── Snakefile
    │   ├── test_github_issue1818/
    │   │   ├── aggregated.txt
    │   │   ├── processed.txt
    │   │   ├── processed2.txt
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   ├── aggregated.txt
    │   │   │   ├── processed.txt
    │   │   │   ├── processed2.txt
    │   │   │   └── my_directory/
    │   │   │       ├── 1.txt
    │   │   │       ├── 2.txt
    │   │   │       ├── 3.txt
    │   │   │       └── .snakemake_timestamp
    │   │   ├── my_directory/
    │   │   │   ├── 1.txt
    │   │   │   ├── 2.txt
    │   │   │   ├── 3.txt
    │   │   │   └── .snakemake_timestamp
    │   │   └── .snakemake/
    │   │       └── metadata/
    │   │           ├── bXlfZGlyZWN0b3J5
    │   │           ├── cHJvY2Vzc2VkLnR4dA==
    │   │           ├── cHJvY2Vzc2VkMi50eHQ=
    │   │           └── YWdncmVnYXRlZC50eHQ=
    │   ├── test_github_issue1882/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── foo.txt
    │   ├── test_github_issue2142/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── data.txt
    │   ├── test_github_issue2154/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_github_issue261/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test1/
    │   │           └── target1/
    │   │               └── config1.done
    │   ├── test_github_issue2732/
    │   │   └── Snakefile
    │   ├── test_github_issue3271/
    │   │   ├── Snakefile
    │   │   └── Snakefile_should_fail
    │   ├── test_github_issue3556/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── dag.mmd
    │   ├── test_github_issue413/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── test_dir/
    │   │   │       ├── other_file -> test_file
    │   │   │       └── subdir/
    │   │   │           └── test_file
    │   │   └── test_dir/
    │   │       ├── other_file -> test_file
    │   │       └── subdir/
    │   │           └── test_file
    │   ├── test_github_issue456/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── test.txt
    │   │   └── references/
    │   │       └── gffs/
    │   │           └── test.gff
    │   ├── test_github_issue52/
    │   │   ├── B
    │   │   ├── other.smk
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_github_issue627/
    │   │   ├── config.yaml
    │   │   ├── job.py
    │   │   ├── local_job_status.sh
    │   │   ├── run_locally.sh
    │   │   ├── Snakefile
    │   │   ├── Snakefile.internal
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_github_issue640/
    │   │   ├── setup.sh
    │   │   └── Snakefile
    │   ├── test_github_issue727/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── checkpoint-test.bin.1.clust.info.csv
    │   │       ├── checkpoint-test.bin.10.clust.info.csv
    │   │       ├── checkpoint-test.bin.11.clust.info.csv
    │   │       ├── checkpoint-test.bin.12.clust.info.csv
    │   │       ├── checkpoint-test.bin.13.clust.info.csv
    │   │       ├── checkpoint-test.bin.14.clust.info.csv
    │   │       ├── checkpoint-test.bin.15.clust.info.csv
    │   │       ├── checkpoint-test.bin.16.clust.info.csv
    │   │       ├── checkpoint-test.bin.17.clust.info.csv
    │   │       ├── checkpoint-test.bin.18.clust.info.csv
    │   │       ├── checkpoint-test.bin.19.clust.info.csv
    │   │       ├── checkpoint-test.bin.2.clust.info.csv
    │   │       ├── checkpoint-test.bin.3.clust.info.csv
    │   │       ├── checkpoint-test.bin.4.clust.info.csv
    │   │       ├── checkpoint-test.bin.5.clust.info.csv
    │   │       ├── checkpoint-test.bin.6.clust.info.csv
    │   │       ├── checkpoint-test.bin.7.clust.info.csv
    │   │       ├── checkpoint-test.bin.8.clust.info.csv
    │   │       ├── checkpoint-test.bin.9.clust.info.csv
    │   │       ├── checkpoint-test.file1
    │   │       ├── checkpoint-test.final_file
    │   │       └── checkpoint-test/
    │   │           └── bins/
    │   │               ├── 1/
    │   │               │   └── read_list.txt
    │   │               ├── 10/
    │   │               │   └── read_list.txt
    │   │               ├── 11/
    │   │               │   └── read_list.txt
    │   │               ├── 12/
    │   │               │   └── read_list.txt
    │   │               ├── 13/
    │   │               │   └── read_list.txt
    │   │               ├── 14/
    │   │               │   └── read_list.txt
    │   │               ├── 15/
    │   │               │   └── read_list.txt
    │   │               ├── 16/
    │   │               │   └── read_list.txt
    │   │               ├── 17/
    │   │               │   └── read_list.txt
    │   │               ├── 18/
    │   │               │   └── read_list.txt
    │   │               ├── 19/
    │   │               │   └── read_list.txt
    │   │               ├── 2/
    │   │               │   └── read_list.txt
    │   │               ├── 3/
    │   │               │   └── read_list.txt
    │   │               ├── 4/
    │   │               │   └── read_list.txt
    │   │               ├── 5/
    │   │               │   └── read_list.txt
    │   │               ├── 6/
    │   │               │   └── read_list.txt
    │   │               ├── 7/
    │   │               │   └── read_list.txt
    │   │               ├── 8/
    │   │               │   └── read_list.txt
    │   │               └── 9/
    │   │                   └── read_list.txt
    │   ├── test_github_issue78/
    │   │   └── Snakefile
    │   ├── test_github_issue806/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── data/
    │   │           ├── batches.en
    │   │           ├── batches.es
    │   │           ├── corpus.txt
    │   │           ├── en/
    │   │           │   ├── 0/
    │   │           │   │   ├── 0/
    │   │           │   │   │   └── text
    │   │           │   │   ├── 1/
    │   │           │   │   │   └── text
    │   │           │   │   └── 2/
    │   │           │   │       └── text
    │   │           │   ├── 1/
    │   │           │   │   ├── 0/
    │   │           │   │   │   └── text
    │   │           │   │   ├── 1/
    │   │           │   │   │   └── text
    │   │           │   │   └── 2/
    │   │           │   │       └── text
    │   │           │   ├── 2/
    │   │           │   │   ├── 0/
    │   │           │   │   │   └── text
    │   │           │   │   ├── 1/
    │   │           │   │   │   └── text
    │   │           │   │   └── 2/
    │   │           │   │       └── text
    │   │           │   └── 3/
    │   │           │       ├── 0/
    │   │           │       │   └── text
    │   │           │       ├── 1/
    │   │           │       │   └── text
    │   │           │       └── 2/
    │   │           │           └── text
    │   │           ├── es/
    │   │           │   ├── 0/
    │   │           │   │   ├── 0/
    │   │           │   │   │   └── text
    │   │           │   │   ├── 1/
    │   │           │   │   │   └── text
    │   │           │   │   └── 2/
    │   │           │   │       └── text
    │   │           │   ├── 1/
    │   │           │   │   ├── 0/
    │   │           │   │   │   └── text
    │   │           │   │   ├── 1/
    │   │           │   │   │   └── text
    │   │           │   │   └── 2/
    │   │           │   │       └── text
    │   │           │   ├── 2/
    │   │           │   │   ├── 0/
    │   │           │   │   │   └── text
    │   │           │   │   ├── 1/
    │   │           │   │   │   └── text
    │   │           │   │   └── 2/
    │   │           │   │       └── text
    │   │           │   └── 3/
    │   │           │       ├── 0/
    │   │           │       │   └── text
    │   │           │       ├── 1/
    │   │           │       │   └── text
    │   │           │       └── 2/
    │   │           │           └── text
    │   │           └── es_en/
    │   │               ├── 0.0_0.combined
    │   │               ├── 0.0_1.combined
    │   │               ├── 0.0_2.combined
    │   │               ├── 0.1_0.combined
    │   │               ├── 0.1_1.combined
    │   │               ├── 0.1_2.combined
    │   │               ├── 0.2_0.combined
    │   │               ├── 0.2_1.combined
    │   │               ├── 0.2_2.combined
    │   │               ├── 1.0_0.combined
    │   │               ├── 1.0_1.combined
    │   │               ├── 1.0_2.combined
    │   │               ├── 1.1_0.combined
    │   │               ├── 1.1_1.combined
    │   │               ├── 1.1_2.combined
    │   │               ├── 1.2_0.combined
    │   │               ├── 1.2_1.combined
    │   │               ├── 1.2_2.combined
    │   │               ├── 2.0_0.combined
    │   │               ├── 2.0_1.combined
    │   │               ├── 2.0_2.combined
    │   │               ├── 2.1_0.combined
    │   │               ├── 2.1_1.combined
    │   │               ├── 2.1_2.combined
    │   │               ├── 2.2_0.combined
    │   │               ├── 2.2_1.combined
    │   │               ├── 2.2_2.combined
    │   │               ├── 3.0_0.combined
    │   │               ├── 3.0_1.combined
    │   │               ├── 3.0_2.combined
    │   │               ├── 3.1_0.combined
    │   │               ├── 3.1_1.combined
    │   │               ├── 3.1_2.combined
    │   │               ├── 3.2_0.combined
    │   │               ├── 3.2_1.combined
    │   │               └── 3.2_2.combined
    │   ├── test_github_issue929/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.file
    │   ├── test_github_issue988/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── all.faa
    │   │       ├── all.fasta
    │   │       └── files/
    │   │           ├── f0.faa
    │   │           ├── f0.fasta
    │   │           ├── f0.txt
    │   │           ├── f1.faa
    │   │           ├── f1.fasta
    │   │           ├── f1.txt
    │   │           ├── f2.faa
    │   │           ├── f2.fasta
    │   │           ├── f2.txt
    │   │           ├── f3.faa
    │   │           ├── f3.fasta
    │   │           ├── f3.txt
    │   │           └── .snakemake_timestamp
    │   ├── test_github_issue_14/
    │   │   ├── local_script.py
    │   │   ├── pythonTest.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── pythonTest
    │   ├── test_github_issue_3265_respect_dryrun_delete_all/
    │   │   ├── infile
    │   │   ├── Snakefile
    │   │   ├── Snakefile_inner
    │   │   └── expected-results/
    │   │       └── all_ok
    │   ├── test_globwildcards/
    │   │   ├── Snakefile
    │   │   ├── test.0.txt
    │   │   ├── test.1.txt
    │   │   ├── test.2.txt
    │   │   ├── test_a_a.txt
    │   │   ├── test_a_b.txt
    │   │   └── test_b_b.txt
    │   ├── test_google_lifesciences/
    │   │   ├── config.json
    │   │   ├── env.yml
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_group_job_fail/
    │   │   ├── qsub
    │   │   └── Snakefile
    │   ├── test_group_jobs/
    │   │   ├── qsub
    │   │   └── Snakefile
    │   ├── test_group_jobs_attempts/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── a
    │   ├── test_group_jobs_resources/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── status_failed
    │   ├── test_group_parallel/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── bar.txt
    │   │       └── foo.txt
    │   ├── test_group_with_pipe/
    │   │   ├── qsub
    │   │   └── Snakefile
    │   ├── test_groupid_expand/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── bar0.txt
    │   │       ├── bar1.txt
    │   │       ├── bar2.txt
    │   │       └── foo.local.txt
    │   ├── test_groupid_expand_cluster/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── bar0.txt
    │   │       ├── bar1.txt
    │   │       ├── bar2.txt
    │   │       ├── foo.412c59b9-e2fc-51ea-9a84-d055fb244f80.txt
    │   │       ├── foo.5792af91-9d58-5430-a941-2d29860112e7.txt
    │   │       └── foo.e1a9b7a0-f48e-568a-bc42-d6c2078055be.txt
    │   ├── test_groups_out_of_jobs/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_gs_requester_pays/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── landsat-data.txt
    │   ├── test_handle_storage_multi_consumers/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── storage/
    │   │   │       ├── test.a.txt
    │   │   │       └── test.b.txt
    │   │   └── storage/
    │   │       └── test.txt
    │   ├── test_handover/
    │   │   ├── script.py
    │   │   └── Snakefile
    │   ├── test_immediate_submit/
    │   │   ├── README.md
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── output/
    │   │   │       └── all
    │   │   └── slurm/
    │   │       ├── config.yaml
    │   │       ├── get_jobid.sh
    │   │       └── sbatch.sh
    │   ├── test_incomplete_params/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_inferred_resources/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_inferred_resources_mib/
    │   │   └── Snakefile
    │   ├── test_inner_call/
    │   │   ├── a.in
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── a.txt
    │   ├── test_inoutput_is_path/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_input_generator/
    │   │   ├── foo.1.in
    │   │   ├── foo.2.in
    │   │   ├── foo.3.in
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── foo.out1
    │   │       ├── foo.out2
    │   │       └── foo.out3
    │   ├── test_ioutils/
    │   │   ├── config.yaml
    │   │   ├── dummy1.tsv
    │   │   ├── in.txt
    │   │   ├── samples.md5
    │   │   ├── samples.tsv
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test.txt
    │   │       ├── b/
    │   │       │   └── 2.txt
    │   │       ├── c/
    │   │       │   ├── 1.txt
    │   │       │   └── 2.txt
    │   │       └── results/
    │   │           └── switch~someswitch.column~sample.txt
    │   ├── test_issue1037/
    │   │   ├── Foo_A.start
    │   │   ├── qsub
    │   │   └── Snakefile
    │   ├── test_issue1041/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── raw.split_0.txt
    │   │       ├── raw.split_1.txt
    │   │       └── raw.split_2.txt
    │   ├── test_issue1046/
    │   │   ├── Snakefile
    │   │   ├── test.csv
    │   │   ├── expected-results/
    │   │   │   └── test_report.html
    │   │   └── my_wrapper/
    │   │       ├── environment.yaml
    │   │       └── wrapper.py
    │   ├── test_issue1083/
    │   │   ├── bar.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── foo.txt
    │   ├── test_issue1085/
    │   │   └── Snakefile
    │   ├── test_issue1092/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── aggregated.txt
    │   │       └── aggregated.txt_WIN
    │   ├── test_issue1093/
    │   │   ├── condaenv.yaml
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── aggregated.txt
    │   ├── test_issue1256/
    │   │   └── Snakefile
    │   ├── test_issue1281/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── apple.sh
    │   │       └── banana.sh
    │   ├── test_issue1284/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── result.c1
    │   │       ├── result.c2
    │   │       ├── result.r1
    │   │       ├── result.r2
    │   │       └── result.r3
    │   ├── test_issue1331/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── aligned_and_sort/
    │   │           ├── 1.txt
    │   │           ├── 2.txt
    │   │           ├── 3.txt
    │   │           ├── 4.txt
    │   │           ├── 5.txt
    │   │           └── 6.txt
    │   ├── test_issue2574/
    │   │   ├── config.yaml
    │   │   └── Snakefile
    │   ├── test_issue2685/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test/
    │   │           ├── a.txt
    │   │           └── complete.txt
    │   ├── test_issue2826_failed_binary_logs/
    │   │   └── Snakefile
    │   ├── test_issue3192/
    │   │   ├── a.txt
    │   │   ├── script.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── c.txt
    │   ├── test_issue3338/
    │   │   ├── other_workflow.smk
    │   │   └── Snakefile
    │   ├── test_issue3361_fail/
    │   │   ├── input.txt
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── output.txt
    │   │   └── some_dir/
    │   │       └── some_other_input.txt
    │   ├── test_issue3361_pass/
    │   │   ├── input.txt
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── output.txt
    │   │   └── some_dir/
    │   │       └── some_other_input.txt
    │   ├── test_issue381/
    │   │   ├── a.in
    │   │   └── Snakefile
    │   ├── test_issue471/
    │   │   └── Snakefile
    │   ├── test_issue584/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── out1
    │   ├── test_issue612/
    │   │   └── Snakefile
    │   ├── test_issue635/
    │   │   ├── input.txt
    │   │   ├── Snakefile
    │   │   ├── envs/
    │   │   │   └── rmarkdown.yaml
    │   │   └── scripts/
    │   │       └── report.Rmd
    │   ├── test_issue805/
    │   │   └── Snakefile
    │   ├── test_issue823_1/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── b.txt
    │   ├── test_issue823_2/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── c.txt
    │   ├── test_issue823_3/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── a.txt
    │   ├── test_issue850/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── 2.qc2
    │   ├── test_issue854/
    │   │   └── Snakefile
    │   ├── test_issue860/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── 1.bam
    │   │       ├── 2.bam
    │   │       ├── 2.qc
    │   │       ├── 2.qc2
    │   │       ├── 3.bam
    │   │       └── 4.bam
    │   ├── test_issue894/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── a
    │   ├── test_issue912/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── fake/
    │   │   │       └── test/
    │   │   │           └── sample.Mutect2.snpSift.hardfilter.vcf.gz
    │   │   └── fake/
    │   │       └── test/
    │   │           └── sample.Mutect2.snpSift.vcf.gz
    │   ├── test_issue916/
    │   │   ├── local_script.py
    │   │   ├── pythonTest.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── pythonTest
    │   ├── test_issue930/
    │   │   ├── qsub
    │   │   └── Snakefile
    │   ├── test_issue956/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── f_A.txt
    │   │       └── f_B.txt
    │   ├── test_issue958/
    │   │   ├── a.txt
    │   │   └── Snakefile
    │   ├── test_issue_3202/
    │   │   ├── Snakefile
    │   │   ├── test-env.yaml
    │   │   └── expected-results/
    │   │       └── output.txt
    │   ├── test_job_properties/
    │   │   ├── qsub.py
    │   │   └── Snakefile
    │   ├── test_jupyter_notebook/
    │   │   ├── env.yaml
    │   │   ├── Notebook.ipynb
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── result_final.txt
    │   │       └── result_intermediate.txt
    │   ├── test_jupyter_notebook_draft/
    │   │   ├── data.txt
    │   │   ├── env.yaml
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── Notebook.py.ipynb
    │   │       └── results/
    │   │           └── .gitkeep
    │   ├── test_jupyter_notebook_nbconvert/
    │   │   ├── env.yaml
    │   │   ├── Notebook.ipynb
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── result_final.txt
    │   │       └── result_intermediate.txt
    │   ├── test_keyword_list/
    │   │   ├── Snakefile
    │   │   ├── test.in1
    │   │   └── test.in2
    │   ├── test_kubernetes/
    │   │   ├── README.md
    │   │   ├── Snakefile
    │   │   └── envs/
    │   │       └── gzip.yaml
    │   ├── test_lazy_resources/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── results/
    │   │           ├── bar.txt
    │   │           └── foo.txt
    │   ├── test_list_untracked/
    │   │   ├── Snakefile
    │   │   ├── Snakefile_inner
    │   │   └── expected-results/
    │   │       ├── leftover_files
    │   │       └── leftover_files_WIN
    │   ├── test_load_metawrapper/
    │   │   ├── Snakefile
    │   │   ├── data/
    │   │   │   ├── a.gtf
    │   │   │   └── genome.fa
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_local_and_retrieve/
    │   │   ├── keep_local.smk
    │   │   └── retrieve.smk
    │   ├── test_local_import/
    │   │   ├── bar.py
    │   │   ├── Snakefile
    │   │   └── foo/
    │   │       └── __init__.py
    │   ├── test_localrule/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── 1.txt
    │   │       └── 2.txt
    │   ├── test_log_input/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.a.txt
    │   ├── test_logfile/
    │   │   └── Snakefile
    │   ├── test_long_shell/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_many_jobs/
    │   │   └── Snakefile
    │   ├── test_match_by_wildcard_names/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.a.r1.fq
    │   ├── test_metadata_migration/
    │   │   └── Snakefile
    │   ├── test_missing_file_dryrun/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_module_checkpoint/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   ├── a/
    │   │   │   │   ├── aggregated/
    │   │   │   │   │   └── a.txt
    │   │   │   │   ├── post/
    │   │   │   │   │   └── a.txt
    │   │   │   │   └── somestep/
    │   │   │   │       └── a.txt
    │   │   │   └── b/
    │   │   │       ├── aggregated/
    │   │   │       │   └── b.txt
    │   │   │       ├── alt/
    │   │   │       │   └── b.txt
    │   │   │       └── somestep/
    │   │   │           └── b.txt
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_module_complex/
    │   │   ├── config.yaml
    │   │   ├── samples.tsv
    │   │   ├── scenario.yaml
    │   │   ├── Snakefile
    │   │   ├── units.tsv
    │   │   ├── data/
    │   │   │   ├── a.1.fq
    │   │   │   └── a.2.fq
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_module_complex2/
    │   │   ├── config.yaml
    │   │   ├── samples.tsv
    │   │   ├── scenario.yaml
    │   │   ├── Snakefile
    │   │   ├── units.tsv
    │   │   ├── data/
    │   │   │   ├── a.1.fq
    │   │   │   └── a.2.fq
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_module_input_func/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── test/
    │   │   │       └── results/
    │   │   │           └── C.txt
    │   │   └── module1/
    │   │       └── Snakefile
    │   ├── test_module_local_git/
    │   │   ├── config.yaml
    │   │   ├── module.tar.gz
    │   │   ├── samples.tsv
    │   │   ├── Snakefile
    │   │   ├── Snakefile_main_missing_rule_and_schema
    │   │   ├── Snakefile_missing_rule
    │   │   ├── Snakefile_missing_schema
    │   │   ├── Snakefile_relative
    │   │   ├── units.tsv
    │   │   ├── data/
    │   │   │   ├── a.1.fq
    │   │   │   └── a.2.fq
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_module_nested/
    │   │   ├── module_deep.smk
    │   │   ├── module_shallow.smk
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── foo.txt
    │   ├── test_module_no_prefixing_modified_paths/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── module2/
    │   │   │       └── test_final.txt
    │   │   ├── module1/
    │   │   │   └── Snakefile
    │   │   └── module2/
    │   │       └── Snakefile
    │   ├── test_module_report/
    │   │   ├── custom-stylesheet.css
    │   │   ├── Snakefile
    │   │   ├── testdir/
    │   │   │   ├── 1.txt
    │   │   │   ├── 2.txt
    │   │   │   ├── 3.txt
    │   │   │   └── .snakemake_timestamp
    │   │   └── .snakemake/
    │   │       └── metadata/
    │   │           ├── dGVzdC40Lm91dA==
    │   │           ├── dGVzdC41Lm91dA==
    │   │           ├── dGVzdC42Lm91dA==
    │   │           ├── dGVzdC43Lm91dA==
    │   │           ├── dGVzdC44Lm91dA==
    │   │           ├── dGVzdC45Lm91dA==
    │   │           ├── dGVzdC4wLm91dA==
    │   │           ├── dGVzdC4xLm91dA==
    │   │           ├── dGVzdC4yLm91dA==
    │   │           ├── dGVzdC4zLm91dA==
    │   │           ├── dGVzdC5jc3Y=
    │   │           ├── dGVzdGRpcg==
    │   │           ├── ZmlnMi5wbmc=
    │   │           └── ZmlnMS5zdmc=
    │   ├── test_module_wildcard_constraints/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── result/
    │   │   │       ├── a.txt
    │   │   │       └── b.txt
    │   │   └── testmodule/
    │   │       └── Snakefile
    │   ├── test_module_with_script/
    │   │   ├── config.yaml
    │   │   └── Snakefile
    │   ├── test_module_workflow_snakefile_usage/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   └── module-test/
    │   │       ├── Snakefile
    │   │       └── some_file.txt
    │   ├── test_modules_all/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_modules_all_exclude/
    │   │   ├── Snakefile
    │   │   ├── Snakefile_exclude
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_modules_dynamic/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   ├── module1/
    │   │   │   └── Snakefile
    │   │   └── module2/
    │   │       └── Snakefile
    │   ├── test_modules_dynamic_import_rules/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   ├── expected-results/
    │   │   │   └── results/
    │   │   │       └── testmodule/
    │   │   │           └── b2/
    │   │   │               └── test.txt
    │   │   ├── module1/
    │   │   │   └── Snakefile
    │   │   ├── module2/
    │   │   │   └── Snakefile
    │   │   └── module3/
    │   │       └── Snakefile
    │   ├── test_modules_dynamic_module_as_alias/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   ├── module1/
    │   │   │   └── Snakefile
    │   │   └── module2/
    │   │       └── Snakefile
    │   ├── test_modules_dynamic_no_as/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   ├── module1/
    │   │   │   └── Snakefile
    │   │   └── module2/
    │   │       └── Snakefile
    │   ├── test_modules_meta_wrapper/
    │   │   ├── config.yaml
    │   │   ├── Snakefile
    │   │   ├── test.1.fastq
    │   │   ├── test.2.fastq
    │   │   └── expected-results/
    │   │       └── .gitempty
    │   ├── test_modules_name/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_modules_no_name/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_modules_peppy/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   ├── module-test/
    │   │   │   └── Snakefile
    │   │   └── pep/
    │   │       ├── config.yaml
    │   │       └── sample_table.csv
    │   ├── test_modules_prefix/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_modules_prefix_local/
    │   │   ├── input.txt
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── out_1/
    │   │   │       └── test_final.txt
    │   │   └── module1/
    │   │       └── Snakefile
    │   ├── test_modules_ruledeps_inheritance/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── test2.txt
    │   │   ├── module/
    │   │   │   └── Snakefile
    │   │   └── module2/
    │   │       └── Snakefile
    │   ├── test_modules_semi_dynamic/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   ├── module1/
    │   │   │   └── Snakefile
    │   │   └── module2/
    │   │       └── Snakefile
    │   ├── test_modules_specific/
    │   │   ├── Snakefile
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_modules_two_names/
    │   │   ├── Snakefile
    │   │   ├── config/
    │   │   │   └── config.yaml
    │   │   └── module-test/
    │   │       └── Snakefile
    │   ├── test_multicomp_group_jobs/
    │   │   ├── qsub
    │   │   └── Snakefile
    │   ├── test_multiext/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── ref/
    │   │           ├── genome.ann
    │   │           ├── genome.bwt
    │   │           └── genome.sa
    │   ├── test_multiext_named/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── a.1
    │   │       └── a.2
    │   ├── test_multiple_includes/
    │   │   ├── Snakefile
    │   │   ├── test_rule.smk
    │   │   ├── test_second_rule.smk
    │   │   ├── test_third_rule.smk
    │   │   └── expected-results/
    │   │       ├── test1.txt
    │   │       ├── test2.txt
    │   │       └── test3.txt
    │   ├── test_name_override/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── out1.txt
    │   │       └── out2.txt
    │   ├── test_no_temp/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── a
    │   │       └── b
    │   ├── test_no_workflow_profile/
    │   │   ├── dummy-general-profile/
    │   │   │   └── config.yaml
    │   │   └── workflow/
    │   │       ├── Snakefile
    │   │       └── profiles/
    │   │           └── default/
    │   │               └── config.yaml
    │   ├── test_nodelocal/
    │   │   ├── qsub
    │   │   ├── qsub_stage2
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── local/
    │   │       │   ├── local
    │   │       │   └── persist.txt
    │   │       └── results/
    │   │           └── result.txt
    │   ├── test_nonstr_params/
    │   │   └── Snakefile
    │   ├── test_omitfrom/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── independent.txt
    │   │       └── levelone.txt
    │   ├── test_output_file_cache/
    │   │   ├── Snakefile
    │   │   ├── cache/
    │   │   │   └── .gitkeep
    │   │   └── expected-results/
    │   │       └── cache/
    │   │           └── 695af872f8dc47c9acd2dd3ebec1caad623f7348dca2c38ef81df8acef58b4eb_out2
    │   ├── test_output_file_cache_storage/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_params/
    │   │   └── Snakefile
    │   ├── test_params_empty_inherit/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_params_outdated_code/
    │   │   ├── Snakefile
    │   │   └── .snakemake/
    │   │       └── metadata/
    │   │           └── c29tZWRpci90ZXN0Lm91dA==
    │   ├── test_params_pickling/
    │   │   ├── Snakefile
    │   │   ├── test.py
    │   │   └── expected-results/
    │   │       ├── testnp.tsv
    │   │       ├── testpd.tsv
    │   │       └── testpl.tsv
    │   ├── test_paramspace/
    │   │   ├── params.tsv
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── results/
    │   │   │       ├── default/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1/
    │   │   │       │       │   └── beta~0.1/
    │   │   │       │       │       └── gamma~0.99.tsv
    │   │   │       │       └── alpha~2/
    │   │   │       │           └── beta~0.0/
    │   │   │       │               └── gamma~3.9.tsv
    │   │   │       ├── empty/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1/
    │   │   │       │       │   └── beta~0.1/
    │   │   │       │       │       └── gamma~0.99.tsv
    │   │   │       │       └── alpha~2/
    │   │   │       │           └── beta~0.0/
    │   │   │       │               └── gamma~3.9.tsv
    │   │   │       ├── filenamesep/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1__beta~0.1__gamma~0.99.tsv
    │   │   │       │       └── alpha~2__beta~0.0__gamma~3.9.tsv
    │   │   │       ├── full/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1_beta~0.1_gamma~0.99.tsv
    │   │   │       │       └── alpha~2_beta~0.0_gamma~3.9.tsv
    │   │   │       ├── full_reorder/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── beta~0.0_gamma~3.9_alpha~2.tsv
    │   │   │       │       └── beta~0.1_gamma~0.99_alpha~1.tsv
    │   │   │       ├── one/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1/
    │   │   │       │       │   └── gamma~0.99/
    │   │   │       │       │       └── beta~0.1.tsv
    │   │   │       │       └── alpha~2/
    │   │   │       │           └── gamma~3.9/
    │   │   │       │               └── beta~0.0.tsv
    │   │   │       ├── sep/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha_is_1/
    │   │   │       │       │   └── beta_is_0.1/
    │   │   │       │       │       └── gamma_is_0.99.tsv
    │   │   │       │       └── alpha_is_2/
    │   │   │       │           └── beta_is_0.0/
    │   │   │       │               └── gamma_is_3.9.tsv
    │   │   │       ├── sep_and_pattern/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── beta=0.0_gamma=3.9_alpha=2.tsv
    │   │   │       │       └── beta=0.1_gamma=0.99_alpha=1.tsv
    │   │   │       └── two/
    │   │   │           └── simulations/
    │   │   │               ├── alpha~1/
    │   │   │               │   └── beta~0.1_gamma~0.99.tsv
    │   │   │               └── alpha~2/
    │   │   │                   └── beta~0.0_gamma~3.9.tsv
    │   │   └── scripts/
    │   │       └── simulate.py
    │   ├── test_paramspace_single_wildcard/
    │   │   ├── params.tsv
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── results/
    │   │   │       ├── default/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1/
    │   │   │       │       │   └── beta~0.1/
    │   │   │       │       │       └── gamma~0.99.tsv
    │   │   │       │       └── alpha~2/
    │   │   │       │           └── beta~0.0/
    │   │   │       │               └── gamma~3.9.tsv
    │   │   │       ├── empty/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1/
    │   │   │       │       │   └── beta~0.1/
    │   │   │       │       │       └── gamma~0.99.tsv
    │   │   │       │       └── alpha~2/
    │   │   │       │           └── beta~0.0/
    │   │   │       │               └── gamma~3.9.tsv
    │   │   │       ├── filenamesep/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1__beta~0.1__gamma~0.99.tsv
    │   │   │       │       └── alpha~2__beta~0.0__gamma~3.9.tsv
    │   │   │       ├── full/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1_beta~0.1_gamma~0.99.tsv
    │   │   │       │       └── alpha~2_beta~0.0_gamma~3.9.tsv
    │   │   │       ├── full_reorder/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── beta~0.0_gamma~3.9_alpha~2.tsv
    │   │   │       │       └── beta~0.1_gamma~0.99_alpha~1.tsv
    │   │   │       ├── one/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha~1/
    │   │   │       │       │   └── gamma~0.99/
    │   │   │       │       │       └── beta~0.1.tsv
    │   │   │       │       └── alpha~2/
    │   │   │       │           └── gamma~3.9/
    │   │   │       │               └── beta~0.0.tsv
    │   │   │       ├── sep/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── alpha_is_1/
    │   │   │       │       │   └── beta_is_0.1/
    │   │   │       │       │       └── gamma_is_0.99.tsv
    │   │   │       │       └── alpha_is_2/
    │   │   │       │           └── beta_is_0.0/
    │   │   │       │               └── gamma_is_3.9.tsv
    │   │   │       ├── sep_and_pattern/
    │   │   │       │   └── simulations/
    │   │   │       │       ├── beta=0.0_gamma=3.9_alpha=2.tsv
    │   │   │       │       └── beta=0.1_gamma=0.99_alpha=1.tsv
    │   │   │       └── two/
    │   │   │           └── simulations/
    │   │   │               ├── alpha~1/
    │   │   │               │   └── beta~0.1_gamma~0.99.tsv
    │   │   │               └── alpha~2/
    │   │   │                   └── beta~0.0_gamma~3.9.tsv
    │   │   └── scripts/
    │   │       └── simulate.py
    │   ├── test_parser/
    │   │   └── Snakefile
    │   ├── test_parsing_terminal_comment_following_statement/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── done
    │   ├── test_path with spaces/
    │   │   └── Snakefile
    │   ├── test_pathlib/
    │   │   ├── existing_file.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── outdir/
    │   │           ├── log_existing
    │   │           ├── log_function_unpacking
    │   │           ├── log_other_rule_output_existing
    │   │           ├── log_protected
    │   │           ├── log_temp
    │   │           ├── output_existing
    │   │           ├── output_other_rule_output_existing
    │   │           ├── output_protected
    │   │           └── function_unpacking/
    │   │               └── output_function_unpacking
    │   ├── test_pathlib_missing_file/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_pep_pathlib/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   ├── a.txt
    │   │   │   └── b.txt
    │   │   ├── pep/
    │   │   │   ├── config.yaml
    │   │   │   └── sample_table.csv
    │   │   └── workflow/
    │   │       └── schemas/
    │   │           └── pep.yaml
    │   ├── test_peppy/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   ├── a.txt
    │   │   │   └── b.txt
    │   │   ├── pep/
    │   │   │   ├── config.yaml
    │   │   │   └── sample_table.csv
    │   │   └── workflow/
    │   │       └── schemas/
    │   │           └── pep.yaml
    │   ├── test_pipe_depend/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_pipes/
    │   │   ├── producer.py
    │   │   └── Snakefile
    │   ├── test_pipes2/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── foo.txt
    │   ├── test_pipes_fail/
    │   │   └── Snakefile
    │   ├── test_pipes_multiple/
    │   │   └── Snakefile
    │   ├── test_prebuilt_conda_script/
    │   │   ├── env.yaml
    │   │   ├── Snakefile
    │   │   ├── test.py
    │   │   ├── dummy_package/
    │   │   │   ├── pyproject.toml
    │   │   │   ├── setup.cfg
    │   │   │   └── src/
    │   │   │       └── dummy/
    │   │   │           ├── __init__.py
    │   │   │           └── dummy.py
    │   │   └── expected-results/
    │   │       └── output.txt
    │   ├── test_profile/
    │   │   ├── config.yaml
    │   │   ├── input.txt
    │   │   ├── Snakefile
    │   │   ├── Snakefile.internal
    │   │   └── workflow-config.yaml
    │   ├── test_protected_symlink_output/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── outfile
    │   │       └── outlink
    │   ├── test_queue_input/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test0.txt
    │   │       ├── test1.txt
    │   │       ├── test10.txt
    │   │       ├── test11.txt
    │   │       ├── test12.txt
    │   │       ├── test13.txt
    │   │       ├── test14.txt
    │   │       ├── test15.txt
    │   │       ├── test16.txt
    │   │       ├── test17.txt
    │   │       ├── test18.txt
    │   │       ├── test19.txt
    │   │       ├── test2.txt
    │   │       ├── test3.txt
    │   │       ├── test4.txt
    │   │       ├── test5.txt
    │   │       ├── test6.txt
    │   │       ├── test7.txt
    │   │       ├── test8.txt
    │   │       └── test9.txt
    │   ├── test_r_wrapper/
    │   │   ├── environment.yaml
    │   │   ├── Snakefile
    │   │   └── wrapper.R
    │   ├── test_remote_auto/
    │   │   └── Snakefile
    │   ├── test_remote_azure/
    │   │   ├── README.md
    │   │   └── Snakefile
    │   ├── test_remote_gs/
    │   │   ├── landsat-data.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── landsat-data.txt
    │   ├── test_remote_irods/
    │   │   ├── README.md
    │   │   ├── Dockerfile
    │   │   ├── Makefile
    │   │   ├── Snakefile
    │   │   ├── Snakefile.local
    │   │   ├── expected-results/
    │   │   │   └── outfile
    │   │   ├── setup-data/
    │   │   │   ├── irods_environment.json
    │   │   │   └── .irodsA
    │   │   └── test-data/
    │   │       └── infile
    │   ├── test_remote_log/
    │   │   ├── Snakefile
    │   │   ├── test.txt
    │   │   └── expected-results/
    │   │       └── motoState.p
    │   ├── test_remote_ncbi/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── sizes.txt
    │   ├── test_remote_ncbi_simple/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── sizes.txt
    │   ├── test_remote_sftp/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── readme.txt
    │   ├── test_remote_zenodo/
    │   │   ├── Snakefile
    │   │   ├── test.txt
    │   │   └── expected-results/
    │   │       └── download.txt
    │   ├── test_report/
    │   │   ├── Snakefile
    │   │   ├── envs/
    │   │   │   └── test.yaml
    │   │   ├── report/
    │   │   │   ├── fig1.rst
    │   │   │   ├── fig2.rst
    │   │   │   ├── table.rst
    │   │   │   ├── testdir.rst
    │   │   │   └── workflow.rst
    │   │   ├── testdir/
    │   │   │   ├── 1.txt
    │   │   │   ├── 2.txt
    │   │   │   ├── 3.txt
    │   │   │   └── .snakemake_timestamp
    │   │   └── .snakemake/
    │   │       └── metadata/
    │   │           ├── dGVzdC40Lm91dA==
    │   │           ├── dGVzdC41Lm91dA==
    │   │           ├── dGVzdC42Lm91dA==
    │   │           ├── dGVzdC43Lm91dA==
    │   │           ├── dGVzdC44Lm91dA==
    │   │           ├── dGVzdC45Lm91dA==
    │   │           ├── dGVzdC4wLm91dA==
    │   │           ├── dGVzdC4xLm91dA==
    │   │           ├── dGVzdC4yLm91dA==
    │   │           ├── dGVzdC4zLm91dA==
    │   │           ├── dGVzdC5jc3Y=
    │   │           ├── dGVzdGRpcg==
    │   │           ├── ZmlnMi5wbmc=
    │   │           └── ZmlnMS5zdmc=
    │   ├── test_report_after_run/
    │   │   ├── Snakefile
    │   │   ├── envs/
    │   │   │   └── test.yaml
    │   │   └── report/
    │   │       ├── fig1.rst
    │   │       ├── fig2.rst
    │   │       ├── table.rst
    │   │       ├── testdir.rst
    │   │       └── workflow.rst
    │   ├── test_report_dir/
    │   │   ├── caption.rst
    │   │   ├── Snakefile
    │   │   ├── template.html
    │   │   └── test/
    │   │       ├── test.html
    │   │       ├── .snakemake_timestamp
    │   │       └── js/
    │   │           └── test.js
    │   ├── test_report_display_code/
    │   │   ├── Snakefile
    │   │   └── test.py
    │   ├── test_report_href/
    │   │   ├── Snakefile
    │   │   ├── test.html
    │   │   ├── test_script.py
    │   │   ├── expected-results/
    │   │   │   └── test2.html
    │   │   └── subdir/
    │   │       └── subdir/
    │   │           └── test3.html
    │   ├── test_report_metadata/
    │   │   ├── report.rst
    │   │   ├── Snakefile
    │   │   └── yte_template.yaml
    │   ├── test_report_zip/
    │   │   ├── custom-stylesheet.css
    │   │   ├── Snakefile
    │   │   ├── envs/
    │   │   │   └── test.yaml
    │   │   ├── report/
    │   │   │   ├── fig1.rst
    │   │   │   ├── fig2.rst
    │   │   │   ├── table.rst
    │   │   │   ├── testdir.rst
    │   │   │   └── workflow.rst
    │   │   ├── testdir/
    │   │   │   ├── 1.txt
    │   │   │   ├── 2.txt
    │   │   │   ├── 3.txt
    │   │   │   └── .snakemake_timestamp
    │   │   └── .snakemake/
    │   │       └── metadata/
    │   │           ├── dGVzdC40Lm91dA==
    │   │           ├── dGVzdC41Lm91dA==
    │   │           ├── dGVzdC42Lm91dA==
    │   │           ├── dGVzdC43Lm91dA==
    │   │           ├── dGVzdC44Lm91dA==
    │   │           ├── dGVzdC45Lm91dA==
    │   │           ├── dGVzdC4wLm91dA==
    │   │           ├── dGVzdC4xLm91dA==
    │   │           ├── dGVzdC4yLm91dA==
    │   │           ├── dGVzdC4zLm91dA==
    │   │           ├── dGVzdC5jc3Y=
    │   │           ├── dGVzdG1vZGVsLmZpZzIucG5n
    │   │           ├── dGVzdGRpcg==
    │   │           ├── ZmlnMi5wbmc=
    │   │           └── ZmlnMS5zdmc=
    │   ├── test_resource_quoting/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── test.txt
    │   │   ├── test-profile/
    │   │   │   └── config.yaml
    │   │   └── test-profile-default/
    │   │       └── config.yaml
    │   ├── test_resource_string_in_cli_or_profile/
    │   │   ├── Snakefile
    │   │   └── profiles/
    │   │       └── config.yaml
    │   ├── test_resource_tbdstring/
    │   │   └── Snakefile
    │   ├── test_restartable_job_cmd_exit_1/
    │   │   ├── qsub
    │   │   ├── qsub.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .done
    │   ├── test_restartable_job_qsub_exit_1/
    │   │   ├── qsub
    │   │   ├── qsub.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .done
    │   ├── test_retries/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_retries_not_overriden/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_rule_defined_in_for_loop/
    │   │   ├── iteration-01.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── iteration-01.txt
    │   │       └── iteration-02.txt
    │   ├── test_rule_inheritance_globals/
    │   │   ├── caption.rst
    │   │   ├── foo.txt
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── report.html
    │   │   └── .snakemake/
    │   │       └── metadata/
    │   │           └── Zm9vLnR4dA==
    │   ├── test_ruledag/
    │   │   └── Snakefile
    │   ├── test_ruledeps/
    │   │   └── Snakefile
    │   ├── test_run_namedlist/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── file.txt
    │   ├── test_runtime_conversion_from_workflow_profile/
    │   │   └── workflow/
    │   │       ├── Snakefile
    │   │       └── profiles/
    │   │           └── default/
    │   │               └── config.yaml
    │   ├── test_same_wildcard/
    │   │   ├── Snakefile
    │   │   └── test_test.in
    │   ├── test_scatter_gather/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── gathered/
    │   │       │   └── all.txt
    │   │       └── splitted/
    │   │           ├── 1-of-2.post.txt
    │   │           ├── 1-of-2.txt
    │   │           ├── 2-of-2.post.txt
    │   │           └── 2-of-2.txt
    │   ├── test_scatter_gather_multiple_processes/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── gathered/
    │   │       │   ├── all_a.txt
    │   │       │   └── all_b.txt
    │   │       ├── split_a/
    │   │       │   ├── 1-of-4.post.txt
    │   │       │   ├── 1-of-4.txt
    │   │       │   ├── 2-of-4.post.txt
    │   │       │   ├── 2-of-4.txt
    │   │       │   ├── 3-of-4.post.txt
    │   │       │   ├── 3-of-4.txt
    │   │       │   ├── 4-of-4.post.txt
    │   │       │   └── 4-of-4.txt
    │   │       └── split_b/
    │   │           ├── 1-of-2.post.txt
    │   │           ├── 1-of-2.txt
    │   │           ├── 2-of-2.post.txt
    │   │           └── 2-of-2.txt
    │   ├── test_scheduler_sequential_all_cores/
    │   │   └── Snakefile
    │   ├── test_script/
    │   │   ├── config.yaml
    │   │   ├── Snakefile
    │   │   ├── test2.in
    │   │   ├── envs/
    │   │   │   ├── bash.yaml
    │   │   │   ├── julia.yaml
    │   │   │   └── r.yaml
    │   │   ├── expected-results/
    │   │   │   ├── test.html
    │   │   │   └── test.in
    │   │   └── scripts/
    │   │       ├── rel_source.R
    │   │       ├── source_me.R
    │   │       ├── test.jl
    │   │       ├── test.py
    │   │       ├── test.R
    │   │       ├── test.Rmd
    │   │       └── test.sh
    │   ├── test_script_pre_py39/
    │   │   ├── env.yaml
    │   │   ├── script.py
    │   │   └── Snakefile
    │   ├── test_script_py/
    │   │   ├── config.yaml
    │   │   ├── Snakefile
    │   │   └── scripts/
    │   │       ├── test.py
    │   │       └── test_explicit_import.py
    │   ├── test_script_rs/
    │   │   ├── config.yaml
    │   │   ├── Snakefile
    │   │   ├── test2.in
    │   │   ├── envs/
    │   │   │   └── rust.yaml
    │   │   └── scripts/
    │   │       ├── test-manifest.rs
    │   │       ├── test-outer-line-doc.rs
    │   │       ├── test.py
    │   │       └── test.rs
    │   ├── test_script_xsh/
    │   │   ├── Snakefile
    │   │   ├── envs/
    │   │   │   └── xonsh.yaml
    │   │   └── scripts/
    │   │       └── test.xsh
    │   ├── test_service_jobs/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test.txt
    │   │       └── test2.txt
    │   ├── test_set_resources/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_set_resources_complex/
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── test-profile/
    │   │       └── config.yaml
    │   ├── test_set_threads/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_shadow/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_shadow_copy/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_shadow_prefix/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── expected-results/
    │   │       └── shadow_prefix
    │   ├── test_shadowed_log/
    │   │   └── Snakefile
    │   ├── test_shell/
    │   │   ├── Snakefile
    │   │   └── test.in
    │   ├── test_shell_exec/
    │   │   └── Snakefile
    │   ├── test_singularity/
    │   │   ├── qsub
    │   │   ├── script.py
    │   │   └── Snakefile
    │   ├── test_singularity_conda/
    │   │   ├── Snakefile
    │   │   └── test-env.yaml
    │   ├── test_singularity_global/
    │   │   └── Snakefile
    │   ├── test_singularity_module/
    │   │   ├── module.smk
    │   │   ├── script.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_singularity_none/
    │   │   └── Snakefile
    │   ├── test_slurm_mpi/
    │   │   ├── pi_MPI.c
    │   │   ├── Snakefile
    │   │   ├── envs/
    │   │   │   └── mpicc.yaml
    │   │   └── expected-results/
    │   │       └── pi.calc
    │   ├── test_solver/
    │   │   └── Snakefile
    │   ├── test_source_path/
    │   │   └── workflow/
    │   │       ├── Snakefile
    │   │       └── resources/
    │   │           └── test.txt
    │   ├── test_spaces_in_fnames/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   ├── test bam file.bam
    │   │   └── expected-results/
    │   │       └── test bam file realigned.bam
    │   ├── test_speed/
    │   │   └── Snakefile
    │   ├── test_storage/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_storage_call/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test.txt
    │   │       └── .gitkeep
    │   ├── test_storage_cleanup_local/
    │   │   ├── qsub
    │   │   └── Snakefile
    │   ├── test_storage_localrule/
    │   │   ├── qsub
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── fs-storage/
    │   │           └── test.txt
    │   ├── test_storage_noretrieve_dryrun/
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── .gitkeep
    │   │   └── fs/
    │   │       └── test.txt
    │   ├── test_strict_mode/
    │   │   └── Snakefile
    │   ├── test_string_resources/
    │   │   ├── qsub.py
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test.txt
    │   │       └── test2.txt
    │   ├── test_subpath/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── results/
    │   │           └── foo/
    │   │               └── bar/
    │   │                   └── b.txt
    │   ├── test_subworkflow_config/
    │   │   ├── environment.yml
    │   │   ├── Snakefile
    │   │   ├── sub.snake
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_subworkflows/
    │   │   ├── Snakefile
    │   │   └── subconfig.yaml
    │   ├── test_symlink_temp/
    │   │   ├── a
    │   │   ├── b
    │   │   └── Snakefile
    │   ├── test_symlink_time_handling/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── time_diff.txt
    │   ├── test_temp/
    │   │   ├── Snakefile
    │   │   ├── test.bam
    │   │   └── expected-results/
    │   │       └── test.realigned.bam
    │   ├── test_temp_and_all_input/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── foo.txt
    │   ├── test_temp_expand/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── test.txt
    │   ├── test_template_engine/
    │   │   ├── Snakefile
    │   │   ├── template.jinja2.txt
    │   │   ├── template.yte.yaml
    │   │   └── expected-results/
    │   │       ├── rendered.jinja2
    │   │       └── rendered.yte
    │   ├── test_tes/
    │   │   ├── Snakefile
    │   │   ├── test_input.txt
    │   │   └── expected-results/
    │   │       └── .gitkeep
    │   ├── test_threads/
    │   │   └── Snakefile
    │   ├── test_threads0/
    │   │   └── Snakefile
    │   ├── test_tibanna/
    │   │   ├── README.md
    │   │   ├── cleanup.py
    │   │   ├── config.json
    │   │   ├── env.yml
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   └── .gitkeep
    │   │   └── scripts/
    │   │       └── step2.py
    │   ├── test_tmpdir/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── test.txt
    │   │       └── test2.txt
    │   ├── test_toposort/
    │   │   └── Snakefile
    │   ├── test_touch/
    │   │   └── Snakefile
    │   ├── test_touch_pipeline_with_temp_dir/
    │   │   ├── out.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       └── out.txt
    │   ├── test_touch_remote_prefix/
    │   │   ├── Snakefile.touch
    │   │   └── expected-results/
    │   │       ├── fileJob1
    │   │       └── fileJob2.txt
    │   ├── test_touch_with_directories/
    │   │   └── Snakefile
    │   ├── test_unpack_dict/
    │   │   ├── prefix.in1.txt
    │   │   ├── prefix.in2.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── prefix.out1.txt
    │   │       └── prefix.out2.txt
    │   ├── test_unpack_list/
    │   │   ├── prefix.in1.txt
    │   │   ├── prefix.in2.txt
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── prefix.out1.txt
    │   │       └── prefix.out2.txt
    │   ├── test_until/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── levelone.txt
    │   │       ├── leveltwo_first.txt
    │   │       └── leveltwo_second.txt
    │   ├── test_update_config/
    │   │   ├── cp.rule
    │   │   ├── echo.rule
    │   │   ├── echo.yaml
    │   │   ├── Snakefile
    │   │   ├── test.yaml
    │   │   └── expected-results/
    │   │       └── config.yaml
    │   ├── test_update_flag/
    │   │   ├── in.txt
    │   │   ├── Snakefile
    │   │   ├── test.txt
    │   │   └── expected-results/
    │   │       ├── out.txt
    │   │       └── test.txt
    │   ├── test_url_include/
    │   │   ├── Snakefile
    │   │   ├── test.in
    │   │   └── expected-results/
    │   │       ├── test.1.inter
    │   │       ├── test.1.inter2
    │   │       ├── test.2.inter
    │   │       ├── test.2.inter2
    │   │       ├── test.3.inter
    │   │       ├── test.3.inter2
    │   │       └── test.predictions
    │   ├── test_use_rule_same_module/
    │   │   └── Snakefile
    │   ├── test_validate/
    │   │   ├── config.fail.yaml
    │   │   ├── config.schema.yaml
    │   │   ├── config.yaml
    │   │   ├── samples.schema.yaml
    │   │   ├── samples.tsv
    │   │   ├── Snakefile
    │   │   ├── expected-results/
    │   │   │   ├── test.A.txt
    │   │   │   └── test.B.txt
    │   │   └── module-test/
    │   │       ├── config.schema.yaml
    │   │       └── Snakefile
    │   ├── test_wildcard_count_ambiguity/
    │   │   └── Snakefile
    │   ├── test_wildcard_keyword/
    │   │   ├── Snakefile
    │   │   └── expected-results/
    │   │       ├── ambig.u.ous.txt
    │   │       ├── globalbar.txt
    │   │       ├── localbar.txt
    │   │       └── stringbar.txt
    │   ├── test_workflow_profile/
    │   │   ├── dummy-general-profile/
    │   │   │   └── config.yaml
    │   │   └── workflow/
    │   │       ├── Snakefile
    │   │       └── profiles/
    │   │           └── default/
    │   │               └── config.yaml
    │   ├── test_wrapper/
    │   │   ├── Snakefile
    │   │   ├── test.vcf
    │   │   └── expected-results/
    │   │       └── test.vcf.gz
    │   ├── test_xrootd/
    │   │   └── Snakefile
    │   ├── test_yaml_config/
    │   │   ├── Snakefile
    │   │   └── test.yaml
    │   └── testHighWorkload/
    │       ├── Snakefile
    │       └── mfa/
    │           ├── 00.mfa
    │           ├── 01.mfa
    │           ├── 02.mfa
    │           ├── 03.mfa
    │           ├── 04.mfa
    │           ├── 05.mfa
    │           ├── 06.mfa
    │           ├── 07.mfa
    │           ├── 08.mfa
    │           ├── 09.mfa
    │           └── 10.mfa
    └── .github/
        ├── CODEOWNERS
        ├── dependabot.yml
        ├── pull_request_template.md
        ├── ISSUE_TEMPLATE/
        │   ├── bug_report.md
        │   └── feature_request.md
        └── workflows/
            ├── codespell.yml
            ├── conventional-prs.yml
            ├── docker-publish.yml
            ├── docs.yml
            ├── main.yml
            ├── release-please.yml
            ├── stale.yml
            └── scripts/
                └── sacct-proxy.py

================================================
FILE: README.md
================================================
[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/snakemake/snakemake)
[![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/main.yml?label=tests)](https://github.com/snakemake/snakemake/actions?query=branch%3Amain++)
[![Sonarcloud Status](https://sonarcloud.io/api/project_badges/measure?project=snakemake_snakemake&metric=alert_status)](https://sonarcloud.io/dashboard?id=snakemake_snakemake)
[![Conda Version](https://img.shields.io/conda/vn/bioconda/snakemake)](https://anaconda.org/bioconda/snakemake)
[![Bioconda](https://img.shields.io/conda/dn/bioconda/snakemake.svg?label=Bioconda)](https://bioconda.github.io/recipes/snakemake/README.html)
[![Pypi](https://img.shields.io/pypi/pyversions/snakemake.svg)](https://pypi.org/project/snakemake)
[![docker container status](https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/docker-publish.yml?color=blue&label=docker%20container)](https://hub.docker.com/r/snakemake/snakemake)
[![Stack Overflow](https://img.shields.io/badge/stack-overflow-orange.svg)](https://stackoverflow.com/questions/tagged/snakemake)
[![Twitter](https://img.shields.io/twitter/follow/johanneskoester.svg?style=social&label=Follow)](https://twitter.com/search?l=&q=%23snakemake%20from%3Ajohanneskoester)
![Mastodon Follow](https://img.shields.io/mastodon/follow/109308140471392959?domain=https%3A%2F%2Ffosstodon.org&label=Follow)
[![Discord](https://img.shields.io/discord/753690260830945390?label=discord%20chat)](https://discord.gg/NUdMtmr)
[![Github stars](https://img.shields.io/github/stars/snakemake/snakemake?style=social)](https://github.com/snakemake/snakemake/stargazers)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md) 

# Snakemake

The Snakemake workflow management system is a tool to create **reproducible and scalable** data analyses.
Snakemake is highly popular, with on average more than 7 new citations per week in 2021, and almost 400k downloads.
Workflows are described via a human readable, Python based language.
They can be seamlessly scaled to server, cluster, grid and cloud environments without the need to modify the workflow definition.
Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.

**Homepage: https://snakemake.github.io**

Copyright (c) 2012-2022 Johannes Köster <johannes.koester@uni-due.com> (see LICENSE)



================================================
FILE: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socioeconomic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported via email to snakemake-code-of-conduct@googlegroups.com.
Messages posted to this address will only be visible to the Snakemake maintainers.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.




================================================
FILE: doc-environment.yml
================================================
channels:
  - conda-forge
dependencies:
  - python >=3.11
  - wrapt
  - pyyaml
  - requests
  - appdirs
  - docutils
  - throttler
  - configargparse
  - jsonschema
  - gitpython
  - sphinx
  - pip
  - pip:
    - sphinxcontrib-napoleon
    - snakemake-interface-common >=1.17.0
    - snakemake-interface-executor-plugins >=9.1.0
    - snakemake-interface-storage-plugins >=3.2.3
  - sphinx-argparse
  - sphinx-tabs
  - sphinx_rtd_theme
  - sphinxawesome-theme
  - docutils
  - recommonmark
  - commonmark
  - myst-parser



================================================
FILE: Dockerfile
================================================
FROM mambaorg/micromamba
LABEL org.opencontainers.image.authors="Johannes Köster <johannes.koester@tu-dortmund.de>"
ADD . /tmp/repo
WORKDIR /tmp/repo
ENV LANG C.UTF-8
ENV SHELL /bin/bash
USER root 

ENV APT_PKGS bzip2 ca-certificates curl wget gnupg2 squashfs-tools git

RUN apt-get update \
    && apt-get install -y --no-install-recommends ${APT_PKGS} \
    && apt-get clean \
    && rm -rf /var/lib/apt /var/lib/dpkg /var/lib/cache /var/lib/log

RUN micromamba create -q -y -c conda-forge -n apptainer apptainer

RUN micromamba create -q -y -c bioconda -c conda-forge -n snakemake \
    snakemake-minimal --only-deps && \
    eval "$(micromamba shell hook --shell bash)" && \
    micromamba activate /opt/conda/envs/snakemake && \
    micromamba install -c conda-forge conda && \
    micromamba clean --all -y

ENV PATH /opt/conda/envs/snakemake/bin:/opt/conda/envs/apptainer/bin:${PATH}
RUN pip install .[reports,messaging,pep]


================================================
FILE: LICENSE.md
================================================
Copyright (c) 2012-2024 The Snakemake team

Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.



================================================
FILE: MANIFEST.in
================================================
global-exclude *

include setup.py
include pyproject.toml
include README.md
include LICENSE.md
include .gitignore
graft src

# Include tests in sdist, otherwise only tests/test*.py are included by default
graft tests

global-exclude *~ *.py[cod] *.so



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools>=64", "setuptools-scm>=8"]
build-backend = "setuptools.build_meta"

[project]
name = "snakemake"
dynamic = ["version"]
description = "Workflow management system to create reproducible and scalable data analyses"
readme = { content-type = "text/markdown", text = """
  Snakemake is a workflow management system that aims to reduce the
  complexity of creating workflows by providing a fast and comfortable
  execution environment, together with a clean and modern specification
  language in python style. Snakemake workflows are essentially Python
  scripts extended by declarative code to define rules. Rules describe
  how to create output files from input files.
""" }
requires-python = ">= 3.11"
license = "MIT"
authors = [{ name = "Johannes Köster", email = "johannes.koester@uni-due.de" }]
classifiers = [
  "Development Status :: 5 - Production/Stable",
  "Environment :: Console",
  "Intended Audience :: Science/Research",
  "Natural Language :: English",
  "Programming Language :: Python :: 3 :: Only",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering",
]

dependencies = [
  "appdirs",
  "immutables",
  "configargparse",
  "connection_pool>=0.0.3",
  "docutils",
  "gitpython",
  "humanfriendly",
  "jinja2>=3.0,<4.0",
  "jsonschema",
  "nbformat",
  "packaging >=24.0",
  "psutil",
  "pulp>=2.3.1,<3.3",
  "pyyaml",
  "referencing",
  "requests>=2.8.1,<3.0",
  "reretry",
  "smart-open>=4.0,<8.0",
  "snakemake-interface-executor-plugins>=9.3.2,<10.0",
  "snakemake-interface-common>=1.20.1,<2.0",
  "snakemake-interface-storage-plugins>=4.1.0,<5.0",
  "snakemake-interface-report-plugins>=1.2.0,<2.0.0",
  "snakemake-interface-logger-plugins>=1.1.0,<2.0.0",
  "snakemake-interface-scheduler-plugins>=2.0.0,<3.0.0",
  "tabulate",
  "throttler",
  "wrapt",
  "yte>=1.5.5,<2.0",
  "dpath>=2.1.6,<3.0.0",
  "conda-inject>=1.3.1,<2.0",
]

[project.urls]
Homepage = "https://snakemake.github.io"
Documentation = "https://snakemake.readthedocs.io"
Source = "https://github.com/snakemake/snakemake"

[project.optional-dependencies]
pep = ["eido", "peppy"]
reports = ["pygments"]
all = ["eido", "peppy", "pygments"]

[project.scripts]
snakemake = "snakemake.cli:main"

[tool.setuptools_scm]
version_file = "src/snakemake/_version.py"
# Ignore Git hash
local_scheme = "no-local-version"

[tool.codespell]
# Ref: https://github.com/codespell-project/codespell#using-a-config-file
skip = '.git,*.pdf,*.svg,versioneer.py,*.css,test_*'
check-hidden = true
ignore-regex = '^\s*"image/\S+": ".*|\b[Mm]anuel[. ][Hh]oltgrewe\b'
ignore-words-list = 'testin'

####################################################################
# Pixi Configuration
####################################################################
[tool.pixi.project]
channels = ["conda-forge", "bioconda"]
platforms = ["linux-64", "osx-arm64", "win-64", "osx-64", "linux-aarch64"]

[tool.pixi.pypi-dependencies]
snakemake = { path = ".", editable = true, extras = ["all"] }

[tool.pixi.environments]
dev = { features = ["test", "docs", "style"] }

py311 = { features = ["py311", "test"] }
py312 = { features = ["py312", "test"] }
py313 = { features = ["py313", "test"] }


docs = { features = ["docs"] }
quality = { features = ["style"] }
publish = { features = ["publish"] }

# Python Envs
[tool.pixi.feature.py311.dependencies]
python = ">=3.11.0,<3.12"

[tool.pixi.feature.py312.dependencies]
python = ">=3.12.0,<3.13"


[tool.pixi.feature.py313.dependencies]
python = ">=3.13.0,<3.14"

# Test
[tool.pytest.ini_options]
addopts = "--verbose --show-capture=stderr" #  --exitfirst
markers = [
  #  (use '-m "not needs_envmodules"' to skip these tests)
  "needs_envmodules: marks tests that require environment modules",
  "needs_s3: marks tests that require S3 credentials",
]
[tool.pixi.feature.test]
channels = ["conda-forge", "bioconda", "nodefaults"]

[tool.pixi.feature.test.tasks.test-simple]
cmd = [
  "pytest",
  "-m",
  "not needs_envmodules",
  "-k",
  "not test_conda_global",
  "tests/tests.py",
]
description = "Run tests only for tests/tests.py"

[tool.pixi.feature.test.target.osx-arm64.tasks.test-simple]
cmd = [
  "pytest",
  "-m",
  "not (needs_s3 or needs_envmodules)", # needs_s3 is not supported on osx-arm64
  "-k",
  "not (test_conda_global or queue)", # queue is buggy on osx-arm64
  "tests/tests.py",
]

[tool.pixi.feature.test.tasks.test-all]
description = "Run all tests in the tests directory"
cmd = [
  "pytest",
  "tests",
  # TODO for the two excluded tests, see https://github.com/snakemake/snakemake/pull/3369#issuecomment-2718056637
  "-m",
  "not needs_envmodules",
  "-k",
  "not test_conda_global",
  "tests/tests.py",                    # tested in test-simple
  "tests/tests_using_conda.py",
  "tests/test_expand.py",
  "tests/test_io.py",
  "tests/test_schema.py",
  "tests/test_linting.py",
  "tests/test_executor_test_suite.py",
  "tests/test_api.py",
  "tests/test_internals.py",
  # the tests below were never tested in the old CI and currently fail
  # "tests/test_args.py",
  # "tests/test_output_index.py",
  # "tests/test_path_modifier.py",
  # "tests/test_persistence.py"
  # "tests/test_prefix_lookup.py",
  # "tests/test_script.py",
  # "tests/test_sourcecache.py",
]

[tool.pixi.feature.test.dependencies]
pytest = ">=8.3.5,<9"
pytest-mock = ">=3.14.0,<4"
pytest-cov = ">=6.0.0,<7"
pytest-split = ">=0.10.0,<0.11"
graphviz = ">=12.2.1,<13"
oauth2client = ">=4.1.3,<5"
numpy = ">=2.2.3,<3"
glpk = ">=5.0,<6"
pulp = ">=2.8.0,<3"
boto3 = ">=1.37.9,<2"
responses = ">=0.25.6,<0.26"
pytools = ">=2024.1.6,<2026"
pandoc = ">=3.6.3,<4"
crc32c = ">=2.7.1,<3"
filelock = ">=3.17.0,<4"
humanfriendly = ">=10.0,<11"
pandas = ">=2.2.3,<3"
pygments = ">=2.19.1,<3"
pysftp = ">=0.2.9,<0.3"
yte = ">=1.5.7,<2"
ftputil = ">=5.1.0,<6"
httpretty = ">=1.1.4,<2"
polars = ">=1.24.0,<2"
requests-mock = ">=1.12.1,<2"
docutils = ">=0.20.1,<0.22"
conda = "*"
pip = "*"

[tool.pixi.feature.test.pypi-dependencies]
snakemake-executor-plugin-cluster-generic = "*"
snakemake-storage-plugin-http = "*"
snakemake-storage-plugin-s3 = "*"
snakemake-storage-plugin-fs = "*"

[tool.pixi.feature.test.target.linux-64.pypi-dependencies]
snakemake-storage-plugin-xrootd = "*"

[tool.pixi.feature.test.target.linux-64.dependencies]
# environment-modules = "*"
xorg-libxrender = "*"
xorg-libxext = "*"
xorg-libxau = "*"
xorg-libxdmcp = "*"
xorg-libsm = "*"
stress-ng = "*"
nodejs = "*"
squashfuse = "*"
cwltool = "*"
cwl-utils = "*"
# # moved to pixi from main.yaml installing via apt
apptainer = "*"
git = "*"
wget = "*"
dash = "*"
openmpi = "*"
xrootd = ">=5.7.3,<6"

# Docs
[tool.pixi.feature.docs.dependencies]
sphinx = ">=7.2.6,<8"
sphinx-argparse = ">=0.4.0,<0.6"
sphinx_rtd_theme = ">=3.0.1,<4"
sphinxawesome-theme = ">=5.2.0,<6"
recommonmark = ">=0.7.1,<0.8"
commonmark = ">=0.9.1,<0.10"
myst-parser = ">=3.0.1,<5"
sphinxcontrib-napoleon = ">=0.7,<0.8"
sphinx-autobuild = ">=2024.10.3,<2025"
sphinx-tabs = ">=3.4.1,<4"
sphinx-autodoc-typehints = ">=2.0.1,<3"


[tool.pixi.feature.docs.tasks.build-docs]
description = "Build the documentation in the docs/ directory"
cwd = "docs"
cmd = 'make html SPHINXOPTS="-W --keep-going -n"'

[tool.pixi.feature.docs.tasks.docs]
description = "Serve the documentation on http://localhost:8000 with live reload"
cmd = "sphinx-autobuild docs/ docs/_build/html --host 0.0.0.0 --port 8000 --watch docs"

[tool.pixi.feature.docs.tasks.build-apidocs]
description = "Build the API documentation in the apidocs/ directory"
cwd = "apidocs"
cmd = 'make html SPHINXOPTS="--keep-going -n"'

# Style
[tool.pixi.feature.style.dependencies]
black = "*"

[tool.pixi.feature.style.tasks]
format = { cmd = "black docs/ apidocs/ src/ tests/*.py", description = "Format the code" }

# Build
[tool.pixi.feature.publish.dependencies]
twine = ">=6.1.0,<7"
python-build = ">=1.2.2,<2"

[tool.pixi.feature.publish.tasks]
build = { cmd = "python -m build", description = "Build the package into the dist/ directory" }
build-check = { cmd = "python -m twine check --strict dist/*", depends-on = [
  "build",
], description = "Check that the package can be uploaded" }



================================================
FILE: setup.py
================================================
from pathlib import Path
import sys

from setuptools import setup

# ensure the current directory is on sys.path so versioneer can be imported
# when pip uses PEP 517/518 build rules.
# https://github.com/python-versioneer/python-versioneer/issues/193

source_dir = Path(__file__).parent
sys.path.append(str(source_dir))

# import Assets, while avoiding that the rest of snakemake is imported here before
# setup has been called.
sys.path.append(str(source_dir / "src" / "snakemake"))
from assets import Assets

# download online assets
Assets.deploy()

setup(
    name="snakemake",
    use_scm_version=True,
    package_data={
        "snakemake": [
            "assets/data/**/*",
            "report/html_reporter/template/**/*",
            "report/html_reporter/template/*",
            "common/tests/testcases/**/*",
            "unit_tests/templates/*",
        ]
    },
)



================================================
FILE: .dockerignore
================================================
env
.env
.eggs
images
misc
docs
tutorial
tests
examples



================================================
FILE: .gitpod.yml
================================================
image: snakemake/snakemake:main


================================================
FILE: .readthedocs.yml
================================================
version: 2

build:
  os: ubuntu-22.04
  tools:
    python: "3.11"
    # this ensures a viable `mamba` is on `$PATH``
    python: mambaforge-latest
  commands:
    - mamba install -c conda-forge -c nodefaults pixi
    - pixi install --environment docs
    - pixi run --environment docs build-docs
    - mkdir -p $READTHEDOCS_OUTPUT/html
    - rm -rf $READTHEDOCS_OUTPUT/html && mv docs/_build/html $READTHEDOCS_OUTPUT/html
sphinx:
  configuration: docs/conf.py



================================================
FILE: .sonarcloud.properties
================================================
sonar.sources=snakemake
sonar.python.version=3.11


================================================
FILE: .test_durations
================================================
{
  "tests/tests.py::test_singularity_conda": 197.75476967299994,
  "tests/tests.py::test_script": 161.43986884500003,
  "tests/tests.py::test_containerized": 132.75854069299993,
  "tests/tests.py::test_prebuilt_conda_script": 127.24543886599997,
  "tests/tests.py::test_upstream_conda": 119.17382170500002,
  "tests/tests.py::test_issue1331": 111.00403144699521,
  "tests/tests.py::test_issue635": 50.981860626000014,
  "tests/tests.py::test_wrapper_local_git_prefix": 36.43735605800009,
  "tests/tests.py::test_jupyter_notebook_draft": 33.67636250900006,
  "tests/tests.py::test_jupyter_notebook": 28.790527739000026,
  "tests/tests.py::test_deploy_script": 27.02208906100003,
  "tests/tests.py::test_converting_path_for_r_script": 25.586826289999863,
  "tests/tests.py::test_conda": 25.435097164000013,
  "tests/tests.py::test_issue1093": 23.854344417999982,
  "tests/tests.py::test_archive": 23.039710936000006,
  "tests/tests.py::test_queue_input": 20.115373177686706,
  "tests/tests.py::test_queue_input_forceall": 20.113109471043572,
  "tests/tests.py::test_queue_input_dryrun": 20.03096218779683,
  "tests/tests.py::test_github_issue806": 18.35432805889286,
  "tests/tests.py::test_conda_custom_prefix": 17.384630909000066,
  "tests/tests.py::test_deploy_hashing": 16.55730940000001,
  "tests/tests.py::test_conda_python_3_7_script": 16.47462713099992,
  "tests/tests.py::test_conda_function": 12.322682386999986,
  "tests/tests.py::test_pipes2": 12.024292239919305,
  "tests/tests.py::test_group_jobs": 11.858973153866827,
  "tests/tests.py::test_multicomp_group_jobs": 11.842080780072138,
  "tests/tests.py::test_scheduler_sequential_all_cores": 11.447925404179841,
  "tests/tests.py::test_conda_global": 11.120370950000051,
  "tests/tests.py::test_conda_python_script": 11.043120718999944,
  "tests/tests.py::test_github_issue1261": 10.863874779082835,
  "tests/tests.py::test_benchmark_jsonl": 10.502822432201356,
  "tests/tests.py::test_benchmark": 10.465204197913408,
  "tests/tests.py::test_wrapper": 9.544016541000019,
  "tests/tests.py::test_ancient": 9.336044118739665,
  "tests/tests.py::test_default_storage": 9.330656361999957,
  "tests/tests.py::test_issue930": 7.76615374092944,
  "tests/tests.py::test_shadow": 7.7587208359036595,
  "tests/tests.py::test_conda_named": 6.323018675000071,
  "tests/tests.py::test_group_job_resources_with_pipe": 6.255495929857716,
  "tests/tests.py::test_group_job_fail": 6.031794314738363,
  "tests/tests.py::test_issue860": 5.957130433991551,
  "tests/tests.py::test_delete_output": 5.808919062139466,
  "tests/tests.py::test_groupid_expand_cluster": 5.797111982945353,
  "tests/tests.py::test_group_jobs_attempts": 5.693795101949945,
  "tests/tests.py::test_checkpoints_many": 5.690685620997101,
  "tests/tests.py::test_set_resources_complex": 5.545762719819322,
  "tests/tests.py::test_singularity_cluster": 5.434438602766022,
  "tests/tests.py::test_github_issue1384": 5.422342301113531,
  "tests/tests.py::test_cwl_singularity": 5.291539239697158,
  "tests/tests.py::test_set_resources_human_readable": 5.221607670886442,
  "tests/tests.py::test_generate_unit_tests": 4.639468893874437,
  "tests/tests.py::test_conda_pin_file": 4.5727264519999835,
  "tests/tests.py::test_job_properties": 4.245566743658856,
  "tests/tests.py::test_url_include": 4.218866904033348,
  "tests/tests.py::test_issue850": 4.173489716136828,
  "tests/tests.py::test_string_resources": 4.169685567030683,
  "tests/tests.py::test05": 4.037952391197905,
  "tests/tests.py::test_pipes_simple": 4.036335143027827,
  "tests/tests.py::test_new_resources_can_be_defined_as_local": 4.0221736386884,
  "tests/tests.py::test_with_parentheses": 3.931770018301904,
  "tests/tests.py::test_symlink_time_handling": 3.85653002304025,
  "tests/tests.py::test_shadow_copy": 3.8008854289073497,
  "tests/tests.py::test_storage_cleanup_local": 3.637012318940833,
  "tests/tests.py::test_github_issue1882": 3.615843378007412,
  "tests/test_executor_test_suite.py::TestWorkflows::test_group_workflow": 3.4086399039999833,
  "tests/tests.py::test_set_resources_complex_profile": 3.274688652716577,
  "tests/tests.py::test_config_merging": 3.273725319188088,
  "tests/tests.py::test_ioutils": 3.259240098996088,
  "tests/tests.py::test_github_issue_3265_respect_dryrun_delete_all": 3.0707865359727293,
  "tests/tests.py::test_service_jobs": 3.0265432668384165,
  "tests/test_executor_test_suite.py::TestWorkflows::test_simple_workflow": 3.00227200300003,
  "tests/tests.py::test_delete_all_output": 2.983958630822599,
  "tests/tests.py::test_set_threads": 2.9736411480698735,
  "tests/tests.py::test_set_resources_complex_cli": 2.900256608147174,
  "tests/tests.py::test_singularity": 2.7677325177937746,
  "tests/tests.py::test_group_jobs_resources_with_limited_resources": 2.619534402852878,
  "tests/tests.py::test_update_config": 2.5604129231069237,
  "tests/tests.py::test_github_issue1158": 2.556309586856514,
  "tests/tests.py::test_group_jobs_resources": 2.5526694608852267,
  "tests/tests.py::test_excluded_resources_not_submitted_to_cluster": 2.4919739472679794,
  "tests/tests.py::test_resources_submitted_to_cluster": 2.43080695415847,
  "tests/tests.py::test_group_jobs_resources_with_max_threads": 2.4228385440073907,
  "tests/tests.py::test_scopes_submitted_to_cluster": 2.411690916167572,
  "tests/tests.py::test_nonstr_params": 2.410959935048595,
  "tests/tests.py::test_github_issue78": 2.3386207851581275,
  "tests/tests.py::test_shell": 2.31836340832524,
  "tests/tests.py::test_issue1083": 2.2626289480831474,
  "tests/tests.py::test_persistent_dict": 2.1322021820000145,
  "tests/tests.py::test_directory2": 2.1306491550058126,
  "tests/tests.py::test_resources_can_be_overwritten_as_global": 2.0925407100003213,
  "tests/tests.py::test10": 2.0778105608187616,
  "tests/tests.py::test_github_issue727": 2.0690123641397804,
  "tests/tests.py::test_global_resource_limits_limit_scheduling_of_groups": 2.054011082975194,
  "tests/tests.py::test_issue823_2": 2.052402160828933,
  "tests/tests.py::test_pipes_multiple": 2.028218562947586,
  "tests/tests.py::test_profile": 2.01697702286765,
  "tests/tests.py::test_github_issue988": 1.9715226762928069,
  "tests/tests.py::test_issue2826_failed_binary_logs": 1.970235870219767,
  "tests/tests.py::test_threads": 1.9690039672423154,
  "tests/tests.py::test_no_workflow_profile": 1.9494761417154223,
  "tests/tests.py::test_rule_defined_in_for_loop": 1.9260783710051328,
  "tests/tests.py::test_run_namedlist": 1.9167879819869995,
  "tests/tests.py::test_issue1092": 1.9126616478897631,
  "tests/tests.py::test_workflow_profile": 1.9099762600380927,
  "tests/tests.py::test_runtime_conversion_from_workflow_profile": 1.9094968668650836,
  "tests/tests.py::test_shadow_prefix": 1.9006822800729424,
  "tests/tests.py::test_protected_symlink_output": 1.88697584788315,
  "tests/tests.py::test_shadowed_log": 1.8734036309178919,
  "tests/tests.py::test_ancient_cli": 1.869531359989196,
  "tests/tests.py::test_paramspace_single_wildcard": 1.8615202540531754,
  "tests/tests.py::test_github_issue1069": 1.846755885053426,
  "tests/tests.py::test_keyword_list": 1.8402204250451177,
  "tests/tests.py::test_default_storage_provider_none": 1.838729496113956,
  "tests/tests.py::test_max_jobs_per_timespan": 1.8365983089897782,
  "tests/tests.py::test_checkpoint_allowed_rules": 1.8321549210231751,
  "tests/tests.py::test_all_temp": 1.8120554538909346,
  "tests/tests.py::test_paramspace": 1.8104857690632343,
  "tests/tests.py::test_retries_not_overriden": 1.8061030060052872,
  "tests/tests.py::test_github_issue261": 1.799018134130165,
  "tests/tests.py::test_storage_localrule": 1.7960972818545997,
  "tests/tests.py::test_match_by_wildcard_names": 1.7796861128881574,
  "tests/tests.py::test_retries": 1.7201219631824642,
  "tests/tests.py::test_storage": 1.4992757990000314,
  "tests/tests.py::test_list_untracked": 1.4284217141103,
  "tests/tests.py::test_params_pickling": 1.3816757071763277,
  "tests/tests.py::test_module_complex2": 1.2224351519253105,
  "tests/tests.py::test_module_complex": 1.173576348926872,
  "tests/tests.py::test_report_dir": 1.1394090682733804,
  "tests/test_linting.py::test_lint[missing_software_definition-positive]": 1.0619196160000115,
  "tests/test_linting.py::test_lint[singularity-positive]": 1.0589239550000684,
  "tests/test_linting.py::test_lint[path_add-positive]": 1.0536393029999545,
  "tests/test_linting.py::test_lint[iofile_by_index-positive]": 1.0443462729999737,
  "tests/test_linting.py::test_lint[mixed_func_and_rules-positive]": 1.0276032310000573,
  "tests/tests.py::test_pipes_fail": 1.0274616319220513,
  "tests/test_linting.py::test_lint[params_prefix-negative]": 1.0222751549999884,
  "tests/tests.py::test_fstring": 1.0164106169249862,
  "tests/tests.py::test_issue2574": 1.007385744014755,
  "tests/test_linting.py::test_lint[absolute_paths-positive]": 0.9946612220001043,
  "tests/test_linting.py::test_lint[version-negative]": 0.9815205209999931,
  "tests/tests.py::test_module_report": 0.9811882062349468,
  "tests/test_linting.py::test_lint[long_run-negative]": 0.9769150150000314,
  "tests/tests.py::test_github_issue_14": 0.9658979957457632,
  "tests/tests.py::test_output_file_cache_storage": 0.9571454370000083,
  "tests/tests.py::test_github_issue627": 0.9014011016115546,
  "tests/tests.py::test_filegraph": 0.8923124340362847,
  "tests/test_linting.py::test_lint[path_add-negative]": 0.8883087350000665,
  "tests/test_linting.py::test_lint[version-positive]": 0.888050124000074,
  "tests/test_linting.py::test_lint[envvars-positive]": 0.8799093959999595,
  "tests/test_linting.py::test_lint[missing_software_definition-negative]": 0.8795691909999732,
  "tests/tests.py::test_list_input_changes": 0.8753871312364936,
  "tests/test_linting.py::test_lint[params_prefix-positive]": 0.8410927619999029,
  "tests/test_linting.py::test_lint[long_run-positive]": 0.8348358939998661,
  "tests/test_linting.py::test_lint[envvars-negative]": 0.8297285379999266,
  "tests/tests.py::test_subpath": 0.8002295501064509,
  "tests/test_linting.py::test_lint[iofile_by_index-negative]": 0.7894975059999751,
  "tests/test_linting.py::test_lint[singularity-negative]": 0.7893011030000707,
  "tests/test_linting.py::test_lint[not_used_params-negative]": 0.7874490210001,
  "tests/tests.py::test_summary": 0.7853767571505159,
  "tests/test_linting.py::test_lint[mixed_func_and_rules-negative]": 0.7841167420000374,
  "tests/test_linting.py::test_lint[not_used_params-positive]": 0.7814260529999615,
  "tests/tests.py::test_issue1256": 0.7784343177918345,
  "tests/tests.py::test_script_python": 0.7777160571422428,
  "tests/test_linting.py::test_lint[tab_usage-positive]": 0.7757818250000241,
  "tests/test_linting.py::test_lint[log_directive-positive]": 0.7707859890000464,
  "tests/test_linting.py::test_lint[log_directive-negative]": 0.7701012800000626,
  "tests/test_linting.py::test_lint[tab_usage-negative]": 0.7681317860000263,
  "tests/test_linting.py::test_lint[absolute_paths-negative]": 0.7670129370000041,
  "tests/tests.py::test_validate": 0.679686674149707,
  "tests/tests.py::test_peppy": 0.6001399550586939,
  "tests/tests.py::test_singularity_global": 0.5909010290633887,
  "tests/tests.py::test_report": 0.5369566651061177,
  "tests/tests.py::test_yaml_config": 0.5217034695670009,
  "tests/tests.py::test_issue956": 0.5047423292417079,
  "tests/tests.py::test_container": 0.5031171480659395,
  "tests/tests.py::test_shell_exec_singularity": 0.45101496507413685,
  "tests/tests.py::test04": 0.4506158637814224,
  "tests/tests.py::test_temp": 0.4445530138909817,
  "tests/tests.py::test_touch": 0.4445505910553038,
  "tests/tests.py::test_spaces_in_fnames": 0.405932076042518,
  "tests/tests.py::test_glpk_solver": 0.3956056737806648,
  "tests/tests.py::test_wildcard_keyword": 0.3845911272801459,
  "tests/tests.py::test_temp_expand": 0.33138796174898744,
  "tests/tests.py::test_touch_pipeline_with_temp_dir": 0.3192523210309446,
  "tests/tests.py::test08": 0.30800895392894745,
  "tests/tests.py::test12": 0.28769562602974474,
  "tests/tests.py::test_report_href": 0.26415959536097944,
  "tests/tests.py::test_same_wildcard": 0.26195169100537896,
  "tests/tests.py::test_get_log_none": 0.2551400419999936,
  "tests/tests.py::test_report_zip": 0.2507889592088759,
  "tests/tests.py::test_get_log_stderr": 0.2368963639999606,
  "tests/tests.py::test_modules_prefix": 0.23490817891433835,
  "tests/tests.py::test_issue916": 0.23486201697960496,
  "tests/tests.py::test_get_log_stdout": 0.21670548999998118,
  "tests/tests.py::test_load_metawrapper": 0.19970647292211652,
  "tests/tests.py::test_modules_meta_wrapper": 0.19560629618354142,
  "tests/tests.py::test_get_log_complex": 0.18728509000004578,
  "tests/tests.py::test_get_log_both": 0.1803411869999536,
  "tests/tests.py::test_issue1046": 0.17822450600004913,
  "tests/tests.py::test_handover": 0.16243270807899535,
  "tests/tests.py::test_pep_pathlib": 0.16194257093593478,
  "tests/tests.py::test_module_with_script": 0.16121072717942297,
  "tests/tests.py::test_directory": 0.15991211123764515,
  "tests/tests.py::test_ancient_dag": 0.15806720708496869,
  "tests/tests.py::test_env_modules": 0.15396998800008532,
  "tests/tests.py::test_conditional": 0.14480079500935972,
  "tests/tests.py::test_handle_storage_multi_consumers": 0.13907945295795798,
  "tests/tests.py::test_storage_noretrieve_dryrun": 0.12248744815587997,
  "tests/tests.py::test_issue823_1": 0.11629820964299142,
  "tests/tests.py::test_checkpoint_open": 0.11530134011991322,
  "tests/tests.py::test_touch_flag_with_directories": 0.11383161088451743,
  "tests/tests.py::test_report_display_code": 0.11131065781228244,
  "tests/tests.py::test_multiple_includes": 0.09923563804477453,
  "tests/tests.py::test_multiext": 0.09572316706180573,
  "tests/tests.py::test_modules_peppy": 0.09459294192492962,
  "tests/tests.py::test13": 0.08550419169478118,
  "tests/tests.py::test_checkpoints_dir": 0.07494424981996417,
  "tests/tests.py::test09": 0.07050320389680564,
  "tests/tests.py::test_globwildcards": 0.06838802504353225,
  "tests/tests.py::test_unpack_dict": 0.062471161829307675,
  "tests/tests.py::test_until": 0.06224554101936519,
  "tests/tests.py::test_name_override": 0.0559288130607456,
  "tests/tests.py::test_template_engine": 0.05578013998456299,
  "tests/tests.py::test_modules_all": 0.055308999260887504,
  "tests/tests.py::test_github_issue52": 0.05508780479431152,
  "tests/tests.py::test_rule_inheritance_globals": 0.05424588895402849,
  "tests/tests.py::test_output_file_cache": 0.053356876131147146,
  "tests/tests.py::test_failed_intermediate": 0.0533049909863621,
  "tests/tests.py::test15": 0.05269804806448519,
  "tests/tests.py::test_issue1284": 0.05095456214621663,
  "tests/tests.py::test_params": 0.048084457870572805,
  "tests/tests.py::test_omitfrom": 0.048055437160655856,
  "tests/tests.py::test_checkpoints": 0.04802724486216903,
  "tests/tests.py::test_scatter_gather": 0.045636147959157825,
  "tests/tests.py::test_coin_solver": 0.045012778136879206,
  "tests/tests.py::test_pathlib": 0.044246486853808165,
  "tests/tests.py::test_batch_final": 0.04209263203665614,
  "tests/tests.py::test01": 0.04153475514613092,
  "tests/tests.py::test_issue3338": 0.041314173955470324,
  "tests/tests.py::test_groupid_expand_local": 0.03896048804745078,
  "tests/tests.py::test_modules_ruledeps_inheritance": 0.038904306245967746,
  "tests/tests.py::test_input_generator": 0.0379973731469363,
  "tests/tests.py::test_wildcard_count_ambiguity": 0.03781299805268645,
  "tests/tests.py::test_issue912": 0.03672460070811212,
  "tests/tests.py::test_config": 0.03655091719701886,
  "tests/tests.py::test_unpack_list": 0.0361270229332149,
  "tests/tests.py::test_github_issue2732": 0.03488529333844781,
  "tests/tests.py::test_modules_all_exclude_2": 0.033172456780448556,
  "tests/tests.py::test_issue1281": 0.03294136002659798,
  "tests/tests.py::test_github_issue1498": 0.03237757459282875,
  "tests/tests.py::test_long_shell": 0.030794878723099828,
  "tests/tests.py::test02": 0.03064008359797299,
  "tests/tests.py::test_update_flag": 0.02980107208713889,
  "tests/tests.py::test_ruledeps": 0.028213901910930872,
  "tests/tests.py::test_github_issue105": 0.0280257451813668,
  "tests/tests.py::test_envvars": 0.0279337412212044,
  "tests/tests.py::test11": 0.027031017001718283,
  "tests/tests.py::test_batch": 0.026748711010441184,
  "tests/tests.py::test_modules_prefix_local": 0.026474938727915287,
  "tests/tests.py::test_log_input": 0.02607565186917782,
  "tests/tests.py::test_group_job_resources_with_pipe_with_too_much_constraint": 0.025978685123845935,
  "tests/tests.py::test07": 0.025449026143178344,
  "tests/tests.py::test_module_no_prefixing_modified_paths": 0.024646114092320204,
  "tests/tests.py::test_lazy_resources": 0.024586975807324052,
  "tests/tests.py::test_github_issue1500": 0.024440034991130233,
  "tests/tests.py::test_github_issue413": 0.024127695010975003,
  "tests/tests.py::test_tmpdir": 0.02404270484112203,
  "tests/tests.py::test_module_wildcard_constraints": 0.02386837196536362,
  "tests/tests.py::test_modules_specific": 0.02259939000941813,
  "tests/tests.py::test_default_target": 0.021905432920902967,
  "tests/tests.py::test_issue2685": 0.021851201308891177,
  "tests/tests.py::test_use_rule_same_module": 0.021139298100024462,
  "tests/tests.py::test_default_resources": 0.02081926236860454,
  "tests/tests.py::test_ensure_success": 0.02022154675796628,
  "tests/tests.py::test_strict_mode": 0.02005258621647954,
  "tests/tests.py::test_local_import": 0.019872518023476005,
  "tests/tests.py::test_singularity_none": 0.019710252061486244,
  "tests/tests.py::test_module_worfklow_namespacing": 0.019580550957471132,
  "tests/tests.py::test_tmpdir_default": 0.01932977419346571,
  "tests/tests.py::test_issue584": 0.019184433855116367,
  "tests/tests.py::test03": 0.019023125991225243,
  "tests/tests.py::test_localrule": 0.019012582022696733,
  "tests/tests.py::test_github_issue1818": 0.018597626825794578,
  "tests/tests.py::test_issue381": 0.01857996778562665,
  "tests/tests.py::test_module_input_func": 0.018308796919882298,
  "tests/tests.py::test_ensure_checksum_fail": 0.018292520893737674,
  "tests/tests.py::test_module_nested": 0.018039944116026163,
  "tests/tests.py::test_call_inner": 0.017293883254751563,
  "tests/tests.py::test_threads0": 0.017197028966620564,
  "tests/tests.py::test_github_issue1618": 0.017136554000899196,
  "tests/tests.py::test_ensure_nonempty_fail": 0.016946704126894474,
  "tests/tests.py::test_github_issue640": 0.01654552691616118,
  "tests/tests.py::test06": 0.0165196661837399,
  "tests/tests.py::test_containerize": 0.016301465000026383,
  "tests/tests.py::test_github_issue1550": 0.016185747925192118,
  "tests/tests.py::test_github_issue2142": 0.016104098642244935,
  "tests/tests.py::test_inferred_resources": 0.015809351345524192,
  "tests/test_schema.py::test_dataframe": 0.015630281999847284,
  "tests/tests.py::test_issue823_3": 0.015485161682590842,
  "tests/tests.py::test_issue471": 0.01528984704054892,
  "tests/tests.py::test_format_wildcards": 0.01527156587690115,
  "tests/tests.py::test_github_issue929": 0.015163294970989227,
  "tests/tests.py::test_deferred_func_eval": 0.01498380396515131,
  "tests/tests.py::test_issue894": 0.014981192303821445,
  "tests/tests.py::test_github_issue456": 0.014806858031079173,
  "tests/tests.py::test_bash": 0.014350920682772994,
  "tests/tests.py::test_expand_list_of_functions": 0.014001698000356555,
  "tests/tests.py::test_source_path": 0.013954584952443838,
  "tests/tests.py::test_groups_out_of_jobs": 0.013661599019542336,
  "tests/tests.py::test_modules_all_exclude_1": 0.013594608288258314,
  "tests/tests.py::test_core_dependent_threads": 0.013498660875484347,
  "tests/test_schema.py::test_config_ref": 0.013302721000002293,
  "tests/tests.py::test_parsing_terminal_comment_following_statement": 0.013247652910649776,
  "tests/tests.py::test_set_resources": 0.013049104018136859,
  "tests/tests.py::test_github_issue1542": 0.013038305100053549,
  "tests/tests.py::test_incomplete_params": 0.011975828092545271,
  "tests/tests.py::test_config_yte": 0.011944804107770324,
  "tests/tests.py::test_format_params": 0.011670383857563138,
  "tests/tests.py::test_params_outdated_metadata": 0.011200228007510304,
  "tests/tests.py::test_conda_list_envs": 0.011109794999924816,
  "tests/tests.py::test_issue1037": 0.011096186237409711,
  "tests/tests.py::test_issue612": 0.010307548334822059,
  "tests/tests.py::test_issue958": 0.010257409885525703,
  "tests/tests.py::test_github_issue1389": 0.009976133238524199,
  "tests/tests.py::test_empty_include": 0.009910424705594778,
  "tests/tests.py::test_resource_tbdstring": 0.009739394998177886,
  "tests/test_schema.py::test_config": 0.008807096999930764,
  "tests/tests.py::test_inoutput_is_path": 0.008132354822009802,
  "tests/tests.py::test_singularity_invalid": 0.00793473538942635,
  "tests/tests.py::test_batch_fail": 0.007326625753194094,
  "tests/tests.py::test_pipe_depend_target_file": 0.007106972159817815,
  "tests/tests.py::test_singularity_module_invalid": 0.007069978164508939,
  "tests/tests.py::test_pipe_depend": 0.007023002952337265,
  "tests/tests.py::test_pathlib_missing_file": 0.005637410096824169,
  "tests/tests.py::test_dup_out_patterns": 0.005502595799043775,
  "tests/tests.py::test_missing_file_dryrun": 0.005422450136393309,
  "tests/tests.py::test_issue1085": 0.005399468820542097,
  "tests/tests.py::test_exists": 0.005003282800316811,
  "tests/tests.py::test_toposort": 0.004886152921244502,
  "tests/tests.py::test_cleanup_metadata_fail": 0.004802477778866887,
  "tests/tests.py::test_validate_fail": 0.0038340582977980375,
  "tests/tests.py::test_issue805": 0.003740105777978897,
  "tests/tests.py::test_expand_flag": 0.0035578489769250154,
  "tests/tests.py::test_issue854": 0.003386870725080371,
  "tests/tests.py::test_cache_multioutput": 0.003321409923955798,
  "tests/test_expand.py::test_allow_missing": 0.002882467000063116,
  "tests/test_io.py::test_expand": 0.0018577370000230076,
  "tests/test_expand.py::test_simple_expand": 0.0009011110000756162,
  "tests/test_io.py::test_wildcard_regex": 0.0008299119999719551,
  "tests/tests.py::test_conda_cmd_exe": 0.00048201100003097963,
  "tests/tests.py::test_convert_to_cwl": 0.0003852460067719221,
  "tests/tests.py::test_filesep_windows_targets": 0.0003132040146738291,
  "tests/tests.py::test_symlink_temp": 0.0002760158386081457,
  "tests/tests.py::test_github_issue1062": 0.0002582769375294447,
  "tests/tests.py::test_filesep_on_windows": 0.0002469560131430626
}



================================================
FILE: .wci.yml
================================================
name: Snakemake
language: Python
documentation:
  general: https://snakemake.readthedocs.io
  installation: https://snakemake.readthedocs.io/en/stable/getting_started/installation.html
  tutorial: https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html

social:
  twitter: johanneskoester

execution_environment:
  resource_managers:
    - any DRM providing a script based submission interface (e.g. slurm, lsf, pbs, ...)
    - kubernetes
    - google life sciences API
    - tibanna
    - TES


================================================
FILE: apidocs/conf.py
================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Snakemake documentation build configuration file, created by
# sphinx-quickstart on Sat Feb  1 16:01:02 2014.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys
import os
from datetime import datetime
from sphinxawesome_theme.postprocess import Icons


# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath("../src/"))

# -- General configuration ------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    "sphinx.ext.autodoc",
    "sphinx_autodoc_typehints",
    "sphinx.ext.autosummary",
    "sphinx.ext.mathjax",
    "sphinx.ext.viewcode",
    "sphinx.ext.napoleon",
    "sphinx.ext.autosectionlabel",
    "sphinxawesome_theme.highlighting",
]
autosummary_generate = True

# skip internal class that Sphinx can't process (#296)
autodoc_default_options = {"exclude-members": "lazy_property"}

# Snakemake theme (made by SciAni).
html_css_files = ["custom.css"]

# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]

# The suffix of source filenames.
source_suffix = [".rst", ".md"]

# The encoding of source files.
# source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = "index"

# General information about the project.
project = "Snakemake-API"
date = datetime.now()
copyright = "2014-{year}, Johannes Koester".format(year=date.timetuple()[0])

import snakemake

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = snakemake.__version__

if os.environ.get("READTHEDOCS") == "True":
    # Because Read The Docs modifies conf.py, versioneer gives a "dirty"
    # version like "5.10.0+0.g28674b1.dirty" that is cleaned here.
    version = version.partition("+0.g")[0]

# The full version, including alpha/beta/rc tags.
release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
# language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
# today = ''
# Else, today_fmt is used as the format for a strftime call.
# today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ["_build"]

nitpick_ignore_regex = [
    ("py:class", r".*"),
    ("py:data", r".*"),
    ("py:exc", r".*"),
    ("py:obj", r".*"),
]

# The reST default role (used for this markup: `text`) to use for all
# documents.
# default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
# add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
# add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
# show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
# pygments_style = "sphinx"

# A list of ignored prefixes for module index sorting.
# modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
# keep_warnings = False


# -- Options for HTML output ----------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = "sphinxawesome_theme"
html_theme_options = {
    "logo_light": "logo-snake.svg",
    "logo_dark": "logo-snake.svg",
    "main_nav_links": {
        "Homepage": "https://snakemake.github.io",
        "Main docs": "https://snakemake.readthedocs.io",
    },
    "awesome_external_links": True,
    "awesome_headerlinks": True,
    "show_prev_next": False,
}
html_permalinks_icon = Icons.permalinks_icon

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
# html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
# html_theme_path = sphinx_bootstrap_theme.get_html_theme_path()

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
# html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
# html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
# html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
# html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["../docs/_static"]

# Add any extra paths that contain custom files (such as robots.txt or
# .htaccess) here, relative to this directory. These files are copied
# directly to the root of the documentation.
# html_extra_path = ["_static/css"]

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
# html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
# html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
# html_sidebars = {"**": ["ethicalads.html"]}

# Additional templates that should be rendered to pages, maps page names to
# template names.
# html_additional_pages = {"index": "index.html"}

# If false, no module index is generated.
# html_domain_indices = True

# If false, no index is generated.
# html_use_index = True

# If true, the index is split into individual pages for each letter.
# html_split_index = False

# If true, links to the reST sources are added to the pages.
# html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
# html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
# html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
# html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
# html_file_suffix = None

# Output file base name for HTML help builder.


def setup(app):
    app.add_css_file("sphinx-argparse.css")



================================================
FILE: apidocs/index.rst
================================================
.. _manual-main:

=========
Snakemake
=========

.. image:: https://img.shields.io/badge/Gitpod-ready--to--code-blue?color=%23022c22
    :target: https://gitpod.io/#https://github.com/snakemake/snakemake

.. image:: https://img.shields.io/conda/dn/bioconda/snakemake.svg?label=Bioconda&color=%23064e3b
    :target: https://bioconda.github.io/recipes/snakemake/README.html

.. image:: https://img.shields.io/pypi/pyversions/snakemake.svg?color=%23065f46
    :target: https://www.python.org

.. image:: https://img.shields.io/pypi/v/snakemake.svg?color=%23047857
    :target: https://pypi.python.org/pypi/snakemake

.. image:: https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/docker-publish.yml?label=docker%20container&branch=main&color=%23059669
    :target: https://hub.docker.com/r/snakemake/snakemake

.. image:: https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/main.yml?label=tests&color=%2310b981
    :target: https://github.com/snakemake/snakemake/actions?query=branch%3Amain+workflow%3ACI

.. image:: https://img.shields.io/badge/stack-overflow-orange.svg?color=%2334d399
    :target: https://stackoverflow.com/questions/tagged/snakemake

.. image:: https://img.shields.io/discord/753690260830945390?label=discord%20chat&color=%23a7f3d0
    :alt: Discord
    :target: https://discord.gg/NUdMtmr

.. image:: https://img.shields.io/twitter/follow/johanneskoester.svg?style=social&label=Follow&color=%236ee7b7
    :target: https://twitter.com/search?l=&q=%23snakemake%20from%3Ajohanneskoester

.. image:: https://img.shields.io/github/stars/snakemake/snakemake?style=social
    :alt: GitHub stars
    :target: https://github.com/snakemake/snakemake/stargazers

.. .. raw:: html
          <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1093/bioinformatics/bts480" data-legend="always" data-style="large_rectangle"></span><script async src="https://badge.dimensions.ai/badge.js" charset="utf-8"></script>

The Snakemake workflow management system is a tool to create **reproducible and scalable** data analyses.
Workflows are described via a human readable, Python based language.
They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition.
Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.

Snakemake is **highly popular**, with `>7 new citations per week <https://badge.dimensions.ai/details/id/pub.1018944052>`_.
For an introduction, please visit https://snakemake.github.io.

This is the home of the **Snakemake API documentation**, which is meant **purely for developers of snakemake's internal functionalities**.
If you are looking for the Snakemake user documentation (for writing workflows or executing them), please visit https://snakemake.readthedocs.io.



.. toctree::
    :caption: API Reference
    :name: api-reference
    :hidden:
    :maxdepth: 1

    api_reference/snakemake_api
    api_reference/snakemake_utils
    Internal API <api_reference/internal/modules>


================================================
FILE: apidocs/Makefile
================================================
# Makefile for Sphinx documentation
#

# You can set these variables from the command line.
SPHINXOPTS    =
SPHINXBUILD   = sphinx-build
PAPER         =
BUILDDIR      = _build

# User-friendly check for sphinx-build
ifeq ($(shell which $(SPHINXBUILD) >/dev/null 2>&1; echo $$?), 1)
$(error The '$(SPHINXBUILD)' command was not found. Make sure you have Sphinx installed, then set the SPHINXBUILD environment variable to point to the full path of the '$(SPHINXBUILD)' executable. Alternatively you can add the directory with the executable to your PATH. If you don't have Sphinx installed, grab it from https://www.sphinx-doc.org/)
endif

# Internal variables.
PAPEROPT_a4     = -D latex_paper_size=a4
PAPEROPT_letter = -D latex_paper_size=letter
ALLSPHINXOPTS   = -d $(BUILDDIR)/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) .
# the i18n builder cannot share the environment and doctrees with the others
I18NSPHINXOPTS  = $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) .

.PHONY: help clean html dirhtml singlehtml pickle json htmlhelp qthelp devhelp epub latex latexpdf text man changes linkcheck doctest gettext

help:
	@echo "Please use \`make <target>' where <target> is one of"
	@echo "  html       to make standalone HTML files"
	@echo "  dirhtml    to make HTML files named index.html in directories"
	@echo "  singlehtml to make a single large HTML file"
	@echo "  pickle     to make pickle files"
	@echo "  json       to make JSON files"
	@echo "  htmlhelp   to make HTML files and a HTML help project"
	@echo "  qthelp     to make HTML files and a qthelp project"
	@echo "  devhelp    to make HTML files and a Devhelp project"
	@echo "  epub       to make an epub"
	@echo "  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter"
	@echo "  latexpdf   to make LaTeX files and run them through pdflatex"
	@echo "  latexpdfja to make LaTeX files and run them through platex/dvipdfmx"
	@echo "  text       to make text files"
	@echo "  man        to make manual pages"
	@echo "  texinfo    to make Texinfo files"
	@echo "  info       to make Texinfo files and run them through makeinfo"
	@echo "  gettext    to make PO message catalogs"
	@echo "  changes    to make an overview of all changed/added/deprecated items"
	@echo "  xml        to make Docutils-native XML files"
	@echo "  pseudoxml  to make pseudoxml-XML files for display purposes"
	@echo "  linkcheck  to check all external links for integrity"
	@echo "  doctest    to run all doctests embedded in the documentation (if enabled)"

clean:
	rm -rf $(BUILDDIR)/*

html:
	$(SPHINXBUILD) -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html
	@echo
	@echo "Build finished. The HTML pages are in $(BUILDDIR)/html."

dirhtml:
	$(SPHINXBUILD) -b dirhtml $(ALLSPHINXOPTS) $(BUILDDIR)/dirhtml
	@echo
	@echo "Build finished. The HTML pages are in $(BUILDDIR)/dirhtml."

singlehtml:
	$(SPHINXBUILD) -b singlehtml $(ALLSPHINXOPTS) $(BUILDDIR)/singlehtml
	@echo
	@echo "Build finished. The HTML page is in $(BUILDDIR)/singlehtml."

pickle:
	$(SPHINXBUILD) -b pickle $(ALLSPHINXOPTS) $(BUILDDIR)/pickle
	@echo
	@echo "Build finished; now you can process the pickle files."

json:
	$(SPHINXBUILD) -b json $(ALLSPHINXOPTS) $(BUILDDIR)/json
	@echo
	@echo "Build finished; now you can process the JSON files."

htmlhelp:
	$(SPHINXBUILD) -b htmlhelp $(ALLSPHINXOPTS) $(BUILDDIR)/htmlhelp
	@echo
	@echo "Build finished; now you can run HTML Help Workshop with the" \
	      ".hhp project file in $(BUILDDIR)/htmlhelp."

qthelp:
	$(SPHINXBUILD) -b qthelp $(ALLSPHINXOPTS) $(BUILDDIR)/qthelp
	@echo
	@echo "Build finished; now you can run "qcollectiongenerator" with the" \
	      ".qhcp project file in $(BUILDDIR)/qthelp, like this:"
	@echo "# qcollectiongenerator $(BUILDDIR)/qthelp/Snakemake.qhcp"
	@echo "To view the help file:"
	@echo "# assistant -collectionFile $(BUILDDIR)/qthelp/Snakemake.qhc"

devhelp:
	$(SPHINXBUILD) -b devhelp $(ALLSPHINXOPTS) $(BUILDDIR)/devhelp
	@echo
	@echo "Build finished."
	@echo "To view the help file:"
	@echo "# mkdir -p $$HOME/.local/share/devhelp/Snakemake"
	@echo "# ln -s $(BUILDDIR)/devhelp $$HOME/.local/share/devhelp/Snakemake"
	@echo "# devhelp"

epub:
	$(SPHINXBUILD) -b epub $(ALLSPHINXOPTS) $(BUILDDIR)/epub
	@echo
	@echo "Build finished. The epub file is in $(BUILDDIR)/epub."

latex:
	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
	@echo
	@echo "Build finished; the LaTeX files are in $(BUILDDIR)/latex."
	@echo "Run \`make' in that directory to run these through (pdf)latex" \
	      "(use \`make latexpdf' here to do that automatically)."

latexpdf:
	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
	@echo "Running LaTeX files through pdflatex..."
	$(MAKE) -C $(BUILDDIR)/latex all-pdf
	@echo "pdflatex finished; the PDF files are in $(BUILDDIR)/latex."

latexpdfja:
	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
	@echo "Running LaTeX files through platex and dvipdfmx..."
	$(MAKE) -C $(BUILDDIR)/latex all-pdf-ja
	@echo "pdflatex finished; the PDF files are in $(BUILDDIR)/latex."

text:
	$(SPHINXBUILD) -b text $(ALLSPHINXOPTS) $(BUILDDIR)/text
	@echo
	@echo "Build finished. The text files are in $(BUILDDIR)/text."

man:
	$(SPHINXBUILD) -b man $(ALLSPHINXOPTS) $(BUILDDIR)/man
	@echo
	@echo "Build finished. The manual pages are in $(BUILDDIR)/man."

texinfo:
	$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo
	@echo
	@echo "Build finished. The Texinfo files are in $(BUILDDIR)/texinfo."
	@echo "Run \`make' in that directory to run these through makeinfo" \
	      "(use \`make info' here to do that automatically)."

info:
	$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo
	@echo "Running Texinfo files through makeinfo..."
	make -C $(BUILDDIR)/texinfo info
	@echo "makeinfo finished; the Info files are in $(BUILDDIR)/texinfo."

gettext:
	$(SPHINXBUILD) -b gettext $(I18NSPHINXOPTS) $(BUILDDIR)/locale
	@echo
	@echo "Build finished. The message catalogs are in $(BUILDDIR)/locale."

changes:
	$(SPHINXBUILD) -b changes $(ALLSPHINXOPTS) $(BUILDDIR)/changes
	@echo
	@echo "The overview file is in $(BUILDDIR)/changes."

linkcheck:
	$(SPHINXBUILD) -b linkcheck $(ALLSPHINXOPTS) $(BUILDDIR)/linkcheck
	@echo
	@echo "Link check complete; look for any errors in the above output " \
	      "or in $(BUILDDIR)/linkcheck/output.txt."

doctest:
	$(SPHINXBUILD) -b doctest $(ALLSPHINXOPTS) $(BUILDDIR)/doctest
	@echo "Testing of doctests in the sources finished, look at the " \
	      "results in $(BUILDDIR)/doctest/output.txt."

xml:
	$(SPHINXBUILD) -b xml $(ALLSPHINXOPTS) $(BUILDDIR)/xml
	@echo
	@echo "Build finished. The XML files are in $(BUILDDIR)/xml."

pseudoxml:
	$(SPHINXBUILD) -b pseudoxml $(ALLSPHINXOPTS) $(BUILDDIR)/pseudoxml
	@echo
	@echo "Build finished. The pseudo-XML files are in $(BUILDDIR)/pseudoxml."



================================================
FILE: apidocs/requirements.txt
================================================
sphinx ==8.1  # TODO workaround for bug in 8.2.0, update as soon as fixed
sphinxcontrib-napoleon
sphinx-argparse
sphinx-autodoc-typehints
docutils
myst-parser
configargparse
appdirs
sphinxawesome-theme
snakemake-interface-common
snakemake-interface-executor-plugins
snakemake-interface-storage-plugins
snakemake-interface-logger-plugins


================================================
FILE: apidocs/.readthedocs.yaml
================================================
version: 2

build:
  os: ubuntu-22.04
  tools:
    python: "3.11"

sphinx:
  configuration: apidocs/conf.py

python:
  install:
    - requirements: apidocs/requirements.txt



================================================
FILE: apidocs/_templates/module_template.rst
================================================
{{ fullname | escape | underline}}

.. automodule:: {{ fullname }}
  
   {% block attributes %}
   {% if attributes %}
   .. rubric:: Module Attributes

   .. autosummary::
      :toctree:
   {% for item in attributes %}
      {{ item }}
   {%- endfor %}
   {% endif %}
   {% endblock %}

   {% block functions %}
   {% if functions %}
   .. rubric:: {{ _('Functions') }}

   .. autosummary::
      :toctree:
   {% for item in functions %}
      {{ item }}
   {%- endfor %}
   {% endif %}
   {% endblock %}

   {% block classes %}
   {% if classes %}
   .. rubric:: {{ _('Classes') }}

   .. autosummary::
      :toctree:
   {% for item in classes %}
      {{ item }}
   {%- endfor %}
   {% endif %}
   {% endblock %}

   {% block exceptions %}
   {% if exceptions %}
   .. rubric:: {{ _('Exceptions') }}

   .. autosummary::
      :toctree:
   {% for item in exceptions %}
      {{ item }}
   {%- endfor %}
   {% endif %}
   {% endblock %}

{% block modules %}
{% if modules %}
.. rubric:: Modules

.. autosummary::
   :toctree:
   :template: module_template.rst
   :recursive:
{% for item in modules %}
   {{ item }}
{%- endfor %}
{% endif %}
{% endblock %}


================================================
FILE: apidocs/_templates/toc.html
================================================
{% extends "!toc.html" %}

{% block toc_after %}
<div id="ethical-ad-placement"></div>
{% endblock %}


================================================
FILE: apidocs/api_reference/snakemake_api.rst
================================================
.. _api_reference_snakemake:

The Snakemake API
=================

The Snakemake API consists of three layers.
The first layer is the central entrypoint, given by the :class:`snakemake.api.SnakemakeApi` class.
From this, a workflow can be loaded via the :meth:`snakemake.api.SnakemakeApi.workflow` method, returning the :class:`snakemake.api.WorkflowApi` class.
From this, the DAG can be processes via the :meth:`snakemake.api.WorkflowApi.dag` method, returning the :class:`snakemake.api.DAGApi` class.

All methods and classes are parameterized via Python `dataclasses <https://docs.python.org/3/library/dataclasses.html>`_, defined in :mod:`snakemake.settings`.

It can be used as follows:

.. code-block:: python

    from pathlib import Path

    from snakemake.api import (
        OutputSettings,
        ResourceSettings,
        SnakemakeApi,
        StorageSettings,
    )

    with SnakemakeApi(
        OutputSettings(
            verbose=False,
            show_failed_logs=True,
        ),
    ) as snakemake_api:
        workflow_api = snakemake_api.workflow(
            storage_settings=StorageSettings(),
            resource_settings=ResourceSettings(),
            snakefile=Path("path/to/Snakefile"),
        )
        dag_api = workflow_api.dag()
        # Go on by calling methods of the dag api.


.. autosummary::
   :toctree: _autosummary
   :template: module_template.rst
   :recursive:
   :noindex:

   snakemake.api
   snakemake.settings
   snakemake.settings.enums
   snakemake.settings.types



================================================
FILE: apidocs/api_reference/snakemake_utils.rst
================================================
.. _utils-api:

Additional utils
================

.. automodule:: snakemake.utils
    :members:
    :noindex:



================================================
FILE: apidocs/api_reference/internal/modules.rst
================================================
snakemake
=========

.. toctree::
   :maxdepth: 4

   snakemake



================================================
FILE: apidocs/api_reference/internal/snakemake.assets.rst
================================================
snakemake.assets package
========================

Module contents
---------------

.. automodule:: snakemake.assets
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.caching.rst
================================================
snakemake.caching package
=========================

Submodules
----------

snakemake.caching.hash module
-----------------------------

.. automodule:: snakemake.caching.hash
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.caching.local module
------------------------------

.. automodule:: snakemake.caching.local
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.caching.storage module
--------------------------------

.. automodule:: snakemake.caching.storage
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.caching
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.common.rst
================================================
snakemake.common package
========================

Subpackages
-----------

.. toctree::
   :maxdepth: 4

   snakemake.common.tests

Submodules
----------

snakemake.common.argparse module
--------------------------------

.. automodule:: snakemake.common.argparse
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.common.configfile module
----------------------------------

.. automodule:: snakemake.common.configfile
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.common.git module
---------------------------

.. automodule:: snakemake.common.git
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.common.tbdstring module
---------------------------------

.. automodule:: snakemake.common.tbdstring
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.common.typing module
------------------------------

.. automodule:: snakemake.common.typing
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.common.workdir\_handler module
----------------------------------------

.. automodule:: snakemake.common.workdir_handler
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.common
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.common.tests.rst
================================================
snakemake.common.tests package
==============================

Subpackages
-----------

.. toctree::
   :maxdepth: 4

   snakemake.common.tests.testcases

Module contents
---------------

.. automodule:: snakemake.common.tests
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.common.tests.testcases.groups.rst
================================================
snakemake.common.tests.testcases.groups package
===============================================

Module contents
---------------

.. automodule:: snakemake.common.tests.testcases.groups
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.common.tests.testcases.rst
================================================
snakemake.common.tests.testcases package
========================================

Subpackages
-----------

.. toctree::
   :maxdepth: 4

   snakemake.common.tests.testcases.groups
   snakemake.common.tests.testcases.simple

Module contents
---------------

.. automodule:: snakemake.common.tests.testcases
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.common.tests.testcases.simple.rst
================================================
snakemake.common.tests.testcases.simple package
===============================================

Module contents
---------------

.. automodule:: snakemake.common.tests.testcases.simple
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.deployment.rst
================================================
snakemake.deployment package
============================

Submodules
----------

snakemake.deployment.conda module
---------------------------------

.. automodule:: snakemake.deployment.conda
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.deployment.containerize module
----------------------------------------

.. automodule:: snakemake.deployment.containerize
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.deployment.env\_modules module
----------------------------------------

.. automodule:: snakemake.deployment.env_modules
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.deployment.singularity module
---------------------------------------

.. automodule:: snakemake.deployment.singularity
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.deployment
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.executors.rst
================================================
snakemake.executors package
===========================

Submodules
----------

snakemake.executors.dryrun module
---------------------------------

.. automodule:: snakemake.executors.dryrun
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.executors.local module
--------------------------------

.. automodule:: snakemake.executors.local
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.executors.touch module
--------------------------------

.. automodule:: snakemake.executors.touch
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.executors
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.ioutils.rst
================================================
snakemake.ioutils package
=========================

Submodules
----------

snakemake.ioutils.branch module
-------------------------------

.. automodule:: snakemake.ioutils.branch
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.ioutils.collect module
--------------------------------

.. automodule:: snakemake.ioutils.collect
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.ioutils.evaluate module
---------------------------------

.. automodule:: snakemake.ioutils.evaluate
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.ioutils.exists module
-------------------------------

.. automodule:: snakemake.ioutils.exists
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.ioutils.lookup module
-------------------------------

.. automodule:: snakemake.ioutils.lookup
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.ioutils.rule\_items\_proxy module
-------------------------------------------

.. automodule:: snakemake.ioutils.rule_items_proxy
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.ioutils.subpath module
--------------------------------

.. automodule:: snakemake.ioutils.subpath
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.ioutils
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.linting.rst
================================================
snakemake.linting package
=========================

Submodules
----------

snakemake.linting.links module
------------------------------

.. automodule:: snakemake.linting.links
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.linting.rules module
------------------------------

.. automodule:: snakemake.linting.rules
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.linting.snakefiles module
-----------------------------------

.. automodule:: snakemake.linting.snakefiles
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.linting
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.remote.rst
================================================
snakemake.remote package
========================

Submodules
----------

snakemake.remote.AzBlob module
------------------------------

.. automodule:: snakemake.remote.AzBlob
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.EGA module
---------------------------

.. automodule:: snakemake.remote.EGA
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.FTP module
---------------------------

.. automodule:: snakemake.remote.FTP
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.GS module
--------------------------

.. automodule:: snakemake.remote.GS
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.HTTP module
----------------------------

.. automodule:: snakemake.remote.HTTP
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.NCBI module
----------------------------

.. automodule:: snakemake.remote.NCBI
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.S3 module
--------------------------

.. automodule:: snakemake.remote.S3
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.S3Mocked module
--------------------------------

.. automodule:: snakemake.remote.S3Mocked
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.SFTP module
----------------------------

.. automodule:: snakemake.remote.SFTP
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.XRootD module
------------------------------

.. automodule:: snakemake.remote.XRootD
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.dropbox module
-------------------------------

.. automodule:: snakemake.remote.dropbox
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.gfal module
----------------------------

.. automodule:: snakemake.remote.gfal
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.gridftp module
-------------------------------

.. automodule:: snakemake.remote.gridftp
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.iRODS module
-----------------------------

.. automodule:: snakemake.remote.iRODS
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.webdav module
------------------------------

.. automodule:: snakemake.remote.webdav
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.remote.zenodo module
------------------------------

.. automodule:: snakemake.remote.zenodo
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.remote
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.report.html_reporter.data.rst
================================================
snakemake.report.html\_reporter.data package
============================================

Submodules
----------

snakemake.report.html\_reporter.data.categories module
------------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.categories
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.html\_reporter.data.common module
--------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.common
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.html\_reporter.data.configfiles module
-------------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.configfiles
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.html\_reporter.data.packages module
----------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.packages
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.html\_reporter.data.results module
---------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.results
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.html\_reporter.data.rulegraph module
-----------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.rulegraph
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.html\_reporter.data.rules module
-------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.rules
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.html\_reporter.data.runtimes module
----------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.runtimes
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.html\_reporter.data.timeline module
----------------------------------------------------

.. automodule:: snakemake.report.html_reporter.data.timeline
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.report.html_reporter.data
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.report.html_reporter.rst
================================================
snakemake.report.html\_reporter package
=======================================

Subpackages
-----------

.. toctree::
   :maxdepth: 4

   snakemake.report.html_reporter.data
   snakemake.report.html_reporter.template

Submodules
----------

snakemake.report.html\_reporter.common module
---------------------------------------------

.. automodule:: snakemake.report.html_reporter.common
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.report.html_reporter
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.report.html_reporter.template.components.rst
================================================
snakemake.report.html\_reporter.template.components package
===========================================================

Module contents
---------------

.. automodule:: snakemake.report.html_reporter.template.components
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.report.html_reporter.template.rst
================================================
snakemake.report.html\_reporter.template package
================================================

Subpackages
-----------

.. toctree::
   :maxdepth: 4

   snakemake.report.html_reporter.template.components

Module contents
---------------

.. automodule:: snakemake.report.html_reporter.template
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.report.rst
================================================
snakemake.report package
========================

Subpackages
-----------

.. toctree::
   :maxdepth: 4

   snakemake.report.html_reporter

Submodules
----------

snakemake.report.common module
------------------------------

.. automodule:: snakemake.report.common
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.report.rulegraph\_spec module
---------------------------------------

.. automodule:: snakemake.report.rulegraph_spec
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.report
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.rst
================================================
snakemake package
=================

Subpackages
-----------

.. toctree::
   :maxdepth: 4

   snakemake.assets
   snakemake.caching
   snakemake.common
   snakemake.deployment
   snakemake.executors
   snakemake.ioutils
   snakemake.linting
   snakemake.remote
   snakemake.report
   snakemake.script
   snakemake.settings
   snakemake.template_rendering
   snakemake.unit_tests

Submodules
----------

snakemake.api module
--------------------

.. automodule:: snakemake.api
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.benchmark module
--------------------------

.. automodule:: snakemake.benchmark
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.checkpoints module
----------------------------

.. automodule:: snakemake.checkpoints
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.cli module
--------------------

.. automodule:: snakemake.cli
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.cwl module
--------------------

.. automodule:: snakemake.cwl
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.dag module
--------------------

.. automodule:: snakemake.dag
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.decorators module
---------------------------

.. automodule:: snakemake.decorators
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.exceptions module
---------------------------

.. automodule:: snakemake.exceptions
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.gui module
--------------------

.. automodule:: snakemake.gui
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.io module
-------------------

.. automodule:: snakemake.io
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.ioflags module
------------------------

.. automodule:: snakemake.ioflags
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.jobs module
---------------------

.. automodule:: snakemake.jobs
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.logging module
------------------------

.. automodule:: snakemake.logging
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.modules module
------------------------

.. automodule:: snakemake.modules
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.notebook module
-------------------------

.. automodule:: snakemake.notebook
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.output\_index module
------------------------------

.. automodule:: snakemake.output_index
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.parser module
-----------------------

.. automodule:: snakemake.parser
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.path\_modifier module
-------------------------------

.. automodule:: snakemake.path_modifier
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.persistence module
----------------------------

.. automodule:: snakemake.persistence
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.profiles module
-------------------------

.. automodule:: snakemake.profiles
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.resources module
--------------------------

.. automodule:: snakemake.resources
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.ruleinfo module
-------------------------

.. automodule:: snakemake.ruleinfo
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.rules module
----------------------

.. automodule:: snakemake.rules
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.scheduler module
--------------------------

.. automodule:: snakemake.scheduler
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.shell module
----------------------

.. automodule:: snakemake.shell
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.sourcecache module
----------------------------

.. automodule:: snakemake.sourcecache
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.spawn\_jobs module
----------------------------

.. automodule:: snakemake.spawn_jobs
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.storage module
------------------------

.. automodule:: snakemake.storage
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.target\_jobs module
-----------------------------

.. automodule:: snakemake.target_jobs
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.utils module
----------------------

.. automodule:: snakemake.utils
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.workflow module
-------------------------

.. automodule:: snakemake.workflow
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.wrapper module
------------------------

.. automodule:: snakemake.wrapper
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.script.rst
================================================
snakemake.script package
========================

Module contents
---------------

.. automodule:: snakemake.script
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.settings.rst
================================================
snakemake.settings package
==========================

Submodules
----------

snakemake.settings.enums module
-------------------------------

.. automodule:: snakemake.settings.enums
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.settings.types module
-------------------------------

.. automodule:: snakemake.settings.types
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.settings
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.template_rendering.rst
================================================
snakemake.template\_rendering package
=====================================

Submodules
----------

snakemake.template\_rendering.jinja2 module
-------------------------------------------

.. automodule:: snakemake.template_rendering.jinja2
   :members:
   :undoc-members:
   :show-inheritance:

snakemake.template\_rendering.yte module
----------------------------------------

.. automodule:: snakemake.template_rendering.yte
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: snakemake.template_rendering
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.unit_tests.rst
================================================
snakemake.unit\_tests package
=============================

Subpackages
-----------

.. toctree::
   :maxdepth: 4

   snakemake.unit_tests.templates

Module contents
---------------

.. automodule:: snakemake.unit_tests
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: apidocs/api_reference/internal/snakemake.unit_tests.templates.rst
================================================
snakemake.unit\_tests.templates package
=======================================

Module contents
---------------

.. automodule:: snakemake.unit_tests.templates
   :members:
   :undoc-members:
   :show-inheritance:



================================================
FILE: docs/conf.py
================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Snakemake documentation build configuration file, created by
# sphinx-quickstart on Sat Feb  1 16:01:02 2014.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import os
from datetime import datetime
from sphinxawesome_theme.postprocess import Icons


# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
# sys.path.insert(0, os.path.abspath("../src/"))

# -- General configuration ------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    "sphinx.ext.mathjax",
    "sphinx.ext.viewcode",
    "sphinx.ext.napoleon",
    "sphinxarg.ext",
    "sphinx.ext.autosectionlabel",
    "sphinx_tabs.tabs",
    "myst_parser",
]

html_css_files = ["custom.css"]

# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]

# The suffix of source filenames.
source_suffix = [".rst", ".md"]

# The encoding of source files.
# source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = "index"

# General information about the project.
project = "Snakemake"
date = datetime.now()
copyright = "2014-{year}, Johannes Koester".format(year=date.timetuple()[0])

import snakemake

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = snakemake.__version__

if os.environ.get("READTHEDOCS") == "True":
    # Because Read The Docs modifies conf.py, versioneer gives a "dirty"
    # version like "5.10.0+0.g28674b1.dirty" that is cleaned here.
    version = version.partition("+0.g")[0]

# The full version, including alpha/beta/rc tags.
release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
# language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
# today = ''
# Else, today_fmt is used as the format for a strftime call.
# today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = [
    "_build",
    "apidocs",
    "tutorial/interaction_visualization_reporting/workdir/workflow/report/cars.rst",
    "tutorial/interaction_visualization_reporting/workdir/workflow/report/horsepower_vs_mpg.rst",
    "tutorial/interaction_visualization_reporting/workdir/workflow/report/workflow.rst",
]

# avoid warnings generated by the autosectionlabel extension
suppress_warnings = ["autosectionlabel.*"]

# The reST default role (used for this markup: `text`) to use for all
# documents.
# default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
# add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
# add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
# show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
# pygments_style = "sphinx"

# A list of ignored prefixes for module index sorting.
# modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
# keep_warnings = False


# -- Options for HTML output ----------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = "sphinxawesome_theme"
html_theme_options = {
    "logo_light": "logo-snake.svg",
    "logo_dark": "logo-snake.svg",
    "main_nav_links": {
        "Homepage": "https://snakemake.github.io",
        "Plugin catalog": "https://snakemake.github.io/snakemake-plugin-catalog",
        "Workflow catalog": "https://snakemake.github.io/snakemake-workflow-catalog",
        "Wrappers": "https://snakemake-wrappers.readthedocs.io",
        "API docs": "https://snakemake-api.readthedocs.io",
    },
    "awesome_external_links": True,
    "awesome_headerlinks": True,
    "show_prev_next": False,
}
html_permalinks_icon = Icons.permalinks_icon

# Add any paths that contain custom themes here, relative to this directory.
# html_theme_path = sphinx_bootstrap_theme.get_html_theme_path()

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
# html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
# html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
# html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
# html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["_static"]
html_js_files = ["gurubase-widget.js"]  # gurubase AI widget

# Add any extra paths that contain custom files (such as robots.txt or
# .htaccess) here, relative to this directory. These files are copied
# directly to the root of the documentation.
# html_extra_path = ["_static/css"]

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
# html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
# html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
# html_sidebars = {"**": ["ethicalads.html"]}

# Additional templates that should be rendered to pages, maps page names to
# template names.
# html_additional_pages = {"index": "index.html"}

# If false, no module index is generated.
# html_domain_indices = True

# If false, no index is generated.
# html_use_index = True

# If true, the index is split into individual pages for each letter.
# html_split_index = False

# If true, links to the reST sources are added to the pages.
# html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
# html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
# html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
# html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
# html_file_suffix = None

# Output file base name for HTML help builder.
# htmlhelp_basename = "Snakemakedoc"


# -- Options for LaTeX output ---------------------------------------------

# latex_elements = {
#     # The paper size ('letterpaper' or 'a4paper').
#     #'papersize': 'letterpaper',
#     # The font size ('10pt', '11pt' or '12pt').
#     #'pointsize': '10pt',
#     # Additional stuff for the LaTeX preamble.
#     #'preamble': '',
# }

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
# latex_documents = [
#     ("index", "Snakemake.tex", "Snakemake Documentation", "Johannes Koester", "manual"),
# ]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
# latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
# latex_use_parts = False

# If true, show page references after internal links.
# latex_show_pagerefs = False

# If true, show URL addresses after external links.
# latex_show_urls = False

# Documents to append as an appendix to all manuals.
# latex_appendices = []

# If false, no module index is generated.
# latex_domain_indices = True


# -- Options for manual page output ---------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
# man_pages = [("index", "snakemake", "Snakemake Documentation", ["Johannes Koester"], 1)]

# If true, show URL addresses after external links.
# man_show_urls = False


# -- Options for Texinfo output -------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
# texinfo_documents = [
#     (
#         "index",
#         "Snakemake",
#         "Snakemake Documentation",
#         "Johannes Koester",
#         "Snakemake",
#         "One line description of project.",
#         "Miscellaneous",
#     ),
# ]

# Documents to append as an appendix to all manuals.
# texinfo_appendices = []

# If false, no module index is generated.
# texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
# texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
# texinfo_no_detailmenu = False


def setup(app):
    app.add_css_file("sphinx-argparse.css")



================================================
FILE: docs/index.rst
================================================
.. _manual-main:

=========
Snakemake
=========

.. image:: https://img.shields.io/badge/Gitpod-ready--to--code-blue?color=%23022c22
    :target: https://gitpod.io/#https://github.com/snakemake/snakemake

.. image:: https://img.shields.io/conda/dn/bioconda/snakemake.svg?label=Bioconda&color=%23064e3b
    :target: https://bioconda.github.io/recipes/snakemake/README.html

.. image:: https://img.shields.io/pypi/pyversions/snakemake.svg?color=%23065f46
    :target: https://www.python.org

.. image:: https://img.shields.io/pypi/v/snakemake.svg?color=%23047857
    :target: https://pypi.python.org/pypi/snakemake

.. image:: https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/docker-publish.yml?label=docker%20container&branch=main&color=%23059669
    :target: https://hub.docker.com/r/snakemake/snakemake

.. image:: https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/main.yml?label=tests&color=%2310b981
    :target: https://github.com/snakemake/snakemake/actions?query=branch%3Amain+workflow%3ACI

.. image:: https://img.shields.io/badge/stack-overflow-orange.svg?color=%2334d399
    :target: https://stackoverflow.com/questions/tagged/snakemake

.. image:: https://img.shields.io/discord/753690260830945390?label=discord%20chat&color=%23a7f3d0
    :alt: Discord
    :target: https://discord.gg/NUdMtmr

.. image:: https://img.shields.io/badge/bluesky-follow-%23d1fae5
   :alt: Bluesky
   :target: https://bsky.app/profile/johanneskoester.bsky.social

.. image:: https://img.shields.io/badge/mastodon-follow-%23ecfdf5
   :alt: Mastodon
   :target: https://fosstodon.org/@johanneskoester

.. image:: https://img.shields.io/github/stars/snakemake/snakemake?style=social
    :alt: GitHub stars
    :target: https://github.com/snakemake/snakemake/stargazers

.. .. raw:: html
          <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1093/bioinformatics/bts480" data-legend="always" data-style="large_rectangle"></span><script async src="https://badge.dimensions.ai/badge.js" charset="utf-8"></script>

The Snakemake workflow management system is a tool to create **reproducible and scalable** data analyses.
Workflows are described via a human readable, Python based language.
They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition.
Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.
Finally, workflow runs can be automatically turned into interactive portable browser based reports, which can be shared with collaborators via email or the cloud and combine results with all used parameters, code, and software.


Snakemake is **highly popular**, with >11 new citations per week (`old <https://badge.dimensions.ai/details/id/pub.1018944052>`__ and `new <https://badge.dimensions.ai/details/id/pub.1137313608>`__ paper).
It has been mentioned in two Nature technology features (`here <https://www.nature.com/articles/d41586-019-02619-z>`__ and `here <https://www.nature.com/articles/d41586-025-01241-6>`__) and has more than `1 million downloads on anaconda.org <https://anaconda.org/bioconda/snakemake>`__.
For an introduction, please visit https://snakemake.github.io.


.. _main-getting-started:

---------------
Getting started
---------------

* To get a first impression, please visit https://snakemake.github.io.
* To properly understand what Snakemake can do for you please read our `"rolling" paper <https://doi.org/10.12688/f1000research.29032.1>`_.
* News about Snakemake are published via `Bluesky <https://bsky.app/profile/johanneskoester.bsky.social>`__ and `Mastodon <https://fosstodon.org/@johanneskoester>`__.
* To learn Snakemake, please do the :ref:`tutorial`, and see the :ref:`FAQ <project_info-faq>`.
* **Best practices** for writing Snakemake workflows can be found :ref:`here <snakefiles-best_practices>`.

.. _main-support:

-------
Support
-------

* For releases, see :ref:`Changelog <changelog>`.
* Check :ref:`frequently asked questions (FAQ) <project_info-faq>`.
* In case of **questions**, please post on `stack overflow <https://stackoverflow.com/questions/tagged/snakemake>`_.
* To **discuss** with other Snakemake users, use the `discord server <https://discord.gg/kHvtG6N>`_. **Please do not post questions there. Use stack overflow for questions.**
* For **bugs and feature requests**, please use the `issue tracker <https://github.com/snakemake/snakemake/issues>`_.
* For **contributions**, visit Snakemake on `Github <https://github.com/snakemake/snakemake>`_ and read the :ref:`guidelines <project_info-contributing>`.
* Check out our `code of conduct <https://github.com/snakemake/snakemake/blob/main/CODE_OF_CONDUCT.md>`_ and refer to it for requests or concerns in that direction.

--------
Citation
--------
When using Snakemake, please cite our "rolling" paper

`Mölder, F., Jablonski, K.P., Letcher, B., Hall, M.B., Tomkins-Tinch, C.H., Sochat, V., Forster, J., Lee, S., Twardziok, S.O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., Köster, J., 2021. Sustainable data analysis with Snakemake. F1000Res 10, 33. <https://doi.org/10.12688/f1000research.29032.1>`_

This paper will also be regularly updated when Snakemake receives new features.
See :doc:`Citations <project_info/citations>` for more information.

.. _maintainers:

-----------
Maintainers
-----------

The Snakemake maintainers are:

* `Johannes Köster <https://github.com/johanneskoester>`_ (lead developer)
* `David Lähnemann <https://github.com/dlaehnemann>`_
* `Christian Meesters <https://github.com/cmeesters>`_
* `Michael B. Hall <https://github.com/mbhall88>`_
* `Filipe G. Vieira <https://github.com/fgvieira>`_
* `Morten E. Lund <https://github.com/melund>`_
* `Michael Jahn <https://github.com/m-jahn>`_
* `Cade Mirchandani <https://github.com/cademirch>`_

.. _main-resources:

---------
Resources
---------

`Snakedeploy <https://snakedeploy.readthedocs.io>`__
    Snakedeploy is a toolbox for maintenance and deployment/setup tasks around Snakemake and Snakemake workflows.

`Snakemake Wrappers Repository <https://snakemake-wrappers.readthedocs.org>`__
    The Snakemake Wrapper Repository is a collection of reusable wrappers that allow to quickly use popular tools from Snakemake rules and workflows.

`Snakemake Workflow Catalog <https://snakemake.github.io/snakemake-workflow-catalog>`__
    An automatically scraped catalog of publicly available Snakemake workflows for any kind of data analysis.

`Snakemake Workflows Project <https://github.com/snakemake-workflows/docs>`__
    This project provides a collection of high quality modularized and reusable workflows.
    The provided code should also serve as a best-practices of how to build production ready workflows with Snakemake.
    Everybody is invited to contribute.

`Snakemake Profiles Project <https://github.com/snakemake-profiles/doc>`__
    This project provides Snakemake configuration profiles for various execution environments.
    Please consider contributing your own if it is still missing.

`Snakemake API documentation <https://snakemake-api.readthedocs.io>`__
    The documentation of the Snakemake API for programmatic access and development on Snakemake.

`Conda-Forge <https://conda-forge.org>`__
    Conda-Forge is a community driven distribution of Conda packages that can be used from Snakemake for creating completely reproducible workflows by defining the used software versions and providing binaries.

`Bioconda <https://bioconda.github.io/>`__
    Bioconda, a partner project of conda-forge, is a community driven distribution of bioinformatics-related Conda packages that can be used from Snakemake for creating completely reproducible workflows by defining the used software versions and providing binaries.


.. toctree::
   :caption: Getting started
   :name: getting_started
   :hidden:
   :maxdepth: 1

   getting_started/installation
   getting_started/migration
   snakefiles/best_practices
   tutorial/tutorial
   tutorial/interaction_visualization_reporting/tutorial

.. toctree::
  :caption: Executing workflows
  :name: execution-toctree
  :hidden:
  :maxdepth: 1

  executing/cli
  executing/grouping
  executing/caching
  executing/interoperability
  executing/monitoring

.. toctree::
    :caption: Defining workflows
    :name: snakefiles
    :hidden:
    :maxdepth: 1

    snakefiles/writing_snakefiles
    snakefiles/rules
    snakefiles/configuration
    snakefiles/modularization
    snakefiles/storage
    snakefiles/utils
    snakefiles/deployment
    snakefiles/reporting
    snakefiles/testing
    snakefiles/foreign_wms

.. toctree::
    :caption: Project Info
    :name: project-info
    :hidden:
    :maxdepth: 1

    project_info/citations
    project_info/more_resources
    project_info/faq
    project_info/contributing
    project_info/codebase
    project_info/authors
    project_info/history
    project_info/license



================================================
FILE: docs/Makefile
================================================
# Makefile for Sphinx documentation
#

# You can set these variables from the command line.
SPHINXOPTS    =
SPHINXBUILD   = sphinx-build
PAPER         =
BUILDDIR      = _build

# User-friendly check for sphinx-build
ifeq ($(shell which $(SPHINXBUILD) >/dev/null 2>&1; echo $$?), 1)
$(error The '$(SPHINXBUILD)' command was not found. Make sure you have Sphinx installed, then set the SPHINXBUILD environment variable to point to the full path of the '$(SPHINXBUILD)' executable. Alternatively you can add the directory with the executable to your PATH. If you don't have Sphinx installed, grab it from https://www.sphinx-doc.org/)
endif

# Internal variables.
PAPEROPT_a4     = -D latex_paper_size=a4
PAPEROPT_letter = -D latex_paper_size=letter
ALLSPHINXOPTS   = -d $(BUILDDIR)/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) .
# the i18n builder cannot share the environment and doctrees with the others
I18NSPHINXOPTS  = $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) .

.PHONY: help clean html dirhtml singlehtml pickle json htmlhelp qthelp devhelp epub latex latexpdf text man changes linkcheck doctest gettext

help:
	@echo "Please use \`make <target>' where <target> is one of"
	@echo "  html       to make standalone HTML files"
	@echo "  dirhtml    to make HTML files named index.html in directories"
	@echo "  singlehtml to make a single large HTML file"
	@echo "  pickle     to make pickle files"
	@echo "  json       to make JSON files"
	@echo "  htmlhelp   to make HTML files and a HTML help project"
	@echo "  qthelp     to make HTML files and a qthelp project"
	@echo "  devhelp    to make HTML files and a Devhelp project"
	@echo "  epub       to make an epub"
	@echo "  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter"
	@echo "  latexpdf   to make LaTeX files and run them through pdflatex"
	@echo "  latexpdfja to make LaTeX files and run them through platex/dvipdfmx"
	@echo "  text       to make text files"
	@echo "  man        to make manual pages"
	@echo "  texinfo    to make Texinfo files"
	@echo "  info       to make Texinfo files and run them through makeinfo"
	@echo "  gettext    to make PO message catalogs"
	@echo "  changes    to make an overview of all changed/added/deprecated items"
	@echo "  xml        to make Docutils-native XML files"
	@echo "  pseudoxml  to make pseudoxml-XML files for display purposes"
	@echo "  linkcheck  to check all external links for integrity"
	@echo "  doctest    to run all doctests embedded in the documentation (if enabled)"

clean:
	rm -rf $(BUILDDIR)/*

html:
	$(SPHINXBUILD) -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html
	@echo
	@echo "Build finished. The HTML pages are in $(BUILDDIR)/html."

dirhtml:
	$(SPHINXBUILD) -b dirhtml $(ALLSPHINXOPTS) $(BUILDDIR)/dirhtml
	@echo
	@echo "Build finished. The HTML pages are in $(BUILDDIR)/dirhtml."

singlehtml:
	$(SPHINXBUILD) -b singlehtml $(ALLSPHINXOPTS) $(BUILDDIR)/singlehtml
	@echo
	@echo "Build finished. The HTML page is in $(BUILDDIR)/singlehtml."

pickle:
	$(SPHINXBUILD) -b pickle $(ALLSPHINXOPTS) $(BUILDDIR)/pickle
	@echo
	@echo "Build finished; now you can process the pickle files."

json:
	$(SPHINXBUILD) -b json $(ALLSPHINXOPTS) $(BUILDDIR)/json
	@echo
	@echo "Build finished; now you can process the JSON files."

htmlhelp:
	$(SPHINXBUILD) -b htmlhelp $(ALLSPHINXOPTS) $(BUILDDIR)/htmlhelp
	@echo
	@echo "Build finished; now you can run HTML Help Workshop with the" \
	      ".hhp project file in $(BUILDDIR)/htmlhelp."

qthelp:
	$(SPHINXBUILD) -b qthelp $(ALLSPHINXOPTS) $(BUILDDIR)/qthelp
	@echo
	@echo "Build finished; now you can run "qcollectiongenerator" with the" \
	      ".qhcp project file in $(BUILDDIR)/qthelp, like this:"
	@echo "# qcollectiongenerator $(BUILDDIR)/qthelp/Snakemake.qhcp"
	@echo "To view the help file:"
	@echo "# assistant -collectionFile $(BUILDDIR)/qthelp/Snakemake.qhc"

devhelp:
	$(SPHINXBUILD) -b devhelp $(ALLSPHINXOPTS) $(BUILDDIR)/devhelp
	@echo
	@echo "Build finished."
	@echo "To view the help file:"
	@echo "# mkdir -p $$HOME/.local/share/devhelp/Snakemake"
	@echo "# ln -s $(BUILDDIR)/devhelp $$HOME/.local/share/devhelp/Snakemake"
	@echo "# devhelp"

epub:
	$(SPHINXBUILD) -b epub $(ALLSPHINXOPTS) $(BUILDDIR)/epub
	@echo
	@echo "Build finished. The epub file is in $(BUILDDIR)/epub."

latex:
	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
	@echo
	@echo "Build finished; the LaTeX files are in $(BUILDDIR)/latex."
	@echo "Run \`make' in that directory to run these through (pdf)latex" \
	      "(use \`make latexpdf' here to do that automatically)."

latexpdf:
	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
	@echo "Running LaTeX files through pdflatex..."
	$(MAKE) -C $(BUILDDIR)/latex all-pdf
	@echo "pdflatex finished; the PDF files are in $(BUILDDIR)/latex."

latexpdfja:
	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
	@echo "Running LaTeX files through platex and dvipdfmx..."
	$(MAKE) -C $(BUILDDIR)/latex all-pdf-ja
	@echo "pdflatex finished; the PDF files are in $(BUILDDIR)/latex."

text:
	$(SPHINXBUILD) -b text $(ALLSPHINXOPTS) $(BUILDDIR)/text
	@echo
	@echo "Build finished. The text files are in $(BUILDDIR)/text."

man:
	$(SPHINXBUILD) -b man $(ALLSPHINXOPTS) $(BUILDDIR)/man
	@echo
	@echo "Build finished. The manual pages are in $(BUILDDIR)/man."

texinfo:
	$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo
	@echo
	@echo "Build finished. The Texinfo files are in $(BUILDDIR)/texinfo."
	@echo "Run \`make' in that directory to run these through makeinfo" \
	      "(use \`make info' here to do that automatically)."

info:
	$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo
	@echo "Running Texinfo files through makeinfo..."
	make -C $(BUILDDIR)/texinfo info
	@echo "makeinfo finished; the Info files are in $(BUILDDIR)/texinfo."

gettext:
	$(SPHINXBUILD) -b gettext $(I18NSPHINXOPTS) $(BUILDDIR)/locale
	@echo
	@echo "Build finished. The message catalogs are in $(BUILDDIR)/locale."

changes:
	$(SPHINXBUILD) -b changes $(ALLSPHINXOPTS) $(BUILDDIR)/changes
	@echo
	@echo "The overview file is in $(BUILDDIR)/changes."

linkcheck:
	$(SPHINXBUILD) -b linkcheck $(ALLSPHINXOPTS) $(BUILDDIR)/linkcheck
	@echo
	@echo "Link check complete; look for any errors in the above output " \
	      "or in $(BUILDDIR)/linkcheck/output.txt."

doctest:
	$(SPHINXBUILD) -b doctest $(ALLSPHINXOPTS) $(BUILDDIR)/doctest
	@echo "Testing of doctests in the sources finished, look at the " \
	      "results in $(BUILDDIR)/doctest/output.txt."

xml:
	$(SPHINXBUILD) -b xml $(ALLSPHINXOPTS) $(BUILDDIR)/xml
	@echo
	@echo "Build finished. The XML files are in $(BUILDDIR)/xml."

pseudoxml:
	$(SPHINXBUILD) -b pseudoxml $(ALLSPHINXOPTS) $(BUILDDIR)/pseudoxml
	@echo
	@echo "Build finished. The pseudo-XML files are in $(BUILDDIR)/pseudoxml."



================================================
FILE: docs/requirements.txt
================================================
sphinx >=3
sphinxcontrib-napoleon
sphinx-argparse
sphinx-tabs
docutils
myst-parser
configargparse
appdirs
immutables
sphinxawesome-theme
snakemake-interface-common
snakemake-interface-executor-plugins
snakemake-interface-storage-plugins
snakemake-interface-report-plugins
snakemake-interface-logger-plugins
humanfriendly


================================================
FILE: docs/_static/custom.css
================================================
.image-reference img {
    display: inline;
    background: none !important;
}

.image-reference {
    border-bottom: none !important;
}

:root {
    --color-links: #047857;
    --color-brand: #047857;
}

#content {
    overflow-x: auto;
}

code {
    font-family: monospace !important;
}

/* Dark mode styles for Sphinx tabs */
html.dark .sphinx-tabs .sphinx-tabs-tab {
    background-color: #333 !important; 
    color: #fff !important;
    border-color: #555 !important;
}

html.dark .sphinx-tabs .sphinx-tabs-tab[aria-selected="true"] {
    background-color: #444 !important;
    border-bottom-color: #444 !important;
}

html.dark .sphinx-tabs-panel {
    background-color: #333 !important;
    color: #fff !important;
    border-color: #555 !important;
}


================================================
FILE: docs/_static/gurubase-widget.js
================================================
document.addEventListener('DOMContentLoaded', function() {
    // Load the GuruBase widget
    const guruScript = document.createElement("script");
    guruScript.src = "https://widget.gurubase.io/widget.latest.min.js";
    guruScript.defer = true;
    guruScript.id = "guru-widget-id";

    // Add widget settings as data attributes
    Object.entries({
        "data-widget-id": "XDztDiqWzDJbUC8HgYE3QTKfoY1r1dXsdkfTzLIzH9g",
        "data-text": "Ask AI",
        "data-margins": JSON.stringify({ bottom: "100px", right: "20px" }),
        "data-overlap-content": "false"
    }).forEach(([key, value]) => {
        guruScript.setAttribute(key, value);
    });

    // Append the script to the document
    document.body.appendChild(guruScript);
});


================================================
FILE: docs/_static/sphinx-argparse.css
================================================
.wy-table-responsive table td {
    white-space: normal !important;
}
.wy-table-responsive {
    overflow: visible !important;
}



================================================
FILE: docs/_templates/toc.html
================================================
{% extends "!toc.html" %}

{% block toc_after %}
<div id="ethical-ad-placement"></div>
{% endblock %}


================================================
FILE: docs/executing/caching.rst
================================================
.. _caching:

========================
Between workflow caching
========================

Within certain data analysis fields, there are certain intermediate results that reoccur in exactly the same way in many analysis.
For example, in bioinformatics, reference genomes or annotations are downloaded, and read mapping indexes are built.
Since such steps are independent of the actual data or measurements that are analyzed, but still computationally or timely expensive to conduct, it has been common practice to externalize their computation and assume the presence of the resulting files before execution of a workflow.

From version 5.8.0 on, Snakemake offers a way to keep those steps inside the actual analysis without requiring from redundant computations.
By hashing all steps, parameters, software stacks (in terms of conda environments or containers), and raw input required up to a certain job via a `Merkle tree <https://en.wikipedia.org/wiki/Merkle_tree>`_, Snakemake is able to recognize **before** the computation whether a certain result is already available in a central cache on the same system.
**Note that this is explicitly intended for caching results between workflows! There is no need to use this feature to avoid redundant computations within a workflow. Snakemake does this already out of the box.**

Such caching has to be explicitly activated per rule, which can be done via the command line interface.
For example,

.. code-block:: console

    $ export SNAKEMAKE_OUTPUT_CACHE=/mnt/snakemake-cache/
    $ snakemake --cache download_data create_index

would instruct Snakemake to cache and reuse the results of the rules ``download_data`` and ``create_index``.
The environment variable definition that happens in the first line (defining the location of the cache) should of course be done only once and system wide in reality.
When Snakemake is executed without a shared filesystem (e.g., in the cloud, see :ref:`tutorial-cloud`), the environment variable has to point to a location compatible with the given remote provider (e.g. an S3 or Google Storage bucket).
In any case, the provided location should be shared between all workflows of your group, institute or computing environment, in order to benefit from the reuse of previously obtained intermediate results.

Alternatively, rules can be marked as eligible for caching via the ``cache`` directive:

.. code-block:: python

    rule download_data:
        output:
            "results/data/worldcitiespop.csv"
        cache: True # allowed values: "all", "omit-software", True
        shell:
            "curl -L https://burntsushi.net/stuff/worldcitiespop.csv > {output}"

Here, the given value defines what information shall be considered for calculating the hash value.
With ``"all"`` or ``True``, all relevant rule information is used as outlined above (this is the recommended default).
With ``"omit-software"``, the software stack is not considered, which is useful if the software stack is not relevant for the result (e.g., if the rule is only a data download).

For workflows defining cache rules like this, it is enough to invoke Snakemake with

.. code-block:: console

    $ snakemake --cache

without explicit rulenames listed.

Note that only rules with just a single output file (or directory) or with :ref:`multiext output files <snakefiles-multiext>` are eligible for caching.
The reason is that for other rules it would be impossible to unambiguously assign the output files to cache entries while being agnostic of the actual file names.
Also note that the rules need to retrieve all their parameters via the ``params`` directive (except input files).
It is not allowed to directly use ``wildcards``, ``config`` or any global variable in the shell command or script, because these are not captured in the hash (otherwise, reuse would be unnecessarily limited).

Also note that Snakemake will store everything in the cache as readable and writeable for **all users** on the system (except in the remote case, where permissions are not enforced and depend on your storage configuration).
Hence, caching is not intended for private data, just for steps that deal with publicly available resources.

Finally, be aware that the implementation should be considered **experimental** until this note is removed.



================================================
FILE: docs/executing/cli.rst
================================================
.. _executable:

======================
Command line interface
======================

This part of the documentation describes the ``snakemake`` executable.  Snakemake
is primarily a command-line tool, so the ``snakemake`` executable is the primary way
to execute, debug, and visualize workflows.

.. user_manual-snakemake_envvars:

-------------------------------
Important environment variables
-------------------------------

Snakemake caches source files for performance and reproducibility.
The location of this cache is determined by the `appdirs <https://github.com/ActiveState/appdirs>`_ package.
If you want to change the location on a unix/linux system, you can define an override path via the environment variable ``XDG_CACHE_HOME``.

.. user_manual-snakemake_options:

-----------------------------
Useful Command Line Arguments
-----------------------------

If called with the number of cores to use, i.e.

.. code-block:: console

    $ snakemake --cores 1

Snakemake tries to execute the workflow specified in a file called ``Snakefile`` in the same directory (the Snakefile can be given via the parameter ``-s``).

By issuing

.. code-block:: console

    $ snakemake -n

a dry-run can be performed.
This is useful to test if the workflow is defined properly and to estimate the amount of needed computation.

Importantly, Snakemake can automatically determine which parts of the workflow can be run in parallel.
By specifying more than one available core, i.e.

.. code-block:: console

    $ snakemake --cores 4

one can tell Snakemake to use up to 4 cores and solve a binary knapsack problem to optimize the scheduling of jobs.
If the number is omitted (i.e., only ``--cores`` is given), the number of used cores is determined as the number of available CPU cores in the machine.

Snakemake workflows usually define the number of used threads of certain rules. Sometimes, it makes sense to overwrite the defaults given in the workflow definition.
This can be done by using the ``--set-threads`` argument, e.g.,

.. code-block:: console

    $ snakemake --cores 4 --set-threads myrule=2

would overwrite whatever number of threads has been defined for the rule ``myrule`` and use ``2`` instead.
Similarly, it is possible to overwrite other resource definitions in rules, via

.. code-block:: console

    $ snakemake --cores 4 --set-resources myrule:partition="foo"

Both mechanisms can be particularly handy when used in combination with :ref:`non-local execution <non-local-exec>`.

.. _non-local-exec:

Non-local execution
^^^^^^^^^^^^^^^^^^^

Non-local execution on cluster or cloud infrastructure is implemented via plugins.
The `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`__ lists available plugins and their documentation.
In general, the configuration boils down to specifying an executor plugin (e.g. for SLURM or Kubernetes) and, if needed, a :ref:`storage <default_storage>` plugin (e.g. in order to use S3 for input and output files or in order to efficiently use a shared network filesystem).
For maximizing the I/O performance over the network, it can be advisable to :ref:`annotate the input file access patterns of rules <storage-access-patterns>`.
Snakemake provides lots of tunables for non-local execution, which can all be found under :ref:`all_options` and in the plugin descriptions of the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`__.
In any case, the cluster or cloud specific configuration will entail lots of command line options to be chosen and set, which should be persisted in a :ref:`profile <executing-profiles>`.

Dealing with very large workflows
---------------------------------

If your workflow has a lot of jobs, Snakemake might need some time to infer the dependencies (the job DAG) and which jobs are actually required to run.
The major bottleneck involved is the filesystem, which has to be queried for existence and modification dates of files.
To overcome this issue, Snakemake allows to run large workflows in batches.
This way, fewer files have to be evaluated at once, and therefore the job DAG can be inferred faster.
By running

.. code-block:: console

    $ snakemake --cores 4 --batch myrule=1/3

you instruct to only compute the first of three batches of the inputs of the rule ``myrule``.
To generate the second batch, run

.. code-block:: console

    $ snakemake --cores 4 --batch myrule=2/3

Finally, when running


.. code-block:: console

    $ snakemake --cores 4 --batch myrule=3/3

Snakemake will process beyond the rule ``myrule``, because all of its input files have been generated, and complete the workflow.
Obviously, a good choice of the rule to perform the batching is a rule that has a lot of input files and upstream jobs, for example a central aggregation step within your workflow.
We advice all workflow developers to inform potential users of the best suited batching rule.

.. _executing-profiles:

--------
Profiles
--------

Adapting Snakemake to a particular environment can entail many flags and options.
Therefore, since Snakemake 4.1, it is possible to specify configuration profiles
to be used to obtain default options.
Since Snakemake 7.29, two kinds of profiles are supported:

* A **global profile** that is defined in a system-wide or user-specific configuration directory (on Linux, this will be ``$HOME/.config/snakemake`` and ``/etc/xdg/snakemake``, you can find the answer for your system via ``snakemake --help``).
* A **workflow specific profile** (introduced in Snakemake 7.29) that is defined via a flag (``--workflow-profile``) or searched in a default location (``profiles/default``) in the working directory or next to the Snakefile.

The workflow specific profile is meant to be used to define default options for a particular workflow, like providing constraints for certain custom resources the workflow uses (e.g. ``api_calls``) or overwriting the threads and resource definitions of individual rules without modifying the workflow code itself.
In contrast, the global profile is meant to be used to define default options for a particular environment, like the default cluster submission command or the default number of jobs to run in parallel.

For example, the command

.. code-block:: console

   $ snakemake --profile myprofile

would expect a folder ``myprofile`` in per-user and global configuration directories (on Linux, this will be ``$HOME/.config/snakemake`` and ``/etc/xdg/snakemake``, you can find the answer for your system via ``snakemake --help``).
Alternatively, an absolute or relative path to the profile folder can be given.
The default profile to use when no ``--profile`` argument is specified can also be set via the environment variable ``SNAKEMAKE_PROFILE``,
e.g. by specifying ``export SNAKEMAKE_PROFILE=myprofile`` in your ``~/.bashrc`` or the system wide shell defaults means that the ``--profile`` flag can be omitted.
In order unset the profile defined by this environment variable for individual runs without specifying and alternative profile you can provide the special value ``none``, i.e. ``--profile none``.

The profile folder is expected to contain a configuration file that file that defines default values for the Snakemake command line arguments.
The file has to be named ``config.vX+.yaml`` with ``X`` denoting the minimum supported Snakemake major version (e.g. ``config.v8+.yaml``).
As fallback, it is also possible to provide a version agnostic ``config.yaml`` that matches any Snakemake version.
For example, the file

.. code-block:: yaml

    executor: slurm
    jobs: 100

would setup Snakemake to always submit to the SLURM cluster middleware and never use more than 100 parallel jobs in total.
The profile can be used to set a default for each option of the Snakemake command line interface.
For this, option ``--someoption`` becomes ``someoption:`` in the profile.
The profile folder can additionally contain auxiliary files, e.g., jobscripts, or any kind of wrappers. See https://github.com/snakemake-profiles/doc for examples.
If options accept multiple arguments these must be given as YAML list in the profile.
If options expect structured arguments (like ``--default-resources RESOURCE=VALUE``, ``--set-threads RULE=VALUE``, or ``--set-resources RULE:RESOURCE=VALUE``), those can be given as strings in the expected forms, i.e.

.. code-block:: yaml

    default-resources: mem_mb=200
    set-threads: myrule=5
    set-resources: myrule:mem=500MB

or as YAML maps, which is easier to read:

.. code-block:: yaml

    default-resources:
        mem_mb: 200
    set-threads:
        myrule: 5
    set-resources:
        myrule:
            mem: 500MB

All of these resource specifications can also be made dynamic, by using expressions and certain variables that are available.
For details of the variables you can use, refer to the callable signatures given in the
documentation sections on the specification of :ref:`threads <snakefiles-threads>`
and :ref:`dynamic resources <snakefiles-dynamic-resources>`.
These enable ``config.yaml`` entries like:

.. code-block:: yaml

    default-resources:
        mem_mb: max(1.5 * input.size_mb, 100)
    set-threads:
        myrule: max(input.size_mb / 5, 2)
    set-resources:
        myrule:
            mem_mb: attempt * 200


Setting resources or threads via the profile is of course rather a job for the workflow profile instead of the global profile (as such settings are likely workflow specific).

Values in profiles can make use of globally available environment variables, e.g. the ``$USER`` variable.
For example, the following would set the default prefix for storing local copies of remote storage files to a user specific directory

.. code-block:: yaml

    local-storage-prefix: /local/work/$USER/snakemake-scratch

Any such environment variables are automatically expanded when evaluating the profile.

Under https://github.com/snakemake-profiles/doc, you can find publicly available global profiles (e.g. for cluster systems).
Feel free to contribute your own.
Workflow specific profiles are either not shared at all, or can be distributed along with the workflow itself where it makes sense.
For example, when the workflow has its Snakefile at ``workflow/Snakefile``, the profile config should be placed at ``workflow/profiles/default/config.yaml``.


Use templating in profiles
^^^^^^^^^^^^^^^^^^^^^^^^^^

In Snakemake 7.30 or newer, when the profile starts with

.. code-block:: yaml

    __use_yte__: true

It will be treated as a `YTE template <https://yte-template-engine.github.io>`_ and parsed accordingly.
This can be handy to e.g. define values inside of the profile that are based on environment variables.
For example, admins could use this to define user-specific settings.
Another application would be the uniform redefinition of resource requirements for a larger set of rules in a workflow profile (see above).
However, it should be noted that templated profiles are harder to keep free of errors and the profile author has to make sure that they always work correctly for the user.


.. _getting_started-visualization:

-------------
Visualization
-------------

To visualize the workflow, one can use the option ``--dag``.
This creates a representation of the DAG in the graphviz dot language which has to be postprocessed by the graphviz tool ``dot``.
E.g. to visualize the DAG that would be executed, you can issue:

.. code-block:: console

    $ snakemake --dag | dot | display

For saving this to a file, you can specify the desired format:

.. code-block:: console

    $ snakemake --dag | dot -Tpdf > dag.pdf

To visualize the whole DAG regardless of the eventual presence of files, the ``forceall`` option can be used:

.. code-block:: console

    $ snakemake --forceall --dag | dot -Tpdf > dag.pdf

Of course the visual appearance can be modified by providing further command line arguments to ``dot``.

**Note:** The DAG is printed in DOT format straight to the standard output, along with other ``print`` statements you may have in your Snakefile. Make sure to comment these other ``print`` statements so that ``dot`` can build a visual representation of your DAG.


.. _all_options:

-----------
All Options
-----------

.. argparse::
   :module: snakemake.cli
   :func: get_argument_parser
   :prog: snakemake

   All command line options can be printed by calling ``snakemake -h``.




================================================
FILE: docs/executing/grouping.rst
================================================
.. _job_grouping:

============
Job Grouping
============

The graph of jobs that Snakemake determines before execution can be partitioned into groups.
Such groups will be executed together in **cluster** or **cloud mode**, as a so-called **group job**, i.e., all jobs of a particular group will be submitted at once, to the same computing node.
When executing locally, group definitions are ignored.

Groups can be defined along with the workflow definition via the ``group`` keyword, see :ref:`snakefiles-grouping`.
This way, queueing and execution time can be saved, in particular by attaching short-running downstream jobs to long running upstream jobs.

From Snakemake 7.11 on, Snakemake will request resources for groups by summing across jobs that can be run in parallel, and taking the max of jobs run in series.
The only exception is ``runtime``, where the max will be taken over parallel jobs, and the sum over series.
If resource constraints are provided (via ``--resources`` or ``--cores``), parallel job layers that exceed the constraints will be stacked in series.
For example, if 6 instances of ``somerule`` are being run, each instance requires ``1000MB`` of memory and ``30 min`` runtime, and only ``3000MB`` are available, Snakemake will request ``3000MB`` and ``60 min`` runtime, enough to run 3 instances of ``somerule``, then another 3 instances of ``somerule`` in series.

Often, the ideal group will be dependent on the specifics of the underlying computing platform.
Hence, it is possible to assign groups via the command line.
For example, with

.. code-block:: bash

    snakemake --groups somerule=group0 someotherrule=group0

we assign the two rules ``somerule`` and ``someotherrule`` to the same group ``group0``.

By default, groups do not span disconnected parts of the DAG.
This means that, for example, jobs of ``somerule`` and ``someotherrule`` only end in the same group if they are directly connected.
It is, however, possible to configure the number of connected DAG components that are spanned by a group via the flag ``--group-components``.
This makes it possible to define batches of jobs of the same kind that shall be executed within one group. For instance:


.. code-block:: bash

    snakemake --groups somerule=group0 --group-components group0=5

means that given ``n`` jobs spawned from rule ``somerule``, Snakemake will create ``n / 5`` groups which each execute 5 jobs of ``somerule`` together.
For example, with 10 jobs from ``somerule`` you would end up with 2 groups of 5 jobs that are submitted as one piece each.

Furthermore, it is possible to use wildcards in group names.
This way, you can e.g. have a group per sample, e.g.:

.. code-block:: bash

    snakemake --groups somerule=group_{sample} --group-components group_{sample}=5



================================================
FILE: docs/executing/interoperability.rst
================================================

================
Interoperability
================

.. _cwl_export:

----------
CWL export
----------

Snakemake workflows can be exported to `CWL <https://www.commonwl.org/>`_, such that they can be executed in any `CWL-enabled workflow engine <https://www.commonwl.org/#Implementations>`_.
Since, CWL is less powerful for expressing workflows than Snakemake (most importantly Snakemake offers more flexible scatter-gather patterns, since full Python can be used), export works such that every Snakemake job is encoded into a single step in the CWL workflow.
Moreover, every step of that workflow calls Snakemake again to execute the job. The latter enables advanced Snakemake features like scripts, benchmarks and remote files to work inside CWL.
So, when exporting keep in mind that the resulting CWL file can become huge, depending on the number of jobs in your workflow.
To export a Snakemake workflow to CWL, simply run

.. code-block:: console

    $ snakemake --export-cwl workflow.cwl

The resulting workflow will by default use the `Snakemake docker image <https://hub.docker.com/r/snakemake/snakemake>`_ for every step, but this behavior can be overwritten via the CWL execution environment.
Then, the workflow can be executed in the same working directory with, e.g.,

.. code-block:: console

    $ cwltool workflow.cwl

Note that due to limitations in CWL, it seems currently impossible to avoid that all target files (output files of target jobs), are written directly to the workdir, regardless of their relative paths in the Snakefile.

Note that export is impossible in case the workflow contains :ref:`checkpoints <snakefiles-checkpoints>` or output files with absolute paths.


================================================
FILE: docs/executing/monitoring.rst
================================================
.. _monitoring:

==========
Monitoring
==========

Since Snakemake 9.0, Snakemake supports monitoring workflow execution through logger plugins.
These plugins enable integration with various monitoring systems and services, allowing you to track workflow progress, collect runtime statistics, and analyze execution patterns.

Logger plugins can capture detailed information about workflow execution, including:

* Job start and completion times
* Resource usage statistics
* Error messages and debugging information
* Workflow topology and dependencies


To use a logger plugin, specify it via the ``--logger`` command line option:

.. code-block:: console

    $ snakemake --logger <plugin-name> --cores 4

Multiple logger plugins can be used simultaneously by specifying the ``--logger`` option multiple times.

Available logger plugins and their configuration options can be found in the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`_.
The catalog provides detailed documentation for each plugin, including installation instructions, configuration examples, and usage guidelines.

For details on developing logger plugins, see the documentation in the `Snakemake logger plugin interface repository <https://github.com/snakemake/snakemake-interface-logger-plugins>`_.



================================================
FILE: docs/getting_started/installation.rst
================================================
.. _Miniconda: https://conda.pydata.org/miniconda.html
.. _Miniforge: https://github.com/conda-forge/miniforge
.. _Mamba: https://prefix.dev/docs/mamba
.. _Conda: https://conda.pydata.org
.. _PyPi: https://pypi.org/project/snakemake/
.. _Pixi: https://pixi.sh
.. _Micromamba: https://prefix.dev/docs/mamba


.. _getting_started-installation:

============
Installation
============

Snakemake is available on PyPi_ as well as through Conda_ and also from source code.
You can use one of the following ways for installing Snakemake.

.. _conda-install:

Installation via Conda/Mamba
============================

This is the **recommended** way to install Snakemake,
because it also enables Snakemake to :ref:`handle software dependencies of your
workflow <integrated_package_management>`.

First, you need to install a Conda-based Python3 distribution.
The recommended choice is Miniforge_.

Of course, any other conda-based package manager can be used as well, e.g. Pixi_, Mamba_ or Micromamba_.
Note however that for the :ref:`conda integration <integrated_package_management>` of Snakemake, for now it requires the `conda` command to be installed in the root environment or in the same environment as Snakemake itself.

Full installation
-----------------

Snakemake can be installed with all goodies needed to run in any environment and for creating interactive reports via

.. code-block:: console

    $ conda create -c conda-forge -c bioconda -n snakemake snakemake

from the `Bioconda <https://bioconda.github.io>`_ channel.
This will install snakemake into an isolated software environment, that has to be activated with

.. code-block:: console

    $ conda activate snakemake
    $ snakemake --help

Installing into isolated environments is best practice in order to avoid side effects with other packages.

Minimal installation
--------------------

A minimal version with only the necessary requirements can be installed with

.. code-block:: console

    $ conda create -c conda-forge -c bioconda -n snakemake snakemake-minimal

Notes on Bioconda as a package source
-------------------------------------

Note that Snakemake is available via Bioconda for historical, reproducibility, and continuity reasons (although it is not limited to biology applications at all).
However, it is easy to combine Snakemake installation with other channels, e.g., by prefixing the package name with ``::bioconda``, i.e.,

.. code-block:: console

    $ conda activate base
    $ conda create -n some-env -c conda-forge bioconda::snakemake ...

Installation via pip
====================

Instead of conda, snakemake can be installed with pip:

.. code-block:: console

    $ pip install snakemake


Installation of a development version via pip
=============================================

If you want to quickly try out an unreleased version from the snakemake repository (which you cannot get via e.g. bioconda or PyPi_, yet), for example to check whether a bug fix works for you workflow, you can get the current state of the main branch with:

.. code-block:: console

    $ conda create --only-deps -n snakemake-dev snakemake
    $ conda activate snakemake-dev
    $ pip install git+https://github.com/snakemake/snakemake

You can also install the current state of another branch or the repository state at a particular commit.
For information on the syntax for this, see `the pip documentation on git support <https://pip.pypa.io/en/stable/topics/vcs-support/#git>`_.


Editor integrations
===================

* `VSCode <https://github.com/snakemake/snakemake-lang-vscode-plugin>`_
* `Vim <https://github.com/snakemake/snakemake/tree/main/misc/vim>`_
* `Zed <https://github.com/lvignoli/zed-snakemake>`_



================================================
FILE: docs/getting_started/migration.rst
================================================
.. _migration:

====================================
Migration between Snakemake versions
====================================

Snakemake is meant to remain backwards compatible as much as possible.
However, sometimes, very rarely, we remove old almost unused features that have since then
been replaced by new ones (so far, this happened only once, for Snakemake 8).
Sometimes, new features are added that do not require, but make it strongly advisable to adapt workflows (e.g. because the new features provide a better user or recipient experience).

Below are migration hints for particular Snakemake versions.

Migrating to Snakemake 9
------------------------

Between Snakemake 8 and Snakemake 9, there is only a single breaking change in how custom loggers are provided, such that hardly any user should be affected.
The new way to specify custom log handlers is specifying a logger plugin via ``--logger`` or ``OutputSettings.log_handler_settings`` in the API.

Migrating to Snakemake 8
------------------------

Workflow definitions
^^^^^^^^^^^^^^^^^^^^

Snakemake 8 removes the support for three syntactical elements, which are all officially deprecated since multiple major releases:

* Support for marking output files as ``dynamic`` has been removed. You should instead use :ref:`checkpoints <snakefiles-checkpoints>`.
* Support for the ``version`` directive has been removed. You should use the :ref:`conda <integrated_package_management>` or :ref:`container <apptainer>` integration instead.
* Support for the ``subworkflow`` directive has been removed. You should use the :ref:`module directive <snakefiles-modules>` instead, which provides the same functionality in a more general way.

In addition, we have moved the former remote provider functionality into so called :ref:`storage plugins <storage-support>`.
Most of the old remote providers have been migrated into the new storage plugins
(see the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`__.).
Two former remote providers have been migrated into Snakemake wrappers instead, namely
the NCBI and EGA remote providers, which are now replaced by the
`entrez/efetch <https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/entrez/efetch.html>`_ and
the `ega <https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/ega/fetch.html>`_ wrappers.
As of writing, the Snakemake storage plugin for xrootd (see `here <https://github.com/snakemake/snakemake-storage-plugin-xrootd>`__) does not yet pass the CI tests. Any help would be greatly appreciated.


Command line interface
^^^^^^^^^^^^^^^^^^^^^^

The command line interface of Snakemake 8 has a lot of new options which are best explored using::

    snakemake --help

Moreover, some options have been renamed:

* All the execution backends have been moved into plugins. When you used e.g. ``--kubernetes`` and corresponding options before, you should now use ``--executor kubernetes`` and check the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog/plugins/executor/kubernetes.html>`_ for the new options. The same holds for all other execution backends, see `here <https://snakemake.github.io/snakemake-plugin-catalog/index.html>`__.
* The ``--use-conda`` and ``--use-singularity`` options are deprecated. Instead you should now use ``--software-deployment-method conda`` or ``--software-deployment-method apptainer`` or ``--software-deployment-method conda apptainer`` if you need both.
* There is a new executor plugin for `Google Cloud Batch <https://cloud.google.com/batch/docs/get-started>`_.
  This is meant as a replacement for the old Google Life Sciences executor. 
  The new executor is called ``googlebatch`` and can be used with ``--executor googlebatch``. 
  Please check out the documentation of the plugin in the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog/plugins/executor/googlebatch.html>`__. 
  Note that in principle it is fine to re-add google-lifesciences support as a plugin as well. 
  We even have skeleton code for this `here <https://github.com/snakemake/snakemake-executor-plugin-google-lifesciences>`__. 
  Any help with getting this tested and released despite the fact that google lifesciences will be shut down this year would still be valued.

.. list-table:: Interface comparison
   :widths: 15 30 15 30 10
   :header-rows: 1
   :align: center

   * - Interface in 7.32
     - Interface description in 7.32
     - Interface in 8.0.1
     - Interface description in 8.0.1
     - Change introduction
   * - preemptible
     -
     -
     -
     -
   * - --preemption-default PREEMPTION_DEFAULT
     -
                        A preemptible instance can be requested when using the
                        Google Life Sciences API. If you set a --preemption-
                        default, all rules will be subject to the default.
                        Specifically, **this integer is the number of restart
                        attempts** that will be made given that the instance is
                        killed unexpectedly. Note that preemptible instances
                        have a maximum running time of 24 hours. If you want
                        to set preemptible instances for only a subset of
                        rules, use --preemptible-rules instead. (default:
                        None)
     - --preemptible-retries PREEMPTIBLE_RETRIES
     -
                        **Number of retries** that shall be made in order to
                        finish a job from of rule that has been marked as
                        preemptible via the --preemptible-rules setting.
                        (default: None)
     - Renamed
   * - --preemptible-rules PREEMPTIBLE_RULES [PREEMPTIBLE_RULES ...]
     -
                        A preemptible instance can be requested when using the
                        Google Life Sciences API. If you want to use these
                        instances for a subset of your rules, you can use
                        --preemptible-rules and then specify a list of rule
                        and integer pairs, where each integer indicates the
                        number of restarts to use for the rule's instance in
                        the case that the instance is terminated unexpectedly.
                        --preemptible-rules can be used in combination with
                        --preemption-default, and will take priority. Note
                        that preemptible instances have a maximum running time
                        of 24. If you want to apply a consistent number of
                        retries across all your rules, use --preemption-
                        default instead. Example: snakemake --preemption-
                        default 10 --preemptible-rules map_reads=3
                        call_variants=0 (default: None)
     - --preemptible-rules [PREEMPTIBLE_RULES ...]
     -
                        Define which rules shall use a preemptible machine
                        which can be prematurely killed by e.g. a cloud
                        provider (also called spot instances). This is
                        currently only supported by the Google Life Sciences
                        executor and ignored by all other executors. If no
                        rule names are provided, all rules are considered to
                        be preemptible. The (default: None)
     - Renamed
   * - list-rules
     -
     -
     -
     -
   * - --list, -l
     -
                        Show available rules in given Snakefile. (default:
                        False)
     - **--list-rules**, --list, -l
     -
                        Show available rules in given Snakefile. (default:
                        False)
     - New alias: --list-rules
   * - list-changes
     -
     -
     -
     -
   * - --list-version-changes, --lv
     -
                        List all output files that have been created with a
                        different version (as determined by the version
                        keyword). (default: False)
     -
     -
     - Deprecated: It seems due to the deprecation of ``version`` directive
   * - --list-code-changes, --lc
     -
                        List all output files for which the rule body (run or
                        shell) have changed in the Snakefile. (default: False)
     - --list-changes {params,input,code}, --lc {params,input,code}
     -
                        List all output files for which the rule body (run or
                        shell) have changed in the Snakefile. (default: None)
     - Redesigned: Please use params such as ``--list-changes params,input,code`` instead of ``--list-code-changes``, ``--list-input-changes``, or ``--list-params-changes``
   * - bash-completion
     -
     -
     -
     -
   * - --bash-completion
     -
                        Output code to register bash completion for snakemake.
                        Put the following in your .bashrc (including the
                        accents): `snakemake --bash-completion` or issue it in
                        an open terminal session. (default: False)
     -
     -
     - Unsupported?
   * - deploy-sources
     -
     -
     -
     -
   * -
     -
     - --deploy-sources QUERY CHECKSUM
     -
                        Deploy sources archive from given storage provider
                        query to the current working sdirectory and control
                        for archive checksum to proceed. Meant for internal
                        use only. (default: None)
     -
   * - reason
     -
     -
     -
     -
   * - --reason, -r
     -
                        Print the reason for each executed rule (deprecated,
                        always true now). (default: False)
     -
     -
     - Deprecated: Drop it and don't worry about anything
   * - gui
     -
     -
     -
     -
   * - --gui [PORT]
     -
                        Serve an HTML based user interface to the given
                        network and port e.g. 168.129.10.15:8000. By default
                        Snakemake is only available in the local network
                        (default port: 8000). To make Snakemake listen to all
                        ip addresses add the special host address 0.0.0.0 to
                        the url (0.0.0.0:8000). This is important if Snakemake
                        is used in a virtualised environment like Docker. If
                        possible, a browser window is opened. (default: None)

     -
     -
     - Unsupported?
   * - stats
     -
     -
     -
     -
   * - --stats FILE
     -
                        Write stats about Snakefile execution in JSON format
                        to the given file. (default: None)
     -
     -
     - Unsupported?
   * - file storage
     -
     -
     -
     -
   * -
     -
     - --unneeded-temp-files FILE [FILE ...]
     -
                        Given files will not be uploaded to storage and
                        immediately deleted after job or group job completion.
                        (default: frozenset())
     -
   * - --keep-remote
     -
                        Keep local copies of remote input files. (default:
                        False)
     - --keep-storage-local-copies
     -
                        Keep local copies of remote input files. (default:
                        False)
     - Renamed
   * - --keep-target-files
     -
                        Do not adjust the paths of given target files relative
                        to the working directory. (default: False)
     -  --target-files-omit-workdir-adjustment
     -
                        Do not adjust the paths of given target files relative
                        to the working directory. (default: False)
     - Renamed
   * - seconds-between-status-checks
     -
     -
     -
     -
   * -
     -
     - --seconds-between-status-checks SECONDS_BETWEEN_STATUS_CHECKS
     -
                        Number of seconds to wait between two rounds of status
                        checks. (default: 10)
     -
   * - remote storage
     -
     -
     -
     -
   * - --default-remote-provider {S3,GS,FTP,SFTP,S3Mocked,gfal,gridftp,iRODS,AzBlob,XRootD}
     -
                        Specify default remote provider to be used for all
                        input and output files that don't yet specify one.
                        (default: None)
     - --default-storage-provider DEFAULT_STORAGE_PROVIDER
     -
                        Specify default storage provider to be used for all
                        input and output files that don't yet specify one
                        (e.g. 's3'). See
                        https://snakemake.github.io/snakemake-plugin-catalog
                        for available storage provider plugins. (default:
                        None)
     - Renamed:
                        See
                        https://snakemake.github.io/snakemake-plugin-catalog
                        for available storage provider plugins.
   * - --default-remote-prefix DEFAULT_REMOTE_PREFIX
     -
                        Specify prefix for default remote provider. E.g. a
                        bucket name. (default: )
     - --default-storage-prefix DEFAULT_STORAGE_PREFIX
     -
                        Specify prefix for default storage provider. E.g. a
                        bucket name. (default: )
     - Renamed
   * -
     -
     - --local-storage-prefix LOCAL_STORAGE_PREFIX
     -
                        Specify prefix for storing local copies of storage
                        files and folders. By default, this is a hidden
                        subfolder in the workdir. It can however be freely
                        chosen, e.g. in order to store those files on a local
                        scratch disk. (default: .snakemake/storage)
     -
   * - shared-fs
     -
     -
     -
     -
   * - --no-shared-fs
     -
                        Do not assume that jobs share a common file system.
                        When this flag is activated, Snakemake will assume
                        that the filesystem on a cluster node is not shared
                        with other nodes. For example, this will lead to
                        downloading remote files on each cluster node
                        separately. Further, it won't take special measures to
                        deal with filesystem latency issues. This option will
                        in most cases only make sense in combination with
                        --default-remote-provider. Further, when using
                        --cluster you will have to also provide --cluster-
                        status. Only activate this if you know what you are
                        doing. (default: False)
     - --shared-fs-usage {input-output,persistence,software-deployment,source-cache,sources,storage-local-copies,none} [{input-output,persistence,software-deployment,source-cache,sources,storage-local-copies,none} ...]
     -
                        Set assumptions on shared filesystem for non-local
                        workflow execution. To disable any sharing via the
                        filesystem, specify 'none'. Usually, the executor
                        plugin sets this to a correct default. However,
                        sometimes it is worth tuning this setting, e.g. for
                        optimizing cluster performance. For example, when
                        using '--default-storage-provider fs' together with a
                        cluster executor like slurm, you might want to set '--
                        shared-fs-usage persistence software-deployment
                        sources source-cache', such that software deployment
                        and data provenance will be handled by NFS but input
                        and output files will be handled exclusively by the
                        storage provider. (default:
                        frozenset({<SharedFSUsage.SOFTWARE_DEPLOYMENT: 2>,
                        <SharedFSUsage.INPUT_OUTPUT: 1>,
                        <SharedFSUsage.PERSISTENCE: 0>,
                        <SharedFSUsage.SOURCES: 3>,
                        <SharedFSUsage.SOURCE_CACHE: 5>,
                        <SharedFSUsage.STORAGE_LOCAL_COPIES: 4>}))
     - Redesigned: Please change ``--no-shared-fs`` to ``--shared-fs-usage none``
   * -
     -
     - --job-deploy-sources
     -
                        Whether the workflow sources shall be deployed before
                        a remote job is started. Only applies if --no-shared-
                        fs is set or executors are used that imply no shared
                        FS (e.g. the kubernetes executor). (default: False)
     - (Clearer description needed)
   * - greediness
     -
     -
     -
     -
   * - --greediness GREEDINESS
     -
                        Set the greediness of scheduling. This value between 0
                        and 1 determines how careful jobs are selected for
                        execution. The default value (1.0) provides the best
                        speed and still acceptable scheduling quality.
                        (default: None)
     - --scheduler-greediness SCHEDULER_GREEDINESS, --greediness SCHEDULER_GREEDINESS
     -
                        Set the greediness of scheduling. This value between 0
                        and 1 determines how careful jobs are selected for
                        execution. The default value (1.0) provides the best
                        speed and still acceptable scheduling quality.
                        (default: None)
     - Renamed
   * - debug
     -
     -
     -
     -
   * - --overwrite-shellcmd OVERWRITE_SHELLCMD
     -
                        Provide a shell command that shall be executed instead
                        of those given in the workflow. This is for debugging
                        purposes only. (default: None)
     -
     -
     - Deprecated
   * -  --mode {0,1,2}
     -
                        Set execution mode of Snakemake (internal use only).
                        (default: 0)
     - --mode {default,remote,subprocess}
     -
                        Set execution mode of Snakemake (internal use only).
                        (default: default)
     - Redesigned: use string instead of integer
   * - APPTAINER/SINGULARITY
     -
     -
     -
     -
   * - --use-singularity
     -
                        If defined in the rule, run job within a singularity
                        container. If this flag is not set, the singularity
                        directive is ignored. (default: False)
     - --use-apptainer, --use-singularity
     -
                        If defined in the rule, run job within a
                        apptainer/singularity container. If this flag is not
                        set, the singularity directive is ignored. (default:
                        False)
     - New alias (more general usage)
   * - --singularity-prefix DIR
     -
                        Specify a directory in which singularity images will
                        be stored. If not supplied, the value is set to the
                        '.snakemake' directory relative to the invocation
                        directory. If supplied, the ``--use-singularity`` flag
                        must also be set. The value may be given as a relative
                        path, which will be extrapolated to the invocation
                        directory, or as an absolute path. (default: None)
     - --apptainer-prefix DIR, --singularity-prefix DIR
     -
                        Specify a directory in which apptainer/singularity
                        images will be stored.If not supplied, the value is
                        set to the '.snakemake' directory relative to the
                        invocation directory. If supplied, the ``--use-
                        apptainer`` flag must also be set. The value may be
                        given as a relative path, which will be extrapolated
                        to the invocation directory, or as an absolute path.
                        (default: None)
     - New alias (more general usage)
   * - --singularity-args ARGS
     -
                        Pass additional args to singularity. (default: )
     - --apptainer-args ARGS, --singularity-args ARGS
     -
                        Pass additional args to apptainer/singularity.
                        (default: )
     - New alias (more general usage)
   * - --cleanup-containers
     -
                        Remove unused (singularity) containers (default:
                        False)
     - --container-cleanup-images
     -
                        Remove unused containers (default: False)
     - New alias (more general usage)
   * - precommand
     -
     -
     -
     -
   * - --precommand PRECOMMAND
     -
                        Any command to execute before snakemake command **on AWS
                        cloud** such as wget, git clone, unzip, etc. This is
                        used with --tibanna.Do not include input/output
                        download/upload commands - file transfer between S3
                        bucket and the run environment (container) is
                        automatically handled by Tibanna. (default: None)
     - --precommand PRECOMMAND
     -
                        Only used in case of remote execution. Command to be
                        executed before Snakemake executes each job on the
                        remote compute node. (default: None)
     - Redesigned: more general usage
   * - software-deployment-method
     -
     -
     -
     -
   * -
     -
     - --software-deployment-method {apptainer,conda,env-modules} [{apptainer,conda,env-modules} ...], --deployment-method {apptainer,conda,env-modules} [{apptainer,conda,env-modules} ...], --deployment {apptainer,conda,env-modules} [{apptainer,conda,env-modules} ...]
     -
                        Specify software environment deployment method.
                        (default: set())
     - New designed
   * - executor
     -
     -
     -
     -
   * - --cluster CMD, (may be --touch, --dryrun, ..., ?)
     -
     - --executor {cluster-generic,local,dryrun,touch}, -e {cluster-generic,local,dryrun,touch}
     -
                        Specify a custom executor, available via an executor
                        plugin: snakemake_executor_<name> (default: None)
     - New designed: Now if you want to use ``--cluster CMD``, please use ``--executor cluster-generic --cluster-generic-submit-cmd CMD`` instead.
        Note you should install ``cluster-generic`` using command ``pip install snakemake-executor-cluster-generic``
   * - --cluster CMD
     -
                        Execute snakemake rules with the given submit command,
                        e.g. qsub. Snakemake compiles jobs into scripts that
                        are submitted to the cluster with the given command,
                        once all input files for a particular job are present.
                        The submit command can be decorated to make it aware
                        of certain job properties (name, rulename, input,
                        output, params, wildcards, log, threads and
                        dependencies (see the argument below)), e.g.: $
                        snakemake --cluster 'qsub -pe threaded {threads}'.
                        (default: None)
     -  --cluster-generic-submit-cmd VALUE
        (Requires the cluster-generic plugin)
     -
                        Command for submitting jobs (default:
                        <dataclasses._MISSING_TYPE object at 0x7fc423088680>)
     - Renamed
   * - --cluster-status CLUSTER_STATUS
     -
                        Status command for cluster execution. This is only
                        considered in combination with the --cluster flag. If
                        provided, Snakemake will use the status command to
                        determine if a job has finished successfully or
                        failed. For this it is necessary that the submit
                        command provided to --cluster returns the cluster job
                        id. Then, the status command will be invoked with the
                        job id. Snakemake expects it to return 'success' if
                        the job was successful, 'failed' if the job failed and
                        'running' if the job still runs. (default: None)
     - --cluster-generic-status-cmd VALUE
       (Requires the cluster-generic plugin)
     -
                        Command for retrieving job status (default:
                        <dataclasses._MISSING_TYPE object at 0x7fc423088680>)
     - Renamed
   * - --cluster-cancel CLUSTER_CANCEL
     -
                        Specify a command that allows to stop currently
                        running jobs. The command will be passed a single
                        argument, the job id. (default: None)
     - --cluster-generic-cancel-cmd VALUE
       (Requires the cluster-generic plugin)
     -
                        Command for cancelling jobs. Expected to take one or
                        more jobids as arguments. (default:
                        <dataclasses._MISSING_TYPE object at 0x7fc423088680>)
     - Renamed
   * - --cluster-cancel-nargs CLUSTER_CANCEL_NARGS
     -
                        Specify maximal number of job ids to pass to
                        --cluster-cancel command, defaults to 1000. (default:
                        1000)
     - --cluster-generic-cancel-nargs VALUE
       (Requires the cluster-generic plugin)
     -
                        Number of jobids to pass to cancel_cmd. If more are
                        given, cancel_cmd will be called multiple times.
                        (default: <dataclasses._MISSING_TYPE object at
                        0x7fc423088680>)
     - Renamed
   * - --cluster-sidecar CLUSTER_SIDECAR
     -
                        Optional command to start a sidecar process during
                        cluster execution. Only active when --cluster is given
                        as well. (default: None)
     - --cluster-generic-sidecar-cmd VALUE
       (Requires the cluster-generic plugin)
     -
                        Command for sidecar process. (default:
                        <dataclasses._MISSING_TYPE object at 0x7fc423088680>)
     - Renamed


Profiles
^^^^^^^^

Profiles can now be versioned.
If your profile makes use of settings that are available in version 8 or later, use the filename ``config.v8+.yaml`` for the profile configuration (see :ref:`executing-profiles`).

API
^^^

The Snakemake API has been completely rewritten into a modern `dataclass <https://docs.python.org/3/library/dataclasses.html>`_ based approach.
The traditional central ``snakemake()`` function is gone.
For an example how to use the new API, check out the Snakemake CLI implementation `here <https://github.com/snakemake/snakemake/blob/04ec2c0262b2cb96cbcd7edbbb2596979c1703ae/snakemake/cli.py#L1767>`__.



================================================
FILE: docs/project_info/authors.rst
================================================
.. project_info-authors:

=======
Credits
=======


Development Lead
----------------

- Johannes Köster

Development Team
----------------

- Christopher Tomkins-Tinch
- David Koppstein
- Tim Booth
- Manuel Holtgrewe
- Christian Arnold
- Wibowo Arindrarto
- Rasmus Ågren
- Soohyun Lee
- Vanessa Sochat

Contributors
------------

In alphabetical order

- Andreas Wilm
- Anthony Underwood
- Ryan Dale
- David Alexander
- Elias Kuthe
- Elmar Pruesse
- Hyeshik Chang
- Jay Hesselberth
- Jesper Foldager
- John Huddleston
- Joona Lehtomäki
- Karel Brinda
- Karl Gutwin
- Kemal Eren
- Kostis Anagnostopoulos
- Kyle A. Beauchamp
- Kyle Meyer
- Lance Parsons
- Manuel Holtgrewe
- Marcel Martin
- Matthew Shirley
- Mattias Franberg
- Matt Shirley
- Paul Moore
- Per Unneberg
- Ryan C. Thompson
- Ryan Dale
- Sean Davis
- Simon Ye
- Tobias Marschall
- Vanessa Sochat
- Willem Ligtenberg



================================================
FILE: docs/project_info/citations.rst
================================================
.. _project_info-citations:

====================
Citing and Citations
====================

This section gives instructions on how to cite Snakemake and lists citing articles.


.. project_info-citing_snakemake:

----------------
Citing Snakemake
----------------

When using Snakemake for a publication, **please cite the following article** in you paper:

`Mölder, F., Jablonski, K.P., Letcher, B., Hall, M.B., Tomkins-Tinch, C.H., Sochat, V., Forster, J., Lee, S., Twardziok, S.O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., Köster, J., 2021. Sustainable data analysis with Snakemake. F1000Res 10, 33. <https://doi.org/10.12688/f1000research.29032.1>`_

This "rolling" paper will be regularly updated when Snakemake receives new features.

More References
===============

The initial Snakemake publication was:

`Köster, Johannes and Rahmann, Sven. "Snakemake - A scalable bioinformatics workflow engine". Bioinformatics 2012. <https://bioinformatics.oxfordjournals.org/content/28/19/2520>`_

Another publication describing more of Snakemake internals:

`Köster, Johannes and Rahmann, Sven. "Building and Documenting Bioinformatics Workflows with Python-based Snakemake". Proceedings of the GCB 2012. <https://drops.dagstuhl.de/opus/volltexte/oasics-complete/oasics-vol26-gcb2012-complete.pdf>`_

And my PhD thesis which describes all algorithmic details as of 2015:

`Johannes Köster, "Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis", TU Dortmund 2014 <https://hdl.handle.net/2003/33940>`_

The most comprehensive publication is our "rolling" paper (see above):

`Mölder, F., Jablonski, K.P., Letcher, B., Hall, M.B., Tomkins-Tinch, C.H., Sochat, V., Forster, J., Lee, S., Twardziok, S.O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., Köster, J., 2021. Sustainable data analysis with Snakemake. F1000Res 10, 33. <https://doi.org/10.12688/f1000research.29032.1>`_


Project Pages
=============

If you publish a Snakemake workflow, consider to add this badge to your project page:

.. image:: https://img.shields.io/badge/snakemake-≥5.6.0-brightgreen.svg?style=flat
   :target: https://snakemake.readthedocs.io

The markdown syntax is

.. sourcecode:: text

    [![Snakemake](https://img.shields.io/badge/snakemake-≥5.6.0-brightgreen.svg?style=flat)](https://snakemake.readthedocs.io)

Replace the ``5.6.0`` with the minimum required Snakemake version.
You can also `change the style <https://shields.io/#styles>`_.



================================================
FILE: docs/project_info/codebase.rst
================================================

.. _codebase_intro:

Codebase and architecture
=========================

Snakemake is organized as a combination of a main package, a set of plugin interface packages for various functionalities (e.g. execution backends, storage, reporting), and plugin packages implementing those interfaces.
The development of the Snakemake main package as well as the plugin interfaces is hosted by the Snakemake GitHub organization (https://github.com/snakemake) and maintained by the :ref:`Snakemake core team <maintainers>`.
Many plugins are hosted by and developed within the Snakemake GitHub organization as well.
However, both Snakemake's plugin detection and the `plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog/>`__ are designed to work seamlessly with plugins developed outside of the Snakemake GitHub organization and such contributions are highly encouraged.
Visually, Snakemake's architecture can be summarized as follows (note that some plugin interfaces are still in development and not yet available to the public):

.. image:: img/architecture.svg
    :alt: Snakemake architecture

The main package
----------------

Snakemake's main package can be partitioned into three levels.

#. The user-facing level, consisting of an API and a command line interface that uses the API under the hood.
#. The language level, implementing the language parser and thereby defining the workflow definition syntax.
#. The core level, implementing Snakemake's interpretation of rules defined via the language, the inference of jobs and their dependencies, as well as the actual execution of workflows as a combined usage of the various plugin types.

The user-facing level
^^^^^^^^^^^^^^^^^^^^^

Most users will interact with the command line interface of Snakemake.
The command line interface of Snakemake is defined in `snakemake/cli.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/cli.py>`__ using Python's argparse module.
The module is responsible for parsing command-line arguments, setting up logging, and invoking the appropriate Snakemake API calls based on the provided arguments. It defines various argument groups for execution, grouping, reports, notebooks, utilities, output, behavior, remote execution, software deployment, conda, apptainer/singularity, environment modules, and internal use.

The main steps in `cli.py` include:

#. **Argument Parsing**: Using argparse to define and parse command-line arguments.
#. **Profile Handling**: Managing profiles for setting default command-line arguments.
#. **Logging Setup**: Configuring log handlers based on the provided arguments.
#. **API Invocation**: Converting parsed arguments into API calls to execute the workflow, generate reports, or perform other tasks.

The API (application programming interface) of Snakemake is defined in `snakemake/api.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/api.py>`__.
It provides a programmatic interface to Snakemake, allowing users to interact with Snakemake workflows through Python code.
It defines classes and methods for setting up and executing workflows, managing resources, and handling various settings.

The main components include:

#. ``SnakemakeApi``: The main entry point for interacting with Snakemake workflows programmatically. It provides methods for setting up workflows, deploying sources, and handling exceptions.
   Via the method ``SnakemakeApi.workflow()`` a workflow can be instantiated from a given snakefile, thereby returning a ``WorkflowApi`` object).
#. ``WorkflowApi``: A class for managing workflow-specific settings and operations, such as creating DAGs, linting workflows, and executing workflows.
   Via the method ``WorkflowApi.dag()`` a Directed Acyclic Graph (DAG) can be instantiated from the workflow, thereby returning a ``DAGApi`` object).
#. ``DAGApi``: A class for managing Directed Acyclic Graph (DAG) settings and executing workflows with specific executors and settings.

All parts of the API are designed to be used in combination with a set of `data classes <https://docs.python.org/3/library/dataclasses.html>`__ and `enumerations <https://docs.python.org/3/library/enum.html>`__ that define various settings in a type safe and readable way.
These classes are in part found under `snakemake/settings <https://github.com/snakemake/snakemake/blob/main/src/snakemake/settings>`__, and in part (where necessary) defined in the plugin interface packages.

The language level
^^^^^^^^^^^^^^^^^^

Snakemake offers a domain specific language (DSL) for defining workflows.
The syntax of the DSL is primarily defined via the implementation of the parser in `snakemake/parser.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/parser.py>`__.
A key feature of Snakemake's power is the fact that the language extends the syntax of Python with *directives* to define rules and other workflow specific controls.
Technically, this is implemented as a hierarchy of `Mealy machines <https://en.wikipedia.org/wiki/Mealy_machine>`__, each of which is responsible for one of the directives offered by the Snakemake DSL.
The parser first tokenizes a given snakefile using Python's builtin tokenizer, and then translates the tokens (that contain DSL tokens) into plain python tokens which translate the DSL specification into a plain Python specification of the workflow and all its items (e.g. the definition of rules).
The latter are then fed into the Python interpreter, thereby building up the workflow with all the defined rules.
During the token translation process, the Mealy machine hierarchy starts in a state accepting (and outputting) any Python token, and switches to appropriate sub-machines whenever a DSL-specifiy keyword is encountered.
The Mealy machines are implemented as Python classes, using abstract base classes to define common functionality.
The generated Python code invokes methods of the ``snakemake.workflow.Workflow`` (see :ref:`codebase_core`) class to build up the workflow.

.. _codebase_core:

The core level
^^^^^^^^^^^^^^

The core level of Snakemake is responsible for interpreting the rules defined via the language, inferring jobs and their dependencies, and executing workflows. The most important components of the core level include:

Workflow
""""""""
The ``Workflow`` class (`snakemake/workflow.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/workflow.py>`__) is the central class representing a Snakemake workflow. It manages the rules, config, resources, and execution settings. The `Workflow` class is responsible for parsing the workflow definition, creating the Directed Acyclic Graph (DAG) of jobs, and orchestrating the execution of the workflow. It interacts with other core components like `Rule`, `DAG`, and `Scheduler` to manage the workflow execution.

Rule
""""
The `Rule` class (`snakemake/rules.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/rules.py>`__) represents a single rule in the workflow. A `Rule` defines the input, output, and parameters for a specific step in the workflow. It also includes directives for resources, conda environments, and containerization. The `Rule` class interacts with the `Workflow` class to define the workflow structure and with the `DAG` class to manage job dependencies.

DAG
"""
The `DAG` class (`snakemake/dag.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/dag.py>`__) represents the Directed Acyclic Graph (DAG) of jobs in the workflow. It is responsible for inferring the order of job execution, detecting cycles, and managing job dependencies. The `DAG` class interacts with the `Workflow`, `Rule`, and `Scheduler` classes to manage the workflow execution.

Persistence
"""""""""""
The `Persistence` class (`snakemake/persistence.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/persistence.py>`__) manages the persistent storage of metadata and provenance information. The `Persistence` class interacts with the `Workflow` and `DAG` classes to manage workflow state.

Scheduler
"""""""""
The `Scheduler` class (`snakemake/scheduler.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/scheduler.py>`__) is responsible for scheduling jobs for execution. It uses the DAG to determine the order of job execution, taking into account resource constraints and job priorities. The `Scheduler` class interacts with the `Workflow`, `DAG`, and `Rule` classes to manage job scheduling.

PathModifier
""""""""""""
The `PathModifier` class (`snakemake/path_modifier.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/path_modifier.py>`__) is a utility class for handling path modifications such as the handling of remote storage or module imports. It ensures that paths are correctly managed and modified according to the workflow's requirements. The `PathModifier` class interacts with the `Workflow` and `_IOFile` classes to manage file paths.

Sourcecache
"""""""""""
The `Sourcecache` class (`snakemake/sourcecache.py <https://github.com/snakemake/snakemake/blob/main/src/snakemake/sourcecache.py>`__) handles the caching of source files. It ensures that remote source files are efficiently managed and reused across workflow executions. The `Sourcecache` class interacts with the `Workflow` and `_IOFile` classes to manage source files.

Besides these central classes, the following modules add additional functionality:

ioutils
"""""""
The ``ioutils`` module (`snakemake/ioutils <https://github.com/snakemake/snakemake/blob/main/src/snakemake/ioutils>`__) implements semantic helper functions for handling input and output files as well as non-file parameters in the workflow.

linting
"""""""
The ``linting`` module (`snakemake/linting <https://github.com/snakemake/snakemake/blob/main/src/snakemake/linting>`__) implements static code analysis functionality for giving hints on discovered anti-patterns in a given workflow definition.

script
""""""
The ``script`` module (`snakemake/script <https://github.com/snakemake/snakemake/blob/main/src/snakemake/script>`__) implements Snakemake's integration with scripting languages like R or Julia.


Plugins
^^^^^^^

Various functionalities of Snakemake are organized via plugins (e.g. supporting different execution backends or remote storage).
For each type of plugin, Snakemake offers an interface package.
Interface packages provide a stable API for the communication between the main Snakemake and the plugin.
They are independently (and semantically) versioned and aim to avoid breaking changes, so that plugins will rarely have to be modified and should remain compatible with a wide range of Snakemake versions for a long time.

Interfaces
""""""""""

Interface packages offer a set of strongly typed abstract base classes that have to be implemented by the respective plugins, thereby rigorously defining what functionality a plugin has to offer and to which structures of the main Snakemake it has access.
Each interface package comes with a test suite, that tests any implementing plugin in terms of (ideally) all kinds of plugin usages.

Naming
""""""

Plugins have to follow a naming convention, controlling how they are findable via `pypi <https://pypi.io>`__.
The convention state that the plugin has to be named as ``snakemake-<type>-plugin-<name>`` with ``<type>`` being the type of plugin (e.g. ``storage``) and ``<name>`` being the name of the plugin (e.g. ``s3``).

Plugin catalog
""""""""""""""

Leveraging this standardized naming, Snakemake automatically collects all plugins from pypi and presents them in the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`__.
The catalog automatically generates usage documentation for each plugin, gives credit to authors, and provides links to the respective github repositories.
Plugin authors can extend the catalog documentation for their plugin by providing markdown files ``docs/intro.md`` and ``docs/further.md`` in the plugin repository in order to show an introductory paragraph as well as extensive non-standard documentation (like usage examples or other plugin specific information) in the catalog.

Scaffolding
"""""""""""

Via `poetry <https://github.com/snakemake/poetry-snakemake-plugin>`__, plugins can be automatically scaffolded, leading to all files for plugin implementation, testing and package building being generated as skeletons.
The developer then only needs to implement extensively annotated abstract methods of base classes provided by the respective interface package.
After pushing the plugin code into a new GitHub repository, testing, release-automation, and pypi upload then work out of the box.

Assuming that the plugin type to create is given as bash variable ``$type`` below, given that poetry is available as a command, the following procedure should be followed for scaffolding a new plugin:

.. code-block:: bash

    # Install latest version of the snakemake poetry plugin
    poetry self add poetry-snakemake-plugin@latest

    # Create a new poetry project via
    poetry new snakemake-$type-plugin-myfancyplugin

    cd snakemake-$type-plugin-myfancyplugin

    poetry scaffold-snakemake-$type-plugin

    # Next, edit the scaffolded code according to your needs, and publish
    # the resulting plugin into a github repository. The scaffold command also 
    # creates github actions workflows that will immediately start to check and test
    # the plugin.

An example class (in this case for storage plugins) created by the scaffold command would be the following:

.. code-block:: python

    # Required:
    # Implementation of your storage provider
    # This class can be empty as the one below.
    # You can however use it to store global information or maintain e.g. a connection
    # pool.
    class StorageProvider(StorageProviderBase):
        # For compatibility with future changes, you should not overwrite the __init__
        # method. Instead, use __post_init__ to set additional attributes and initialize
        # further stuff.

        def __post_init__(self):
            # This is optional and can be removed if not needed.
            # Alternatively, you can e.g. prepare a connection to your storage backend here.
            # and set additional attributes.
            pass

        @classmethod
        def example_queries(cls) -> List[ExampleQuery]:
            # Return example queries with description for this storage provider (at
            # least one).
            ...

        def rate_limiter_key(self, query: str, operation: Operation) -> Any:
            # Return a key for identifying a rate limiter given a query and an operation.

            # This is used to identify a rate limiter for the query.
            # E.g. for a storage provider like http that would be the host name.
            # For s3 it might be just the endpoint URL.
            ...

        def default_max_requests_per_second(self) -> float:
            # Return the default maximum number of requests per second for this storage
            # provider.
            ...

        def use_rate_limiter(self) -> bool:
            # Return False if no rate limiting is needed for this provider.
            ...

        @classmethod
        def is_valid_query(cls, query: str) -> StorageQueryValidationResult:
            # Return whether the given query is valid for this storage provider.
            # Ensure that also queries containing wildcards (e.g. {sample}) are accepted
            # and considered valid. The wildcards will be resolved before the storage
            # object is actually used.
            ...

Once all methods of all scaffolded classes are implemented, the plugin is ready to be tested.

Continuous testing is conducted via Github Actions, defined in the file ``.github/workflows/ci.yml``.
In case the testing needs additional software or services to be deployed for the plugin to be tested, this can happen inside that file, prior to the step that invokes pytest.



================================================
FILE: docs/project_info/contributing.rst
================================================
.. _project_info-contributing:

************
Contributing
************

Contributions are welcome, and they are greatly appreciated!
Every little bit helps, and credit will always be given.
A detailed description of the Snakemake codebase and architecture can be found :ref:`here <codebase_intro>`.

You can contribute in many ways:

Types of contributions
======================


Report bugs or suggest enhancements
-----------------------------------

Report bugs or suggest enhancements at https://github.com/snakemake/snakemake/issues

If you are reporting a bug, follow the template and fill out the requested information, including:

* Your operating system name and version.
* Any details about your local setup that might be helpful in troubleshooting.
* Detailed steps to reproduce the bug and ideally a test case.

If you are proposing am enhancement or a new feature:

* Explain in detail how it would work.
* Keep the scope as narrow as possible, to make it easier to implement.
* Remember that this is a volunteer-driven project, and that contributions are welcome :).


Fix Bugs
--------

Look through the Github issues for bugs.
If you want to start working on a bug then please write short message on the issue tracker to prevent duplicate work.


Implement Features
------------------

Look through the Github issues for feature/enhancement requests.
If you want to start working on an issue then please write short message on the issue tracker to prevent duplicate work.

Contributing a plugin
---------------------

Currently, Snakemake supports executor plugins, storage plugins, and report plugins.
The `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`_ shows which plugins are available and how to contribute new ones.

Write Documentation
-------------------

Snakemake could always use more documentation, whether as part of the official docs, in docstrings, or even on the web in blog posts, articles, and such.

.. _Sphinx: https://sphinx-doc.org

Snakemake uses `Sphinx`_ for the user manual (that you are currently reading).
See :ref:`project_info-doc_guidelines` on how the reStructuredText is used for the documentation.



Pull Request Guidelines
=======================

To update the documentation, fix bugs or add new features you need to create a Pull Request
. A PR is a change you make to your local copy of the code for us to review and potentially integrate into the code base.

To create a Pull Request you need to do these steps:

1. Create a Github account.
2. Fork the repository.
3. Clone your fork locally.
4. Go to the created snakemake folder with :code:`cd snakemake`.
5. Create a new branch with :code:`git checkout -b <descriptive_branch_name>`.
6. Make your changes to the code or documentation.
7. Format your changes with `Snakefmt <https://github.com/snakemake/snakefmt>`_ or `BLACK <https://github.com/psf/black>`_.
8. Run :code:`git add .` to add all the changed files to the commit (to see what files will be added you can run :code:`git add . --dry-run`).
9. To commit the added files use :code:`git commit`. (This will open a command line editor to write a commit message. These should have a descriptive 80 line header, followed by an empty line, and then a description of what you did and why. To use your command line text editor of choice use (for example) :code:`export GIT_EDITOR=vim` before running :code:`git commit`).
10. Now you can push your changes to your Github copy of Snakemake by running :code:`git push origin <descriptive_branch_name>`.
11. If you now go to the webpage for your Github copy of Snakemake you should see a link in the sidebar called "Create Pull Request".
12. Now you need to choose your PR from the menu and click the "Create pull request" button. Be sure to change the pull request target branch to <descriptive_branch_name>!

If you want to create more pull requests, first run :code:`git checkout main` and then start at step 5. with a new branch name.

Feel free to ask questions about this if you want to contribute to Snakemake :)

.. _pixi-getting_started:

Development with ``pixi``
=========================

`pixi <https://pixi.sh/>`_ is a tool that is designed to help you manage 
your development environment.It acts as a drop-in replacement for
`conda <https://docs.conda.io/en/latest/>`_, offering:

- **Easy installation & Updating**: `install pixi <https://pixi.sh/latest/#installation>`_ 
  through many methods and for different shells.
  Updating ``pixi`` is as simple as ``pixi self-update``

- **Ease of Use**: A streamlined CLI (similar to Yarn or Cargo) for quick
  environment creation and management. Try commands like ``pixi init``,
  ``pixi add <package>``, or ``pixi run`` to see how intuitive it is.

- **Multiple Environments**: Define and switch between multiple sets of
  dependencies under one project.
  Pixi uses a ``feature`` system to compose an ``environment``.
  Think of ``features`` as a way to group dependencies and settings together.
  and an environment is a collection of features.
  This allows easy management of different environments for multiple use.
  See the ``pyproject.toml`` file for an example of how the ``test`` feature
  is used to define the ``dev``, ``py311`` and ``py312`` environments.

- **Cross-Platform Solving**: Target Linux, macOS, and Windows from a single
  config. Pixi resolves the correct packages for each platform and captures
  them in a lockfile for reproducible setups—no Docker needed.

- **Speed & Conda Compatibility**: Written in Rust, Pixi downloads and solves
  packages in parallel for faster operations. It uses the Conda ecosystem
  and channels, so you get the same packages with improved performance. In
  many cases, Pixi can outperform both Conda and Mamba.

To learn more, visit the `Pixi docs <https://pixi.sh>`__ or check out helpful
guides on `prefix.dev <https://prefix.dev/>`__. 

Testing Guidelines
==================

To ensure that you do not introduce bugs into Snakemake, you should test your code thoroughly.

Putting these tests into repeatable test cases ensures they can be checked on multiple platforms and Python versions.

Continuous integration
----------------------
For any pull request, all tests are automatically executed within Github Actions, providing feedback to you and the official development team whether the proposed changes are working as expected and do not hamper other functionality Snakemake provides.
However, it is useful to be able to run the tests locally, thereby being able to quickly debug any occurring failures.

Setup to run the test suite locally
-----------------------------------

Unit tests and regression tests are written to be run by `pytest <https://docs.pytest.org/en/stable/>`_.


.. _pixi-test-guide:

Testing Guide using ``pixi``
=============================

**Prerequisites**: Make sure you have ``pixi`` installed: See :ref:`pixi-getting_started`.

**Activate your environment**:
--------------------------------

There are a few environments you can use to run the tests.

The ``dev`` environment is most useful for overall development.
This environment will also install the ``docs`` and ``style`` features
which will allow you to also build documentation and run ``black``.

.. code-block:: console

    $ pixi shell -e dev

The ``py311`` and ``py312`` environments are what are used in the 
CI tests which isolate the Python version and the `test` dependencies.
Use this if you want to test your code against the same environment
as any failing CI tests.

.. code-block:: console

    $ pixi shell -e py311

**Run a comprehensive, simple, or single test**:
The test suite defines two types of tests via ``pixi tasks`` that you can run:

**test-all**: This task runs the comprehensive test suite, which includes 
*most* of the tests in the ``tests/`` directory.

.. code-block:: console

    $ pixi run test-all

**test-simple**: This task runs the main tests located in ``tests/tests.py``.

.. code-block:: console

    $ pixi run test-simple

**Single test**: You can also run a single test by using ``pytest`` 
directly with the test file and the test name.

.. code-block:: console

    $ pixi run pytest tests/tests.py::test_log_input

.. tip::
    This test suite is quite long, and can be run in parts similar to the 
    CI/CD tests which run it in 1/10 parts.

    To do so, you can use the ``--splits`` and ``--group`` flags to run
    a subset of the tests. For example, to run the first group of tests
    in a 10 part split:

    .. code-block:: console

        $ pixi run test-simple \
            --splits 10 \
            --group 1 \
            --splitting-algorithm=least_duration

Marked tests
------------
Some tests have been marked using `pytest markers <https://docs.pytest.org/en/stable/mark.html>`_.
These allow for running specific tests or *excluding* specific tests.
For example, the `pixi run test-simple` currently excludes the `needs_envmodules` tests.
There is also another marker for ``needs_s3`` which will skip tests that require an S3 connection.
If you are not looking to test the S3 functionality, you can modify the
test command to exclude these tests.

.. code-block:: console

    $ pixi run test-simple -m "not needs_envmodules and not needs_s3"

For a full list of available markers, you can run:

.. code-block:: console

    $ pixi run pytest --markers

Warnings and oddities
---------------------

You will likely see warnings related to deprecated functions in dependent libraries, especially botocore.

You may also get intermittent failures from tests that rely on external connectivity. The default test suite makes connections to multiple external services.

Tests that require singularity will be auto-skipped if no singularity or apptainer installation is available.
At the time of writing neither the ``singularity`` package on conda-forge nor the ``apptainer`` package are reliable, in that there are multiple failing tests on a standard Ubuntu system.
This is likely due to system security profiles that conda, being a non-root application, cannot change.
The Debian/Ubuntu ``singularity-container`` DEB package, which must be installed by the system administrator, does work.
The equivalent RPM package should also work on RedHat-type systems.

Depending on how the Snakemake code was downloaded and installed in the test environment, Snakemake may not be able to determine its own version and may think that it is version 0.
The existing unit tests should all cope with this, and in general you should avoid writing tests that rely on explicit version checks.


.. _project_info-doc_guidelines:

Documentation Guidelines
========================

The documentation uses `Sphinx`_ and is written in ``reStructuredText``.
For details on the syntax, see the `Sphinx primer on reStructuredText <https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html#rst-primer>`_ and the `Sphinx documentation on cross-references <https://www.sphinx-doc.org/en/master/usage/referencing.html>`_.

For the documentation, please adhere to the following guidelines:

- Put each sentence on its own line, this makes tracking changes through Git SCM easier.
- Provide `hyperlink targets <https://www.sphinx-doc.org/en/master/usage/referencing.html#cross-referencing-arbitrary-locations>`_, at least for the first two section levels.
  For this, use the format ``<document_part>-<section_name>``, for example ``project_info-doc_guidelines`` for the current section.
  Set the hyperlink target right above the section heading with ``.. _project_info-doc_guidelines:``.
  Reference the hyperlink (i.e. link to it) with ``:ref:`project_info-doc_guidelines```.
- Use the `section structure recommended by Sphinx <https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html#sections>`_, which references the `recommendations in the Python Developer's Guide <https://devguide.python.org/documentation/markup/#sections>`_.
  Namely, the levels are:

::

    .. document_part-section_heading:

    ===============
    Section heading
    ===============


    .. document_part-subsection_heading:

    Subsection heading
    ------------------

    .. document_part-subsubsection_heading:

    Subsubsection heading
    ^^^^^^^^^^^^^^^^^^^^^

    .. document_part-paragraph_heading:

    Paragraph heading
    """""""""""""""""

.. _doc_setup:

Documentation Setup
-------------------

To get started, make sure you have ``pixi`` installed: 
See :ref:`pixi-getting_started`.
We use ``pixi`` to manage the docs environment and tasks to streamline
the developer experience.

.. code-block:: console

    $ ➜ pixi task list --environment docs
    Tasks that can run on this machine:
    -----------------------------------
    build-apidocs, build-docs, docs

    - build-apidocs   Build the API documentation in the apidocs/ directory
    - build-docs      Build the documentation in the docs/ directory
    - docs            Serve the documentation on http://localhost:8000 with live reload

**Test if the docs build**:
To only build the documentation, you can use the ``build-docs`` task.

.. code-block:: console

    $ pixi run build-docs

**Live server with auto-reload**:
To serve the documentation on a local server with live reload, 
use the ``docs`` task.

.. code-block:: console

    $ pixi run docs
    [sphinx-autobuild] Starting initial build
    [sphinx-autobuild] > python -m sphinx build docs/ docs/_build/html
    ...
    The HTML pages are in docs/_build/html.
    [sphinx-autobuild] Serving on http://0.0.0.0:8000
    [sphinx-autobuild] Waiting to detect changes...


================================================
FILE: docs/project_info/faq.rst
================================================
.. _project_info-faq:

==========================
Frequently Asked Questions
==========================

.. contents::

What is the key idea of Snakemake workflows?
--------------------------------------------

The key idea is very similar to GNU Make. The workflow is determined automatically from top (the files you want) to bottom (the files you have), by applying very general rules with wildcards you give to Snakemake:

.. image:: img/idea.png
    :alt: Snakemake idea

When you start using Snakemake, please make sure to walk through the :ref:`official tutorial <tutorial>`.
It is crucial to understand how to properly use the system.

How does Snakemake interpret relative paths?
--------------------------------------------

Relative paths in Snakemake are interpreted depending on their context.

* Input, output, log, and benchmark files are considered to be relative to the working directory (either the directory in which you have invoked Snakemake or whatever was specified for ``--directory`` or the ``workdir:`` directive).
* Any other directives (e.g. ``conda:``, ``include:``, ``script:``, ``notebook:``) consider paths to be relative to the Snakefile they are defined in.

If you have to manually specify a file that has to be relative to the currently evaluated Snakefile, you can use ``workflow.source_path(filepath)``.

.. code-block:: python

    rule read_a_file_relative_to_snakefile:
        input:
            workflow.source_path("resources/some-file.txt")
        output:
            "results/some-output.txt"
        shell:
            "somecommand {input} {output}"


This will in particular also work in combination with :ref:`modules <snakefiles-modules>`.

Snakemake does not connect my rules as I have expected, what can I do to debug my dependency structure?
-------------------------------------------------------------------------------------------------------

Since dependencies are inferred implicitly, results can sometimes be surprising when little errors are made in filenames or when input functions raise unexpected errors.
For debugging such cases, Snakemake provides the command line flag ``--debug-dag`` that leads to printing details each decision that is taken while determining the dependencies.

In addition, it is advisable to check whether certain intermediate files would be created by targeting them individually via the command line.

Finally, it is possible to constrain the rules that are considered for DAG creating via ``--allowed-rules``. 
This way, you can easily check rule by rule if it does what you expect.
However, note that ``--allowed-rules`` is only meant for debugging.
A workflow should always work fine without it.

My shell command fails with with errors about an "unbound variable", what's wrong?
----------------------------------------------------------------------------------

This happens often when calling virtual environments from within Snakemake. Snakemake is using `bash strict mode <http://redsymbol.net/articles/unofficial-bash-strict-mode/>`_, to ensure e.g. proper error behavior of shell scripts.
Unfortunately, virtualenv and some other tools violate bash strict mode.
The quick fix for virtualenv is to temporarily deactivate the check for unbound variables

.. code-block:: bash

    set +u; source /path/to/venv/bin/activate; set -u

For more details on bash strict mode, see the `here <http://redsymbol.net/articles/unofficial-bash-strict-mode/>`__.


My shell command fails with exit code != 0 from within a pipe, what's wrong?
----------------------------------------------------------------------------

Snakemake is using `bash strict mode <http://redsymbol.net/articles/unofficial-bash-strict-mode/>`_ to ensure best practice error reporting in shell commands.
This entails the pipefail option, which reports errors from within a pipe to outside. If you don't want this, e.g., to handle empty output in the pipe, you can disable pipefail via prepending

.. code-block:: bash

    set +o pipefail;

to your shell command in the problematic rule.


I don't want Snakemake to detect an error if my shell command exits with an exitcode > 1. What can I do?
---------------------------------------------------------------------------------------------------------

Sometimes, tools encode information in exit codes bigger than 1. Snakemake by default treats anything > 0 as an error. Special cases have to be added by yourself. For example, you can write

.. code-block:: python

    shell:
        """
        set +e
        somecommand ...
        exitcode=$?
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

This way, Snakemake only treats exit code 1 as an error, and thinks that everything else is fine.
Note that such tools are an excellent use case for contributing a `wrapper <https://snakemake-wrappers.readthedocs.io>`_.


.. _glob-wildcards:

How do I run my rule on all files of a certain directory?
---------------------------------------------------------

In Snakemake, similar to GNU Make, the workflow is determined from the top, i.e. from the target files. Imagine you have a directory with files ``1.fastq, 2.fastq, 3.fastq, ...``, and you want to produce files ``1.bam, 2.bam, 3.bam, ...`` you should specify these as target files, using the ids ``1,2,3,...``. You could end up with at least two rules like this (or any number of intermediate steps):


.. code-block:: python

    IDS = "1 2 3 ...".split() # the list of desired ids

    # a pseudo-rule that collects the target files
    rule all:
        input:  expand("otherdir/{id}.bam", id=IDS)

    # a general rule using wildcards that does the work
    rule:
        input:  "thedir/{id}.fastq"
        output: "otherdir/{id}.bam"
        shell:  "..."

Snakemake will then go down the line and determine which files it needs from your initial directory.

In order to infer the IDs from present files, Snakemake provides the ``glob_wildcards`` function, e.g.

.. code-block:: python

    IDS, = glob_wildcards("thedir/{id}.fastq")

The function matches the given pattern against the files present in the filesystem and thereby infers the values for all wildcards in the pattern. A named tuple that contains a list of values for each wildcard is returned. Here, this named tuple has only one item, that is the list of values for the wildcard ``{id}``.

I don't want expand to use the product of every wildcard, what can I do?
------------------------------------------------------------------------

By default the expand function uses ``itertools.product`` to create every combination of the supplied wildcards.
Expand takes an optional, second positional argument which can customize how wildcards are combined.
To create the list ``["a_1.txt", "b_2.txt", "c_3.txt"]``, invoke expand as:
``expand("{sample}_{id}.txt", zip, sample=["a", "b", "c"], id=["1", "2", "3"])``

I don't want expand to use every wildcard, what can I do?
---------------------------------------------------------

Sometimes partially expanding wildcards is useful to define inputs which still depend on some wildcards.
Expand takes an optional keyword argument, allow_missing=True, that will format only wildcards which are supplied, leaving others as is.
To create the list ``["{sample}_1.txt", "{sample}_2.txt"]``, invoke expand as:
``expand("{sample}_{id}.txt", id=["1", "2"], allow_missing=True)``
If the filename contains the wildcard ``allow_missing``, it will be formatted normally:
``expand("{allow_missing}.txt", allow_missing=True)`` returns ``["True.txt"]``.


Snakemake complains about a cyclic dependency or a PeriodicWildcardError. What can I do?
----------------------------------------------------------------------------------------

One limitation of Snakemake is that graphs of jobs have to be acyclic (similar to GNU Make). This means, that no path in the graph may be a cycle. Although you might have considered this when designing your workflow, Snakemake sometimes runs into situations where a cyclic dependency cannot be avoided without further information, although the solution seems obvious for the developer. Consider the following example:

.. code-block:: text

    rule all:
        input:
            "a"

    rule unzip:
        input:
            "{sample}.tar.gz"
        output:
            "{sample}"
        shell:
            "tar -xf {input}"

If this workflow is executed with

.. code-block:: console

    snakemake -n

two things may happen.

1. If the file ``a.tar.gz`` is present in the filesystem, Snakemake will propose the following (expected and correct) plan:

    .. code-block:: text

        rule a:
	        input: a.tar.gz
    	    output: a
    	    wildcards: sample=a
        localrule all:
	        input: a
        Job counts:
	        count	jobs
	        1	a
	        1	all
	        2

2. If the file ``a.tar.gz`` is not present and cannot be created by any other rule than rule ``a``, Snakemake will try to run rule ``a`` again, with ``{sample}=a.tar.gz``. This would infinitely go on recursively. Snakemake detects this case and produces a ``PeriodicWildcardError``.

In summary, ``PeriodicWildcardErrors`` hint to a problem where a rule or a set of rules can be applied to create its own input. If you are lucky, Snakemake can be smart and avoid the error by stopping the recursion if a file exists in the filesystem. Importantly, however, bugs upstream of that rule can manifest as ``PeriodicWildcardError``, although in reality just a file is missing or named differently.
In such cases, it is best to restrict the wildcard of the output file(s), or follow the general rule of putting output files of different rules into unique subfolders of your working directory. This way, you can discover the true source of your error.


Is it possible to pass variable values to the workflow via the command line?
----------------------------------------------------------------------------

Yes, this is possible. Have a look at :ref:`snakefiles_configuration`.
Previously it was necessary to use environment variables like so:
E.g. write

.. code-block:: bash

    $ SAMPLES="1 2 3 4 5" snakemake

and have in the Snakefile some Python code that reads this environment variable, i.e.

.. code-block:: python

    SAMPLES = os.environ.get("SAMPLES", "10 20").split()

I get a NameError with my shell command. Are braces unsupported?
----------------------------------------------------------------

You can use the entire Python `format minilanguage <https://docs.python.org/3/library/string.html#formatspec>`_ in shell commands. Braces in shell commands that are not intended to insert variable values thus have to be escaped by doubling them:

This:

.. code-block:: python

    ...
    shell: "awk '{print $1}' {input}"

becomes:

.. code-block:: python

    ...
    shell: "awk '{{print $1}}' {input}"

Here the double braces are escapes, i.e. there will remain single braces in the final command. In contrast, ``{input}`` is replaced with an input filename.

In addition, if your shell command has literal backslashes, ``\\``, you must escape them with a backslash, ``\\\\``. For example:

This:

.. code-block:: python

    shell: """printf \">%s\"" {{input}}"""

becomes:

.. code-block:: python

    shell: """printf \\">%s\\"" {{input}}"""

How do I incorporate files that do not follow a consistent naming scheme?
-------------------------------------------------------------------------

The best solution is to have a dictionary that translates a sample id to the inconsistently named files and use a function (see :ref:`snakefiles-input_functions`) to provide an input file like this:

.. code-block:: python

    FILENAME = dict(...)  # map sample ids to the irregular filenames here

    rule:
        # use a function as input to delegate to the correct filename
        input: lambda wildcards: FILENAME[wildcards.sample]
        output: "somefolder/{sample}.csv"
        shell: ...

How do I force Snakemake to rerun all jobs from the rule I just edited?
-----------------------------------------------------------------------

This can be done by invoking Snakemake with the ``--forcerun`` or ``-R`` flag, followed by the rules that should be re-executed:

.. code-block:: console

    $ snakemake -R somerule

This will cause Snakemake to re-run all jobs of that rule and everything downstream (i.e. directly or indirectly depending on the rules output).

How should Snakefiles be formatted?
--------------------------------------

To ensure readability and consistency, you can format Snakefiles with our tool `snakefmt <https://github.com/snakemake/snakefmt>`_. 

Python code gets formatted with `black <https://github.com/psf/black>`_ and Snakemake-specific blocks are formatted using similar principles (such as `PEP8 <https://www.python.org/dev/peps/pep-0008/>`_).

How do I enable syntax highlighting in Vim for Snakefiles?
----------------------------------------------------------

Instructions for doing this are located `here
<https://github.com/snakemake/snakemake/tree/main/misc/vim>`__.

Note that you can also format Snakefiles in Vim using :ref:`snakefmt
<How should Snakefiles be formatted?>`, with instructions located `here
<https://github.com/snakemake/snakefmt/blob/master/docs/editor_integration.md#vim>`__!

I want to import some helper functions from another python file. Is that possible?
----------------------------------------------------------------------------------

Yes, from version 2.4.8 on, Snakemake allows to import python modules (and also simple python files) from the same directory where the Snakefile resides.

How can I run Snakemake on a cluster where its main process is not allowed to run on the head node?
---------------------------------------------------------------------------------------------------

This can be achieved by submitting the main Snakemake invocation as a job to the cluster. If it is not allowed to submit a job from a non-head cluster node, you can provide a submit command that goes back to the head node before submitting:

.. code-block:: bash

    qsub -N PIPE -cwd -j yes python snakemake --cluster "ssh user@headnode_address 'qsub -N pipe_task -j yes -cwd -S /bin/sh ' " -j

This hint was provided by Inti Pedroso.

Can the output of a rule be a symlink?
--------------------------------------

Yes. As of Snakemake 3.8, output files are removed before running a rule and then touched after the rule completes to ensure they are newer than the input.  Symlinks are treated just the same as normal files in this regard, and Snakemake ensures that it only modifies the link and not the target when doing this.

Here is an example where you want to merge N files together, but if N == 1 a symlink will do.  This is easier than attempting to implement workflow logic that skips the step entirely.  Note the **-r** flag, supported by modern versions of ln, is useful to achieve correct linking between files in subdirectories.

.. code-block:: python

    rule merge_files:
        output: "{foo}/all_merged.txt"
        input: my_input_func  # some function that yields 1 or more files to merge
        run:
            if len(input) > 1:
                shell("cat {input} | sort > {output}")
            else:
                shell("ln -sr {input} {output}")

Do be careful with symlinks in combination with :ref:`tutorial_temp-and-protected-files`.
When the original file is deleted, this can cause various errors once the symlink does not point to a valid file any more.

If you get a message like ``Unable to set utime on symlink .... Your Python build does not support it.`` this means that Snakemake is unable to properly adjust the modification time of the symlink.
In this case, a workaround is to add the shell command `touch -h {output}` to the end of the rule.

Can the input of a rule be a symlink?
-------------------------------------

Yes.  In this case, since Snakemake 3.8, one extra consideration is applied.  If *either* the link itself or the target of the link is newer than the output files for the rule then it will trigger the rule to be re-run.

I would like to receive a mail upon snakemake exit. How can this be achieved?
-----------------------------------------------------------------------------

On unix, you can make use of the commonly pre-installed `mail` command:

.. code-block:: bash

    snakemake 2> snakemake.log
    mail -s "snakemake finished" youremail@provider.com < snakemake.log

In case your administrator does not provide you with a proper configuration of the sendmail framework, you can configure `mail` to work e.g. via Gmail (see `here <https://www.cyberciti.biz/tips/linux-use-gmail-as-a-smarthost.html>`__).

I want to pass Python variables between rules. Is that possible?
----------------------------------------------------------------

Because of the cluster support and the ability to resume a workflow where you stopped last time, Snakemake in general should be used in a way that information is stored in the output files of your jobs.
A common approach to pass non file variable data between rules is to use json or parquet for writing in the one rule and reading in a consuming rule the variable shall be passed to.

Why do my global variables behave strangely when I run my job on a cluster?
---------------------------------------------------------------------------

This is closely related to the question above.  Any Python code you put outside of a rule definition is normally run once before Snakemake starts to process rules, but on a cluster it is re-run again for each submitted job, because Snakemake implements jobs by re-running itself.

Consider the following...

.. code-block:: python

    from mydatabase import get_connection

    dbh = get_connection()
    latest_parameters = dbh.get_params().latest()

    rule a:
        input: "{foo}.in"
        output: "{foo}.out"
        shell: "do_op -params {latest_parameters}  {input} {output}"


When run a single machine, you will see a single connection to your database and get a single value for *latest_parameters* for the duration of the run.  On a cluster you will see a connection attempt from the cluster node for each job submitted, regardless of whether it happens to involve rule a or not, and the parameters will be recalculated for each job.

I want to configure the behavior of my shell for all rules. How can that be achieved with Snakemake?
----------------------------------------------------------------------------------------------------

You can set a prefix that will prepended to all shell commands by adding e.g.

.. code-block:: python

    shell.prefix("set -o pipefail; ")

to the top of your Snakefile. Make sure that the prefix ends with a semicolon, such that it will not interfere with the subsequent commands.
To simulate a bash login shell, you can do the following:

.. code-block:: python

    shell.executable("/bin/bash")
    shell.prefix("source ~/.bashrc; ")

Some command line arguments like --config cannot be followed by rule or file targets. Is that intended behavior?
----------------------------------------------------------------------------------------------------------------

This is a limitation of the argparse module, which cannot distinguish between the perhaps next arg of ``--config`` and a target.
As a solution, you can put the `--config` at the end of your invocation, or prepend the target with a single ``--``, i.e.


.. code-block:: console

    $ snakemake --config foo=bar -- mytarget
    $ snakemake mytarget --config foo=bar


How do I enforce config values given at the command line to be interpreted as strings?
--------------------------------------------------------------------------------------

When passing config values like this

.. code-block:: console

    $ snakemake --config version=2018_1

Snakemake will first try to interpret the given value as number.
Only if that fails, it will interpret the value as string.
Here, it does not fail, because the underscore `_` is interpreted as thousand separator.
In order to ensure that the value is interpreted as string, you have to pass it in quotes.
Since bash otherwise automatically removes quotes, you have to also wrap the entire entry into quotes, e.g.:

.. code-block:: console

    $ snakemake --config 'version="2018_1"'


How do I make my rule fail if an output file is empty?
------------------------------------------------------

Snakemake expects shell commands to behave properly, meaning that failures should cause an exit status other than zero. If a command does not exit with a status other than zero, Snakemake assumes everything worked fine, even if output files are empty. This is because empty output files are also a reasonable tool to indicate progress where no real output was produced. However, sometimes you will have to deal with tools that do not properly report their failure with an exit status. Here, you can use the :ref:`ensure function <snakefiles_ensure>` to mark output files that should not be empty, e.g.:

.. code-block:: python

    rule NAME:
        input:  ...
        output:
            ensure("test.txt", non_empty=True)
        shell:
            "somecommand {input} {output}"


How does Snakemake lock the working directory?
----------------------------------------------

Per default, Snakemake will lock a working directory by output and input files. Two Snakemake instances that want to create the same output file are not possible. Two instances creating disjoint sets of output files are possible.
With the command line option ``--nolock``, you can disable this mechanism on your own risk. With ``--unlock``, you can be remove a stale lock. Stale locks can appear if your machine is powered off with a running Snakemake instance.



How do I trigger re-runs for rules with updated code or parameters?
-------------------------------------------------------------------

Similar to the solution above, you can use

.. code-block:: console

    $ snakemake -n -R `snakemake --list-params-changes`

and

.. code-block:: console


    $ snakemake -n -R `snakemake --list-code-changes`

Again, the list commands in backticks return the list of output files with changes, which are fed into ``-R`` to trigger a re-run.


How do I remove all files created by snakemake, i.e. like ``make clean``
------------------------------------------------------------------------

To remove all files created by snakemake as output files to start from scratch, you can use

.. code-block:: console

    $ snakemake some_target --delete-all-output

Only files that are output of snakemake rules will be removed, not those that serve as primary inputs to the workflow.
Note that this will only affect the files involved in reaching the specified target(s).
It is strongly advised to first run together with ``--dry-run`` to list the files that would be removed without actually deleting anything.
The flag ``--delete-temp-output`` can be used in a similar manner to only delete files flagged as temporary.


Why can't I use the conda directive with a run block?
-----------------------------------------------------

The run block of a rule (see :ref:`snakefiles-rules`) has access to anything defined in the Snakefile, outside of the rule.
Hence, it has to share the conda environment with the main Snakemake process.
To avoid confusion we therefore disallow the conda directive together with the run block.
It is recommended to use the script directive instead (see :ref:`snakefiles-external_scripts`).


My workflow is very large, how do I stop Snakemake from printing all this rule/job information in a dry-run?
------------------------------------------------------------------------------------------------------------

Indeed, the information for each individual job can slow down a dry-run if there are tens of thousands of jobs.
If you are just interested in the final summary, you can use the ``--quiet`` flag to suppress this.

.. code-block:: console

    $ snakemake -n --quiet

Git is messing up the modification times of my input files, what can I do?
--------------------------------------------------------------------------

When you checkout a git repository, the modification times of updated files are set to the time of the checkout.
If you rely on these files as input **and** output files in your workflow, this can cause trouble.
For example, Snakemake could think that a certain (git-tracked) output has to be re-executed, just because its input has been checked out a bit later.
In such cases, it is advisable to set the file modification dates to the last commit date after an update has been pulled.
One solution is to add the following lines to your ``.bashrc`` (or similar):

.. code-block:: bash

    gitmtim(){
        local f
        for f; do
            touch -d @0`git log --pretty=%at -n1 -- "$f"` "$f"
        done
    }
    gitmodtimes(){
        for f in $(git ls-tree -r $(git rev-parse --abbrev-ref HEAD) --name-only); do
            gitmtim $f
        done
    }

(inspired by the answer `here <https://stackoverflow.com/questions/2458042/restore-files-modification-time-in-git/22638823#22638823>`__).
You can then run ``gitmodtimes`` to update the modification times of all tracked files on the current branch to their last commit time in git; BE CAREFUL--this does not account for local changes that have not been committed.

How do I exit a running Snakemake workflow?
-------------------------------------------

There are two ways to exit a currently running workflow.

1. If you want to kill all running jobs, hit Ctrl+C. Note that when using ``--cluster``, this will only cancel the main Snakemake process.
2. If you want to stop the scheduling of new jobs and wait for all running jobs to be finished, you can send a TERM signal, e.g., via

   .. code-block:: bash

       killall -TERM snakemake

How can I make use of node-local storage when running cluster jobs?
-------------------------------------------------------------------
When running jobs on a cluster you might want to make use of a node-local scratch
directory in order to reduce cluster network traffic and/or get more efficient disk
storage for temporary files. There is currently no way of doing this in Snakemake,
but a possible workaround involves the ``shadow`` directive and setting the
``--shadow-prefix`` flag to e.g. ``/scratch``.

.. code-block:: python

  rule:
      output:
          "some_summary_statistics.txt"
      shadow: "minimal"
      shell:
          """
          generate huge_file.csv
          summarize huge_file.csv > {output}
          """

The following would then lead to the job being executed in ``/scratch/shadow/some_unique_hash/``, and the
temporary file ``huge_file.csv`` could be kept at the compute node.

.. code-block:: console

   $ snakemake --shadow-prefix /scratch some_summary_statistics.txt --cluster ...

If you want the input files of your rule to be copied to the node-local scratch directory
instead of just using symbolic links, you can use ``copy-minimal`` in the ``shadow`` directive.
This is useful for example for benchmarking tools as a black-box.

.. code-block:: python

  rule:
      input:
          "input_file.txt"
      output:
          file = "output_file.txt",
          benchmark = "benchmark_results.txt",
      shadow: "copy-minimal"
      shell:
          """
          /usr/bin/time -v command "{input}" "{output.file}" > "{output.benchmark}"
          """

Executing snakemake as above then leads to the shell script accessing only node-local storage.

How do I access elements of input or output by a variable index?
----------------------------------------------------------------

Assuming you have something like the following rule

   .. code-block:: python

      rule a:
          output:
              expand("test.{i}.out", i=range(20))
          run:
              for i in range(20):
                  shell("echo test > {output[i]}")

Snakemake will fail upon execution with the error ``'OutputFiles' object has no attribute 'i'``. The reason is that the shell command is using the `Python format mini language <https://docs.python.org/3/library/string.html#formatspec>`_, which only allows indexing via constants, e.g., ``output[1]``, but not via variables. Variables are treated as attribute names instead. The solution is to write

   .. code-block:: python

      rule a:
          output:
              expand("test.{i}.out", i=range(20))
          run:
              for i in range(20):
                  f = output[i]
                  shell("echo test > {f}")

or, more concise in this special case:

   .. code-block:: python

      rule a:
          output:
              expand("test.{i}.out", i=range(20))
          run:
              for f in output:
                  shell("echo test > {f}")

There is a compiler error when installing Snakemake with pip or easy_install, what shall I do?
----------------------------------------------------------------------------------------------

Snakemake itself is plain Python, hence the compiler error must come from one of the dependencies.
You should have a look if maybe you are missing some library or a certain compiler package.
If everything seems fine, please report to the upstream developers of the failing dependency.

Note that in general it is recommended to install Snakemake via `Conda <https://conda.io>`_ which gives you precompiled packages and the additional benefit of having :ref:`automatic software deployment <integrated_package_management>` integrated into your workflow execution.

How to enable autocompletion for the zsh shell?
-----------------------------------------------

For users of the `Z shell <https://www.zsh.org/>`_ (zsh), just run the following (assuming an activated zsh) to activate autocompletion for snakemake:

.. code-block:: console

    compdef _gnu_generic snakemake

Example:
Say you have forgotten how to use the various options starting ``force``, just type the partial match i.e. ``--force`` which results in a list of all potential hits along with a description:


.. code-block:: console

    $snakemake --force**pressing tab**

    --force              -- Force the execution of the selected target or the
    --force-use-threads  -- Force threads rather than processes. Helpful if shared
    --forceall           -- Force the execution of the selected (or the first)
    --forcerun           -- (TARGET (TARGET ...)), -R (TARGET (TARGET ...))

To activate this autocompletion permanently, put this line in ``~/.zshrc``.

`Here <https://github.com/zsh-users/zsh-completions/blob/master/zsh-completions-howto.org>`__ is some further reading.

How can I avoid system /tmp to be used when combining apptainer and conda?
--------------------------------------------------------------------------

When using both apptainer and conda the idea is that inside the apptainer container the conda environment is being installed.
Some apptainer instances are set to share the system /tmp with the containers.
This can lead to unexpected behaviour where the system /tmp gets full.
To stop this behaviour you'd have to run apptainer with the ``--contain`` option. 


.. _consider_ancient:

Snakemake wants to rerun a rule that has been already executed, what can I do?
------------------------------------------------------------------------------

Snakemake tries to ensure consistency between input and output files.
This is based on file modification dates (input files may not be newer than output files of the same job), as well as execution metadata like the used software stack (e.g. conda env or container image), the non-file parameters, the set of input files, and the code of the rule.
If Snakemake wants to rerun a rule that has been already executed, it is because one of these criteria has changed and detailed information about the reasoning is given in the job description of Snakemake's output as well as in the final summary at the end of a dry-run.

If your job is triggered by newer input files, but you are sure that the input files did not change on a semantic level (i.e. won't yield different results), you can mark those input files as ancient via the command line, or (usually better) via a :ref:`workflow specific profile <profiles>`.
Let us assume you have the following rule from which such an unwanted job is triggered:

.. code-block:: python

    rule myrule:
        input:
            foo="inputfile.txt"
        output:
            "outputfile.txt"
        shell:
            "somecommand {input.foo} > {output}"

In case of directly using the command line option, you can run Snakemake like this:

.. code-block:: console

    $ snakemake --consider-ancient myrule=foo

This will mark the file ``inputfile.txt`` as ancient for the rule ``myrule``.
If the setting shall be persisted for all upcoming runs of Snakemake, you can store it e.g. in the default workflow specific profile (``profiles/default/config.yaml``), which will be automatically considered when being present in a working directory:

.. code-block:: python

    consider-ancient:
        myrule: foo

If the input file is not named (does not have something like ``foo=`` in front of it), you can instead refer it by index, i.e.:

.. code-block:: console

    $ snakemake --consider-ancient myrule=0

Or alternatively in the profile:

.. code-block:: python

    consider-ancient:
        myrule: 0


================================================
FILE: docs/project_info/history.md
================================================
(project_info-history)=

(_changelog)=

```{include} ../../CHANGELOG.md
```



================================================
FILE: docs/project_info/license.rst
================================================
.. project_info-license:

=======
License
=======

Snakemake is licensed under the MIT License:

.. literalinclude:: ../../LICENSE.md
    :language: text


================================================
FILE: docs/project_info/more_resources.rst
================================================
.. _project_info-more_resources:

==============
More Resources
==============

.. _project_info-talks_and_posters:

-----------------
Talks and Posters
-----------------

* `Poster at ECCB 2016, The Hague, Netherlands. <https://johanneskoester.bitbucket.io/posters/snakemake+bioconda-2016.pdf>`_
* `Invited talk by Johannes Köster at the Broad Institute, Boston 2015. <https://slides.com/johanneskoester/snakemake-broad-2015>`_
* `Introduction to Snakemake. Tutorial Slides presented by Johannes Köster at the GCB 2015, Dortmund, Germany. <https://slides.com/johanneskoester/deck-1>`_
* `Invited talk by Johannes Köster at the DTL Focus Meeting: "NGS Production Pipelines", Dutch Techcentre for Life Sciences, Utrecht 2014. <https://speakerdeck.com/johanneskoester/workflow-management-with-snakemake>`_
* `Taming Snakemake by Jeremy Leipzig, Bioinformatics software developer at Children's Hospital of Philadelphia, 2014. <https://de.slideshare.net/jermdemo/taming-snakemake>`_
* `"Snakemake makes ... snakes?" - An Introduction by Marcel Martin from SciLifeLab, Stockholm 2015 <https://marcelm.net/talks/2015/snakemake>`_
* `"Workflow Management with Snakemake" by Johannes Köster, 2015. Held at the Department of Biostatistics and Computational Biology, Dana-Farber Cancer Institute <https://speakerdeck.com/johanneskoester/workflow-management-with-snakemake-1>`_


.. _project_info-external_resources:

------------------
External Resources
------------------

These resources are not part of the official documentation.

* `A number of tutorials on the subject "Tools for reproducible research" <https://nbis-reproducible-research.readthedocs.io>`_
* `Snakemake workflow used for the Kallisto paper <https://github.com/pachterlab/kallisto_paper_analysis>`_
* `An alternative tutorial for Snakemake <https://slowkow.com/notes/snakemake-tutorial/>`_
* `An Emacs mode for Snakemake <https://melpa.org/#/snakemake-mode>`_
* `Flexible bioinformatics pipelines with Snakemake <http://watson.nci.nih.gov/~sdavis/blog/flexible_bioinformatics_pipelines_with_snakemake/>`_
* `Sandwiches with Snakemake <https://github.com/leipzig/SandwichesWithSnakemake>`_
* `A visualization of the past years of Snakemake development <https://youtu.be/bq3vXrWw1yk>`_
* `Japanese version of the Snakemake tutorial <https://github.com/joemphilips/Translate_Snakemake_Tutorial>`_
* `Basic <https://bioinfo-fr.net/snakemake-pour-les-nuls>`_ and `advanced <https://bioinfo-fr.net/snakemake-aller-plus-loin-avec-la-parallelisation>`_ french Snakemake tutorial.
* `Mini tutorial on Snakemake and Bioconda <https://github.com/dlaehnemann/TutMinicondaSnakemake>`_
* `Snakeparse: a utility to expose Snakemake workflow configuration via a command line interface <https://github.com/nh13/snakeparse>`_



================================================
FILE: docs/snakefiles/best_practices.rst
================================================
.. _snakefiles-best_practices:

==============
Best practices
==============

Care about code quality
-----------------------

Snakemake (>=5.11) comes with a code quality checker (a so called linter), that analyzes your workflow and highlights issues that should be solved in order to follow best practices, achieve maximum readability, and reproducibility.
The linter can be invoked with 

.. code-block:: bash

      snakemake --lint

given that a ``Snakefile`` or ``workflow/Snakefile`` is accessible from your working directory.
It is **highly recommended** to run the linter before publishing any workflow, asking questions on Stack Overflow or filing issues on Github.

Care about code readability
---------------------------

1. There is an automatic formatter for Snakemake workflows, called `Snakefmt <https://github.com/snakemake/snakefmt>`_, which should be applied to any Snakemake workflow before publishing it.
2. Try to keep filenames short (thus easier on the eye), but informative. Avoid mixing of too many special characters (e.g. decide whether to use ``_`` or ``-`` as a separator and do that consistently throughout the workflow).
3. Try to keep Python code like helper functions separate from rules (e.g. in a ``workflow/rules/common.smk`` file). This way, you help non-experts to read the workflow without needing to parse internals that are irrelevant for them. The helper function names should be chosen in a way that makes them sufficiently informative without looking at their content. Also avoid ``lambda`` expressions inside of rules.
4. Use Snakemake's :ref:`semantic helper functions <snakefiles-semantic-helpers>` in order to increase readability and to avoid the reimplementation of common functionality for aggregation, parameter lookup or path modifications.

Ensure portability
------------------

Annotate all your rules with versioned :ref:`Conda <integrated_package_management>` or :ref:`container <apptainer>` based software environment definitions. This ensures that your workflow utilizes the exactly same isolated software stacks, independently of the underlying system.

Generate interactive reports (for free)
---------------------------------------

Annotate your final results for including into Snakemake's automatic :ref:`interactive reports <snakefiles-reports>` (thereby make sure to use all the features, including categories and labels).
This makes them explorable in a high-level way, while connecting them to the workflow code, parameters, and software stack.

Enable configurability
----------------------

Configuration of a workflow should be handled via :ref:`config files <snakefiles_standard_configuration>` and, if needed, tabular configuration like sample sheets (either via :ref:`Pandas <snakefiles_tabular_configuration>` or :ref:`PEPs <snakefiles-peps>`).
Use such configuration for metadata and experiment information, **not for runtime specific configuration** like threads, resources and output folders.
For those, just rely on Snakemake's CLI arguments like ``--set-threads``, ``--set-resources``, ``--set-default-resources``, and ``--directory``. 
This makes workflows more readable, scalable, and portable.

Avoid duplication of efforts
----------------------------

Make use of `Snakemake wrappers <https://snakemake-wrappers.readthedocs.io>`_ whenever possible. Consider contributing to the wrapper repo whenever you have a rule that reoccurs in at least two of your workflows.

Test your workflow continuously
-------------------------------

When hosting your workflow in a `Github <https://github.com>`_ repository, it is a good idea to add some minimal test data and configure `Github Actions <https://github.com/features/actions>`_ for continuously testing the workflow on each new commit.For this purpose, we provide predefined Github actions for both running tests and linting `here <https://github.com/snakemake/snakemake-github-action>`__, as well as formatting `here <https://github.com/snakemake/snakefmt#github-actions>`__.

Follow the standards
--------------------

1. For publishing and distributing a Snakemake workflow, it is a good idea to stick to a :ref:`standardized folder structure <distribution_and_reproducibility>` that is expected by frequent users of Snakemake. This simplifies the navigation through the codebase and keeps the workflow repository and the working directory clean.
2. The `Snakemake workflow catalog <https://snakemake.github.io/snakemake-workflow-catalog>`_ automatically lists Snakemake workflows hosted on `Github <https://github.com>`_ if they follow certain `rules <https://snakemake.github.io/snakemake-workflow-catalog/?rules=true>`_.
   By complying to these `rules <https://snakemake.github.io/snakemake-workflow-catalog/?rules=true>`_ you can make your workflow more discoverable and even automate its usage documentation (see `"Standardized usage" <https://snakemake.github.io/snakemake-workflow-catalog/?rules=true>`_).


Simplify maintenance
--------------------

Use `Snakedeploy <https://snakedeploy.readthedocs.io>`_ to simplify and automate the maintenance of your Snakemake workflows, or to deploy publicly available workflows for application on your own data.


================================================
FILE: docs/snakefiles/configuration.rst
================================================
.. _snakefiles_configuration:

=============
Configuration
=============

Snakemake allows you to use configuration files for making your workflows more flexible and also for abstracting away direct dependencies to a fixed HPC cluster scheduler.


.. _snakefiles_standard_configuration:

----------------------
Standard Configuration
----------------------

Snakemake directly supports the configuration of your workflow.
A configuration is provided as a JSON or YAML file and can be loaded with:

.. code-block:: python

    configfile: "path/to/config.yaml"

The given path is interpreted relative to the working directory, not relative to the location of the snakefile that contains the statement.
The config file can be used to define a dictionary of configuration parameters and their values.
In case of YAML, the file can optionally be processed with `YTE <https://yte-template-engine.github.io>`_.
To activate this, you have to add the top-level key ``__use_yte__ = true`` to the YAML file.

In the workflow, the configuration is accessible via the global variable `config`, e.g.

.. code-block:: python

    rule all:
        input:
            expand("{sample}.{param}.output.pdf", sample=config["samples"], param=config["yourparam"])

If the `configfile` statement is not used, the config variable provides an empty dictionary.
In addition to the `configfile` statement, config values can be overwritten via the command line or the `snakemake.utils API <https://snakemake-api.readthedocs.io/en/stable/api_reference/snakemake_utils.html#snakemake.utils.update_config>`__, e.g.:

.. code-block:: console

    $ snakemake --config yourparam=1.5

Further, you can manually alter the config dictionary using any Python code **outside** of your rules. Changes made from within a rule won't be seen from other rules.
Finally, you can use the ``--configfile`` command line argument to overwrite values from the `configfile` statement.
Note that any values parsed into the ``config`` dictionary with any of above mechanisms are merged, i.e., all keys defined via a ``configfile``
statement, or the ``--configfile`` and ``--config`` command line arguments will end up in the final `config` dictionary, but if two methods define the same key, command line
overwrites the ``configfile`` statement.

For adding config placeholders into a shell command, Python string formatting syntax requires you to leave out the quotes around the key name, like so:

.. code-block:: python

    shell:
        "mycommand {config[foo]} ..."

.. _snakefiles_tabular_configuration:

---------------------
Tabular configuration
---------------------

It is usually advisable to complement YAML based configuration (see above) by a sheet based approach for meta-data that is of tabular form. For example, such
a sheet can contain per-sample information.
With the `Pandas library <https://pandas.pydata.org/>`_ such data can be read and used with minimal overhead, e.g.,

.. code-block:: python

    import pandas as pd

    samples = pd.read_table("samples.tsv").set_index("samples", drop=False)

reads in a table ``samples.tsv`` in TSV format and makes every record accessible by the sample name.
For details, see the `Pandas documentation <https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html?highlight=read_table#pandas-read-table>`_.
A fully working real-world example containing both types of configuration can be found `here <https://github.com/snakemake-workflows/rna-seq-star-deseq2>`_.

---------------------
Environment variables
---------------------

Sometimes, it is not desirable to put configuration information into text files.
For example, this holds for secrets like access tokens or passwords.
Here, `environment variables <https://en.wikipedia.org/wiki/Environment_variable>`_ are the method of choice.
Snakemake allows to assert the existence of environment variables by adding a statement like:

.. code-block:: python

    envvars:
        "SOME_VARIABLE",
        "SOME_OTHER_VARIABLE"

When executing, Snakemake will fail with a reasonable error message if the variables ``SOME_VARIABLE`` and ``SOME_OTHER_VARIABLE`` are undefined.
Otherwise, it will take care of passing them to cluster and cloud environments. However, note that this does **not** mean that Snakemake makes them available e.g. in the jobs shell command.
Instead, for data provenance and reproducibility reasons, you are required to pass them explicitly to your job via the params directive, e.g. like this:

.. code-block:: python

    envvars:
        "SOME_VARIABLE"

    rule do_something:
        output:
             "test.txt"
        params:
            x=os.environ["SOME_VARIABLE"]
        shell:
            "echo {params.x} > {output}"


.. _snakefiles_config_validation:

----------
Validation
----------

With Snakemake 5.1, it is possible to validate both types of configuration (standard and tabular) via `JSON schemas <https://json-schema.org>`_.
The function ``snakemake.utils.validate`` takes a loaded configuration (a config dictionary, a Pandas DataFrame, Polars DataFrame or Polars LazyFrame) and validates it with a given JSON schema.
Thereby, the schema can be provided in JSON or YAML format. Also, by using the defaults property it is possible to populate entries with default values. See `jsonschema FAQ on setting default values <https://python-jsonschema.readthedocs.io/en/latest/faq/>`_ for details.
In case of the data frame, the schema should model the record that is expected in each row of the data frame.
In the following example,

.. code-block:: python

  import pandas as pd
  from snakemake.utils import validate

  configfile: "config.yaml"
  validate(config, "config.schema.yaml")

  samples = pd.read_table(config["samples"]).set_index("sample", drop=False)
  validate(samples, "samples.schema.yaml")


  rule all:
      input:
          expand("test.{sample}.txt", sample=samples.index)


  rule a:
      output:
          "test.{sample}.txt"
      shell:
          "touch {output}"

the schema for validating the samples data frame looks like this:

.. code-block:: yaml

  $schema: "https://json-schema.org/draft-06/schema#"
  description: an entry in the sample sheet
  properties:
    sample:
      type: string
      description: sample name/identifier
    condition:
      type: string
      description: sample condition that will be compared during differential expression analysis (e.g. a treatment, a tissue time, a disease)
    case:
      type: boolean
      default: true
      description: boolean that indicates if sample is case or control

  required:
    - sample
    - condition

Here, in case the case column is missing, the validate function will
populate it with True for all entries.

.. _snakefiles-peps:

-------------------------------------------
Configuring scientific experiments via PEPs
-------------------------------------------

Often scientific experiments consist of a set of samples (with optional subsamples), for which raw data and metainformation is known.
Instead of writing custom sample sheets as shown above, Snakemake allows to use `portable encapsulated project (PEP) <http://pep.databio.org>`_ definitions to configure a workflow.
This is done via a special directive `pepfile`, that can optionally complemented by a schema for validation (which is recommended for production workflows):

.. code-block:: python

    pepfile: "pep/config.yaml"
    pepschema: "schemas/pep.yaml"

    rule all:
        input:
            expand("{sample}.txt", sample=pep.sample_table["sample_name"])

    rule a:
        output:
            "{sample}.txt"
        shell:
            "touch {output}"

Using the ``pepfile`` directive leads to parsing of the provided PEP with `peppy <http://peppy.databio.org>`_.
The resulting project object is made globally available under the name ``pep``.
Here, we use it to aggregate over the set of sample names that is defined in the corresponding PEP.

**Importantly**, note that PEPs are meant to contain sample metadata and any global information about a project or experiment. 
They should **not** be used to encode workflow specific configuration options.
For those, one should always complement the pepfile with an ordinary :ref:`config file <snakefiles_standard_configuration>`.
The rationale is that PEPs should be portable between different data analysis workflows (that could be applied to the same data) and even between workflow management systems.
In other words, a PEP should describe everything needed about the data, while a workflow and its configuration should describe everything needed about the analysis that is applied to it.

^^^^^^^^^^^^^^^
Validating PEPs
^^^^^^^^^^^^^^^

Using the ``pepschema`` directive leads to an automatic parsing of the provided schema *and* PEP validation with the PEP validation tool -- `eido <http://eido.databio.org>`_. Eido schemas extend `JSON Schema <https://json-schema.org>`_ vocabulary to accommodate the powerful PEP features. Follow the `How to write a PEP schema <http://eido.databio.org/en/latest/writing-a-schema>`_ guide to learn more.

---------------------------
Configure Working Directory
---------------------------

All paths in the snakefile are interpreted relative to the directory snakemake is executed in. This behaviour can be overridden by specifying a workdir in the snakefile:

.. code-block:: python

    workdir: "path/to/workdir"

Usually, it is preferred to only set the working directory via the command line, because above directive limits the portability of Snakemake workflows.


.. _snakefiles-cluster_configuration:

---------------------------------------------
Cluster Configuration (not supported anymore)
---------------------------------------------

The previously supported cluster configuration has been replaced by configuration profiles (see :ref:`executing-profiles`).



================================================
FILE: docs/snakefiles/deployment.rst
================================================
.. _Mamba: https://github.com/mamba-org/mamba

.. _distribution_and_reproducibility:

================================
Distribution and Reproducibility
================================

It is recommended to store each workflow in a dedicated git repository of the
following structure:

.. code-block:: none

    ├── .gitignore
    ├── README.md
    ├── LICENSE.md
    ├── workflow
    │   ├── rules
    |   │   ├── module1.smk
    |   │   └── module2.smk
    │   ├── envs
    |   │   ├── tool1.yaml
    |   │   └── tool2.yaml
    │   ├── scripts
    |   │   ├── script1.py
    |   │   └── script2.R
    │   ├── notebooks
    |   │   ├── notebook1.py.ipynb
    |   │   └── notebook2.r.ipynb
    │   ├── report
    |   │   ├── plot1.rst
    |   │   └── plot2.rst
    |   └── Snakefile
    ├── config
    │   ├── config.yaml
    │   └── some-sheet.tsv
    ├── results
    └── resources

In other words, the workflow code goes into a subfolder ``workflow``, while the configuration is stored in a subfolder ``config``.
Inside of the ``workflow`` subfolder, the central ``Snakefile`` marks the entrypoint of the workflow (it will be automatically discovered when running snakemake from the root of above structure.
This main structure and the recommendations below are implemented in `this Snakemake workflow template <https://github.com/snakemake-workflows/snakemake-workflow-template>`_ that you can use to `create your own workflow repository with a single click on "Use this template" <https://github.com/snakemake-workflows/snakemake-workflow-template/generate>`_.
In addition to the central ``Snakefile``, rules can be stored in a modular way, using the optional subfolder ``workflow/rules``.
Such modules should end with ``.smk``, the recommended file extension of Snakemake.
Further, :ref:`scripts <snakefiles-external_scripts>` should be stored in a subfolder ``workflow/scripts`` and notebooks in a subfolder ``workflow/notebooks``.
Conda environments (see :ref:`integrated_package_management`) should be stored in a subfolder ``workflow/envs`` (make sure to keep them as finegrained as possible to improve transparency and maintainability).
Finally, :ref:`report caption files <snakefiles-reports>` should be stored in ``workflow/report``.
All output files generated in the workflow should be stored under ``results``, unless they are rather retrieved resources, in which case they should be stored under ``resources``.
The latter subfolder may also contain small resources that shall be delivered along with the workflow via git (although it might be tempting, please refrain from trying to generate output file paths with string concatenation of a central ``outdir`` variable or so, as this hampers readability).

Workflows set up in above structure can be easily used and combined via :ref:`the Snakemake module system <use_with_modules>`.
Such deployment can even be automated via  `Snakedeploy <https://snakedeploy.readthedocs.io>`_.
Moreover, by publishing a workflow on `Github <https://github.com>`_ and following a set of additional `rules <https://snakemake.github.io/snakemake-workflow-catalog/?rules=true>`_ the workflow will be automatically included in the `Snakemake workflow catalog <https://snakemake.github.io/snakemake-workflow-catalog>`_, thereby easing discovery and even automating its usage documentation.
For an example of such automated documentation, see `here <https://snakemake.github.io/snakemake-workflow-catalog/?usage=snakemake-workflows%2Fdna-seq-varlociraptor>`_.

Visit the `Snakemake Workflows Project <https://github.com/snakemake-workflows/docs>`_ for more best-practice workflows.

.. _use_with_modules:

-----------------------------------------
Using and combining pre-exising workflows
-----------------------------------------

Via the :ref:`module/use <snakefiles-modules>` system introduced with Snakemake 6.0, it is very easy to deploy existing workflows for new projects.
This ranges from the simple application to new data to the complex combination of several complementary workflows in order to perform an integrated analysis over multiple data types.

Consider the following example:

.. code-block:: python

    from snakemake.utils import min_version
    min_version("6.0")

    configfile: "config/config.yaml"

    module dna_seq:
        snakefile:
            # here, it is also possible to provide a plain raw URL like "https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling/raw/v2.0.1/workflow/Snakefile"
            github("snakemake-workflows/dna-seq-gatk-variant-calling", path="workflow/Snakefile", tag="v2.0.1")
        config:
            config

    use rule * from dna_seq

First, we load a local configuration file.
Next, we define the module ``dna_seq`` to be loaded from the URL ``https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling/raw/v2.0.1/workflow/Snakefile``, while using the contents of the local configuration file.
Note that it is possible to either specify the full URL pointing to the raw Snakefile as a string or to use the github marker as done here.
With the latter, Snakemake can however cache the used source files persistently (if a tag is given), such that they don't have to be downloaded on each invocation.
Finally we declare all rules of the dna_seq module to be used.

This kind of deployment is equivalent to just cloning the original repository and modifying the configuration in it.
However, the advantage here is that we are (a) able to easily extend of modify the workflow, while making the changes transparent, and (b) we can store this workflow in a separate (e.g. private) git repository, along with for example configuration and meta data, without the need to duplicate the workflow code.
Finally, we are always able to later combine another module into the current workflow, e.g. when further kinds of analyses are needed.
The ability to modify rules upon using them (see :ref:`snakefiles-modules`) allows for arbitrary rewiring and configuration of the combined modules.

For example, we can easily add another rule to extend the given workflow:

.. code-block:: python

    from snakemake.utils import min_version
    min_version("6.0")

    configfile: "config/config.yaml"

    module dna_seq:
        snakefile:
            # here, it is also possible to provide a plain raw URL like "https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling/raw/v2.0.1/workflow/Snakefile"
            github("snakemake-workflows/dna-seq-gatk-variant-calling", path="workflow/Snakefile", tag="v2.0.1")
        config: config

    use rule * from dna_seq as dna_seq_*

    # easily extend the workflow
    rule plot_vafs:
        input:
            "filtered/all.vcf.gz"
        output:
            "results/plots/vafs.svg"
        notebook:
            "notebooks/plot-vafs.py.ipynb"

    # Define a new default target that collects both the targets from the dna_seq module as well as
    # the new plot.
    rule all:
        input:
            rules.dna_seq_all.input,
            "results/plots/vafs.svg",
        default_target: True

Above, we have added a prefix to all rule names of the dna_seq module, such that there is no name clash with the added rules (``as dna_seq_*`` in the ``use rule`` statement).
In addition, we have added a new rule ``all``, defining the default target in case the workflow is executed (as usually) without any specific target files or rule.
The new target rule collects both all input files of the rule ``all`` from the dna_seq workflow, as well as additionally collecting the new plot.

It is possible to further extend the workflow with other modules, thereby generating an integrative analysis.
Here, let us assume that we want to conduct another kind of analysis, say RNA-seq, using a different external workflow.
We can extend above example in the following way:

.. code-block:: python

    from snakemake.utils import min_version
    min_version("6.0")

    configfile: "config/config.yaml"

    module dna_seq:
        snakefile:
            github("snakemake-workflows/dna-seq-gatk-variant-calling", path="workflow/Snakefile", tag="v2.0.1")
        config: config["dna-seq"]
        prefix: "dna-seq"

    use rule * from dna_seq as dna_seq_*

    rule plot_vafs:
        input:
            "filtered/all.vcf.gz"
        output:
            "results/plots/vafs.svg"
        notebook:
            "notebooks/plot-vafs.py.ipynb"

    module rna_seq:
        snakefile:
            github("snakemake-workflows/rna-seq-kallisto-sleuth", path="workflow/Snakefile", tag="v2.0.1")
        config: config["rna-seq"]
        prefix: "rna-seq"

    use rule * from rna_seq as rna_seq_*


    # Define a new default target that collects all the targets from the dna_seq and rna_seq module.
    rule all:
        input:
            rules.dna_seq_all.input,
            rules.rna_seq_all.input,
        default_target: True

Above, several things have changed.

* First, we have added another module ``rna_seq``.
* Second, we have added a prefix to all non-absolute input and output file names of both modules (``prefix: "dna-seq"`` and ``prefix: "rna-seq"``) in order to avoid file name clashes.
* Third, we have added a default target rule that collects both the default targets from the module ``dna_seq`` as well as the module ``rna_seq``.
* Finally, we provide the config of the two modules via two separate sections in the common config file (``config["dna-seq"]`` and ``config["rna-seq"]``).

----------------------------------
Uploading workflows to WorkflowHub
----------------------------------

In order to share a workflow with the scientific community it is advised to upload the repository to `WorkflowHub <https://workflowhub.eu/>`_, where each submission will be automatically parsed and encapsulated into a `Research Object Crate <https://w3id.org/ro/crate>`_. That way a *snakemake* workflow is annotated with proper metadata and thus complies with the `FAIR <https://en.wikipedia.org/wiki/FAIR_data>`_ principles of scientific data.

To adhere to the high WorkflowHub standards of scientific workflows the recommended *snakemake* repository structure presented above needs to be extended by the following elements:

- Code of Conduct
- Contribution instructions
- Workflow rule graph
- Workflow documentation
- Test directory

A code of conduct for the repository developers as well as instruction on how to contribute to the project should be placed in the top-level files: ``CODE_OF_CONDUCT.md`` and ``CONTRIBUTING.md``, respectively. Each *snakemake* workflow repository needs to contain an SVG-formatted rule graph placed in a subdirectory ``images/rulegraph.svg``. Additionally, the workflow should be annotated with a technical documentation of all of its subsequent steps, described in ``workflow/documentation.md``. Finally, the repository should contain a ``.tests`` directory with two subdirectories: ``.tests/integration`` and ``.tests/unit``. The former has to contain all the input data, configuration specifications and shell commands required to run an integration test of the whole workflow. The latter shall contain subdirectories dedicated to testing each of the separate workflow steps independently. To simplify the testing procedure *snakemake* can automatically generate unit tests from a successful workflow execution (see :ref:`snakefiles-testing`).

Therefore, the repository structure should comply with:

.. code-block:: none

    ├── .gitignore
    ├── README.md
    ├── LICENSE.md
    ├── CODE_OF_CONDUCT.md
    ├── CONTRIBUTING.md
    ├── .tests
    │   ├── integration
    │   └── unit
    ├── images
    │   └── rulegraph.svg
    ├── workflow
    │   ├── rules
    |   │   ├── module1.smk
    |   │   └── module2.smk
    │   ├── envs
    |   │   ├── tool1.yaml
    |   │   └── tool2.yaml
    │   ├── scripts
    |   │   ├── script1.py
    |   │   └── script2.R
    │   ├── notebooks
    |   │   ├── notebook1.py.ipynb
    |   │   └── notebook2.r.ipynb
    │   ├── report
    |   │   ├── plot1.rst
    |   │   └── plot2.rst
    │   ├── Snakefile
    |   └── documentation.md
    ├── config
    │   ├── config.yaml
    │   └── some-sheet.tsv
    ├── results
    └── resources


.. _integrated_package_management:

-----------------------------
Integrated Package Management
-----------------------------

It is possible (and highly encouraged, see :ref:`snakefiles-best_practices`) to define isolated software environments per rule.
Upon execution of a workflow, the `Conda package manager <https://conda.pydata.org>`_ is used to obtain and deploy the defined software packages in the specified versions. Packages will be installed into your working directory, without requiring any admin/root privileges.
Given that conda is available on your system (see `Miniconda <https://conda.pydata.org/miniconda.html>`_), to use the Conda integration, add the ``--software-deployment-method conda`` option (``--sdm`` for short) to your workflow execution command, e.g. ``snakemake --cores 8 --sdm conda``.
When ``--software-deployment-method conda`` (``--sdm`` for short) is activated, Snakemake will automatically create software environments for any used wrapper (see :ref:`snakefiles-wrappers`).
Further, you can manually define environments via the ``conda`` directive, e.g.:

.. code-block:: python

    rule NAME:
        input:
            "table.txt"
        output:
            "plots/myplot.pdf"
        conda:
            "envs/ggplot.yaml"
        script:
            "scripts/plot-stuff.R"

with the following `environment definition <https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually>`_:


.. code-block:: yaml

    channels:
     - r
    dependencies:
     - r=3.3.1
     - r-ggplot2=2.1.0

Please note that in the environment definition, conda determines the priority of channels depending on their order of appearance in the channels list. For instance, the channel that comes first in the list gets the highest priority. Default packages defined in the user configuration of conda (`.condarc`)) are ignored by Snakemake.

The path to the environment definition is interpreted as **relative to the Snakefile that contains the rule** (unless it is an absolute path, which is discouraged).

Instead of using a concrete path, it is also possible to provide a path containing wildcards (which must also occur in the output files of the rule), analogous to the specification of input files.

In addition, it is possible to use a callable which returns a ``str`` value.
The signature of the callable has to be ``callable(wildcards [, params] [, input])`` (``params`` and ``input`` are optional parameters).

Note that the use of distinct conda environments for different jobs from the same rule is currently not properly displayed in the generated reports.
At the moment, only a single, random conda environment is shown.

.. note::

   Note that conda environments can be used with the ``shell``, ``script``, ``notebook``, ``wrapper`` and ``run`` directives.
   
   However, the ``run`` directive is a special case, as it has access to the rest of the Snakefile (e.g. globally defined variables) and therefore must be executed in the same process as Snakemake itself. 
   The ``conda`` directive for rules with a ``run`` directive therefore only affects ``shell`` function calls that are executed from the within ``run`` script.
   
   If used with ``notebook`` directive, the associated conda environment should have package ``jupyter`` installed (this package contains dependencies required to execute the notebook).

   Further, note that search path modifying environment variables like ``R_LIBS`` and ``PYTHONPATH`` can interfere with your conda environments.
   Therefore, Snakemake automatically deactivates them for a job when a conda environment definition is used.
   If you know what you are doing, in order to deactivate this behavior, you can use the flag ``--conda-not-block-search-path-envvars``.

Snakemake will store the environment persistently in ``.snakemake/conda/$hash`` with ``$hash`` being the MD5 hash of the environment definition file content. This way, updates to the environment definition are automatically detected.
Note that you need to clean up environments manually for now. However, in many cases they are lightweight and consist of symlinks to your central conda installation.

Conda deployment also works well for offline or air-gapped environments. Running ``snakemake --sdm conda --conda-create-envs-only`` will only install the required conda environments without running the full workflow. Subsequent runs with ``--sdm conda`` will make use of the local environments without requiring internet access.

Freezing environments to exactly pinned packages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If Snakemake finds a special file ending on ``<platform>.pin.txt`` next to a conda environment file (with ``<platform>`` being the current platform, e.g. ``linux-64``), it will try to use the contents of that file to determine the conda packages to deploy.
The file is expected to contain conda's `explicit specification file format <https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#building-identical-conda-environments>`_.
Snakemake will first try to deploy the environment using that file, and only if that fails it will use the regular environment file.

This enables to freeze an environment to a certain state, and will ensure that people using a workflow will get exactly the same environments down to the individual package builds, which is in fact very similar to providing the environment encapsulated in a container image.
Generating such pin files for conda environments can be automatically done using `Snakedeploy <https://snakedeploy.readthedocs.io>`_.
Let ``envs/ggplot.yaml`` be the conda environment file used in the example above.
Then, the pinning can be generated with

.. code-block:: bash

    snakedeploy pin-conda-envs envs/ggplot.yaml

Multiple paths to environments can be provided at the same time; also see ``snakedeploy pin-conda-envs --help``.

Of course, it is **important to update the pinnings** whenever the original environment is modified, such that they do not diverge.

Updating environments
~~~~~~~~~~~~~~~~~~~~~

When a workflow contains many conda environments, it can be helpful to automatically update them to the latest versions of all packages.
This can be done automatically via `Snakedeploy <https://snakedeploy.readthedocs.io>`_:

.. code-block:: bash

    snakedeploy update-conda-envs envs/ggplot.yaml

Multiple paths to environments can be provided at the same time; also see ``snakedeploy update-conda-envs --help``.


Providing post-deployment scripts
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

From Snakemake 6.14 onwards post-deployment shell-scripts can be provided to perform additional adjustments of a conda environment.
This might be helpful in case a conda package is missing components or requires further configuration for execution.
Post-deployment scripts must be placed next to their corresponding environment-file and require the suffix ``.post-deploy.sh``, e.g.:

.. code-block:: python

    rule NAME:
        input:
            "seqs.fastq"
        output:
            "results.tsv"
        conda:
            "envs/interproscan.yaml"
        shell:
            "interproscan.sh -i {input} -f tsv -o {output}"

.. code-block:: none

    ├── Snakefile
    └── envs
        ├── interproscan.yaml
        └── interproscan.post-deploy.sh

The path of the conda environment can be accessed within the script via ``$CONDA_PREFIX``.
Importantly, if the script relies on certain shell specific syntax, (e.g. `set -o pipefail` for bash), make sure to add a matching shebang to the script, e.g.:

.. code-block:: bash

    #!env bash
    set -o pipefail
    # ...

If no shebang line like above (``#!env bash``) is provided, the script will be executed with the ``sh`` command.

.. _conda_named_env:

-----------------------------------------
Using already existing conda environments
-----------------------------------------

Sometimes it can be handy to refer to an already existing conda environment from a rule, instead of defining a new one from scratch.
Importantly, one should be aware that this can **hamper reproducibility**, because the workflow then relies on this environment to be present
**in exactly the same way** on any new system where the workflow is executed. Essentially, you will have to take care of this manually in such a case.
Therefore, the approach using environment definition files described above is highly recommended and preferred.
Referring to an existing environment can however be useful during development, e.g. when a certain software package is developed in parallel to a workflow that uses it.

It is possible to refer to a named environment:

.. code-block:: python

    rule NAME:
        input:
            "table.txt"
        output:
            "plots/myplot.pdf"
        conda:
            "some-env-name"
        script:
            "scripts/plot-stuff.R"

Or alternatively to the filesystem path of an environment:

.. code-block:: python

    rule NAME:
        input:
            "table.txt"
        output:
            "plots/myplot.pdf"
        conda:
            "/home/johannes/miniforge/envs/some-env-name"
        script:
            "scripts/plot-stuff.R"

For any such rules, Snakemake will just activate the given environment, instead of automatically deploying anything.
Instead of using a concrete name or path, it is also possible to provide one containing wildcards (which must also occur in the output files of the rule), analogous to the specification of input files.
Finally it is also possible to use a callable which returns a ``str`` value and takes ``wildcards`` as single argument, similar to input functions.

Note that Snakemake distinguishes file based environments from named ones as follows:
if the given specification ends on ``.yaml`` or ``.yml``, Snakemake assumes it to be a path to an environment definition file;
otherwise, it assumes the given specification to point to an existing environment.


.. _apptainer:

--------------------------
Running jobs in containers
--------------------------

As an alternative to using Conda (see above), it is possible to define, for each rule, a (Docker) container to use, e.g.,

.. code-block:: python

    rule NAME:
        input:
            "table.txt"
        output:
            "plots/myplot.pdf"
        container:
            "docker://joseespinosa/docker-r-ggplot2:1.0"
        script:
            "scripts/plot-stuff.R"

When executing Snakemake with

.. code-block:: bash

    snakemake --software-deployment-method apptainer
    # or the shorthand version
    snakemake --sdm apptainer

it will execute the job within a container that is spawned from the given image.
Allowed image urls entail everything supported by apptainer (e.g., ``shub://`` and ``docker://``).
However, ``docker://`` is preferred, as other container runtimes will be supported in the future (e.g. podman).

Additionally, instead of using a concrete url or path, it is also possible to provide one containing wildcards (which must also occur in the output files of the rule), analogous to the specification of input files.
Finally it is also possible to use a callable which returns a ``str`` value and takes ``wildcards`` as single argument, similar to input functions.

Note that the isolation of jobs running in containers depends on the container engine.
For example, Docker does not pass any host environment variables to the container, whereas Apptainer/Singularity passes everything.
To override the default behaviour, consider using ``--apptainer-args`` or ``--singularity-args``, e.g. to pass ``--cleanenv``.

Files that are mounted using `params` using `workflow.source_path` are also automatically available in the container. This is realized by mounting the snakemake cache in the container (/home/<user>/.cache/snakemake/snakemake/source-cache) where the sourced files will be cached. 

In general, it should be noted that only trusted containers should be used!

Defining global container images
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::

   Note that apptainer integration can be used with the ``shell``, ``script``, ``wrapper`` and ``run`` directives.
   
   However, the ``run`` directive is a special case, as it has access to the rest of the Snakefile (e.g. globally defined variables) and therefore must be executed in the same process as Snakemake itself. 
   The ``container`` directive for rules with a ``run`` directive therefore only affects ``shell`` function calls that are executed from within the ``run`` script.

A global definition of a container image can be given:

.. code-block:: python

    container: "docker://joseespinosa/docker-r-ggplot2:1.0"

    rule NAME:
        ...

In this case all jobs will be executed in a container. You can disable execution in container
by setting the container directive of the rule to ``None``.

.. code-block:: python

    container: "docker://joseespinosa/docker-r-ggplot2:1.0"

    rule NAME:
        container: None


Handling shell executable
~~~~~~~~~~~~~~~~~~~~~~~~~

Snakemake executes rules using the bash shell by default.
If your container image does not contain bash, you can specify a different shell executable, see :ref:`shell_settings`.

-----------------------------------------
Containerization of Conda based workflows
-----------------------------------------
While :ref:`integrated_package_management` provides control over the used software in exactly
the desired versions, it does not control the underlying operating system.
However, given a workflow with conda environments for each rule, Snakemake can automatically
generate a container image specification (in the form of a ``Dockerfile``) that contains
all required environments via the flag --containerize:

.. code-block:: bash

    snakemake --containerize > Dockerfile

The container image specification generated by Snakemake aims to be transparent and readable, e.g. by displaying each contained environment in a human readable way.
Via the special directive ``containerized`` this container image can be used in the workflow (both globally or per rule) such that no further conda package downloads are necessary, for example:

.. code-block:: python

    containerized: "docker://username/myworkflow:1.0.0"

    rule NAME:
        input:
            "table.txt"
        output:
            "plots/myplot.pdf"
        conda:
            "envs/ggplot.yaml"
        script:
            "scripts/plot-stuff.R"

Using the containerization of Snakemake has three advantages over manually crafting a container image for a workflow:

1. A workflow with conda environment definitions is much more transparent to the reader than a black box container image, as each rule directly shows which software stack is used. Containerization just persistently projects those environments into a container image.
2. It remains possible to run the workflow without containers, just via the conda environments.
3. During development, testing can first happen without the container and just on the conda environments. When releasing a production version of the workflow the image can be uploaded just once and for future stable releases, thereby limiting the overhead created in container registries.

--------------------------------------------------------------
Ad-hoc combination of Conda package management with containers
--------------------------------------------------------------

While :ref:`integrated_package_management` provides control over the used software in exactly
the desired versions, it does not control the underlying operating system.
Here, it becomes handy that Snakemake >=4.8.0 allows to combine Conda-based package management
with :ref:`apptainer`.
For example, you can write

.. code-block:: python

    container: "docker://continuumio/miniconda3:4.4.10"

    rule NAME:
        input:
            "table.txt"
        output:
            "plots/myplot.pdf"
        conda:
            "envs/ggplot.yaml"
        script:
            "scripts/plot-stuff.R"

in other words, a global definition of a container image can be combined with a
per-rule conda directive.
Then, upon invocation with

.. code-block:: bash

    snakemake --software-deployment-method conda apptainer
    # or the shorthand version
    snakemake --sdm conda apptainer

Snakemake will first pull the defined container image, and then create the requested conda environment from within the container.
The conda environments will still be stored in your working environment, such that they don't have to be recreated unless they have changed.
The hash under which the environments are stored includes the used container image url, such that changes to the container image also lead to new environments to be created.
When a job is executed, Snakemake will first enter the container and then activate the conda environment.

By this, both packages and OS can be easily controlled without the overhead of creating and distributing specialized container images.
Of course, it is also possible (though less common) to define a container image per rule in this scenario.

The user can, upon execution, freely choose the desired level of reproducibility:

* no package management (use whatever is on the system)
* Conda based package management (use versions defined by the workflow developer)
* Conda based package management in containerized OS (use versions and OS defined by the workflow developer)

-------------------------
Using environment modules
-------------------------

In high performance cluster systems (HPC), it can be preferable to use environment modules for deployment of optimized versions of certain standard tools.
Snakemake allows to define environment modules per rule:

.. code-block:: python

    rule bwa:
        input:
            "genome.fa"
            "reads.fq"
        output:
            "mapped.bam"
        conda:
            "envs/bwa.yaml"
        envmodules:
            "bio/bwa/0.7.9",
            "bio/samtools/1.9"
        shell:
            "bwa mem {input} | samtools view -Sbh - > {output}"

Here, when Snakemake is executed with ``snakemake --use-envmodules``, it will load the defined modules in the given order, instead of using the also defined conda environment.
Note that although not mandatory, one should always provide either a conda environment or a container (see above), along with environment module definitions.
The reason is that environment modules are often highly platform specific, and cannot be assumed to be available somewhere else, thereby limiting reproducibility.
By definition an equivalent conda environment or container as a fallback, people outside of the HPC system where the workflow has been designed can still execute it, e.g. by running ``snakemake --software-deployment-method conda`` instead of ``snakemake --use-envmodules``.

--------------------------------------
Sustainable and reproducible archiving
--------------------------------------

With Snakemake 3.10.0 it is possible to archive a workflow into a
`tarball <https://en.wikipedia.org/wiki/Tar_(computing)>`_
(`.tar`, `.tar.gz`, `.tar.bz2`, `.tar.xz`), via

.. code-block:: bash

    snakemake --archive my-workflow.tar.gz

If above layout is followed, this will archive any code and config files that
is under git version control. Further, all input files will be included into the
archive. Finally, the software packages of each defined conda environment are included.
This results in a self-contained workflow archive that can be re-executed on a
vanilla machine that only has Conda and Snakemake installed via

.. code-block:: bash

    tar -xf my-workflow.tar.gz
    snakemake -n

Note that the archive is platform specific. For example, if created on Linux, it will
run on any Linux newer than the minimum version that has been supported by the used
Conda packages at the time of archiving (e.g. CentOS 6).

A useful pattern when publishing data analyses is to create such an archive,
upload it to `Zenodo <https://zenodo.org/>`_ and thereby obtain a
`DOI <https://en.wikipedia.org/wiki/Digital_object_identifier>`_.
Then, the DOI can be cited in manuscripts, and readers are able to download
and reproduce the data analysis at any time in the future.

.. _global-workflow-dependencies:

----------------------------
Global workflow dependencies
----------------------------

Often, your workflow will depend on some additional packages that need to be present
along with Snakemake in order to handle actions before any rule is executed.
Classical examples for this are `pandas <https://pandas.pydata.org/>`_,
`pep <https://pep.databio.org>`_ (also see :ref:`snakefiles-peps`) and
:ref:`storage plugins <storage-support>`.

Snakemake allows to define such global dependencies using a global ``conda`` directive
that should occur at the beginning of your workflow, before you import or use any of
those additional packages::

    conda:
        "envs/global.yaml"

With ``envs/global.yaml`` containing e.g.::

    channels:
      - conda-forge
      - bioconda
      - nodefaults
    dependencies:
      - pandas=1.0.3
      - snakemake-storage-plugin-s3

Under the hood, this is implemented using `conda-inject <https://github.com/koesterlab/conda-inject>`_,
which modifies the python searchpath and the PATH variable on the fly during execution,
pointing to additional environments that do not alter the environment in which Snakemake
has been installed.

This mechanism requires that you use Mamba_ or Conda and activate conda-based software deployment via::

    --software-deployment-method conda
    # or the shorthand version
    --sdm conda



================================================
FILE: docs/snakefiles/foreign_wms.rst
================================================

.. _snakefiles-foreign-wms:

===============================================
Integrating foreign workflow management systems
===============================================

Snakemake 6.2 and later allows to hand over execution steps to other workflow management systems.
By this, it is possible to make use of workflows written for other systems, while performing any further pre- or postprocessing within Snakemake.
Such a handover is indicated with the ``handover`` directive.
Consider the following example:

.. code-block:: python

    rule chipseq_pipeline:
        input:
            input="design.csv",
            fasta="data/genome.fasta",
            gtf="data/genome.gtf",
        output:
            "multiqc/broadPeaks/multiqc_report.html",
        params:
            pipeline="nf-core/chipseq",
            revision="1.2.1",
            profile=["conda"],
        handover: True
        wrapper:
            "0.74.0/utils/nextflow"

Here, the workflow is executed as usual until this rule is reached.
Then, Snakemake passes all resources to the nextflow workflow management system, which generates certain files.
The rule is executed as a :ref:`local rule <snakefiles-local-rules>`, meaning that it would not be submitted to a cluster or cloud system by Snakemake.
Instead, the invoked other workflow management system is responsible for that.
E.g., in case of `Nextflow <https://nextflow.io>`_, submission behavior can be configured via a ``nextflow.conf`` file or environment variables.
After the step is done, Snakemake continues execution with the output files produced by the foreign workflow.



================================================
FILE: docs/snakefiles/modularization.rst
================================================
.. _Snakemake Wrapper Repository: https://snakemake-wrappers.readthedocs.io

.. _snakefiles-modularization:

==============
Modularization
==============

Modularization in Snakemake comes at four different levels.

1. The most fine-grained level are wrappers. They are available and can be published at the `Snakemake Wrapper Repository`_. These wrappers can then be composed and customized according to your needs, by copying skeleton rules into your workflow. In combination with conda integration, wrappers also automatically deploy the needed software dependencies into isolated environments.
2. For larger, reusable parts that shall be integrated into a common workflow, it is recommended to write small Snakefiles and include them into a main Snakefile via the include statement. In such a setup, all rules share a common config file.
3. The third level is provided via the :ref:`module statement <snakefiles-modules>`, which enables arbitrary combination and reuse of rules.


.. _snakefiles-wrappers:

--------
Wrappers
--------

The wrapper directive allows to have reusable wrapper scripts around e.g. command line tools.
In contrast to modularization strategies like ``include`` or subworkflows, the wrapper directive allows to re-wire the DAG of jobs.
For example

.. code-block:: python

    rule samtools_sort:
        input:
            "mapped/{sample}.bam"
        output:
            "mapped/{sample}.sorted.bam"
        params:
            "-m 4G"
        threads: 8
        wrapper:
            "0.0.8/bio/samtools/sort"

.. note::

    It is possible to refer to wildcards and params in the wrapper identifier, e.g. by specifying ``"0.0.8/bio/{params.wrapper}"`` or ``"0.0.8/bio/{wildcards.wrapper}"``.

Refers to the wrapper ``"0.0.8/bio/samtools/sort"`` to create the output from the input.
Snakemake will automatically download the wrapper from the `Snakemake Wrapper Repository`_.
Thereby, ``0.0.8`` can be replaced with the git `version tag <https://github.com/snakemake/snakemake-wrappers/releases>`_ you want to use, or a `commit id <https://github.com/snakemake/snakemake-wrappers/commits>`_.
This ensures reproducibility since changes in the wrapper implementation will only be propagated to your workflow once you update the version tag.
Examples for each wrapper can be found in the READMEs located in the wrapper subdirectories at the `Snakemake Wrapper Repository`_.

Alternatively, for example during development, the wrapper directive can also point to full URLs, including URLs to local files with absolute paths ``file://`` or relative paths ``file:``.
Such a URL will have to point to the folder containing the ``wrapper.*`` and ``environment.yaml`` files.
In the above example, the full GitHub URL could for example be provided with ``wrapper: https://github.com/snakemake/snakemake-wrappers/raw/0.0.8/bio/samtools/sort``.
Note that it needs to point to the ``/raw/`` version of the folder, not the rendered HTML version.

In addition, the `Snakemake Wrapper Repository`_ offers so-called meta-wrappers, which can be used as modules, see :ref:`snakefiles-meta-wrappers`.

The `Snakemake Wrapper Repository`_ is meant as a collaborative project and pull requests are very welcome.


.. _cwl:

--------------------------------------
Common-Workflow-Language (CWL) support
--------------------------------------

With Snakemake 4.8.0, it is possible to refer to `CWL <https://www.commonwl.org/>`__ tool definitions in rules instead of specifying a wrapper or a plain shell command.
A CWL tool definition can be used as follows.

.. code-block:: python

    rule samtools_sort:
        input:
            input="mapped/{sample}.bam"
        output:
            output_name="mapped/{sample}.sorted.bam"
        params:
            threads=lambda wildcards, threads: threads,
            memory="4G"
        threads: 8
        cwl:
            "https://github.com/common-workflow-language/workflows/blob/"
            "fb406c95/tools/samtools-sort.cwl"

.. note::

    It is possible to refer to wildcards and params in the tool definition URL, e.g. by specifying something like ``"https://.../tools/{params.tool}.cwl"`` or ``"https://.../tools/{wildcards.tool}.cwl"``.

It is advisable to use a github URL that includes the commit as above instead of a branch name, in order to ensure reproducible results.
Snakemake will execute the rule by invoking `cwltool`, which has to be available via your `$PATH` variable, and can be, e.g., installed via `conda` or `pip`.
When using in combination with :ref:`--software-deployment-method apptainer <apptainer>` (``--sdm`` for short), Snakemake will instruct `cwltool` to execute the command via Singularity in user space.
Otherwise, `cwltool` will in most cases use a Docker container, which requires Docker to be set up properly.

The advantage is that predefined tools available via any `repository of CWL tool definitions <https://www.commonwl.org/#Repositories_of_CWL_Tools_and_Workflows>`__ can be used in any supporting workflow management system.
In contrast to a :ref:`Snakemake wrapper <snakefiles-wrappers>`, CWL tool definitions are in general not suited to alter the behavior of a tool, e.g., by normalizing output names or special input handling.
As you can see in comparison to the analog :ref:`wrapper declaration <snakefiles-wrappers>` above, the rule becomes slightly more verbose, because input, output, and params have to be dispatched to the specific expectations of the CWL tool definition.

.. _snakefiles-includes:

--------
Includes
--------

Another Snakefile with all its rules can be included into the current:

.. code-block:: python

    include: "path/to/other/snakefile"

The default target rule (often called the ``all``-rule), won't be affected by the include.
I.e. it will always be the first rule in your Snakefile, no matter how many includes you have above your first rule.
Includes are relative to the directory of the Snakefile in which they occur.
For example, if above Snakefile resides in the directory ``my/dir``, then Snakemake will search for the include at ``my/dir/path/to/other/snakefile``, regardless of the working directory.


.. _snakefiles-modules:

-------
Modules
-------

With Snakemake 6.0 and later, it is possible to define external workflows as modules, from which rules can be used by explicitly "importing" them.

.. code-block:: python

    from snakemake.utils import min_version
    min_version("6.0")

    module other_workflow:
        snakefile:
            # here, plain paths, URLs and the special markers for code hosting providers (see below) are possible.
            "other_workflow/Snakefile"
    
    use rule * from other_workflow exclude ruleC as other_*

The ``module other_workflow:`` statement registers the external workflow as a module, by defining the path to the main snakefile of ``other_workflow``.
Here, plain paths, HTTP/HTTPS URLs and special markers for code hosting providers like Github or Gitlab are possible (see :ref:`snakefile-code-hosting-providers`).
The second statement, ``use rule * from other_workflow exclude ruleC as other_*``, declares all rules of that module to be used in the current one, except for ruleC.
Thereby, the ``as other_*`` at the end renames all those rules with a common prefix.
This can be handy to avoid rule name conflicts (note that rules from modules can otherwise overwrite rules from your current workflow or other modules).

.. note::

    The imported module cannot be named as `workflow`, which is a reserved name.

The module is evaluated in a separate namespace, and only the selected rules are added to the current workflow.
Non-rule Python statements inside the module are also evaluated in that separate namespace.
They are available in the module-defining workflow under the name of the module (e.g. here ``other_workflow.myfunction()`` would call the function ``myfunction`` that has been defined in the model, e.g. in ``other_workflow/Snakefile``).
Also note that this means that any Python variables and functions available in the module-defining namespace will **not** be visible from inside the module.
However, it is possible to pass information to the module using the ``config`` mechanism described in the following.

It is possible to overwrite the global config dictionary for the module, which is usually filled by the ``configfile`` statement (see :ref:`snakefiles_standard_configuration`):

.. code-block:: python

    from snakemake.utils import min_version
    min_version("6.0")

    configfile: "config/config.yaml"

    module other_workflow:
        # here, plain paths, URLs and the special markers for code hosting providers (see below) are possible.
        snakefile: "other_workflow/Snakefile"
        config: config["other-workflow"]
    
    use rule * from other_workflow as other_*

In this case, any ``configfile`` statements inside the module are ignored.
In addition, it is possible to skip any :ref:`validation <snakefiles_config_validation>` statements in the module, by specifying ``skip_validation: True`` in the module statement.
Moreover, one can automatically move all relative input and output files of a module into a dedicated folder by specifying ``prefix: "foo"`` in the module definition, e.g. any output file ``path/to/output.txt`` in the module would be stored under ``foo/path/to/output.txt`` instead.
This becomes particularly useful when combining multiple modules, see :ref:`use_with_modules`.
However, if you have some input files that come from outside the workflow, you can use the ``local`` flag so that their path is not modified (see :ref:`snakefiles-storage-local-files`)..

Instead of using all rules, it is possible to import specific rules.
Specific rules may even be modified before using them, via a final ``with:`` followed by a block that lists items to overwrite.
This modification can be performed after a general import, and will overwrite any unmodified import of the same rule.

.. code-block:: python

    from snakemake.utils import min_version
    min_version("6.0")

    module other_workflow:
        # here, plain paths, URLs and the special markers for code hosting providers (see below) are possible.
        snakefile: "other_workflow/Snakefile"
        config: config["other-workflow"]

    use rule * from other_workflow as other_*

    use rule some_task from other_workflow as other_some_task with:
        output:
            "results/some-result.txt"

By such a modifying use statement, any properties of the rule (``input``, ``output``, ``log``, ``params``, ``benchmark``, ``threads``, ``resources``, etc.) can be overwritten, except the actual execution step (``shell``, ``notebook``, ``script``, ``cwl``, or ``run``).

.. note::
    Modification of `params` allows the replacement of single keyword arguments. Keyword `params` arguments of the original rule that are not defined after `with` are inherited. Positional `params` arguments of the original rule are overwritten, if positional `params` arguments are given after `with`.
    All other properties are overwritten with the values specified after `with`.

Note that the second use statement has to use the original rule name, not the one that has been prefixed with ``other_`` via the first use statement (there is no rule ``other_some_task`` in the module ``other_workflow``).
In order to overwrite the rule ``some_task`` that has been imported with the first ``use rule`` statement, it is crucial to ensure that the rule is used with the same name in the second statement, by adding an equivalent ``as`` clause (here ``other_some_task``).
Otherwise, you will have two versions of the same rule, which might be unintended (a common symptom of such unintended repeated uses would be ambiguous rule exceptions thrown by Snakemake).

Of course, it is possible to combine the use of rules from multiple modules (see :ref:`use_with_modules`), and via modifying statements they can be rewired and reconfigured in an arbitrary way.

.. _snakefiles-dynamic-modules:

---------------
Dynamic Modules
---------------

With Snakemake 9.0 and later, it is possible to load modules dynamically by providing the ``name`` keyword inside the module definition.
For example, by reading the module name from a config file or by iterating over several modules in a loop.
For this, the module name is not specified directly after the ``module`` keyword, but by specifying the ``name`` parameter. 


.. code-block:: python

    for module_name in ['module1', 'module2']:
        module:
            name: module_name
            snakefile: f"{module_name}/Snakefile"
            config: config[module_name]

        use rule * from module_name as module_name*

.. note::
    It is not allowed to specify the module name both after the ``module`` keyword and inside the module definition after the ``name`` parameter.

In the ``use rule`` statement, it is first checked if the module name (here, ``'module_name'``) corresponds to a loaded module. If yes, the rules are imported from the loaded module and an arbitrary alias can be provided after the ``as`` keyword.

If ``module_name`` was not registered as a module (as in the example above), the module name is resolved dynamically by searching the name in the current python variable scope. In the example, it resolves to ``'module1'`` and ``'module2'``.
Note that this means that if ``use rule`` is used with the optional ``as`` keyword inside the loop, the alias after ``as`` must be specified using a variable to ensure a one-to-one mapping between module names and their aliases. This can either be the same name variable (as in the above example) or a second variable (as in the example below).

In particular, it is not possible to modify the alias name in the ``use rule`` statement (e.g., writing directly ``use rule * from module as module_*`` is not allowed for dynamic modules).

.. code-block:: python

    for module_name, alias in zip(['module1', 'module2'], ['module1_', 'module2_']):
        module:
            name: module_name
            snakefile: f"{module_name}/Snakefile"
            config: config[module_name]

        use rule * from module_name as alias*

..  _snakefiles-meta-wrappers:

~~~~~~~~~~~~~
Meta-Wrappers
~~~~~~~~~~~~~

Snakemake wrappers offer a simple way to include commonly used tools in Snakemake workflows.
In addition the `Snakemake Wrapper Repository`_ offers so-called meta-wrappers, which are combinations of wrappers, meant to perform common tasks.
Both wrappers and meta-wrappers are continuously tested.
The module statement also allows to easily use meta-wrappers, for example:

.. code-block:: python

    from snakemake.utils import min_version
    min_version("6.0")

    configfile: "config.yaml"


    module bwa_mapping:
        meta_wrapper: "0.72.0/meta/bio/bwa_mapping"


    use rule * from bwa_mapping


    def get_input(wildcards):
        return config["samples"][wildcards.sample]


    use rule bwa_mem from bwa_mapping with:
        input:
            get_input


First, we define the meta-wrapper as a module.
Next, we declare all rules from the module to be used.
And finally, we overwrite the input directive of the rule ``bwa_mem`` such that the raw data is taken from the place where our workflow configures it via it's config file.


.. _snakefile-code-hosting-providers:

----------------------
Code hosting providers
----------------------

To obtain the correct URL to an external source code resource (e.g. a snakefile, see :ref:`snakefiles-modules`), Snakemake provides markers for code hosting providers.
Currently, Github 

.. code-block:: python

    github("owner/repo", path="workflow/Snakefile", tag="v1.0.0")


and Gitlab are supported:

.. code-block:: python

    gitlab("owner/repo", path="workflow/Snakefile", tag="v1.0.0")

For the latter, it is also possible to specify an alternative host, e.g.

.. code-block:: python

    gitlab("owner/repo", path="workflow/Snakefile", tag="v1.0.0", host="somecustomgitlab.org")


While specifying a tag is highly encouraged, it is alternatively possible to specify a `commit` or a `branch` via respective keyword arguments.
Note that only when specifying a tag or a commit, Snakemake is able to persistently cache the source, thereby avoiding to repeatedly query it in case of multiple executions.

~~~~~~~~~~~~~~~~~~~~
Private repositories
~~~~~~~~~~~~~~~~~~~~

To access source code resources located in private repositories you can define an
access token in the ``GITHUB_TOKEN`` and/or ``GITLAB_TOKEN`` environment variables.



================================================
FILE: docs/snakefiles/reporting.rst
================================================
.. _snakefiles-reports:

=======
Reports
=======

From Snakemake 5.1 on, it is possible to automatically generate detailed self-contained HTML reports that you can easily move and share. 
They encompass runtime statistics, provenance information and workflow topology by default, and workflow developers can also specify and annotate results for inclusion.
For smaller default reports, these can be a single :ref:`snakefiles-self_contained_html_file`.
For more complex reports, one can generate a :ref:`snakefiles-self_contained_zip_archive`, with a contained ``report.html`` as the main entry point.
**As an example, the report of the Snakemake rolling paper can be found** `here <https://snakemake.github.io/resources/report.html>`__.

.. _snakefiles-including_results_in_a_report:

Including results in a report
-----------------------------

For including results into the report, the Snakefile has to be annotated with additional information.
Each output file that shall be part of the report has to be marked with the ``report`` flag, which optionally points to a caption in `restructured text format <https://docutils.sourceforge.io/docs/user/rst/quickstart.html>`_ and allows to define a ``category`` for grouping purposes.
Moreover, a global workflow description can be defined via the ``report`` directive.
Consider the following example:

.. code-block:: python

  report: "report/workflow.rst"


  rule all:
      input:
          ["fig1.svg", "fig2.png", "testdir"]


  rule c:
      output:
          "test.{i}.out"
      container:
          "docker://continuumio/miniconda3:4.4.10"
      conda:
          "envs/test.yaml"
      shell:
          "sleep `shuf -i 1-3 -n 1`; touch {output}"


  rule a:
      input:
          expand("test.{i}.out", i=range(10))
      output:
          report("fig1.svg", caption="report/fig1.rst", category="Step 1")
      shell:
          "sleep `shuf -i 1-3 -n 1`; cp data/fig1.svg {output}"


  rule b:
      input:
          expand("{model}.{i}.out", i=range(10))
      output:
          report("fig2.png", caption="report/fig2.rst", category="Step 2", subcategory="{model}")
      shell:
          "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png {output}"

  rule d:
      output:
          report(
              directory("testdir"), 
              patterns=["{name}.txt"], 
              caption="report/somedata.rst", 
              category="Step 3")
      shell:
          "mkdir {output}; for i in 1 2 3; do echo $i > {output}/$i.txt; done"

As can be seen, we define a global description which is contained in the file ``report/workflow.rst``.
In addition, we mark ``fig1.svg`` and ``fig2.png`` for inclusion into the report, while in both cases specifying a caption text via again referring to a restructured text file.
Note the paths to the ``.rst``-files are interpreted relative to the current Snakefile.

Inside the ``.rst``-files you can use `Jinja2 <https://jinja.palletsprojects.com>`_ templating to access context information.
In case of the global description, you can access the config dictionary via ``{{ snakemake.config }}``, (e.g., use ``{{ snakemake.config["mykey"] }}`` to access the key ``mykey``).
In case of output files, you can access the same values as available with the :ref:`script directive <snakefiles-external_scripts>` (e.g., ``snakemake.wildcards``).

When marking files for inclusion in the report, a ``category`` and a ``subcategory`` can be given, allowing to group results in of the report.
For both, wildcards (like ``{model}`` see rule b in the example), are automatically replaced with the respective values from the corresponding job.

The last rule ``d`` creates a directory with several files, here mimicking the case that it is impossible to specify exactly which files will be created while writing the workflow (e.g. it might depend on the data).
Nevertheless, it is still possible to include those files one by one into the report by defining inclusion patterns (here ``patterns=["{name}.txt"]``) along with the report flag.
When creating the report, Snakemake will scan the directory for files matching the given patterns and include all of them in the report.
Wildcards in those patterns are made available in the jinja-templated caption document along with the rules wildcards in the ``snakemake.wildcards`` object.

If the output of a rule is a directory with an HTML file hierarchy, it is also possible to specify an entry-point HTML file for inclusion into the report, instead of the ``patterns`` approach from above.
This works as follows:

.. code-block:: python

    rule generate_html_hierarchy:
        output:
            report(directory("test"), caption="report/caption.rst", htmlindex="test.html")
        shell:
            """
            # mimic writing of an HTML hierarchy
            mkdir test
            cp template.html test/test.html
            mkdir test/js
            echo \"alert('test')\" > test/js/test.js
            """

Defining file labels
^^^^^^^^^^^^^^^^^^^^

In addition to category, and subcategory, it is possible (and highly recommended!) to define a dictionary of labels for each report item.
By that, the actual filename will be hidden in the report and instead a table with the label keys as columns and the values in the respective row for the file will be displayed.
This can lead to less technical reports that abstract away the fact that the results of the analysis are actually files.
Consider the following modification of rule ``b`` from above:

.. code-block:: python

    rule b:
      input:
          expand("{model}.{i}.out", i=range(10))
      output:
          report(
              "fig2.png", 
              caption="report/fig2.rst", 
              category="Step 2", 
              subcategory="{model}",
              labels={
                  "model": "{model}",
                  "figure": "some plot"
              }
          )
      shell:
          "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png {output}"

If all results in a particular category/subcategory share the same label and both values occur once for each combination of other labels,
Snakemake displays the label as a toggle switch above the result menu.
This behavior can be used to, for example, switch between different versions of a plot, one with and one without a legend, see the example below (there, the legend label, with values yes/no, is automatically rendered as a toggle switch):

.. image:: images/report-toggles.png
    :scale: 100%
    :alt: Example toggle switch for labels
    :align: center


Determining category, subcategory, and labels dynamically via functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Similar to e.g. with input file and parameter definition (see :ref:`snakefiles-input_functions`), ``category`` and a ``subcategory`` and ``labels`` can be specified by pointing to a function that takes ``wildcards`` as the first argument (and optionally in addition ``input``, ``output``, ``params`` in any order).
The function is expected to return a string or number (int, float, numpy types), or, in case of labels, a dict with strings as keys and strings or numbers as values.


Linking between items
^^^^^^^^^^^^^^^^^^^^^

From captions
"""""""""""""

In every ``.rst`` document (i.e. in the captions), you can link to

* the **Workflow** panel (with ``Rules_``),
* the **Statistics** panel (with ``Statistics_``),
* any **category** panel (with ``Mycategory_``, while ``Mycategory`` is the name given for the category argument of the report flag). E.g., with above example, you could write ``see `Step 2`_`` in order to link to the section with the results that have been assigned to the category ``Step 2``.
* any **file** marked with the report flag (with ``myfile.txt_``, while ``myfile.txt`` is the basename of the file, without any leading directories). E.g., with above example, you could write ``see fig2.png_`` in order to link to the result in the report document.

For details about the hyperlink mechanism of restructured text see `here <https://docutils.sourceforge.io/docs/user/rst/quickref.html#hyperlink-targets>`__.

From results
""""""""""""

From within results that are included into the report, you can link to other report items.
This works by using the ``snakemake.report_href()`` method that is available from within :ref:`python scripts <snakefiles-external_scripts>`.
The method takes the path to the target report item in exactly the same form as it is given in the Snakefile,
and optionally can be extended to target child paths or by URL arguments.
For example, consider the following Snakefile:

.. code-block:: python

    rule a:
        input:
            report("test.html"),
            report(
                "subdir",
                patterns=["{name}.html"],
            )
        output:
            report(
                "test2.html",
            )
        script:
            "test_script.py"

Inside of the script, we can now use ``snakemake.report_href()`` to create a link to the file ``test.html`` such that it can be accessed from the file ``test2.html``:

.. code-block:: python

    import textwrap

    with open(snakemake.output[0], "w") as f:
        print(
            textwrap.dedent(f"""
            <html>
                <head>
                    <title>Report</title>
                </head>
                <body>
                    <a href={snakemake.report_href("test.html")}>Link to test.html</a>
                </body>
            </html>
            """
            ),
            file=f,
        )

Note that you will rarely directly generate HTML like this in a Python script within a Snakemake workflow.
Rather, you might want to access ``snakemake.report_href()`` when e.g. generating a table which is later rendered into HTML by e.g. `Datavzrd <https://datavzrd.github.io>`__ (also see :ref:`interaction_visualization_reporting_tutorial`).

In case you want to refer to a file that is inside of a directory that is included into the Snakemake report, you can do so using the ``child_path`` method:

.. code-block:: python

    import textwrap

    with open(snakemake.output[0], "w") as f:
        print(
            textwrap.dedent(f"""
            <html>
                <head>
                    <title>Report</title>
                </head>
                <body>
                    <a href={snakemake.report_href("subdir").child_path("foo.html")}>Link to test.html</a>
                </body>
            </html>
            """
            ),
            file=f,
        )

Further, using ``url_args()`` you can add URL arguments and using ``anchor()`` you can add a target anchor to the link, e.g. to scroll to a specific section of the target document:

.. code-block:: python

    import textwrap

    with open(snakemake.output[0], "w") as f:
        print(
            textwrap.dedent(f"""
            <html>
                <head>
                    <title>Report</title>
                </head>
                <body>
                    <a href={snakemake.report_href("subdir").child_path("foo.html").url_args(someparam=5).anchor("mysection")}>Link to test.html</a>
                </body>
            </html>
            """
            ),
            file=f,
        )

.. _snakefiles-rendering_reports:

Rendering reports
-----------------

All the metadata contained in the report (e.g. runtime statistics) are automatically collected during the rendering of the report.
These statistics are obtained from the metadata that is stored in the ``.snakemake`` directory inside your working directory.

.. _snakefiles-self_contained_html_file:

Self-contained HTML file
^^^^^^^^^^^^^^^^^^^^^^^^

To create a default report simply run

.. code-block:: bash

    snakemake --report

after your workflow has finished successfully.
This will create a self-contained HTML report with the default filename ``report.html``.
If you want to give your report a custom name, you can do so by specifying this on the command-line:

.. code-block:: bash

    snakemake --report my_report.html

As specifying an HTML file for the report will always embed any user-defined report outputs into the same HTML file, this report output type is only suitable for smaller reports.

.. _snakefiles-self_contained_zip_archive:

Self-contained ZIP archive
^^^^^^^^^^^^^^^^^^^^^^^^^^

For anything more complex, it is recommended to generate a ``ZIP`` archive report.
For example, if you have lots of samples you report on, or if you specified lots of different workflow outputs for report inclusion.
To get such a self-contained ``ZIP`` archive, simply specify a ``.zip`` file name instead:

.. code-block:: bash

    snakemake --report some_report.zip

You can move this file wherever you want to view it and then unpack it right there.
This includes sending this file to collaboration partners or customers for whom you might have done an analysis.
The main entry point is always the ``report.html`` file in the main folder that this creates (for the above example, this folder will be named ``some_report``).
Just open that file in a web browser and explore your results.

Partial reports
^^^^^^^^^^^^^^^

The report can be restricted to particular jobs and results by specifying targets at the command line, analog to normal Snakemake execution.
For example, with

.. code-block:: bash

    snakemake fig1.svg --report report-short.html

the report contains only ``fig1.svg``.
This can be useful when a larger workflow has not yet run to completion, but you already want to explore some intermediate outputs in the report.
Or when you have multiple alternative target rules within the same workflow.

Custom layout
^^^^^^^^^^^^^

You can define an institute specific layout by providing a custom stylesheet:

.. code-block:: bash

    snakemake --report report.html --report-stylesheet custom-stylesheet.css

For example, this allows you to set a logo at the top (by using CSS to inject a background for the placeholder ``<div id="brand">``), or overwrite colors.

For an example with a custom stylesheet defining a logo, see :download:`the report here <../../tests/test_report/expected-results/report.html>` (with a custom branding for the University of Duisburg-Essen).
For the complete mechanics, you can also have a look at the `full example source code  <https://github.com/snakemake/snakemake/tree/main/tests/test_report/>`__ and :download:`the custom stylesheet with the logo definition <../../tests/test_report/custom-stylesheet.css>`.

Custom report metadata
^^^^^^^^^^^^^^^^^^^^^^

You can define custom metadata that is displayed on the landing page of the report.
The metadata is provided as a `YTE <https://yte-template-engine.github.io>`_ yaml template.

.. code-block:: bash

    snakemake --report report.html --report-metadata yte_template.yaml

An example metadata yaml template that contains information about the work directory in which the workflow was run contains the following definitions.

.. code-block:: yaml

    __definitions__:
    - import os

    Workflow name: Test Workflow
    Workdir: ?os.getcwd()
    Contributors:
      - Test Contributor
      - Another Contributor




================================================
FILE: docs/snakefiles/storage.rst
================================================
.. _Mamba: https://github.com/mamba-org/mamba

.. _storage-support:

===============
Storage support
===============

By default, input and output files or directories defined and used in Snakemake rules are
written to the local filesystem, thereby interpreting relative paths relative to the current working directory.

However, Snakemake also allows to transparently map input and output files to storage providers.
Storage providers are implemented via plugins.
All available storage providers and their documentation are available in the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`_.


Deployment
----------

Storage plugins can be used by deploying them to your local environment via pip::
   
   pip install snakemake-storage-plugin-s3

or Mamba_::

   mamba install -c conda-forge -c bioconda snakemake-storage-plugin-s3

If you choose to register a storage plugin within your workflow (see below), it is advisable to add
the respective plugin package as a dependency of your workflow itself
(see :ref:`global-workflow-dependencies`).

Usage
-----

In general, there are four ways to use a storage provider.

1. Use the ``--default-storage-provider`` command line argument to set a default storage provider.
   This will be used for all input and output files that are not explicitly mapped to a storage provider.
2. Register a storage provider in the workflow and use it only for particular input and output, not all of them.
3. Register multiple entities of the same storage provider with different names/tags. This allows to e.g. use the same protocol to access multiple different remote storages.
4. Let Snakemake automatically find a matching storage provider.

Using the S3 storage plugin, we will provide an example for all of the cases below.
For provider specific options (also for all options of the S3 plugin which are omitted here for brevity) and all available plugins see the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`_.

.. _default_storage:

As default provider
^^^^^^^^^^^^^^^^^^^
If you want all your input and output (which is not explicitly marked to come from 
another storage) to be written to and read from this storage, you can use it as a 
default provider via::

    snakemake --default-storage-provider s3 --default-storage-prefix s3://mybucket/

Custom settings can be passed as well::

    snakemake --default-storage-provider s3 --default-storage-prefix s3://mybucket/ \
        --storage-s3-max-requests-per-second 10


.. _snakefiles-storage-local-files:

Local input/output files
""""""""""""""""""""""""

Despite using a default storage provider, you might have certain files in your workflow
that still come from the local filesystem. Likewise, when importing a module while
specifying a prefix (see :ref:`snakefiles-modules`), you might have some input files
that come from outside the workflow. In either cases, you can use the ``local`` flag::

    rule example:
        input:
            local("resources/example-input.txt")
        output:
            "example-output.txt"
        shell:
            "..."

Here, ``resources/example-input.txt`` will be interpreted as a local file, while
``example-output.txt`` will be written to the default storage provider that you
have specified (with the prefix prepended).

Note that source paths (see :ref:`snakefiles-aux_source_files`) are also not mapped to the default storage provider.
There is no need to additionally mark them as local.

Within the workflow
^^^^^^^^^^^^^^^^^^^

If you want to use this storage plugin only for specific items, you can register it
inside of your workflow::

    # register storage provider (not needed if no custom settings are to be defined here)
    storage:
        provider="s3",
        # optionally add custom settings here if needed
        # alternatively they can be passed via command line arguments
        # starting with --storage-s3-...
        max_requests_per_second=10,

    rule example:
        input:
            storage.s3(
                # define query to the storage backend here
                ...
            ),
        output:
            "example.txt"
        shell:
            "..."

Using multiple entities of the same storage plugin
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In case you have to use this storage plugin multiple times, but with different settings
(e.g. to connect to different storage servers), you can register it multiple times,
each time providing a different tag::

    # register shared settings
    storage:
        provider="s3",
        # optionally add custom settings here if needed
        # alternatively they can be passed via command line arguments
        # starting with --storage-s3-...
        max_requests_per_second=10,

    # register multiple tagged entities
    storage awss3:
        provider="s3",
        endpoint_url="s3.us-east-2.amazonaws.com"

    rule example:
        input:
            storage.awss3(
                # define query to the storage backend here
                ...
            ),
        output:
            "example.txt"
        shell:
            "..."

Retrieving and keeping remote files locally
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When the input file is a remote file, the default behaviour is to download the remote
file to the local area and then remove it after the workflow no longer needs it.

This behaviour can be configured from the command line arguments, using::

    snakemake --keep-storage-local-copies --not-retrieve-storage

where ``--keep-storage-local-copies`` directs snakemake to keep the local copies of
remote files that it makes and ``--not-retrieve-storage`` directs snakemake to not download
copies of remote files.

Additionally, this behaviour can be set at the level of the storage directive e.g.::

    storage:
        provider="http",
        retrieve=False,

    storage http_local:
        provider="http",
        keep_local=True,

    rule example_remote:
        input:
            storage.http("http://example.com/example.txt")
        output:
            "example_remote.txt"
        shell:
            "..."

    rule example_local:
        input:
            storage.http_local("http://example.com/example.txt")
        output:
            "example_local.txt"
        shell:
            "..."

Finally, ``retrieve`` and ``keep_local`` can also be set inside the call to the storage
plugin within a rule::

    storage:
        provider="http",
        retrieve=False,

    rule example_remote:
        input:
            storage.http("http://example.com/example.txt", retrieve=False)
        output:
            "example_remote.txt"
        shell:
            "..."

    rule example_local:
        input:
            storage.http("http://example.com/example.txt", keep_local=True)
        output:
            "example_local.txt"
        shell:
            "..."

Automatic inference
^^^^^^^^^^^^^^^^^^^

If the query for a storage plugin is unique given those plugins that you have currently installed,
you can let Snakemake automatically infer the plugin to use::

    rule example:
        input:
            storage("s3://mybucket/example.txt")
        output:
            "example.txt"
        shell:
            "..."

You can also pass additional arguments to the storage call within a rule. 
These arguments will be forwarded to the storage provider settings.
For example::

    rule example:
        input:
            storage("s3://mybucket/example.txt", retries=10)
        output:
            "example.txt"
        shell:
            "..."


Credentials
^^^^^^^^^^^

Depending on the storage provider, you might have to provide credentials.
Usually, this can be done via environment variables, e.g. for S3::

    export SNAKEMAKE_STORAGE_S3_ACCESS_KEY=...
    export SNAKEMAKE_STORAGE_S3_SECRET_KEY=...

.. _storage-access-patterns:

Access pattern annotation
^^^^^^^^^^^^^^^^^^^^^^^^^

Storage providers can automatically optimize the provision of files based on how the files will be accessed by the respective job.
For example, if a file is only read sequentially, the storage provider can avoid downloading it and instead mount or symlink it (depending on the protocol) for ondemand access.
This can be beneficial, in particular if the sequential access involves only a small part of an otherwise large file.
The three access patterns that can be annotated are:

* ``access.sequential``: The file is read sequentially either from start to end or in (potentially disjoint) chunks, but always in order from the start to the end.
* ``access.random``: The file is read in a non-sequential order.
* ``access.multi``: The file is read sequentially, but potentially multiple times in parallel.

Snakemake considers an input file eligible for on-demand provisioning if it is accessed sequentially by one job in parallel.
In all other cases, multi-access, random access, or sequential access by multiple jobs in parallel, the storage provider will download the file to the local filesystem before it is accessed by jobs.
In case no access pattern is annotated (the default), Snakemake will also download the file.

The access patterns can be annotated via flags.
Usually, one would define sequential access as the default pattern (it should usually be the most common pattern in a workflow).
This can be done via the ``inputflags`` directive before defining any rule.
For specific files, the access pattern can be annotated by the respective flags ``access.sequential``, ``access.random``, or ``access.multi``.

.. code-block:: python

    inputflags:
        access.sequential


    rule a:
        input:
            access.random("test1.in")  # expected as local copy (because accessed randomly)
        output:
            "test1.out"
        shell:
            "cmd_b {input} {output}"


    rule b:
        input:
            access.multi("test1.out") # expected as local copy (because accessed multiple times)
        output:
            "test2.{dataset}.out"
        shell:
            "cmd_b {input} {output}"


    rule c:
        input:
            "test2.{dataset}.out" # expected as on-demand provisioning (because accessed sequentially, the default defined above)
        output:
            "test3.{dataset}.out"
        shell:
            "cmd_c {input} {output}"

Note that there is no guarantee that the storage provider makes use of this information, since the possibilities can vary between storage protocols and the development stage of the storage plugin.


================================================
FILE: docs/snakefiles/testing.rst
================================================
.. _snakefiles-testing:

===================================
Automatically generating unit tests
===================================

Snakemake can automatically generate unit tests from a workflow that has already been successfully executed.
By running

.. code-block:: bash

    snakemake --generate-unit-tests

Snakemake is instructed to take one representative job for each rule and copy its input files to a hidden folder ``.tests/unit``,
along with generating test cases for Pytest_.
Pytest_ tests can be run as:

.. code-block:: bash

    pytest .tests/unit/

or, optionally, if you want to use a local conda cache and disable pytest caching:

.. code-block:: bash

    pytest -p no:cacheprovider .tests/unit/ --conda-prefix /path/to/cache/conda/

Each auto-generated unit test is stored in a file ``.tests/unit/test_<rulename>.py``, and executes just the one representative job of the respective rule.
After successful execution of the job, it will compare the obtained results with those that have been present when running ``snakemake --generate-unit-tests``.
By default, the comparison happens byte by byte (using ``cmp/zcmp/bzcmp/xzcmp``). This behavior can be overwritten by modifying the test file.

NOTE: Importantly, such unit tests shall not be generated from big data, as they should usually be finished in a few seconds.
Furthermore, it makes sense to store the generated unit tests in version control (e.g. git), such that huge files are also not recommended.
Instead, we suggest to first execute the workflow that shall be tested with some kind of small dummy datasets while keeping all temp files (``--notemp``),
and then use the results thereof to generate the unit tests.
The small dummy datasets can in addition be used to generate an integration test, that could e.g. be stored under ``.tests/integration``, next to the unit tests.

.. _Pytest: https://pytest.org



================================================
FILE: docs/snakefiles/utils.rst
================================================
.. _snakefiles-utils:

=====
Utils
=====

The module ``snakemake.utils`` provides a collection of helper functions for common tasks in Snakemake workflows. Details can be found in `the API documentation <https://snakemake-api.readthedocs.io/en/stable/api_reference/snakemake_utils.html>`__.



================================================
FILE: docs/snakefiles/writing_snakefiles.rst
================================================
.. _user_manual-writing_snakefiles:

=================
Writing Workflows
=================

In Snakemake, workflows are specified as Snakefiles.
Inspired by GNU Make, a Snakefile contains rules that denote how to create output files from input files.
Dependencies between rules are handled implicitly, by matching filenames of input files against output files.
Thereby wildcards can be used to write general rules.

.. _snakefiles-grammar:

-------
Grammar
-------

The Snakefile syntax obeys the following grammar, given in `extended Backus-Naur form (EBNF) <https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form>`_.

.. code-block:: text

    snakemake    = statement | rule | include | workdir | module | configfile | container
    rule         = "rule" (identifier | "") ":" ruleparams
    include      = "include:" stringliteral
    workdir      = "workdir:" stringliteral
    module       = "module" identifier ":" moduleparams
    configfile   = "configfile" ":" stringliteral
    userule      = "use" "rule" (identifier | "*") "from" identifier ["as" identifier] ["with" ":" norunparams]
    ni           = NEWLINE INDENT
    norunparams  = [ni input] [ni output] [ni params] [ni message] [ni threads] [ni resources] [ni log] [ni conda] [ni container] [ni benchmark] [ni cache] [ni priority]
    ruleparams   = norunparams [ni (run | shell | script | notebook)] NEWLINE snakemake
    input        = "input" ":" parameter_list
    output       = "output" ":" parameter_list
    params       = "params" ":" parameter_list
    log          = "log" ":" parameter_list
    benchmark    = "benchmark" ":" statement
    cache        = "cache" ":" bool
    message      = "message" ":" stringliteral
    threads      = "threads" ":" integer
    priority     = "priority" ":" integer
    resources    = "resources" ":" parameter_list
    version      = "version" ":" statement
    conda        = "conda" ":" stringliteral
    container    = "container" ":" stringliteral
    run          = "run" ":" ni statement
    shell        = "shell" ":" stringliteral
    script       = "script" ":" stringliteral
    notebook     = "notebook" ":" stringliteral
    moduleparams = [ni snakefile] [ni metawrapper] [ni config] [ni skipval]
    snakefile    = "snakefile" ":" stringliteral
    metawrapper  = "meta_wrapper" ":" stringliteral
    config       = "config" ":" stringliteral
    skipval      = "skip_validation" ":" stringliteral
    

while all not defined non-terminals map to their Python equivalents.

.. _snakefiles-depend_version:

Depend on a Minimum Snakemake Version
-------------------------------------

From Snakemake 3.2 on, if your workflow depends on a minimum Snakemake version, you can easily ensure that at least this version is installed via

.. code-block:: python

    from snakemake.utils import min_version

    min_version("3.2")

given that your minimum required version of Snakemake is 3.2. The statement will raise a WorkflowError (and therefore abort the workflow execution) if the version is not met.



================================================
FILE: docs/tutorial/additional_features.rst
================================================
.. _tutorial-additional_features:

Additional features
-------------------

.. _Snakemake: https://snakemake.readthedocs.io
.. _Snakemake homepage: https://snakemake.readthedocs.io
.. _GNU Make: https://www.gnu.org/software/make
.. _Python: https://www.python.org
.. _BWA: http://bio-bwa.sourceforge.net
.. _SAMtools: https://www.htslib.org
.. _BCFtools: https://www.htslib.org
.. _Pandas: https://pandas.pydata.org
.. _Miniconda: https://conda.pydata.org/miniconda.html
.. _Conda: https://conda.pydata.org
.. _Bash: https://www.tldp.org/LDP/Bash-Beginners-Guide/html
.. _Atom: https://atom.io
.. _Anaconda: https://anaconda.org
.. _Graphviz: https://www.graphviz.org
.. _RestructuredText: https://docutils.sourceforge.io/docs/user/rst/quickstart.html
.. _data URI: https://developer.mozilla.org/en-US/docs/Web/HTTP/data_URIs
.. _JSON: https://json.org
.. _YAML: https://yaml.org
.. _DRMAA: https://www.drmaa.org
.. _rpy2: https://rpy2.github.io
.. _R: https://www.r-project.org
.. _Rscript: https://stat.ethz.ch/R-manual/R-devel/library/utils/html/Rscript.html
.. _PyYAML: https://pyyaml.org
.. _Docutils: https://docutils.sourceforge.io
.. _Bioconda: https://bioconda.github.io
.. _Vagrant: https://www.vagrantup.com
.. _Vagrant Documentation: https://docs.vagrantup.com
.. _Blogpost: https://blog.osteel.me/posts/2015/01/25/how-to-use-vagrant-on-windows.html
.. _slides: https://slides.com/johanneskoester/deck-1

In the following, we introduce some features that are beyond the scope of above example workflow.
For details and even more features, see :ref:`user_manual-writing_snakefiles`, :ref:`project_info-faq` and the command line help (``snakemake --help``).


Benchmarking
::::::::::::

With the ``benchmark`` directive, Snakemake can be instructed to **measure the wall clock time of a job**.
We activate benchmarking for the rule ``bwa_map``:

.. code:: python

    rule bwa_map:
        input:
            "data/genome.fa",
            lambda wildcards: config["samples"][wildcards.sample]
        output:
            temp("mapped_reads/{sample}.bam")
        params:
            rg="@RG\tID:{sample}\tSM:{sample}"
        log:
            "logs/bwa_mem/{sample}.log"
        benchmark:
            "benchmarks/{sample}.bwa.benchmark.txt"
        threads: 8
        shell:
            "(bwa mem -R '{params.rg}' -t {threads} {input} | "
            "samtools view -Sb - > {output}) 2> {log}"

The ``benchmark`` directive takes a string that points to the file where benchmarking results shall be stored.
Similar to output files, the path can contain wildcards (it must be the same wildcards as in the output files).
When a job derived from the rule is executed, Snakemake will measure the wall clock time and memory usage (in MiB) and store it in the file in tab-delimited format.
It is possible to repeat a benchmark multiple times in order to get a sense for the variability of the measurements.
This can be done by annotating the benchmark file, e.g., with ``repeat("benchmarks/{sample}.bwa.benchmark.txt", 3)`` Snakemake can be told to run the job three times.
The repeated measurements occur as subsequent lines in the tab-delimited benchmark file.

Modularization
::::::::::::::

In order to reuse building blocks or simply to structure large workflows, it is sometimes reasonable to **split a workflow into multiple Snakefiles**.
For this, Snakemake provides the ``include`` directive to include another Snakefile into the current one, e.g.:

.. code:: python

    include: "path/to/other.smk"

As can be seen, the default file extensions for snakefiles other than the main snakefile is ``.smk``.
Alternatively, Snakemake allows to **define external workflows as modules**.
A module refers to a working directory with a complete Snakemake workflow.
Output files of that sub-workflow can be used in the current Snakefile.
When executing, Snakemake ensures that the output files of the sub-workflow are up-to-date before executing the current workflow.
This mechanism is particularly useful when you want to extend a previous analysis without modifying it.
For details about modules, see the :ref:`documentation <snakefiles-modules>`.


Exercise
........

* Put the read mapping related rules into a separate Snakefile and use the ``include`` directive to make them available in our example workflow again.


.. _tutorial-conda:

Automatic deployment of software dependencies
:::::::::::::::::::::::::::::::::::::::::::::

In order to get a fully reproducible data analysis, it is not sufficient to
be able to execute each step and document all used parameters.
The used software tools and libraries have to be documented as well.
In this tutorial, you have already seen how Conda_ can be used to specify an
isolated software environment for a whole workflow. With Snakemake, you can
go one step further and specify Conda environments per rule.
This way, you can even make use of conflicting software versions (e.g. combine
Python 2 with Python 3).

In our example, instead of using an external environment we can specify
environments per rule, e.g.:

.. code:: python

  rule samtools_index:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam.bai"
    conda:
        "envs/samtools.yaml"
    shell:
        "samtools index {input}"

with ``envs/samtools.yaml`` defined as

.. code:: yaml

  channels:
    - bioconda
    - conda-forge
  dependencies:
    - samtools =1.9

.. note::

  The conda directive does not work in combination with ``run`` blocks, because
  they have to share their Python environment with the surrounding snakefile.

When Snakemake is executed with

.. code:: console

  snakemake --software-deployment-method conda --cores 1

or the short form

.. code:: console

    snakemake --sdm conda -c 1

it will automatically create required environments and
activate them before a job is executed.
It is best practice to specify at least the `major and minor version <https://semver.org/>`_ of any packages
in the environment definition. Specifying environments per rule in this way has two
advantages.
First, the workflow definition also documents all used software versions.
Second, a workflow can be re-executed (without admin rights)
on a vanilla system, without installing any
prerequisites apart from Snakemake and Miniconda_.


Tool wrappers
:::::::::::::

In order to simplify the utilization of popular tools, Snakemake provides a
repository of so-called wrappers
(the `Snakemake wrapper repository <https://snakemake-wrappers.readthedocs.io>`_).
A wrapper is a short script that wraps (typically)
a command line application and makes it directly addressable from within Snakemake.
For this, Snakemake provides the ``wrapper`` directive that can be used instead of
``shell``, ``script``, or ``run``.
For example, the rule ``samtools_index`` could alternatively look like this:

.. code:: python

  rule samtools_index:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam.bai"
    conda:
        "envs/samtools.yaml"
    log:
        "logs/samtools_index/{sample}.log",
    params:
        extra="",  # optional params string
    wrapper:
        "v6.2.0/bio/samtools/index"

.. note::

  Updates to the Snakemake wrapper repository are automatically tested via
  `continuous integration <https://en.wikipedia.org/wiki/Continuous_integration>`_.

The wrapper directive expects a (partial) URL that points to a wrapper in the repository.
These can be looked up in the corresponding `database <https://snakemake-wrappers.readthedocs.io>`_.
The first part of the URL is a Git version tag. Upon invocation, Snakemake
will automatically download the requested version of the wrapper.
Furthermore, in combination with ``--software-deployment-method conda`` (see :ref:`tutorial-conda`),
the required software will be automatically deployed before execution.

.. _tutorial-cloud:

Cluster or cloud execution
::::::::::::::::::::::::::

Executing jobs on a cluster or in the cloud is supported by so-called executor plugins, which are distributed and documented via the `Snakemake plugin catalog <https://snakemake.github.io/snakemake-plugin-catalog>`_.

Constraining wildcards
::::::::::::::::::::::

Snakemake uses regular expressions to match output files to input files and determine dependencies between the jobs.
Sometimes it is useful to constrain the values a wildcard can have.
This can be achieved by adding a regular expression that describes the set of allowed wildcard values.
For example, the wildcard ``sample`` in the output file ``"sorted_reads/{sample}.bam"`` can be constrained to only allow alphanumeric sample names as ``"sorted_reads/{sample,[A-Za-z0-9]+}.bam"``.
Constraints may be defined per rule or globally using the ``wildcard_constraints`` keyword, as demonstrated in :ref:`snakefiles-wildcards`.
This mechanism helps to solve two kinds of ambiguity.

* It can help to avoid ambiguous rules, i.e. two or more rules that can be applied to generate the same output file. Other ways of handling ambiguous rules are described in the Section :ref:`snakefiles-ambiguous-rules`.
* It can help to guide the regular expression based matching so that wildcards are assigned to the right parts of a file name. Consider the output file ``{sample}.{group}.txt`` and assume that the target file is ``A.1.normal.txt``. It is not clear whether ``dataset="A.1"`` and ``group="normal"`` or ``dataset="A"`` and ``group="1.normal"`` is the right assignment. Here, constraining the dataset wildcard by ``{sample,[A-Z]+}.{group}`` solves the problem.

When dealing with ambiguous rules, it is best practice to first try to solve the ambiguity by using a proper file structure, for example, by separating the output files of different steps in different directories.

As follow-up to this tutorial, we recommend to have a look at the :ref:`interaction, visualization and reporting tutorial <interaction_visualization_reporting_tutorial>`, which focuses on Snakemake's ability to cover the last mile of data analysis, i.e., the generation of publication ready reports and interactive visualizations.


================================================
FILE: docs/tutorial/advanced.rst
================================================
.. _tutorial-advanced:

Advanced: Decorating the example workflow
-----------------------------------------

.. _Snakemake: https://snakemake.readthedocs.io
.. _Snakemake homepage: https://snakemake.readthedocs.io
.. _GNU Make: https://www.gnu.org/software/make
.. _Python: https://www.python.org
.. _BWA: http://bio-bwa.sourceforge.net
.. _SAMtools: https://www.htslib.org
.. _BCFtools: https://www.htslib.org
.. _Pandas: https://pandas.pydata.org
.. _Miniconda: https://conda.pydata.org/miniconda.html
.. _Conda: https://conda.pydata.org
.. _Bash: https://www.tldp.org/LDP/Bash-Beginners-Guide/html
.. _Atom: https://atom.io
.. _Anaconda: https://anaconda.org
.. _Graphviz: https://www.graphviz.org
.. _RestructuredText: https://docutils.sourceforge.io/docs/user/rst/quickstart.html
.. _data URI: https://developer.mozilla.org/en-US/docs/Web/HTTP/data_URIs
.. _JSON: https://json.org
.. _YAML: https://yaml.org
.. _DRMAA: https://www.drmaa.org
.. _rpy2: https://rpy2.github.io
.. _R: https://www.r-project.org
.. _Rscript: https://stat.ethz.ch/R-manual/R-devel/library/utils/html/Rscript.html
.. _PyYAML: https://pyyaml.org
.. _Docutils: https://docutils.sourceforge.io
.. _Bioconda: https://bioconda.github.io
.. _Vagrant: https://www.vagrantup.com
.. _Vagrant Documentation: https://docs.vagrantup.com
.. _Blogpost: https://blog.osteel.me/posts/2015/01/25/how-to-use-vagrant-on-windows.html
.. _slides: https://slides.com/johanneskoester/deck-1

Now that the basic concepts of Snakemake have been illustrated, we can introduce some advanced functionality.

Step 1: Specifying the number of used threads
:::::::::::::::::::::::::::::::::::::::::::::

For some tools, it is advisable to use more than one thread in order to speed up the computation.
**Snakemake can be made aware of the threads a rule needs** with the ``threads`` directive.
In our example workflow, it makes sense to use multiple threads for the rule ``bwa_map``:

.. code:: python

    rule bwa_map:
        input:
            "data/genome.fa",
            "data/samples/{sample}.fastq"
        output:
            "mapped_reads/{sample}.bam"
        threads: 8
        shell:
            "bwa mem -t {threads} {input} | samtools view -Sb - > {output}"

The number of threads can be propagated to the shell command with the familiar braces notation (i.e. ``{threads}``).
If no ``threads`` directive is given, a rule is assumed to need 1 thread.

When a workflow is executed, **the number of threads the jobs need is considered by the Snakemake scheduler**.
In particular, the scheduler ensures that the sum of the threads of all jobs running at the same time does not exceed a given number of available CPU cores.
This number is given with the ``--cores`` command line argument, which is mandatory for ``snakemake`` calls that actually run the workflow.
For example

.. code:: console

    $ snakemake --cores 10


.. note::

  Apart from the very common thread resource, Snakemake provides a ``resources`` directive that can be used to **specify arbitrary resources**, e.g., memory usage or auxiliary computing devices like GPUs.
  Similar to threads, these can be considered by the scheduler when an available amount of that resource is given with the command line argument ``--resources`` (see :ref:`snakefiles-resources`).

would execute the workflow with 10 cores.
Since the rule ``bwa_map`` needs 8 threads, only one job of the rule can run at a time, and the Snakemake scheduler will try to saturate the remaining cores with other jobs like, e.g., ``samtools_sort``.
The threads directive in a rule is interpreted as a maximum: when **less cores than threads** are provided, the number of threads a rule uses will be **reduced to the number of given cores**.

If ``--cores all`` is given, all available cores are used.

Exercise
........

* With the flag ``--forceall`` you can enforce a complete re-execution of the workflow. Combine this flag with different values for ``--cores`` and examine how the scheduler selects jobs to run in parallel.

Step 2: Config files
::::::::::::::::::::

So far, we specified which samples to consider by providing a Python list in the Snakefile.
However, often you want your workflow to be customizable, so that it can easily be adapted to new data.
For this purpose, Snakemake provides a `config file mechanism <https://snakemake.readthedocs.io/en/latest/snakefiles/configuration.html>`_.
Config files can be written in JSON_ or YAML_, and are used with the ``configfile`` directive.
In our example workflow, we add the line

.. code:: python

    configfile: "config.yaml"

to the top of the Snakefile.
Snakemake will load the config file and store its contents into a globally available `dictionary <https://docs.python.org/3/tutorial/datastructures.html#dictionaries>`_ named ``config``.
In our case, it makes sense to specify the samples in ``config.yaml`` as

.. code:: yaml

    samples:
        A: data/samples/A.fastq
        B: data/samples/B.fastq

Now, we can remove the statement defining ``SAMPLES`` from the Snakefile and change the rule ``bcftools_call`` to

.. code:: python

    rule bcftools_call:
        input:
            fa="data/genome.fa",
            bam=expand("sorted_reads/{sample}.bam", sample=config["samples"]),
            bai=expand("sorted_reads/{sample}.bam.bai", sample=config["samples"])
        output:
            "calls/all.vcf"
        shell:
            "bcftools mpileup -f {input.fa} {input.bam} | "
            "bcftools call -mv - > {output}"

.. _tutorial-input_functions:

Step 3: Input functions
:::::::::::::::::::::::

Since we have stored the path to the FASTQ files in the config file, we can also generalize the rule ``bwa_map`` to use these paths.
This case is different to the rule ``bcftools_call`` we modified above.
To understand this, it is important to know that Snakemake workflows are executed in three phases.

1. In the **initialization** phase, the files defining the workflow are parsed and all rules are instantiated.
2. In the **DAG** phase, the directed acyclic dependency graph of all jobs is built by filling wildcards and matching input files to output files.
3. In the **scheduling** phase, the DAG of jobs is executed, with jobs started according to the available resources.

The expand functions in the list of input files of the rule ``bcftools_call`` are executed during the initialization phase.
In this phase, we don't know about jobs, wildcard values and rule dependencies.
Hence, we cannot determine the FASTQ paths for rule ``bwa_map`` from the config file in this phase, because we don't even know which jobs will be generated from that rule.
Instead, we need to defer the determination of input files to the DAG phase.
This can be achieved by specifying an **input function** instead of a string as inside of the input directive.
For the rule ``bwa_map`` this works as follows:

.. code:: python

    def get_bwa_map_input_fastqs(wildcards):
        return config["samples"][wildcards.sample]
    
    rule bwa_map:
        input:
            "data/genome.fa",
            get_bwa_map_input_fastqs
        output:
            "mapped_reads/{sample}.bam"
        threads: 8
        shell:
            "bwa mem -t {threads} {input} | samtools view -Sb - > {output}"

Any normal function would work as well.
Input functions take as **single argument** a ``wildcards`` object, that allows to access the wildcards values via attributes (here ``wildcards.sample``).
They have to **return a string or a list of strings**, that are interpreted as paths to input files (here, we return the path that is stored for the sample in the config file).
Input functions are evaluated once the wildcard values of a job are determined.


Exercise
........

* In the ``data/samples`` folder, there is an additional sample ``C.fastq``. Add that sample to the config file and see how Snakemake wants to recompute the part of the workflow belonging to the new sample, when invoking with ``snakemake -n --forcerun bcftools_call``.

Step 4: Rule parameters
:::::::::::::::::::::::

Sometimes, shell commands are not only composed of input and output files and some static flags.
In particular, it can happen that additional parameters need to be set depending on the wildcard values of the job.
For this, Snakemake allows to **define arbitrary parameters** for rules with the ``params`` directive.
In our workflow, it is reasonable to annotate aligned reads with so-called read groups, that contain metadata like the sample name.
We modify the rule ``bwa_map`` accordingly:

.. code:: python

    rule bwa_map:
        input:
            "data/genome.fa",
            get_bwa_map_input_fastqs
        output:
            "mapped_reads/{sample}.bam"
        params:
            rg=r"@RG\tID:{sample}\tSM:{sample}"
        threads: 8
        shell:
            "bwa mem -R '{params.rg}' -t {threads} {input} | samtools view -Sb - > {output}"

.. note::

  The ``params`` directive can also take functions like in Step 3 to defer
  initialization to the DAG phase. In contrast to input functions, these can
  optionally take additional arguments ``input``, ``output``, ``threads``, and ``resources``.

Similar to input and output files, ``params`` can be accessed from the shell command, the Python based ``run`` block, or the script directive (see :ref:`tutorial-script`).

Exercise
........

* Variant calling can consider a lot of parameters. A particularly important one is the prior mutation rate (1e-3 per default). It is set via the flag ``-P`` of the ``bcftools call`` command. Consider making this flag configurable via adding a new key to the config file and using the ``params`` directive in the rule ``bcftools_call`` to propagate it to the shell command.

Step 5: Logging
:::::::::::::::

When executing a large workflow, it is usually desirable to store the logging output of each job into a separate file, instead of just printing all logging output to the terminal---when multiple jobs are run in parallel, this would result in chaotic output.
For this purpose, Snakemake allows to **specify log files** for rules.
Log files are defined via the ``log`` directive and handled similarly to output files, but they are not subject of rule matching and are not cleaned up when a job fails.
We modify our rule ``bwa_map`` as follows:

.. code:: python

    rule bwa_map:
        input:
            "data/genome.fa",
            get_bwa_map_input_fastqs
        output:
            "mapped_reads/{sample}.bam"
        params:
            rg=r"@RG\tID:{sample}\tSM:{sample}"
        log:
            "logs/bwa_mem/{sample}.log"
        threads: 8
        shell:
            "(bwa mem -R '{params.rg}' -t {threads} {input} | "
            "samtools view -Sb - > {output}) 2> {log}"

.. note::

  It is best practice to store all log files in a subdirectory ``logs/``, prefixed by the rule or tool name.

The shell command is modified to `collect STDERR output <https://tldp.org/LDP/abs/html/io-redirection.html>`_ of both ``bwa`` and ``samtools`` and pipe it into the file referred to by ``{log}``.
Log files must contain exactly the same wildcards as the output files to avoid file name clashes between different jobs of the same rule.

Exercise
........

* Add a log directive to the ``bcftools_call`` rule as well.
* Time to re-run the whole workflow (remember the command line flags to force re-execution). See how log files are created for variant calling and read mapping.
* The ability to track the provenance of each generated result is an important step towards reproducible analyses. Apart from the ``report`` functionality discussed before, Snakemake can summarize various provenance information for all output files of the workflow. The flag ``--summary`` prints a table associating each output file with the rule used to generate it, the creation date and optionally the version of the tool used for creation is provided. Further, the table informs about updated input files and changes to the source code of the rule after creation of the output file. Invoke Snakemake with ``--summary`` to examine the information for our example.

.. _tutorial_temp-and-protected-files:

Step 6: Temporary and protected files
:::::::::::::::::::::::::::::::::::::

In our workflow, we create two BAM files for each sample, namely
the output of the rules ``bwa_map`` and ``samtools_sort``.
When not dealing with examples, the underlying data is usually huge.
Hence, the resulting BAM files need a lot of disk space and their creation takes some time.
To save disk space, you can **mark output files as temporary**.
Snakemake will delete the marked files for you, once all the consuming jobs (that need it as input) have been executed.
We use this mechanism for the output file of the rule ``bwa_map``:

.. code:: python

    rule bwa_map:
        input:
            "data/genome.fa",
            get_bwa_map_input_fastqs
        output:
            temp("mapped_reads/{sample}.bam")
        params:
            rg=r"@RG\tID:{sample}\tSM:{sample}"
        log:
            "logs/bwa_mem/{sample}.log"
        threads: 8
        shell:
            "(bwa mem -R '{params.rg}' -t {threads} {input} | "
            "samtools view -Sb - > {output}) 2> {log}"

This results in the deletion of the BAM file once the corresponding ``samtools_sort`` job has been executed.
Since the creation of BAM files via read mapping and sorting is computationally expensive, it is reasonable to **protect** the final BAM file **from accidental deletion or modification**.
We modify the rule ``samtools_sort`` to mark its output file as ``protected``:

.. code:: python

    rule samtools_sort:
        input:
            "mapped_reads/{sample}.bam"
        output:
            protected("sorted_reads/{sample}.bam")
        shell:
            "samtools sort -T sorted_reads/{wildcards.sample} "
            "-O bam {input} > {output}"

After successful execution of the job, Snakemake will write-protect the output file in the filesystem, so that it can't be overwritten or deleted by accident.

Exercise
........

* Re-execute the whole workflow and observe how Snakemake handles the temporary and protected files.
* Run Snakemake with the target ``mapped_reads/A.bam``. Although the file is marked as temporary, you will see that Snakemake does not delete it because it is specified as a target file.
* Try to re-execute the whole workflow again with the dry-run option. You will see that it fails (as intended) because Snakemake cannot overwrite the protected output files.

After having a look at the summary, please go on with the :ref:`"additional features" part of the tutorial <tutorial-additional_features>`.

Summary
:::::::

For this advanced part of the tutorial, we have now created a ``config.yaml`` configuration file:

.. code:: yaml

    samples:
        A: data/samples/A.fastq
        B: data/samples/B.fastq
    
    prior_mutation_rate: 0.001


With this, the final version of our workflow in the ``Snakefile`` looks like this:

.. code:: python

    configfile: "config.yaml"


    rule all:
        input:
            "plots/quals.svg"


    def get_bwa_map_input_fastqs(wildcards):
        return config["samples"][wildcards.sample]
    

    rule bwa_map:
        input:
            "data/genome.fa",
            get_bwa_map_input_fastqs
        output:
            temp("mapped_reads/{sample}.bam")
        params:
            rg=r"@RG\tID:{sample}\tSM:{sample}"
        log:
            "logs/bwa_mem/{sample}.log"
        threads: 8
        shell:
            "(bwa mem -R '{params.rg}' -t {threads} {input} | "
            "samtools view -Sb - > {output}) 2> {log}"


    rule samtools_sort:
        input:
            "mapped_reads/{sample}.bam"
        output:
            protected("sorted_reads/{sample}.bam")
        shell:
            "samtools sort -T sorted_reads/{wildcards.sample} "
            "-O bam {input} > {output}"


    rule samtools_index:
        input:
            "sorted_reads/{sample}.bam"
        output:
            "sorted_reads/{sample}.bam.bai"
        shell:
            "samtools index {input}"


    rule bcftools_call:
        input:
            fa="data/genome.fa",
            bam=expand("sorted_reads/{sample}.bam", sample=config["samples"]),
            bai=expand("sorted_reads/{sample}.bam.bai", sample=config["samples"])
        output:
            "calls/all.vcf"
        params:
            rate=config["prior_mutation_rate"]
        log:
            "logs/bcftools_call/all.log"
        shell:
            "(bcftools mpileup -f {input.fa} {input.bam} | "
            "bcftools call -mv -P {params.rate} - > {output}) 2> {log}"


    rule plot_quals:
        input:
            "calls/all.vcf"
        output:
            "plots/quals.svg"
        script:
            "scripts/plot-quals.py"

Now, please go on with the :ref:`"additional features" part of the tutorial <tutorial-additional_features>`.


================================================
FILE: docs/tutorial/basics.rst
================================================
.. _tutorial-basics:

Basics: An example workflow
---------------------------

.. _Snakemake: https://snakemake.readthedocs.io
.. _Snakemake homepage: https://snakemake.readthedocs.io
.. _GNU Make: https://www.gnu.org/software/make
.. _Python: https://www.python.org
.. _BWA: http://bio-bwa.sourceforge.net
.. _SAMtools: https://www.htslib.org
.. _BCFtools: https://www.htslib.org
.. _Pandas: https://pandas.pydata.org
.. _Miniconda: https://conda.pydata.org/miniconda.html
.. _Conda: https://conda.pydata.org
.. _Bash: https://www.tldp.org/LDP/Bash-Beginners-Guide/html
.. _Visual Studio Code: https://code.visualstudio.com/
.. _Snakemake extension: https://marketplace.visualstudio.com/items?itemName=Snakemake.snakemake-lang
.. _remote extension: https://marketplace.visualstudio.com/items?itemName=ms-vscode.remote-explorer
.. _Anaconda: https://anaconda.org
.. _Graphviz: https://www.graphviz.org
.. _RestructuredText: https://docutils.sourceforge.io/docs/user/rst/quickstart.html
.. _data URI: https://developer.mozilla.org/en-US/docs/Web/HTTP/data_URIs
.. _JSON: https://json.org
.. _YAML: https://yaml.org
.. _DRMAA: https://www.drmaa.org
.. _rpy2: https://rpy2.github.io
.. _R: https://www.r-project.org
.. _Rscript: https://stat.ethz.ch/R-manual/R-devel/library/utils/html/Rscript.html
.. _PyYAML: https://pyyaml.org
.. _Docutils: https://docutils.sourceforge.io
.. _Bioconda: https://bioconda.github.io
.. _Vagrant: https://www.vagrantup.com
.. _Vagrant Documentation: https://docs.vagrantup.com
.. _Blogpost: https://blog.osteel.me/posts/2015/01/25/how-to-use-vagrant-on-windows.html
.. _slides: https://slides.com/johanneskoester/deck-1

Please make sure that you have **activated** the environment we created before, and that you have an open terminal in the working directory you have created.

**A Snakemake workflow is defined by specifying rules in a Snakefile**.
**Rules decompose the workflow into small steps** (for example, the application of a single tool) by specifying how to create sets of **output files** from sets of **input files**.
Snakemake automatically **determines the dependencies** between the rules by matching file names.

The Snakemake language extends the Python language, adding syntactic structures for rule definition and additional controls.
All added syntactic structures begin with a keyword followed by a code block that is either in the same line or indented and consisting of multiple lines.
The resulting syntax resembles that of original Python constructs.

In the following, we will introduce the Snakemake syntax by creating an example workflow.
The workflow comes from the domain of genome analysis.
It maps sequencing reads to a reference genome and calls variants on the mapped reads.
The tutorial does not require you to know what this is about.
Nevertheless, we provide some background in the following paragraph.

.. _tutorial-background:

Background
::::::::::

The genome of a living organism encodes its hereditary information.
It serves as a blueprint for proteins, which form living cells, carry information
and drive chemical reactions.
Differences between species, populations or individuals can be reflected by differences in the genome.
Certain variants can cause syndromes or predisposition for certain diseases, or cause cancerous growth in the case of tumour cells that have accumulated changes with respect to healthy cells.
This makes the genome a major target of biological and medical research.
Today, it is often analyzed with DNA sequencing, producing gigabytes of data from
a single biological sample (for example a biopsy of some tissue).
For technical reasons, DNA sequencing cuts the DNA of a sample into millions
of small pieces, called **reads**.
In order to recover the genome of the sample, one has to map these reads against
a known **reference genome** (for example, the human one obtained during the famous
`human genome project <https://en.wikipedia.org/wiki/Human_Genome_Project>`_).
This task is called **read mapping**.
Often, it is of interest where an individual genome is different from the species-wide consensus
represented with the reference genome.
Such differences are called **variants**. They are responsible for harmless individual
differences (like eye color), but can also cause diseases like cancer.
By investigating the differences between the mapped reads
and the reference sequence at a particular genome position, variants can be detected.
This is a statistical challenge, because they have
to be distinguished from artifacts generated by the sequencing process.

Step 1: Mapping reads
:::::::::::::::::::::

Our first Snakemake rule maps reads of a given sample to a given reference genome (see :ref:`tutorial-background`).
For this, we will use the tool bwa_, specifically the subcommand ``bwa mem``.
In the working directory, **create a new file** called ``Snakefile`` with an editor of your choice.
We propose to use the integrated development environment (IDE) tool `Visual Studio Code`_, since it provides a good syntax highlighting `Snakemake extension`_ and a `remote extension`_ for directly using the IDE on a remote server.
In the Snakefile, define the following rule:

.. code:: python

    rule bwa_map:
        input:
            "data/genome.fa",
            "data/samples/A.fastq"
        output:
            "mapped_reads/A.bam"
        shell:
            "bwa mem {input} | samtools view -Sb - > {output}"

.. note::

    A common error is to forget the comma between the input or output items.
    Since Python concatenates subsequent strings, this can lead to unexpected behavior.

A Snakemake rule has a name (here ``bwa_map``) and a number of directives, here ``input``, ``output`` and ``shell``.
The ``input`` and ``output`` directives are followed by lists of files that are expected to be used or created by the rule.
In the simplest case, these are just explicit Python strings.
The ``shell`` directive is followed by a Python string containing the shell command to execute.
In the shell command string, we can refer to elements of the rule via braces notation (similar to the Python format function).
Here, we refer to the output file by specifying ``{output}`` and to the input files by specifying ``{input}``.
Since the rule has multiple input files, Snakemake will concatenate them, separated by a whitespace.
In other words, Snakemake will replace ``{input}`` with ``data/genome.fa data/samples/A.fastq`` before executing the command.
The shell command invokes ``bwa mem`` with reference genome and reads, and pipes the output into ``samtools`` which creates a compressed `BAM <https://en.wikipedia.org/wiki/Binary_Alignment_Map>`_ file containing the alignments.
The output of ``samtools`` is redirected into the output file defined by the rule with ``>``.

.. note::

  It is best practice to have subsequent steps of a workflow in separate, unique, output folders. This keeps the working directory structured. Further, such unique prefixes allow Snakemake to quickly discard most rules in its search for rules that can provide the requested input. This accelerates the resolution of the rule dependencies in a workflow.

When a workflow is executed, Snakemake tries to generate given **target** files.
Target files can be specified via the command line.
By executing

.. code:: console

    $ snakemake -np mapped_reads/A.bam

in the working directory containing the Snakefile, we tell Snakemake to generate the target file ``mapped_reads/A.bam``.
Since we used the ``-n`` (or ``--dry-run``) flag, Snakemake will only show the execution plan instead of actually performing the steps.
The ``-p`` flag instructs Snakemake to also print the resulting shell command for illustration.
To generate the target files, **Snakemake applies the rules given in the Snakefile in a top-down way**.
The application of a rule to generate a set of output files is called **job**.
For each input file of a job, Snakemake again (i.e. recursively) determines rules that can be applied to generate it.
This yields a `directed acyclic graph (DAG) <https://en.wikipedia.org/wiki/Directed_acyclic_graph>`_ of jobs where the edges represent dependencies.
So far, we only have a single rule, and the DAG of jobs consists of a single node.
Nevertheless, we can **execute our workflow** with

.. code:: console

    $ snakemake --cores 1 mapped_reads/A.bam

Whenever executing a workflow, you need to specify the number of cores to use.
For this tutorial, we will use a single core for now.
Later you will see how parallelization works.
Note that, after completion of above command, Snakemake will not try to create ``mapped_reads/A.bam`` again, because it is already present in the file system.
Snakemake **only re-runs jobs if one of the input files is newer than one of the output files or one of the input files will be updated by another job**.

Step 2: Generalizing the read mapping rule
::::::::::::::::::::::::::::::::::::::::::

Obviously, the rule will only work for a single sample with reads in the file ``data/samples/A.fastq``.
However, Snakemake allows **generalizing rules by using named wildcards**.
Simply replace the ``A`` in the second input file and in the output file with the wildcard ``{sample}``, leading to

.. code:: python

    rule bwa_map:
        input:
            "data/genome.fa",
            "data/samples/{sample}.fastq"
        output:
            "mapped_reads/{sample}.bam"
        shell:
            "bwa mem {input} | samtools view -Sb - > {output}"

.. note::

  Note that if a rule has multiple output files, Snakemake requires them to all
  have exactly the same wildcards. Otherwise, it could happen that two jobs
  running the same rule in parallel want to write to the same file.

When Snakemake determines that this rule can be applied to generate a target file by replacing the wildcard ``{sample}`` in the output file with an appropriate value, it will propagate that value to all occurrences of ``{sample}`` in the input files and thereby determine the necessary input for the resulting job.
Note that you can have multiple wildcards in your file paths, however, to avoid conflicts with other jobs of the same rule, **all output files** of a rule have to **contain exactly the same wildcards**.

When executing

.. code:: console

    $ snakemake -np mapped_reads/B.bam

Snakemake will determine that the rule ``bwa_map`` can be applied to generate the target file by replacing the wildcard ``{sample}`` with the value ``B``.
In the output of the dry-run, you will see how the wildcard value is propagated to the input files and all filenames in the shell command.
You can also **specify multiple targets**, for example:

.. code:: console

    $ snakemake -np mapped_reads/A.bam mapped_reads/B.bam

Some Bash_ magic can make this particularly handy. For example, you can alternatively compose our multiple targets in a single pass via

.. code:: console

    $ snakemake -np mapped_reads/{A,B}.bam

Note that this is not a special Snakemake syntax.
Bash_ is just applying its `brace expansion <https://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_04.html>`_ to the set ``{A,B}``, creating the given path for each element and separating the resulting paths by a whitespace.

In both cases, you will see that Snakemake only proposes to create the output file ``mapped_reads/B.bam``.
This is because you already executed the workflow before (see the previous step) and no input file is newer than the output file ``mapped_reads/A.bam``.
You can update the file modification date of the input file
``data/samples/A.fastq`` via

.. code:: console

    $ touch data/samples/A.fastq

and see how Snakemake wants to re-run the job to create the file ``mapped_reads/A.bam`` by executing

.. code:: console

    $ snakemake -np mapped_reads/A.bam mapped_reads/B.bam


Step 3: Sorting read alignments
:::::::::::::::::::::::::::::::

For later steps, we need the read alignments in the BAM files to be sorted.
This can be achieved with the samtools_ ``sort`` command.
We add the following rule beneath the ``bwa_map`` rule:

.. code:: python

    rule samtools_sort:
        input:
            "mapped_reads/{sample}.bam"
        output:
            "sorted_reads/{sample}.bam"
        shell:
            "samtools sort -T sorted_reads/{wildcards.sample} "
            "-O bam {input} > {output}"

.. note::

  In the shell command above we split the string into two lines, which are however automatically concatenated into one by Python.
  This is a handy pattern to avoid too long shell command lines. When using this, make sure to have a trailing whitespace in each line but the last,
  in order to avoid arguments to become not properly separated.

This rule will take the input file from the ``mapped_reads`` directory and store a sorted version in the ``sorted_reads`` directory.
Note that Snakemake **automatically creates missing directories** before jobs are executed.
For sorting, ``samtools`` requires a prefix specified with the flag ``-T``.
Here, we need the value of the wildcard ``sample``.
Snakemake allows to access wildcards in the shell command via the ``wildcards`` object that has an attribute with the value for each wildcard.

When issuing

.. code:: console

    $ snakemake -np sorted_reads/B.bam

you will see how Snakemake wants to run first the rule ``bwa_map`` and then the rule ``samtools_sort`` to create the desired target file:
as mentioned before, the dependencies are resolved automatically by matching file names.

Step 4: Indexing read alignments and visualizing the DAG of jobs
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

.. note::

  Snakemake uses the `Python format mini language <https://docs.python.org/3/library/string.html#formatexamples>`_ to format shell commands.
  Sometimes you have to use braces (``{}``) for something else in a shell command.
  In that case, you have to escape them by doubling, for example when relying on the bash brace expansion we mentioned above:
  ``ls {{A,B}}.txt``.

Next, we need to use samtools_ again to index the sorted read alignments so that we can quickly access reads by the genomic location they were mapped to.
This can be done with the following rule:

.. code:: python

    rule samtools_index:
        input:
            "sorted_reads/{sample}.bam"
        output:
            "sorted_reads/{sample}.bam.bai"
        shell:
            "samtools index {input}"

Having three steps already, it is a good time to take a closer look at the resulting directed acyclic graph (DAG) of jobs.
By executing

.. code:: console

    $ snakemake sorted_reads/{A,B}.bam.bai --dag | dot -Tsvg > dag.svg


.. note::

  If you went with: :ref:`tutorial-free-on-gitpod`, you can easily view the resulting ``dag.svg`` by right-clicking on the file in the explorer panel on the left and selecting ``Open With -> Preview``.


we create a **visualization of the DAG** using the ``dot`` command provided by Graphviz_.
For the given target files, Snakemake specifies the DAG in the dot language and pipes it into the ``dot`` command, which renders the definition into `SVG format <https://en.wikipedia.org/wiki/Scalable_Vector_Graphics>`_.
The rendered DAG is piped into the file ``dag.svg`` and will look similar to this:

.. image:: workflow/dag_index.png
   :align: center

The DAG contains a node for each job with the edges connecting them representing the dependencies.
The frames of jobs that don't need to be run (because their output is up-to-date) are dashed.
For rules with wildcards, the value of the wildcard for the particular job is displayed in the job node.

Exercise
........

* Run parts of the workflow using different targets. Recreate the DAG and see how different rules' frames become dashed because their output is present and up-to-date.

Step 5: Calling genomic variants
::::::::::::::::::::::::::::::::

The next step in our workflow will aggregate the mapped reads from all samples and jointly call genomic variants on them (see :ref:`tutorial-background`).
For the variant calling, we will combine the two utilities samtools_ and bcftools_.
Snakemake provides a **helper function for collecting input files** that helps us to describe the aggregation in this step.
With

.. code:: python

    expand("sorted_reads/{sample}.bam", sample=SAMPLES)

we obtain a list of files where the given pattern ``"sorted_reads/{sample}.bam"`` was formatted with the values in a given list of samples ``SAMPLES``, i.e.

.. code:: python

    ["sorted_reads/A.bam", "sorted_reads/B.bam"]

The function is particularly useful when the pattern contains multiple wildcards.
For example,

.. code:: python

    expand("sorted_reads/{sample}.{replicate}.bam", sample=SAMPLES, replicate=[0, 1])

would create the product of all elements of ``SAMPLES`` and the list ``[0, 1]``, yielding

.. code:: python

    ["sorted_reads/A.0.bam", "sorted_reads/A.1.bam", "sorted_reads/B.0.bam", "sorted_reads/B.1.bam"]

Here, we use only the simple case of ``expand``.
We first let Snakemake know which samples we want to consider.
Remember that Snakemake works backwards from requested output, and not from available input.
Thus, it does not automatically infer all possible output from, for example, the fastq files in the data folder.
Also remember that Snakefiles are in principle Python code enhanced by some declarative statements to define workflows.
Hence, we can define the list of samples ad-hoc in plain Python at the top of the Snakefile:

.. code:: python

    SAMPLES = ["A", "B"]


.. note::

  If you name input or output files like above, their order won't be preserved when referring to them as ``{input}``.
  Further, note that named and unnamed (i.e., positional) input and output files can be combined, but the positional ones must come first, equivalent to Python functions with keyword arguments.

Later, we will learn about more sophisticated ways like **config files**.
But for now, this is enough so that we can add the following rule to our Snakefile:

.. code:: python

    rule bcftools_call:
        input:
            fa="data/genome.fa",
            bam=expand("sorted_reads/{sample}.bam", sample=SAMPLES),
            bai=expand("sorted_reads/{sample}.bam.bai", sample=SAMPLES)
        output:
            "calls/all.vcf"
        shell:
            "bcftools mpileup -f {input.fa} {input.bam} | "
            "bcftools call -mv - > {output}"

With multiple input or output files, it is sometimes handy to refer to them separately in the shell command.
This can be done by **specifying names for input or output files**, for example with ``fa=...``.
The files can then be referred to in the shell command by name, for example with ``{input.fa}``.
For **long shell commands** like this one, it is advisable to **split the string over multiple indented lines**.
Python will automatically merge it into one.
Further, you will notice that the **input or output file lists can contain arbitrary Python statements**, as long as it returns a string, or a list of strings.
Here, we invoke our ``expand`` function to aggregate over the aligned reads of all samples.


Exercise
........

* obtain the updated DAG of jobs for the target file ``calls/all.vcf``, it should look like this:

.. image:: workflow/dag_call.png
   :align: center


.. _tutorial-script:

Step 6: Using custom scripts
::::::::::::::::::::::::::::

Usually, a workflow not only consists of invoking various tools, but also contains custom code to for example calculate summary statistics or create plots.
While Snakemake also allows you to directly :ref:`write Python code inside a rule <snakefiles-external_scripts>`, it is usually reasonable to move such logic into separate scripts.
For this purpose, Snakemake offers the ``script`` directive.
Add the following rule to your Snakefile:

.. code:: python

    rule plot_quals:
        input:
            "calls/all.vcf"
        output:
            "plots/quals.svg"
        script:
            "scripts/plot-quals.py"


.. note::

  ``snakemake.input`` and ``snakemake.output`` always contain a list of file names, even if the lists each contain only one file name.
  Therefore, to refer to a particular file name, you have to index into that list.
  ``snakemake.output[0]`` will give you the first element of the output file name list, something that always has to be there.

With this rule, we will eventually generate a histogram of the quality scores that have been assigned to the variant calls in the file ``calls/all.vcf``.
The actual Python code to generate the plot is hidden in the script ``scripts/plot-quals.py``.
Script paths are always relative to the referring Snakefile.
In the script, all properties of the rule like ``input``, ``output``, ``wildcards``, etc. are available as attributes of a global ``snakemake`` object.
Create the file ``scripts/plot-quals.py``, with the following content:

.. code:: python

    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
    from pysam import VariantFile

    quals = [record.qual for record in VariantFile(snakemake.input[0])]
    plt.hist(quals)

    plt.savefig(snakemake.output[0])


.. note::

  It is best practice to use the script directive whenever an inline code block would have
  more than a few lines of code.

Although there are other strategies to invoke separate scripts from your workflow
(for example, invoking them via shell commands), the benefit of this is obvious:
the script logic is separated from the workflow logic (and can even be shared between workflows),
but **boilerplate code like the parsing of command line arguments is unnecessary**.

Apart from Python scripts, it is also possible to use R scripts. In R scripts,
an S4 object named ``snakemake`` analogous to the Python case above is available and
allows access to input and output files and other parameters. Here, the syntax
follows that of S4 classes with attributes that are R lists, for example we can access
the first input file with ``snakemake@input[[1]]`` (note that the first file does
not have index 0 here, because R starts counting from 1). Named input and output
files can be accessed in the same way, by just providing the name instead of an
index, for example ``snakemake@input[["myfile"]]``.

For details and examples, see the :ref:`snakefiles-external_scripts` section in the Documentation.


Step 7: Adding a target rule
::::::::::::::::::::::::::::

So far, we always executed the workflow by specifying a target file at the command line.
Apart from filenames, Snakemake **also accepts rule names as targets** if the requested rule does not have wildcards.
Hence, it is possible to write target rules collecting particular subsets of the desired results or all results.
Moreover, if no target is given at the command line, Snakemake will define the **first rule** of the Snakefile as the target.
Hence, it is best practice to have a rule ``all`` at the top of the workflow which has all typically desired target files as input files.

Here, this means that we add a rule

.. code:: python

    rule all:
        input:
            "plots/quals.svg"

to the top of our workflow.
When executing Snakemake with

.. code:: console

    $ snakemake -n

.. note::

   In case you have multiple reasonable sets of target files,
   you can add multiple target rules at the top of the Snakefile. While
   Snakemake will execute the first per default, you can target any of them via
   the command line (for example, ``snakemake -n mytarget``).

the execution plan for creating the file ``plots/quals.svg``, which contains and summarizes all our results, will be shown.
Note that, apart from Snakemake considering the first rule of the workflow as the default target, **the order of rules in the Snakefile is arbitrary and does not influence the DAG of jobs**.

Exercise
........

* Create the DAG of jobs for the complete workflow.
* Execute the complete workflow and have a look at the resulting ``plots/quals.svg``.
* Snakemake provides handy flags for forcing re-execution of parts of the workflow. Have a look at the command line help with ``snakemake --help`` and search for the flag ``--forcerun``. Then, use this flag to re-execute the rule ``samtools_sort`` and see what happens.
* Snakemake displays the reason for each job (under ``reason:``). Perform a dry-run that forces some rules to be reexecuted (using the ``--forcerun`` flag in combination with some rulename) to understand the decisions of Snakemake.

After having a look at the summary, please go on with the :ref:`advanced part of the tutorial <tutorial-advanced>`.

Summary
:::::::

In total, the resulting workflow looks like this:

.. code:: python

    SAMPLES = ["A", "B"]


    rule all:
        input:
            "plots/quals.svg"


    rule bwa_map:
        input:
            "data/genome.fa",
            "data/samples/{sample}.fastq"
        output:
            "mapped_reads/{sample}.bam"
        shell:
            "bwa mem {input} | samtools view -Sb - > {output}"


    rule samtools_sort:
        input:
            "mapped_reads/{sample}.bam"
        output:
            "sorted_reads/{sample}.bam"
        shell:
            "samtools sort -T sorted_reads/{wildcards.sample} "
            "-O bam {input} > {output}"


    rule samtools_index:
        input:
            "sorted_reads/{sample}.bam"
        output:
            "sorted_reads/{sample}.bam.bai"
        shell:
            "samtools index {input}"


    rule bcftools_call:
        input:
            fa="data/genome.fa",
            bam=expand("sorted_reads/{sample}.bam", sample=SAMPLES),
            bai=expand("sorted_reads/{sample}.bam.bai", sample=SAMPLES)
        output:
            "calls/all.vcf"
        shell:
            "bcftools mpileup -f {input.fa} {input.bam} | "
            "bcftools call -mv - > {output}"


    rule plot_quals:
        input:
            "calls/all.vcf"
        output:
            "plots/quals.svg"
        script:
            "scripts/plot-quals.py"


Now, please go on with the :ref:`advanced part of the tutorial <tutorial-advanced>`.



================================================
FILE: docs/tutorial/setup.rst
================================================

.. _tutorial-setup:

Setup
-----

.. _Snakemake: https://snakemake.readthedocs.io
.. _Snakemake homepage: https://snakemake.readthedocs.io
.. _GNU Make: https://www.gnu.org/software/make
.. _Python: https://www.python.org
.. _BWA: http://bio-bwa.sourceforge.net
.. _SAMtools: https://www.htslib.org
.. _BCFtools: https://www.htslib.org
.. _Pandas: https://pandas.pydata.org
.. _Miniconda: https://conda.pydata.org/miniconda.html
.. _Miniforge: https://github.com/conda-forge/miniforge
.. _Mamba: https://github.com/mamba-org/mamba
.. _Conda: https://conda.pydata.org
.. _Pixi: https://pixi.sh/
.. _Pixi installation: https://pixi.sh/latest/#installation
.. _Pixi automated switching: https://pixi.sh/latest/switching_from/conda/#automated-switching
.. _Bash: https://www.tldp.org/LDP/Bash-Beginners-Guide/html
.. _Atom: https://atom.io
.. _Graphviz: https://www.graphviz.org
.. _PyYAML: https://pyyaml.org
.. _Docutils: https://docutils.sourceforge.io
.. _Jinja2: https://jinja.palletsprojects.com
.. _NetworkX: https://networkx.github.io
.. _Matplotlib: https://matplotlib.org
.. _Pysam: https://pysam.readthedocs.io
.. _Bioconda: https://bioconda.github.io
.. _WSL: https://docs.microsoft.com/en-us/windows/wsl/about
.. _WSL Documentation: https://docs.microsoft.com/en-us/windows/wsl/install-win10
.. _Vagrant: https://www.vagrantup.com
.. _Vagrant Documentation: https://docs.vagrantup.com
.. _Blogpost: https://blog.osteel.me/posts/2015/01/25/how-to-use-vagrant-on-windows.html

Requirements
::::::::::::

**Please wait to install the tools listed below, as this tutorial will guide you through better and more reliable setup processes in the next sections.**
For your reference, the following tools will be used:

* Python_ ≥3.5
* Snakemake_ ≥5.24.1
* BWA_ 0.7
* SAMtools_ 1.9
* Pysam_ 0.15
* BCFtools_ 1.9
* Graphviz_ 2.42
* Jinja2_ 2.11
* NetworkX_ 2.5
* Matplotlib_ 3.3


.. _tutorial-free-on-gitpod:

Run tutorial for free in the cloud via Gitpod
:::::::::::::::::::::::::::::::::::::::::::::

.. note::

    A common thing to happen while using the development environment in GitPod is to hit ``Ctrl-s`` while in the terminal window, because you wanted to save a file in the editor window.
    This will freeze up you terminal.
    To get it back, make sure you selected the terminal window by clicking on it and then hit ``Ctrl-q``.

The easiest way to run this tutorial is to use Gitpod, which enables performing the exercises via your browser---including all required software, for free and in the cloud.
In order to do this, simply open the predefined `snakemake-tutorial GitPod workspace <https://gitpod.io/#https://github.com/snakemake/snakemake-tutorial-data>`_ in your browser.
GitPod provides you with a `Theia development environment <https://theia-ide.org/docs>`_, which you can learn about in the linked documentation.
Once you have a basic understanding of this environment, you can go on directly with :ref:`tutorial-basics`.

Running the tutorial on your local machine
::::::::::::::::::::::::::::::::::::::::::

If you prefer to run the tutorial on your local machine, please follow the steps below.

The easiest way to set these prerequisites up, is to use the Pixi_ package management tool.
This option is listed below as tutorial-installing_pixi_.
An alternative is to use the Miniforge_ Python 3 distribution
(Miniforge_ is a Conda based distribution like Miniconda_, which however uses Mamba_ a fast and more robust replacement for the Conda_ package manager).
This option is listed below as tutorial-installing_miniforge_.
The tutorial assumes that you are using either Linux or MacOS X.
Snakemake, Pixi_, and Miniforge_ work also under Windows, but the Windows shell is too different to be able to provide generic examples.

**Currently, the setup currently only works for Intel based machines (x86_64), not ARM based machines like the new Apple M1/2/3 architecture.**
This will change in the coming months. In the meantime, if you are on an ARM based Mac, you can use Rosetta to emulate an intel architecture.
Otherwise, you can simply use the Gitpod approach outlined above.

Setup on Windows
::::::::::::::::

If you already use Linux or MacOS X, go on with **Step 1**.

Windows Subsystem for Linux
"""""""""""""""""""""""""""

If you use Windows 10, you can set up the Windows Subsystem for Linux (`WSL`_) to natively run linux applications.
Install the WSL following the instructions in the `WSL Documentation`_. You can chose any Linux distribution available for the WSL, but the most popular and accessible one is Ubuntu.
Start the WSL and set up your account; now, you can follow the steps of our tutorial from within your Linux environment in the WSL.

Vagrant virtual machine
"""""""""""""""""""""""

If you are using a version of Windows older than 10 or if you do not wish to install the WSL, you can instead setup a Linux virtual machine (VM) with Vagrant_.
First, install Vagrant following the installation instructions in the `Vagrant Documentation`_.
Then, create a new directory you want to share with your Linux VM, for example, create a folder named ``vagrant-linux`` somewhere.
Open a command line prompt, and change into that directory.
Here, you create a 64-bit Ubuntu Linux environment with

.. code:: console

    > vagrant init hashicorp/precise64
    > vagrant up

If you decide to use a 32-bit image, you will need to download the 32-bit version of Miniconda in the next step.
The contents of the ``vagrant-linux`` folder will be shared with the virtual machine that is set up by vagrant.
You can log into the virtual machine via

.. code:: console

    > vagrant ssh

If this command tells you to install an SSH client, you can follow the instructions in this Blogpost_.
Now, you can follow the steps of our tutorial from within your Linux VM.

Step 1: Installing Pixi or Miniforge
::::::::::::::::::::::::::::::::::::

First, please **open a terminal** or make sure you are logged into your Vagrant Linux VM.
Choose **one** of the following options,
`Pixi <tutorial-installing_pixi_>`_ **or**
`Miniforge <tutorial-installing_miniforge_>`_,
to install the required software.


.. _tutorial-installing_pixi:

Step 1a: Installing Pixi
""""""""""""""""""""""""

To install ``pixi`` you can run the following command in your terminal:

.. code:: console

    $ curl -fsSL https://pixi.sh/install.sh | bash

The above invocation will automatically download the latest version of ``pixi``,
extract it, and move the pixi binary to ``~/.pixi/bin``.
If this directory does not already exist, the script will create it.

The script will also update your ``~/.bashrc`` or ``~/.zshrc`` to include ``~/.pixi/bin`` in your PATH,
allowing you to invoke the ``pixi`` command from anywhere after opening a **new terminal** window.

Please also see the official `Pixi installation`_ instructions for more information.

.. _tutorial-installing_miniforge:

Step 1b: Installing Miniforge
"""""""""""""""""""""""""""""

Download and install Miniconda 3 with the following commands depending on
your operating system and architecture:

.. tabs::
    
    .. tab:: Linux

        .. code:: console

            $ curl -L https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh -o Miniforge3-Linux-x86_64.sh
            $ bash Miniforge3-Linux-x86_64.sh

    .. tab:: MacOS X (x86_64)

        .. code:: console

            $ curl -L https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-x86_64.sh -o Miniforge3-MacOSX-x86_64.sh
            $ bash Miniforge3-MacOSX-x86_64.sh

    .. tab:: MacOS X (arm64)

        .. code:: console

            $ curl -L https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh -o Miniforge3-MacOSX-arm64.sh
            $ bash Miniforge3-MacOSX-arm64.sh

When you are asked the question

.. code::

    Do you wish the installer to prepend the install location to PATH ...? [yes|no]

answer with **yes**.
Along with a minimal Python 3 environment, Miniforge contains the package manager Mamba_.
After closing your current terminal and opening a **new terminal**, you can use the new ``conda`` command to install software packages and create isolated environments to, for example, use different versions of the same package.
We will later use Conda_ to create an isolated environment with all the required software for this tutorial.

Step 2: Preparing a working directory
:::::::::::::::::::::::::::::::::::::

First, **create a new directory** ``snakemake-tutorial`` at a **place you can easily remember** and change into that directory in your terminal:

.. code:: console

    $ mkdir snakemake-tutorial
    $ cd snakemake-tutorial

If you use a Vagrant Linux VM from Windows as described above, create that directory under ``/vagrant/``, so that the contents are shared with your host system (you can then edit all files from within Windows with an editor that supports Unix line breaks).
Then, **change to the newly created directory**.
In this directory, we will later create an example workflow that illustrates the Snakemake syntax and execution environment.
First, we download some example data on which the workflow shall be executed:

.. code:: console

    $ curl -L https://api.github.com/repos/snakemake/snakemake-tutorial-data/tarball -o snakemake-tutorial-data.tar.gz

Next we extract the data:

.. tabs::
    
    .. tab:: Linux

        .. code:: console

            $ tar --wildcards -xf snakemake-tutorial-data.tar.gz --strip 1 "*/data" "*/environment.yaml"

    .. tab:: MacOS X

        .. code:: console

            $ tar -xf snakemake-tutorial-data.tar.gz --strip 1 "*/data" "*/environment.yaml"

This will create a folder ``data`` and a file ``environment.yaml`` in the working directory.

Step 3: Creating an environment with the required software
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

The procedure is again different depending on whether you use
`Pixi <tutorial-creating_environment_pixi_>`_ or
`Miniforge_ <tutorial-creating_environment_miniforge_>`_.

Step 3a: Pixi environment
"""""""""""""""""""""""""

.. _tutorial-creating_environment_pixi:

Pixi supports importing environments from a ``conda`` or ``mamba`` ``environment.yaml`` file using
`automated switching <Pixi automated switching_>`_:

.. code:: console

    $ pixi init --import environment.yaml


Step 3b: Miniforge environment
""""""""""""""""""""""""""""""

.. _tutorial-creating_environment_miniforge:

If using **Miniforge**, all interactions with Conda package management below can be conducted with either ``conda``, ``mamba`` or ``micromamba``.
For the steps in the :ref:`"advanced" part of the tutorial <tutorial-advanced>`, you have to install ``mamba`` though in case you don't have it.

First, make sure to activate the base environment with

.. code:: console

    $ conda activate base

The ``environment.yaml`` file that you have obtained with the previous step (Step 2) can be used to install all required software into an isolated Conda environment with the name ``snakemake-tutorial`` via

.. code:: console

    $ mamba env create --name snakemake-tutorial --file environment.yaml

If you don't have the Mamba_ command because you used a different conda distribution than Miniforge_, you can also first install Mamba_
(which is a faster and more robust replacement for Conda_) in your base environment with

.. code:: console

    $ conda install -n base -c conda-forge mamba

and then run the ``mamba env create`` command shown above.

Step 4: Activating the environment
::::::::::::::::::::::::::::::::::

To activate the ``snakemake-tutorial`` environment, execute

.. tabs::

    .. group-tab:: Pixi

        .. code:: console

            $ pixi shell

    .. group-tab:: Miniforge

        .. code:: console

            $ conda activate snakemake-tutorial

Now you can use the installed tools.
Execute

.. code:: console

    $ snakemake --help

to test this and get information about the command-line interface of Snakemake.
To exit the environment, you can execute

.. tabs::

    .. group-tab:: Pixi

        .. code:: console

            $ exit

    .. group-tab:: Miniforge

        .. code:: console

            $ conda deactivate

but **don't do that now**, since we finally want to start working with Snakemake :-).



================================================
FILE: docs/tutorial/tutorial.rst
================================================
.. _tutorial:

=====================
Tutorial: General use
=====================

.. _Snakemake: https://snakemake.readthedocs.io
.. _GNU Make: https://www.gnu.org/software/make
.. _Python: https://www.python.org
.. _slides: https://slides.com/johanneskoester/snakemake-tutorial
.. _Conda: https://conda.io
.. _Singularity: https://www.sylabs.io

This tutorial introduces the text-based workflow system Snakemake_.
Snakemake follows the `GNU Make`_ paradigm: workflows are defined in terms of rules that define how to create output files from input files.
Dependencies between the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized.

Snakemake sets itself apart from other text-based workflow systems in the following way.
Hooking into the Python interpreter, Snakemake offers a definition language that is an extension of Python_ with syntax to define rules and workflow specific properties.
This allows Snakemake to combine the flexibility of a plain scripting language with a pythonic workflow definition.
The Python language is known to be concise yet readable and can appear almost like pseudo-code.
The syntactic extensions provided by Snakemake maintain this property for the definition of the workflow.
Further, Snakemake's scheduling algorithm can be constrained by priorities, provided cores and customizable resources and it provides a generic support for distributed computing (e.g., cluster or batch systems).
Hence, a Snakemake workflow scales without modification from single core workstations and multi-core servers to cluster or batch systems.
Finally, Snakemake integrates with the package manager Conda_ and the container engine Singularity_ such that defining the software stack becomes part of the workflow itself.

The examples presented in this tutorial come from Bioinformatics.
However, Snakemake is a general-purpose workflow management system for any discipline.
We ensured that no bioinformatics knowledge is needed to understand the tutorial.

Also have a look at the corresponding slides_.

As follow-up to this tutorial, we recommend to have a look at the :ref:`interaction, visualization and reporting tutorial <interaction_visualization_reporting_tutorial>`, which focuses on Snakemake's ability to cover the last mile of data analysis, i.e., the generation of publication ready reports and interactive visualizations.


.. toctree::
   :maxdepth: 2

   setup
   basics
   advanced
   additional_features



================================================
FILE: docs/tutorial/interaction_visualization_reporting/tutorial.rst
================================================
.. _interaction_visualization_reporting_tutorial:

==================================================================
Tutorial: Interaction, Visualization, and Reporting with Snakemake
==================================================================

.. _Snakemake: https://snakemake.github.io
.. _Jupyter: https://jupyter.org
.. _Datavzrd: https://datavzrd.github.io
.. _Altair: https://altair-viz.github.io

Via its Jupyter_ :ref:`notebook <snakefiles_notebook-integration>` and :ref:`script <snakefiles-external_scripts>` integration, Snakemake_ offers powerful support for interactive data exploration.
This is particularly useful for the last mile of data analysis, where results obtained from established tools and libraries are summarized and visualized for e.g. presentation in a scientific publication.
Moreover, via its :ref:`reporting <snakefiles-reports>` capabilities, Snakemake can be used to generate reproducible reports that contain the results of a data analysis pipeline, the code that was used to generate them, and the environment in which the pipeline was executed.
The latter integrates well with Datavzrd_, a tool for generating interactive views of tabular data.

In this short tutorial, we bring all these capabilities together in order to demonstrate how Snakemake enables last mile data analysis without losing reproducibility, adaptability, and transparency.
The tutorial assumes that you already know how to write Snakemake workflows in general, at least by having conducted the entire :ref:`general tutorial <tutorial>`.
For simplicity, this tutorial does not use any :ref:`wildcards <snakefiles-wildcards>`, which render both the notebook integration and Snakemake's last mile capabilities even more powerful.

Setup
-----

Install Snakemake into a new Conda environment as instructed in the :ref:`installation guide <getting_started-installation>`.
In the following, we assume that you conduct all commands within this environment.
If you already have Snakemake installed, make sure that you use the latest stable version.

Next, create an empty working directory at a reasonable place in your file system.
In the following, we assume that you have an open terminal inside of that directory.
If you use visual studio code (recommended), open a new instance inside of that directory and open a terminal via the terminal menu.

Step 1: Obtain example data
---------------------------

First, we need some example data.
We will conduct this step as our first interactive exploration via Snakemake's Jupyter integration.
We create a new directory ``workflow`` and inside of that an empty ``Snakefile``.
In the ``Snakefile``, define the following rule:

.. code-block:: python

    
    rule get_data:
        output:
            "resources/data/cars.tsv"
        conda:
            "envs/download.yaml"
        log:
            notebook="logs/get_data.ipynb"
        notebook:
            "notebooks/get_data.py.ipynb"

The rule shall create an output TSV file ``resources/data/cars.tsv`` that will contain our example dataset.
It defines a Conda environment with the required software in ``envs/download.yaml``.
Finally, it specifies that the output shall be created via the Jupyter_ notebook ``notebooks/get_data.py.ipynb``.
Snakemake's notebook support allows to write a notebook with the per-cell output as a log file, which we utilize here by specifying ``notebook="logs/get_data.ipynb"`` in the ``log`` directive.
After saving the new rule into the ``Snakefile``, we store the following in the Conda environment file ``envs/download.yaml``:

.. code-block:: yaml

    channels:
      - conda-forge
      - nodefaults
    dependencies:
      - python =3.11
      - polars =1.1
      - vega_datasets =0.9
      - ipykernel =6.29
      - notebook =7.2

Since we use a notebook for the step, we not only need ``python``, ``polars`` and ``vega_datasets`` for data processing, but also ``ipykernel`` (for Jupyter's Python support) and ``notebook`` for being able to run a Jupyter notebook.

Now, instead of creating the notebook ourselves, we let Snakemake do the job by executing the following in a terminal:

.. code-block:: console

    $ snakemake --sdm conda --cores 1 --edit-notebook resources/data/cars.tsv

This tells Snakemake to create a skeleton notebook and start a Jupyter server.
The output of the server provides three options to open the notebook in the browser.
Use Ctrl-Click on one of the options to open the notebook server in your browser.
In the presented interface, select the temporary notebook file and start editing.
We aim for the following content:

.. code-block:: python
    
    import polars as pl
    from vega_datasets import data

    cars = pl.from_pandas(data.cars()).with_columns(
        pl.col("Year").dt.year()
    ).select(
        pl.col("*").name.map(lambda name: name.lower().replace("_", " "))
    )

    cars.write_csv(snakemake.output[0], separator="\t")

This code snippet loads the cars dataset from the ``vega_datasets`` package, converts the ``Year`` column (which actually contains dates) to an integer representing just the year, normalizes column names, and writes the resulting table to the output file.
After saving the notebook, stop the Jupyter server by selecting shut down in the ``File`` menu.
Snakemake then cleans up your notebook and stores it in the desired place.

Future reruns of this rule can just treat the notebook as an ordinary script.
Whenever you want to modify the notebook, you can do so in interactive mode again using the ``--edit-notebook`` option in combination with the output file path.

Step 2: Create a plot with R
----------------------------

We now add a rule that creates a plot from the given dataset using the `R tidyverse <https://www.tidyverse.org>`__.
Add the following rule to the ``Snakefile``:

.. code-block:: python

    
    rule plot_with_r:
        input:
            "resources/data/cars.tsv"
        output:
            "results/plots/horsepower_vs_mpg.ggplot.svg",
        log:
            notebook="logs/plot_horsepower_vs_mpg.r.ipynb"
        conda:
            "envs/rstats.yaml"
        notebook:
            "notebooks/plot_horsepower_vs_mpg.r.ipynb"

Analogously to before, we specify a Conda environment in ``envs/rstats.yaml`` with the following content:

.. code-block:: yaml

    channels:
      - conda-forge
      - nodefaults
    dependencies:
      - r-base =4.4
      - r-readr =2.1.5
      - r-dplyr =1.1
      - r-ggplot2 =3.5
      - notebook =7.2
      - r-irkernel =1.3

Here, we require ``r-irkernel`` instead of the ``ipykernel`` for the Python case.
Now recall the terminal command from the previous step and run the equivalent for the new rule:

.. code-block:: console

    $ snakemake --sdm conda --cores 1 --edit-notebook results/plots/horsepower_vs_mpg.ggplot.svg

Again open the notebook in your browser and start editing.
We want to create a plot of the horsepower vs. miles per gallon from the cars dataset.
The plot shall finally be saves as an SVG file:

.. code-block:: r
    
    library(readr)
    library(ggplot2)

    cars <- read_tsv(snakemake@input[[1]], show_col_types = FALSE)
    svg(snakemake@output[[1]])
    ggplot(cars, aes(`miles per gallon`, horsepower)) + geom_point() + theme_classic(16)
    dev.off()

After saving the notebook, shut down the Jupyter server as before (the ``File`` menu) and let Snakemake clean up and store the notebook automatically.

Step 3: Create a plot with Python
---------------------------------

Now, for illustration purposes, we want to create the same plot with Python, using the plotting library Altair_.
Add the following rule to the ``Snakefile``:

.. code-block:: python

    
    rule plot_with_python:
        input:
            "resources/data/cars.tsv"
        output:
            "results/plots/horsepower_vs_mpg.altair.html",
        log:
            notebook="logs/plot_horsepower_vs_mpg.py.ipynb"
        conda:
            "envs/pystats.yaml"
        notebook:
            "notebooks/plot_horsepower_vs_mpg.py.ipynb"

The corresponding Conda environment in ``envs/pystats.yaml`` is:

.. code-block:: yaml

    channels:
      - conda-forge
      - nodefaults
    dependencies:
      - python =3.11
      - polars =1.1
      - altair =5.3
      - altair_saver =0.5
      - vl-convert-python =1.5
      - vegafusion =1.6
      - vegafusion-python-embed =1.6
      - notebook =7.2
      - ipykernel =6.29

In addition to the packages already used for the download step, we now require ``altair``, ``altair_saver`` and ``vl-convert-python`` for output format support in Altair_. In addition, adding the two ``vegafusion`` packages, enables support for efficient plotting that involves a lot of datapoints.
While we don't need that in this example, it is a good practice to include them in the environment file in order to be prepared for such cases.

Again, recall the terminal command from the previous step and run the equivalent for the new rule:

.. code-block:: console

    $ snakemake --sdm conda --cores 1 --edit-notebook results/plots/horsepower_vs_mpg.altair.html

The content of the notebook shall be:

.. code-block:: python
    
    import altair as alt
    import polars as pl
    alt.data_transformers.enable("vegafusion")

    data = pl.read_csv(snakemake.input[0], separator="\t")

    chart = alt.Chart(data).mark_point(tooltip=True).encode(
        alt.X("miles per gallon"),
        alt.Y("horsepower"),
        alt.Color("origin").scale(scheme="accent"),
    ).interactive()

    chart.save(snakemake.output[0])

Here, in addition to the plot before, we color the points by the origin of the car.
Moreover, we define the chart to be interactive and offer tooltips at each point.
After running and saving the notebook, followed by shutting down the Jupyter server like before, open the generated output file in your browser and explore the interactivity (zoom with the mouse wheel, pan by click-hold-move, and hover the points for tooltips).

Step 4: Create an interactive table view with Datavzrd
------------------------------------------------------

While plots are a central part of data exploration and enable to reveal e.g. relationships between variables, providing transparent access to the underlying dataset is crucial for generating trust and easing communication of the results.
Datavzrd_ is a tool that allows to rapidly generate interactive views of tabular data without requiring a web server to be set up and maintained.
It is available as a `Snakemake wrapper <https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/datavzrd.html>`__ and can be included here as follows:

.. code-block:: python

    
    rule view_with_datavzrd:
        input:
            config=workflow.source_path("resources/datavzrd/cars.yaml"),
            table="resources/data/cars.tsv",
        output:
            report(
                directory("results/tables/cars"),
                htmlindex="index.html",
                caption="report/cars.rst",
                category="Tables",
                labels={"table": "cars"},
            ),
        log:
            "logs/datavzrd.log",
        wrapper:
            "v4.7.2/utils/datavzrd"

Note that the wrapper already suggests to annotate the output for inclusion into the Snakemake report.
While we ignore that for now, we will revisit it at a later step.
Snakemake wrappers are a powerful way to integrate external tools into a Snakemake workflow.
They come with predefined Conda environments (so that we don't need to specify any software here) and usually wrap around a library or command line tool using e.g. Python or R scripts.
The Datavzrd wrapper expects one or multiple tables as input, as well as a configuration file that defines the appearance and behavior of the table view(s).
In this case, this config file shall be stored in ``resources/datavzrd/cars.yaml`` and contain the following:

.. code-block:: yaml

    __use_yte__: true

    datasets:
      cars:
        path: ?input.table
        separator: "\t"

    views:
      cars:
        dataset: cars
        render-table:
          columns:
            name:
              link-to-url:
                Wikipedia:
                    url: "https://en.wikipedia.org/wiki/{value}"
            miles per gallon:
              plot:
                ticks:
                  scale: linear
            cylinders:
              plot:
                heatmap:
                  scale: linear
                  domain:
                    - 0
                    - 16
                  range:
                    # white to blue
                    - "#ffffff"
                    - "#6baed6"
            displacement:
              plot:
                ticks:
                  scale: linear
              display-mode: detail
            horsepower:
              plot:
                ticks:
                  scale: linear
              display-mode: detail
            weight in lbs:
              plot:
                ticks:
                  scale: linear
            acceleration:
              plot:
                ticks:
                  scale: linear
            year:
              plot:
                ticks:
                  scale: linear
            origin:
              plot:
                heatmap:
                  scale: ordinal
                  color-scheme: category10

While we refer to the Datavzrd_ documentation for the details, let us highlight a few important aspects here:
The configuration file defines a dataset named ``cars`` that is loaded from the input table file.
Since the path of the table file shall rather not be hardcoded into the configuration we use the `YTE template engine <https://yte-template-engine.github.io>`__ to render the path dynamically into the config file, thereby accessing ``input.table`` as provided by the Snakemake job.
In this case, we define one view on the dataset (in principle there can be multiple views on one or multiple tables encoded into one Datavzrd report).
For each column of the table, we specify how it shall be visualized (as a link, as tick or heatmap plot).
Beyond these, Datavzrd offers a lot more possibilities, including the ability to publish `spells <https://datavzrd.github.io/docs/spells.html>`__ with custom visualizations for common column types.

Step 5: Adding default targets
------------------------------

While we have so far generated each output file manually via the ``--edit-notebook`` option, it is time to define the default targets of our workflow.
Add a rule ``all`` to the top of the ``Snakefile``:

.. code-block:: python

    
    rule all:
        input:
            "results/plots/horsepower_vs_mpg.ggplot.svg",
            "results/plots/horsepower_vs_mpg.altair.html",
            "results/tables/cars",

This rule just defines input files.
Since Snakemake wants to run the first rule in the workflow by default, all the inputs have to be created by combinations of other rules, such that the desired files are generated.

Run the workflow with

.. code-block:: console

    $ snakemake --sdm conda --cores 1

Since the plots are already present, Snakemake will just run the Datavzrd rule.
Afterwards explore the Datavzrd output by opening ``results/tables/cars/index.html`` in your browser.

Step 6: Reporting
-----------------

At the end of the last mile, the data analysis results are usually communicated, e.g. in a scienficic manuscript or by first sending them to collaborators.
Just communicating the individual outputs disconnects them from the code that generated them, which is a problem for transparency.
To overcome this caveat, Snakemake offers the generation of automatic reports, which connect code and parameters with results.
Since these reports are standalone, server free HTML files, they can be easily added as supplementary files to a manuscript or shared with collaborators.
The Datavrzrd rule already marks the output for inclusion in the report.
It does so by specifying a category, a caption, and labels for showing the output inside of the report menu structure.
Let us now add equivalent annotations to the two plot rules by editing them into the following form:

.. code-block:: python

    rule plot_with_r:
        input:
            "resources/data/cars.tsv"
        output:
            report(
                "results/plots/horsepower_vs_mpg.ggplot.svg",
                category="Plots",
                labels={"plot": "horsepower_vs_mpg", "approach": "ggplot"},
                caption="report/horsepower_vs_mpg_plot_with_r.rst",
            ),
        log:
            notebook="logs/plot_horsepower_vs_mpg.r.ipynb"
        conda:
            "envs/rstats.yaml"
        notebook:
            "notebooks/plot_horsepower_vs_mpg.r.ipynb"


    rule plot_with_python:
        input:
            "resources/data/cars.tsv"
        output:
            report(
                "results/plots/horsepower_vs_mpg.altair.html",
                category="Plots",
                labels={"plot": "horsepower_vs_mpg", "approach": "altair"},
                caption="report/horsepower_vs_mpg_plot_with_python.rst",
            ),
        log:
            notebook="logs/plot_horsepower_vs_mpg.py.ipynb"
        conda:
            "envs/pystats.yaml"
        notebook:
            "notebooks/plot_horsepower_vs_mpg.py.ipynb"

The caption files shall contain a human readable description, which we will keep short in this example case.
Create the file ``workflow/report/cars.rst`` with the following content:

.. code-block:: rst

    The cars dataset as provided by the vega project: https://github.com/vega/vega-datasets.

Create the file ``workflow/report/horsepower_vs_mpg_plot_with_r.rst`` with the following content:

.. code-block:: rst

    A plot of the horsepower vs. miles per gallon from the cars dataset, created with ggplot2.

Create the file ``workflow/report/horsepower_vs_mpg_plot_with_python.rst`` with the following content:

.. code-block:: rst

    A plot of the horsepower vs. miles per gallon from the cars dataset, created with Altair.

Finally, add a global report directive to the top of the ``Snakefile``

.. code-block:: python

    report: "report/workflow.rst"

and create the file ``workflow/report/workflow.rst`` with the following content:

.. code-block:: rst

    Workflow illustrating Snakemake's capabilities for last mile data analysis with Jupyter, Datavzrd, and R/Python plotting libraries.

Now create the report by running

.. code-block:: console

    $ snakemake --sdm conda --report report.zip

This will create a report in the form of a zip file.
Unzip it with

.. code-block:: console

    $ unzip report.zip

and open the file ``report/report.html`` file in your browser.
You will see that the report brings all desired outputs together in a structured way, including the captions and the global description.
It not only allows to view the results, but also to explore the code of each rule and all involved parameters and software tools.
This way, it generates transparency without requiring people to manually inspect the workflow codebase.
In many ways, it can be seen as a self-contained next generation supplementary file of a scientific manuscript.



================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/Snakefile
================================================
report: "report/workflow.rst"

rule all:
    input:
        "results/plots/horsepower_vs_mpg.ggplot.svg",
        "results/plots/horsepower_vs_mpg.altair.html",
        "results/tables/cars",


rule get_data:
    output:
        "resources/data/cars.tsv"
    conda:
        "envs/download.yaml"
    log:
        notebook="logs/get_data.ipynb"
    notebook:
        "notebooks/get_data.py.ipynb"


rule plot_with_r:
    input:
        "resources/data/cars.tsv"
    output:
        report(
            "results/plots/horsepower_vs_mpg.ggplot.svg",
            category="Plots",
            labels={"plot": "horsepower_vs_mpg", "approach": "ggplot"},
            caption="report/horsepower_vs_mpg.rst",
        ),
    log:
        notebook="logs/plot_horsepower_vs_mpg.r.ipynb"
    conda:
        "envs/rstats.yaml"
    notebook:
        "notebooks/plot_horsepower_vs_mpg.r.ipynb"


rule plot_with_python:
    input:
        "resources/data/cars.tsv"
    output:
        report(
            "results/plots/horsepower_vs_mpg.altair.html",
            category="Plots",
            labels={"plot": "horsepower_vs_mpg", "approach": "altair"},
            caption="report/horsepower_vs_mpg.rst",
        ),
    log:
        notebook="logs/plot_horsepower_vs_mpg.py.ipynb"
    conda:
        "envs/pystats.yaml"
    notebook:
        "notebooks/plot_horsepower_vs_mpg.py.ipynb"


rule view_with_datavzrd:
    input:
        config=workflow.source_path("resources/datavzrd/cars.yaml"),
        table="resources/data/cars.tsv",
    output:
        report(
            directory("results/tables/cars"),
            htmlindex="index.html",
            caption="report/cars.rst",
            category="Tables",
            labels={"table": "cars"},
        ),
    log:
        "logs/datavzrd.log",
    wrapper:
        "v4.7.2/utils/datavzrd"



================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/envs/download.yaml
================================================
channels:
  - conda-forge
  - nodefaults
dependencies:
  - python =3.11
  - polars =1.1
  - vega_datasets =0.9
  - ipykernel =6.29
  - notebook =7.2


================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/envs/pystats.yaml
================================================
channels:
  - conda-forge
  - nodefaults
dependencies:
  - python =3.11
  - polars =1.1
  - altair =5.3
  - altair_saver =0.5
  - vegafusion =1.6
  - vegafusion-python-embed =1.6
  - vl-convert-python =1.5
  - notebook =7.2
  - ipykernel =6.29


================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/envs/rstats.yaml
================================================
channels:
  - conda-forge
  - nodefaults
dependencies:
  - r-base =4.4
  - r-readr =2.1.5
  - r-dplyr =1.1
  - r-ggplot2 =3.5
  - notebook =7.2
  - r-irkernel =1.3


================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/notebooks/get_data.py.ipynb
================================================
# Jupyter notebook converted to Python script.

# start coding here
import polars as pl
from vega_datasets import data

cars = pl.from_pandas(data.cars()).with_columns(
    pl.col("Year").dt.year()
).select(
    pl.col("*").name.map(lambda name: name.lower().replace("_", " "))
)

cars.write_csv(snakemake.output[0], separator="\t")



================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/notebooks/plot_horsepower_vs_mpg.py.ipynb
================================================
# Jupyter notebook converted to Python script.

import altair as alt
import polars as pl
alt.data_transformers.enable("vegafusion")

data = pl.read_csv(snakemake.input[0], separator="\t")
data

chart = alt.Chart(data).mark_point(tooltip=True).encode(
    alt.X("miles per gallon"),
    alt.Y("horsepower"),
    alt.Color("origin").scale(scheme="accent"),
).interactive()
chart

chart.save(snakemake.output[0])



================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/notebooks/plot_horsepower_vs_mpg.r.ipynb
================================================
# Jupyter notebook converted to Python script.

library(readr)
library(ggplot2)

cars <- read_tsv(snakemake@input[[1]], show_col_types = FALSE)

cars

svg(snakemake@output[[1]])
ggplot(cars, aes(`miles per gallon`, horsepower)) + geom_point() + theme_classic(16)
dev.off()



================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/report/cars.rst
================================================
The cars dataset as provided by the vega project: https://github.com/vega/vega-datasets.


================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/report/horsepower_vs_mpg.rst
================================================
Visualization of the relationship between horsepower and miles per gallon (MPG) in the cars dataset.


================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/report/workflow.rst
================================================
Workflow illustrating Snakemake's capabilities for last mile data analysis with Jupyter, Datavzrd, and R/Python plotting libraries.


================================================
FILE: docs/tutorial/interaction_visualization_reporting/workdir/workflow/resources/datavzrd/cars.yaml
================================================
__use_yte__: true

datasets:
  cars:
    path: ?input.table
    separator: "\t"

views:
  cars:
    dataset: cars
    render-table:
      columns:
        name:
          link-to-url:
            Wikipedia:
              url: "https://en.wikipedia.org/wiki/{value}"
        miles per gallon:
          plot:
            ticks:
              scale: linear
        cylinders:
          plot:
            heatmap:
              scale: linear
              domain:
                - 0
                - 16
              range:
                # white to blue
                - "#ffffff"
                - "#6baed6"
        displacement:
          plot:
            ticks:
              scale: linear
          display-mode: detail
        horsepower:
          plot:
            ticks:
              scale: linear
          display-mode: detail
        weight in lbs:
          plot:
            ticks:
              scale: linear
        acceleration:
          plot:
            ticks:
              scale: linear
        year:
          plot:
            ticks:
              scale: linear
        origin:
          plot:
            heatmap:
              scale: ordinal
              color-scheme: category10


================================================
FILE: examples/c/README.txt
================================================
https://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/



================================================
FILE: examples/c/include/hello.h
================================================
void myPrintHelloMake(void);



================================================
FILE: examples/c/src/hello.c
================================================
#include <stdio.h>

int main() {
  // call a function in another file
  myPrintHello();

  return(0);
}



================================================
FILE: examples/c/src/hellofunc.c
================================================
#include <stdio.h>

void myPrintHello(void) {

  printf("Hello makefiles!\n");

  return;
}



================================================
FILE: examples/c/src/Makefile
================================================


IDIR=../include
ODIR=obj
LDIR=../lib

LIBS=-lm

CC=gcc
CFLAGS=-I$(IDIR)

_HEADERS = hello.h
HEADERS = $(patsubst %,$(IDIR)/%,$(_HEADERS))

_OBJS = hello.o hellofunc.o 
OBJS = $(patsubst %,$(ODIR)/%,$(_OBJS))

# build the executable from the object files
hello: $(OBJS)
	$(CC) -o $@ $^ $(CFLAGS)

# compile a single .c file to an .o file
$(ODIR)/%.o: %.c $(HEADERS)
	$(CC) -c -o $@ $< $(CFLAGS)


# clean up temporary files
.PHONY: clean
clean:
	rm -f $(ODIR)/*.o *~ core $(IDIR)/*~ 



================================================
FILE: examples/c/src/Snakefile
================================================
from os.path import join

IDIR = '../include'
ODIR = 'obj'
LDIR = '../lib'

LIBS = '-lm'

CC = 'gcc'
CFLAGS = '-I' + IDIR


_HEADERS = ['hello.h']
HEADERS = [join(IDIR, hfile) for hfile in _HEADERS]

_OBJS = ['hello.o', 'hellofunc.o']
OBJS = [join(ODIR, ofile) for ofile in _OBJS]


rule hello:
    """build the executable from the object files"""
    output:
        'hello'
    input:
        OBJS
    shell:
        "{CC} -o {output} {input} {CFLAGS} {LIBS}"

rule c_to_o:
    """compile a single .c file to an .o file"""
    output:
        temp('{ODIR}/{name}.o')
    input:
        '{name}.c'
    shell:
        "{CC} -c -o {output} {input} {CFLAGS}"

rule clean:
    """clean up temporary files"""
    shell:
        "rm -f   *~  core  {IDIR}/*~"



================================================
FILE: examples/cufflinks/hg19.fa
================================================
[Empty file]


================================================
FILE: examples/cufflinks/hg19.gtf
================================================
[Empty file]


================================================
FILE: examples/cufflinks/Snakefile
================================================
# path to track and reference
TRACK   = 'hg19.gtf'
REF     = 'hg19.fa'


# sample names and classes
CLASS1  = '101 102'.split()
CLASS2  = '103 104'.split()
SAMPLES = CLASS1 + CLASS2


# path to bam files
CLASS1_BAM = expand('mapped/{sample}.bam', sample=CLASS1)
CLASS2_BAM = expand('mapped/{sample}.bam', sample=CLASS2)


rule all:
    input:
        'diffexp/isoform_exp.diff',
        'assembly/comparison'


rule assembly:
    input:
        local('mapped/{sample}.bam')
    output:
        'assembly/{sample}/transcripts.gtf',
        dir='assembly/{sample}'
    threads: 4
    shell:
        'cufflinks --num-threads {threads} -o {output.dir} '
        '--frag-bias-correct {REF} {input}'


rule compose_merge:
    input:
        expand('assembly/{sample}/transcripts.gtf', sample=SAMPLES)
    output:
        txt='assembly/assemblies.txt'
    run:
        with open(output.txt, 'w') as out:
            print(*input, sep="\n", file=out)


rule merge_assemblies:
    input:
        'assembly/assemblies.txt'
    output:
        'assembly/merged/merged.gtf', dir='assembly/merged'
    shell:
        'cuffmerge -o {output.dir} -s {REF} {input}'


rule compare_assemblies:
    input:
        'assembly/merged/merged.gtf'
    output:
        'assembly/comparison/all.stats',
        dir='assembly/comparison'
    shell:
        'cuffcompare -o {output.dir}all -s {REF} -r {TRACK} {input}'


rule diffexp:
    input:
        class1=CLASS1_BAM,
        class2=CLASS2_BAM,
        gtf='assembly/merged/merged.gtf'
    output:
        'diffexp/gene_exp.diff', 'diffexp/isoform_exp.diff'
    params:
        class1=",".join(CLASS1_BAM),
        class2=",".join(CLASS2_BAM)
    threads: 8
    shell:
        'cuffdiff --num-threads {threads} {input.gtf} {params.class1} {params.class2}'



================================================
FILE: examples/cufflinks/mapped/101.bam
================================================
[Empty file]


================================================
FILE: examples/cufflinks/mapped/102.bam
================================================
[Empty file]


================================================
FILE: examples/cufflinks/mapped/103.bam
================================================
[Empty file]


================================================
FILE: examples/cufflinks/mapped/104.bam
================================================
[Empty file]


================================================
FILE: examples/flux/Dockerfile
================================================
FROM fluxrm/flux-sched:focal
# docker build -t flux-snake .
# docker run -it flux-snake
USER root
ENV PATH=/opt/conda:/bin:$PATH
RUN pip install git+https://github.com/snakemake/snakemake@main && \
    ln -s /bin/python3 /usr/local/bin/python
USER fluxuser
COPY ./Snakefile /home/fluxuser/Snakefile



================================================
FILE: examples/flux/Snakefile
================================================
# By convention, the first pseudorule should be called "all"
# We're using the expand() function to create multiple targets
rule all:
	input:
		expand(
			"{greeting}/world.txt",
			greeting = ['hello', 'hola'],
		),

# First real rule, this is using a wildcard called "greeting"
rule multilingual_hello_world:
	output:
		"{greeting}/world.txt",
	shell:
		"""
		mkdir -p "{wildcards.greeting}"
		sleep 5
		echo "{wildcards.greeting}, World!" > {output}
		"""



================================================
FILE: examples/hello-world/config.yaml
================================================
countries:
  - fr
  - at
  - sp



================================================
FILE: examples/hello-world/Snakefile
================================================
configfile: "config.yaml"


rule all:
    input:
        expand(
            "plots/{country}.hist.pdf",
            country=config["countries"]
        )


rule select_by_country:
    input:
        "data/worldcitiespop.csv"
    output:
        "by-country/{country}.csv"
    conda:
        "envs/xsv.yaml"
    shell:
        "xsv search -s Country '{wildcards.country}' "
        "{input} > {output}"


rule plot_histogram:
    input:
        "by-country/{country}.csv"
    output:
        "plots/{country}.hist.svg"
    container:
        "docker://faizanbashir/python-datascience:3.6"
    script:
        "scripts/plot-hist.py"


rule convert_to_pdf:
    input:
        "{prefix}.svg"
    output:
        "{prefix}.pdf"
    wrapper:
        "0.47.0/utils/cairosvg"


rule download_data:
    output:
        "data/worldcitiespop.csv"
    shell:
        "curl -L https://burntsushi.net/stuff/worldcitiespop.csv > {output}"



================================================
FILE: examples/hello-world/envs/matplotlib.yaml
================================================
channels:
  - conda-forge
dependencies:
  - python 3.7
  - matplotlib 3.1
  - pandas 0.25



================================================
FILE: examples/hello-world/envs/xsv.yaml
================================================
channels:
  - conda-forge
dependencies:
  - xsv 0.13



================================================
FILE: examples/hello-world/scripts/plot-hist.py
================================================
import matplotlib.pyplot as plt
import pandas as pd

cities = pd.read_csv(snakemake.input[0])

plt.hist(cities["Population"], bins=50)

plt.savefig(snakemake.output[0])



================================================
FILE: examples/latex/document.tex
================================================
[Empty file]


================================================
FILE: examples/latex/response-to-editor.tex
================================================
[Empty file]


================================================
FILE: examples/latex/Snakefile
================================================
DOCUMENTS = ['document', 'response-to-editor']
TEXS = [doc+".tex" for doc in DOCUMENTS]
PDFS = [doc+".pdf" for doc in DOCUMENTS]
FIGURES = ['fig1.pdf']

include:
    'tex.rules'

rule all:
    input:
        PDFS

rule zipit:
    output:
        'upload.zip'
    input:
        TEXS, FIGURES, PDFS
    shell:
        'zip -T {output} {input}'

rule pdfclean:
    shell:
        "rm -f  {PDFS}"



================================================
FILE: examples/latex/tex.rules
================================================
ruleorder:  tex2pdf_with_bib > tex2pdf_without_bib

rule tex2pdf_with_bib:
    input:
        '{name}.tex',
        '{name}.bib'
    output:
        '{name}.pdf'
    shell:
        """
        pdflatex {wildcards.name}
        bibtex {wildcards.name}
        pdflatex {wildcards.name}
        pdflatex {wildcards.name}
        """

rule tex2pdf_without_bib:
    input:
        '{name}.tex'
    output:
        '{name}.pdf'
    shell:
        """
        pdflatex {wildcards.name}
        pdflatex {wildcards.name}
        """

rule texclean:
    shell:
        "rm -f  *.log *.aux *.bbl *.blg *.synctex.gz"



================================================
FILE: examples/mirna/dag.dot
================================================
digraph snakemake_dag {
	35[label = "do_readlengths_of_one_file"];
	34 -> 35;
	103[label = "count_mirbase_mappable"];
	102 -> 103;
	143[label = "count_mirna_expression_of_one_dataset"];
	142 -> 143;
	141 -> 143;
	42[label = "do_readlengths_pdfs"];
	41 -> 42;
	164[label = "annotate"];
	147 -> 164;
	90 -> 164;
	2[label = "compute_readcounts\nds: 554"];
	10[label = "run_cutadapt\nds: 552"];
	34[label = "run_cutadapt\nds: 560"];
	153[label = "rna_type"];
	152 -> 153;
	11[label = "do_readlengths_of_one_file"];
	10 -> 11;
	138[label = "sort_a_bam_file"];
	110 -> 138;
	38[label = "do_readlengths_of_one_file"];
	37 -> 38;
	56[label = "do_readlengths_of_one_file"];
	55 -> 56;
	36[label = "do_readlengths_pdfs"];
	35 -> 36;
	137[label = "count_mirna_expression_of_one_dataset"];
	135 -> 137;
	136 -> 137;
	13[label = "run_cutadapt\nds: 553"];
	37[label = "run_cutadapt\nds: 561"];
	41[label = "do_readlengths_of_one_file"];
	40 -> 41;
	109[label = "count_mirbase_mappable"];
	108 -> 109;
	132[label = "sort_a_bam_file"];
	106 -> 132;
	165[label = "rna_type"];
	164 -> 165;
	18[label = "do_readlengths_pdfs"];
	17 -> 18;
	40[label = "do_shortreads\nds: 552"];
	16[label = "run_cutadapt\nds: 554"];
	84[label = "map_ds_against_hg"];
	16 -> 84;
	81 -> 84;
	151[label = "rna_type"];
	150 -> 151;
	155[label = "rna_type"];
	154 -> 155;
	51[label = "do_readlengths_pdfs"];
	50 -> 51;
	166[label = "annotate"];
	91 -> 166;
	147 -> 166;
	20[label = "do_readlengths_of_one_file"];
	19 -> 20;
	43[label = "do_shortreads\nds: 553"];
	64[label = "do_shortreads\nds: 560"];
	0[label = "compute_readcounts\nds: 552"];
	139[label = "index_a_bam_file"];
	138 -> 139;
	5[label = "compute_readcounts\nds: 557"];
	19[label = "run_cutadapt\nds: 555"];
	54[label = "do_readlengths_pdfs"];
	53 -> 54;
	21[label = "do_readlengths_pdfs"];
	20 -> 21;
	152[label = "annotate"];
	84 -> 152;
	147 -> 152;
	47[label = "do_readlengths_of_one_file"];
	46 -> 47;
	46[label = "do_shortreads\nds: 554"];
	23[label = "do_readlengths_of_one_file"];
	22 -> 23;
	150[label = "annotate"];
	147 -> 150;
	83 -> 150;
	24[label = "do_readlengths_pdfs"];
	23 -> 24;
	57[label = "do_readlengths_pdfs"];
	56 -> 57;
	118[label = "index_a_bam_file"];
	117 -> 118;
	22[label = "run_cutadapt\nds: 556"];
	49[label = "do_shortreads\nds: 555"];
	140[label = "count_mirna_expression_of_one_dataset"];
	138 -> 140;
	139 -> 140;
	170[label = "all"];
	57 -> 170;
	101 -> 170;
	54 -> 170;
	21 -> 170;
	24 -> 170;
	85 -> 170;
	103 -> 170;
	2 -> 170;
	27 -> 170;
	144 -> 170;
	63 -> 170;
	78 -> 170;
	82 -> 170;
	105 -> 170;
	3 -> 170;
	86 -> 170;
	6 -> 170;
	0 -> 170;
	107 -> 170;
	97 -> 170;
	80 -> 170;
	36 -> 170;
	12 -> 170;
	39 -> 170;
	8 -> 170;
	109 -> 170;
	88 -> 170;
	18 -> 170;
	71 -> 170;
	83 -> 170;
	5 -> 170;
	4 -> 170;
	89 -> 170;
	60 -> 170;
	48 -> 170;
	145 -> 170;
	113 -> 170;
	90 -> 170;
	42 -> 170;
	169 -> 170;
	73 -> 170;
	7 -> 170;
	84 -> 170;
	91 -> 170;
	30 -> 170;
	51 -> 170;
	1 -> 170;
	146 -> 170;
	87 -> 170;
	9 -> 170;
	95 -> 170;
	33 -> 170;
	69 -> 170;
	111 -> 170;
	99 -> 170;
	45 -> 170;
	15 -> 170;
	66 -> 170;
	55[label = "do_shortreads\nds: 557"];
	26[label = "do_readlengths_of_one_file"];
	25 -> 26;
	60[label = "do_readlengths_pdfs"];
	59 -> 60;
	93[label = "build_bwa_index"];
	92 -> 93;
	27[label = "do_readlengths_pdfs"];
	26 -> 27;
	52[label = "do_shortreads\nds: 556"];
	135[label = "sort_a_bam_file"];
	108 -> 135;
	4[label = "compute_readcounts\nds: 556"];
	59[label = "do_readlengths_of_one_file"];
	58 -> 59;
	63[label = "do_readlengths_pdfs"];
	62 -> 63;
	154[label = "annotate"];
	147 -> 154;
	85 -> 154;
	29[label = "do_readlengths_of_one_file"];
	28 -> 29;
	6[label = "compute_readcounts\nds: 558"];
	14[label = "do_readlengths_of_one_file"];
	13 -> 14;
	3[label = "compute_readcounts\nds: 555"];
	114[label = "sort_a_bam_file"];
	94 -> 114;
	73[label = "do_readlengths_pdfs"];
	72 -> 73;
	7[label = "compute_readcounts\nds: 559"];
	120[label = "sort_a_bam_file"];
	98 -> 120;
	142[label = "index_a_bam_file"];
	141 -> 142;
	58[label = "do_shortreads\nds: 558"];
	158[label = "annotate"];
	87 -> 158;
	147 -> 158;
	62[label = "do_readlengths_of_one_file"];
	61 -> 62;
	9[label = "compute_readcounts\nds: 561"];
	28[label = "run_cutadapt\nds: 558"];
	157[label = "rna_type"];
	156 -> 157;
	12[label = "do_readlengths_pdfs"];
	11 -> 12;
	31[label = "run_cutadapt\nds: 559"];
	61[label = "do_shortreads\nds: 559"];
	168[label = "rnatypes"];
	165 -> 168;
	151 -> 168;
	149 -> 168;
	153 -> 168;
	161 -> 168;
	163 -> 168;
	159 -> 168;
	157 -> 168;
	155 -> 168;
	167 -> 168;
	65[label = "do_readlengths_of_one_file"];
	64 -> 65;
	136[label = "index_a_bam_file"];
	135 -> 136;
	126[label = "sort_a_bam_file"];
	102 -> 126;
	50[label = "do_readlengths_of_one_file"];
	49 -> 50;
	122[label = "count_mirna_expression_of_one_dataset"];
	121 -> 122;
	120 -> 122;
	147[label = "hgtrack"];
	39[label = "do_readlengths_pdfs"];
	38 -> 39;
	144[label = "normalize_expressions"];
	128 -> 144;
	116 -> 144;
	122 -> 144;
	119 -> 144;
	137 -> 144;
	131 -> 144;
	125 -> 144;
	143 -> 144;
	140 -> 144;
	134 -> 144;
	113[label = "count_mirbase_mappable"];
	112 -> 113;
	68[label = "do_readlengths_of_one_file"];
	67 -> 68;
	45[label = "do_readlengths_pdfs"];
	44 -> 45;
	82[label = "map_ds_against_hg"];
	10 -> 82;
	81 -> 82;
	86[label = "map_ds_against_hg"];
	22 -> 86;
	81 -> 86;
	117[label = "sort_a_bam_file"];
	96 -> 117;
	159[label = "rna_type"];
	158 -> 159;
	67[label = "do_shortreads\nds: 561"];
	8[label = "compute_readcounts\nds: 560"];
	88[label = "map_ds_against_hg"];
	28 -> 88;
	81 -> 88;
	99[label = "count_mirbase_mappable"];
	98 -> 99;
	71[label = "do_readlengths_pdfs"];
	70 -> 71;
	161[label = "rna_type"];
	160 -> 161;
	25[label = "run_cutadapt\nds: 557"];
	89[label = "map_ds_against_hg"];
	31 -> 89;
	81 -> 89;
	32[label = "do_readlengths_of_one_file"];
	31 -> 32;
	124[label = "index_a_bam_file"];
	123 -> 124;
	90[label = "map_ds_against_hg"];
	34 -> 90;
	81 -> 90;
	141[label = "sort_a_bam_file"];
	112 -> 141;
	131[label = "count_mirna_expression_of_one_dataset"];
	129 -> 131;
	130 -> 131;
	160[label = "annotate"];
	147 -> 160;
	88 -> 160;
	91[label = "map_ds_against_hg"];
	37 -> 91;
	81 -> 91;
	95[label = "count_mirbase_mappable"];
	94 -> 95;
	97[label = "count_mirbase_mappable"];
	96 -> 97;
	87[label = "map_ds_against_hg"];
	25 -> 87;
	81 -> 87;
	70[label = "do_readlengths_summary"];
	23 -> 70;
	11 -> 70;
	17 -> 70;
	26 -> 70;
	29 -> 70;
	32 -> 70;
	14 -> 70;
	35 -> 70;
	20 -> 70;
	38 -> 70;
	133[label = "index_a_bam_file"];
	132 -> 133;
	149[label = "rna_type"];
	148 -> 149;
	94[label = "map_ds_against_mirbase"];
	93 -> 94;
	40 -> 94;
	92 -> 94;
	92[label = "compute_mirnas_with_context"];
	116[label = "count_mirna_expression_of_one_dataset"];
	114 -> 116;
	115 -> 116;
	125[label = "count_mirna_expression_of_one_dataset"];
	123 -> 125;
	124 -> 125;
	33[label = "do_readlengths_pdfs"];
	32 -> 33;
	96[label = "map_ds_against_mirbase"];
	93 -> 96;
	43 -> 96;
	92 -> 96;
	78[label = "do_readlengths_pdfs"];
	77 -> 78;
	101[label = "count_mirbase_mappable"];
	100 -> 101;
	17[label = "do_readlengths_of_one_file"];
	16 -> 17;
	98[label = "map_ds_against_mirbase"];
	93 -> 98;
	46 -> 98;
	92 -> 98;
	146[label = "correlate_seq_with_rtpcr"];
	144 -> 146;
	119[label = "count_mirna_expression_of_one_dataset"];
	117 -> 119;
	118 -> 119;
	69[label = "do_readlengths_pdfs"];
	68 -> 69;
	72[label = "do_readlengths_summary"];
	50 -> 72;
	41 -> 72;
	56 -> 72;
	53 -> 72;
	47 -> 72;
	44 -> 72;
	68 -> 72;
	59 -> 72;
	65 -> 72;
	62 -> 72;
	100[label = "map_ds_against_mirbase"];
	93 -> 100;
	49 -> 100;
	92 -> 100;
	162[label = "annotate"];
	89 -> 162;
	147 -> 162;
	53[label = "do_readlengths_of_one_file"];
	52 -> 53;
	105[label = "count_mirbase_mappable"];
	104 -> 105;
	167[label = "rna_type"];
	166 -> 167;
	48[label = "do_readlengths_pdfs"];
	47 -> 48;
	102[label = "map_ds_against_mirbase"];
	93 -> 102;
	52 -> 102;
	92 -> 102;
	107[label = "count_mirbase_mappable"];
	106 -> 107;
	148[label = "annotate"];
	82 -> 148;
	147 -> 148;
	111[label = "count_mirbase_mappable"];
	110 -> 111;
	169[label = "plot_rnatypes"];
	168 -> 169;
	104[label = "map_ds_against_mirbase"];
	93 -> 104;
	55 -> 104;
	92 -> 104;
	80[label = "plot_mirna_distancematrix_histogram"];
	79 -> 80;
	127[label = "index_a_bam_file"];
	126 -> 127;
	106[label = "map_ds_against_mirbase"];
	93 -> 106;
	58 -> 106;
	92 -> 106;
	134[label = "count_mirna_expression_of_one_dataset"];
	133 -> 134;
	132 -> 134;
	108[label = "map_ds_against_mirbase"];
	93 -> 108;
	61 -> 108;
	92 -> 108;
	77[label = "compute_mirna_lengths"];
	79[label = "compute_fasta_distancematrix\nprefix: mirbase/mirnas"];
	85[label = "map_ds_against_hg"];
	19 -> 85;
	81 -> 85;
	163[label = "rna_type"];
	162 -> 163;
	110[label = "map_ds_against_mirbase"];
	64 -> 110;
	92 -> 110;
	93 -> 110;
	66[label = "do_readlengths_pdfs"];
	65 -> 66;
	129[label = "sort_a_bam_file"];
	104 -> 129;
	121[label = "index_a_bam_file"];
	120 -> 121;
	30[label = "do_readlengths_pdfs"];
	29 -> 30;
	112[label = "map_ds_against_mirbase"];
	67 -> 112;
	92 -> 112;
	93 -> 112;
	1[label = "compute_readcounts\nds: 553"];
	128[label = "count_mirna_expression_of_one_dataset"];
	126 -> 128;
	127 -> 128;
	130[label = "index_a_bam_file"];
	129 -> 130;
	44[label = "do_readlengths_of_one_file"];
	43 -> 44;
	81[label = "build_bwa_index\nprefix: hgref/hg1kv37.fasta.gz"];
	15[label = "do_readlengths_pdfs"];
	14 -> 15;
	145[label = "differential_expressions"];
	144 -> 145;
	156[label = "annotate"];
	147 -> 156;
	86 -> 156;
	83[label = "map_ds_against_hg"];
	13 -> 83;
	81 -> 83;
	123[label = "sort_a_bam_file"];
	100 -> 123;
	115[label = "index_a_bam_file"];
	114 -> 115;
}



================================================
FILE: misc/nano/README.md
================================================
A nano syntax highlighting definition for Snakemake.

To use this file in nano, copy the `syntax/snakemake.nanorc` file
to your `$HOME` directory and add a line to your ~/.nanorc saying:

    include $HOME/snakemake.nanorc


NB. Line 12 of the syntax file contains a regular expression for
identifying a shebang (#!) header line. This command is not
supported in some versions of nano, so is commented out. If you
wish to enable it, simply remove the # at the beginning of the
line. If you are uncertain if your version of nano supports this
command, you may remove it and try. You will see an error if it
does not. Leaving the line commented out will result in the header
being treated as a regular comment for highlighting purposes. 



================================================
FILE: misc/nano/syntax/snakemake.nanorc
================================================
# A nano syntax sheet for Snakemake files

# Language: Snakemake (extended from python.nanorc
# Maintainer: Bailey Harrington (baileythegreen@gmail.com)
# Created: 28 September 2020
# Last change: 13 October 2020

# To use this file in nano, add a
# line to your ~/.nanorc saying:
# include <path_to_this_file>/snakemake.nanorc

# Specify the filename patterns this syntax applies to
syntax "python" "\.smk$" "Snakefile$" "\.snake$"

# describes the shebang line as the header
# header "^#!.*/python[-0-9._]*"

# Python keywords
color blue "\<(and|as|assert|break|class|continue|del|elif|else|except|exec|finally|for|from|global|if|import|in|is|lambda|not|or|pass|print|raise|return|try|while|with|yield)\>"

# the function definition keyword for Python
color brightblue "def [0-9A-Za-z_]+"

# strings
color green "['][^']*[^\\][']" "[']{3}.*[^\\][']{3}"
color green "["][^"]*[^\\]["]" "["]{3}.*[^\\]["]{3}"

# multi-line comments
color green start=""""[^"]" end=""""" start="'''[^']" end="'''"

# regular comments
color brightmagenta "#.*$"

# Snakemake keywords
color blue "\<(include|workdir|onsuccess|onerror|ruleorder|localrule|localrules|configfile|touch|protected|temp|input|output|params|message|threads|resources|version|run|shell|benchmark|snakefile|log)\>"

# rule or subworkflow names
color brightmagenta "(rule|subworkflow)\s*[A-Za-z_]*"

# the Snakemake rule and subworkflow keywords
color blue "(rule|subworkflow)"



================================================
FILE: misc/vim/README.md
================================================
A vim syntax highlighting definition for Snakemake.

To install via Vundle use:

    Plugin 'https://github.com/snakemake/snakemake.git', {'rtp': 'misc/vim/'}

To install via [vim-plug]( https://github.com/junegunn/vim-plug):

    Plug 'snakemake/snakemake', {'rtp': 'misc/vim'}

To install via [packer.nvim](https://github.com/wbthomason/packer.nvim):

    use {'snakemake/snakemake', rtp='misc/vim', ft='snakemake'}

To install via [lazy.nvim](https://github.com/folke/lazy.nvim):

    ``` lua
    return {
      {
        "snakemake/snakemake",
        ft = "snakemake",
        config = function(plugin)
          vim.opt.rtp:append(plugin.dir .. "/misc/vim")
        end,
        init = function(plugin)
          require("lazy.core.loader").ftdetect(plugin.dir .. "/misc/vim")
        end,
      },
    }
    ```

To manually install, copy `syntax/snakemake.vim` file to `$HOME/.vim/syntax`
directory and `ftdetect/snakemake.vim` file to `$HOME/.vim/ftdetect`.

Highlighting can be forced in a vim session with `:set syntax=snakemake`.

By default, all rules will be folded.  To unfold all levels, use `zR`.  `zM`
will refold all levels.  If you'd like to change the default, add
`set nofoldenable` to your `.vimrc`.  To learn more, see `:h fold`.



================================================
FILE: misc/vim/ftdetect/snakemake.vim
================================================
" Vim ftdetect file
" Language: Snakemake (extended from python.vim)
" Maintainer: Jay Hesselberth (jay.hesselberth@gmail.com)
" Last Change: 2020 Oct 6
"
" Usage
"
" copy to $HOME/.vim/ftdetect directory
au BufNewFile,BufRead Snakefile,*.smk set filetype=snakemake 



================================================
FILE: misc/vim/ftplugin/snakemake/comment.vim
================================================
setlocal commentstring=#\ %s



================================================
FILE: misc/vim/ftplugin/snakemake/folding.vim
================================================
setlocal foldmethod=expr
setlocal foldexpr=GetSnakemakeFold(v:lnum)

function! GetSnakemakeFold(lnum)
    " fold preamble
    if a:lnum == 1
        return '>1'
    endif

    let thisline = getline(a:lnum)

    " blank lines end folds
    if thisline =~? '\v^\s*$'
        return '-1'
    " start fold on top level rules, checkpoints, use statements, or python objects
    elseif thisline =~? '\v^(rule|def|checkpoint|class|use|onstart|onsuccess|onerror)'
        return ">1"
    elseif thisline =~? '\v^\S'
        if PreviousLineIndented(a:lnum) && NextRuleIndented(a:lnum)
            return ">1"
        endif
    endif

    return "="

endfunction

function! NextRuleIndented(lnum)
    let numlines = line('$')
    let current = a:lnum + 1

    while current <= numlines
        let thisline = getline(current)
        if thisline =~? '\v^(rule|def|checkpoint|class|use)'
            return 0
        elseif thisline =~? '\v^\s+(rule|checkpoint|use)'
            return 1
        endif

        let current += 1
    endwhile

    return 0
endfunction

function! PreviousLineIndented(lnum)
    let current = a:lnum - 1

    while current >= 1
        let thisline = getline(current)
        if thisline =~? '\v^\S'
            return 0
        elseif thisline =~? '\v^\s+\S'
            return 1
        endif

        let current -= 1
    endwhile

    return 0
endfunction



================================================
FILE: misc/vim/ftplugin/snakemake/sections.vim
================================================
function! s:NextSection(type, backwards, visual)
    if a:visual
        normal! gv
    endif

    if a:type == 1
        let pattern = '\v(^rule|^checkpoint|^def|^use|^onstart|^onsuccess|^onerror|%^)' 
    elseif a:type == 2
        let pattern = '\v\n\zs\n^(rule|checkpoint|def|use|onstart|onsuccess|onerror)'
    endif

    if a:backwards
        let dir = '?'
    else
        let dir = '/'
    endif

    execute 'silent normal! ' . dir . pattern . dir . "\r"
endfunction

noremap <script> <buffer> <silent> ]]
        \ :call <SID>NextSection(1, 0, 0)<cr>

noremap <script> <buffer> <silent> [[
        \ :call <SID>NextSection(1, 1, 0)<cr>

noremap <script> <buffer> <silent> ][
        \ :call <SID>NextSection(2, 0, 0)<cr>

noremap <script> <buffer> <silent> []
        \ :call <SID>NextSection(2, 1, 0)<cr>

vnoremap <script> <buffer> <silent> ]]
        \ :<c-u>call <SID>NextSection(1, 0, 1)<cr>

vnoremap <script> <buffer> <silent> [[
        \ :<c-u>call <SID>NextSection(1, 1, 1)<cr>

vnoremap <script> <buffer> <silent> ][
        \ :<c-u>call <SID>NextSection(2, 0, 1)<cr>

vnoremap <script> <buffer> <silent> []
        \ :<c-u>call <SID>NextSection(2, 1, 1)<cr>



================================================
FILE: misc/vim/syntax/snakemake.vim
================================================
" Vim syntax file
" Language: Snakemake (extended from python.vim)
" Maintainer: Jay Hesselberth (jay.hesselberth@gmail.com)
" Last Change: 2025 Mar 6
"
" Usage
"
" copy to $HOME/.vim/syntax directory and
" copy to ftdetect/snakemake.vim to $HOME/.vim/ftdetect directory
"
" force coloring in a vim session with:
"
" :set syntax=snakemake
"
if exists("b:current_syntax")
    finish
endif

" load settings from system python.vim (7.4)
source $VIMRUNTIME/syntax/python.vim
source $VIMRUNTIME/indent/python.vim

" Snakemake rules, as of version 8.29
"
"
" rule         = "rule" (identifier | "") ":" ruleparams
" include      = "include:" stringliteral
" workdir      = "workdir:" stringliteral
" module       = "module" identifier ":" moduleparams
" configfile   = "configfile" ":" stringliteral
" userule      = "use" "rule" (identifier | "*") "from" identifier ["as" identifier] ["with" ":" norunparams]
" ni           = NEWLINE INDENT
" norunparams  = [ni input] [ni output] [ni params] [ni message] [ni threads] [ni resources] [ni log] [ni conda] [ni container] [ni benchmark] [ni cache] [ni priority]
" ruleparams   = norunparams [ni (run | shell | script | notebook)] NEWLINE snakemake
" input        = "input" ":" parameter_list
" output       = "output" ":" parameter_list
" params       = "params" ":" parameter_list
" log          = "log" ":" parameter_list
" benchmark    = "benchmark" ":" statement
" cache        = "cache" ":" bool
" message      = "message" ":" stringliteral
" threads      = "threads" ":" integer
" priority     = "priority" ":" integer
" resources    = "resources" ":" parameter_list
" version      = "version" ":" statement
" conda        = "conda" ":" stringliteral
" container    = "container" ":" stringliteral
" run          = "run" ":" ni statement
" shell        = "shell" ":" stringliteral
" script       = "script" ":" stringliteral
" notebook     = "notebook" ":" stringliteral
" moduleparams = [ni snakefile] [ni metawrapper] [ni config] [ni skipval]
" snakefile    = "snakefile" ":" stringliteral
" metawrapper  = "meta_wrapper" ":" stringliteral
" config       = "config" ":" stringliteral
" skipval      = "skip_validation" ":" stringliteral


" general directives (e.g. input)
syn keyword pythonStatement 
      \ benchmark
      \ conda
      \ configfile
      \ container
      \ default_target
      \ envmodules
      \ group
      \ include
      \ input
      \ localrule
      \ localrules
      \ log
      \ message
      \ notebook
      \ onerror
      \ onstart
      \ onsuccess
      \ output
      \ params
      \ priority
      \ resources
      \ ruleorder
      \ run
      \ scattergather
      \ script
      \ shadow
      \ shell
      \ singularity
      \ snakefile
      \ template_engine
      \ threads
      \ version
      \ wildcard_constraints
      \ wildcards
      \ workdir
      \ wrapper

" directives with a label (e.g. rule)
syn keyword pythonStatement 
      \ checkpoint
      \ rule
      \ subworkflow
      \ use
      \ nextgroup=pythonFunction skipwhite

" common snakemake objects
syn keyword pythonBuiltinObj 
      \ Paramspace
      \ checkpoints
      \ config
      \ gather
      \ rules
      \ scatter
      \ workflow

" snakemake functions
syn keyword pythonBuiltinFunc 
      \ ancient
      \ before_update
      \ directory
      \ expand
      \ exists
      \ multiext
      \ pipe
      \ protected
      \ read_job_properties
      \ service
      \ temp
      \ touch
      \ unpack
      \ update

" similar to special def and class treatment from python.vim, except
" parenthetical part of def and class
syn match pythonFunction
      \ "\%(\%(rule\s\|subworkflow\s\|checkpoint\s\)\s*\)\@<=\h\w*" contained

syn sync match pythonSync grouphere NONE "^\s*\%(rule\|subworkflow\|checkpoint\)\s\+\h\w*\s*"

let b:current_syntax = "snakemake"

" vim:set sw=2 sts=2 ts=8 noet:



================================================
FILE: playground/.gitkeep
================================================
[Empty file]


================================================
FILE: src/snakemake/__init__.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2023, Johannes Köster"
__email__ = "johannes.koester@protonmail.com"
__license__ = "MIT"

import sys

from snakemake._version import version as __version__

PIP_DEPLOYMENTS_PATH = ".snakemake/pip-deployments"

sys.path.append(PIP_DEPLOYMENTS_PATH)

# Reexports that are part of the public API:
from snakemake.shell import shell


if __name__ == "__main__":
    from snakemake.cli import main
    import sys

    main(sys.argv)



================================================
FILE: src/snakemake/__main__.py
================================================
# This script makes it possible to invoke snakemake with 'python3 -m snakemake'
from snakemake.cli import main

main()



================================================
FILE: src/snakemake/api.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from abc import ABC
from dataclasses import dataclass, field
import functools
import hashlib
from pathlib import Path
import sys
from typing import Dict, List, Mapping, Optional, Set
import os
import tarfile
import uuid
from snakemake.common import MIN_PY_VERSION, SNAKEFILE_CHOICES, async_run
from snakemake.settings.types import (
    ChangeType,
    GroupSettings,
    SchedulingSettings,
    WorkflowSettings,
    GlobalReportSettings,
)

if sys.version_info < MIN_PY_VERSION:
    raise ValueError(
        f"Snakemake requires at least Python {'.'.join(map(str, MIN_PY_VERSION))}."
    )

from snakemake.common.workdir_handler import WorkdirHandler
from snakemake.settings.types import (
    DAGSettings,
    DeploymentMethod,
    DeploymentSettings,
    ExecutionSettings,
    OutputSettings,
    ConfigSettings,
    RemoteExecutionSettings,
    ResourceSettings,
    StorageSettings,
    SharedFSUsage,
)
from snakemake.scheduling.greedy import SchedulerSettings as GreedySchedulerSettings
from snakemake.scheduling.milp import SchedulerSettings as IlpSchedulerSettings

from snakemake_interface_executor_plugins.settings import ExecMode, ExecutorSettingsBase
from snakemake_interface_executor_plugins.registry import ExecutorPluginRegistry
from snakemake_interface_common.exceptions import ApiError
from snakemake_interface_storage_plugins.registry import StoragePluginRegistry
from snakemake_interface_common.plugin_registry.plugin import TaggedSettings
from snakemake_interface_report_plugins.settings import ReportSettingsBase
from snakemake_interface_report_plugins.registry import ReportPluginRegistry
from snakemake_interface_logger_plugins.registry import LoggerPluginRegistry
from snakemake_interface_logger_plugins.common import LogEvent
from snakemake_interface_scheduler_plugins.settings import SchedulerSettingsBase
from snakemake_interface_scheduler_plugins.registry import SchedulerPluginRegistry

from snakemake.workflow import Workflow
from snakemake.exceptions import print_exception
from snakemake.logging import logger, logger_manager
from snakemake.shell import shell
from snakemake.common import (
    MIN_PY_VERSION,
    __version__,
)
from snakemake.resources import DefaultResources


class ApiBase(ABC):
    def __post_init__(self):
        self._check()

    def _check(self):
        # nothing to check by default
        # override in subclasses if needed
        pass


def resolve_snakefile(path: Optional[Path], allow_missing: bool = False):
    """Get path to the snakefile.

    Arguments
    ---------
    path: Optional[Path] -- The path to the snakefile. If not provided, default locations will be tried.
    """
    if path is None:
        for p in SNAKEFILE_CHOICES:
            if p.exists():
                return p
        if not allow_missing:
            raise ApiError(
                f"No Snakefile found, tried {', '.join(map(str, SNAKEFILE_CHOICES))}."
            )
    return path


@dataclass
class SnakemakeApi(ApiBase):
    """The Snakemake API.

    Arguments
    ---------

    output_settings: OutputSettings -- The output settings for the Snakemake API.
    """

    output_settings: OutputSettings = field(default_factory=OutputSettings)
    _workflow_api: Optional["WorkflowApi"] = field(init=False, default=None)
    _is_in_context: bool = field(init=False, default=False)

    def workflow(
        self,
        resource_settings: ResourceSettings,
        config_settings: Optional[ConfigSettings] = None,
        storage_settings: Optional[StorageSettings] = None,
        workflow_settings: Optional[WorkflowSettings] = None,
        deployment_settings: Optional[DeploymentSettings] = None,
        storage_provider_settings: Optional[Mapping[str, TaggedSettings]] = None,
        snakefile: Optional[Path] = None,
        workdir: Optional[Path] = None,
    ):
        """Create the workflow API.

        Note that if provided, this also changes to the provided workdir.
        It will change back to the previous working directory when the workflow API object is deleted.

        Arguments
        ---------
        config_settings: ConfigSettings -- The config settings for the workflow.
        resource_settings: ResourceSettings -- The resource settings for the workflow.
        storage_settings: StorageSettings -- The storage settings for the workflow.
        snakefile: Optional[Path] -- The path to the snakefile. If not provided, default locations will be tried.
        workdir: Optional[Path] -- The path to the working directory. If not provided, the current working directory will be used.
        """

        if config_settings is None:
            config_settings = ConfigSettings()
        if storage_settings is None:
            storage_settings = StorageSettings()
        if workflow_settings is None:
            workflow_settings = WorkflowSettings()
        if deployment_settings is None:
            deployment_settings = DeploymentSettings()
        if storage_provider_settings is None:
            storage_provider_settings = dict()

        self._check_is_in_context()

        self._check_default_storage_provider(storage_settings=storage_settings)

        snakefile = resolve_snakefile(snakefile)

        self._workflow_api = WorkflowApi(
            snakemake_api=self,
            snakefile=snakefile,
            workdir=workdir,
            config_settings=config_settings,
            resource_settings=resource_settings,
            storage_settings=storage_settings,
            workflow_settings=workflow_settings,
            deployment_settings=deployment_settings,
            storage_provider_settings=storage_provider_settings,
        )
        return self._workflow_api

    def _cleanup(self):
        """Cleanup the workflow."""
        if not self.output_settings.keep_logger:
            logger_manager.cleanup_logfile()
            logger_manager.stop()
        if self._workflow_api is not None:
            self._workflow_api._workdir_handler.change_back()
            if self._workflow_api._workflow_store is not None:
                self._workflow_api._workflow_store.tear_down()

    def deploy_sources(
        self,
        query: str,
        checksum: str,
        storage_settings: StorageSettings,
        storage_provider_settings: Dict[str, TaggedSettings],
    ):
        if (
            storage_settings.default_storage_provider is None
            or storage_settings.default_storage_prefix is None
        ):
            raise ApiError(
                "A default storage provider and prefix has to be set for deployment of "
                "sources."
            )

        self._check_default_storage_provider(storage_settings=storage_settings)

        plugin = StoragePluginRegistry().get_plugin(
            storage_settings.default_storage_provider
        )
        if not plugin.is_read_write():
            raise ApiError(
                f"Default storage provider {storage_settings.default_storage_provider} "
                "is not a read-write storage provider."
            )

        plugin_settings = storage_provider_settings.get(
            storage_settings.default_storage_provider
        ).get_settings(None)

        if plugin_settings is not None:
            plugin.validate_settings(plugin_settings)

        provider_instance = plugin.storage_provider(
            logger=logger,
            local_prefix=storage_settings.local_storage_prefix,
            settings=plugin_settings,
            is_default=True,
            wait_for_free_local_storage=storage_settings.wait_for_free_local_storage,
        )
        query_validity = provider_instance.is_valid_query(query)
        if not query_validity:
            raise ApiError(
                f"Error when applying default storage provider "
                f"{storage_settings.default_storage_provider} to upload workflow "
                f"sources. {query_validity}"
            )
        storage_object = provider_instance.object(query)
        async_run(storage_object.managed_retrieve())
        with open(storage_object.local_path(), "rb") as f:
            obtained_checksum = hashlib.file_digest(f, "sha256").hexdigest()
        if obtained_checksum != checksum:
            raise ApiError(
                f"Checksum of retrieved sources ({obtained_checksum}) does not match "
                f"expected checksum ({checksum})."
            )
        with tarfile.open(storage_object.local_path(), "r") as tar:
            tar.extractall()

    def print_exception(self, ex: Exception):
        """Print an exception during workflow execution in a human readable way
        (with adjusted line numbers for exceptions raised in Snakefiles and stack
        traces that hide Snakemake internals for better readability).

        Arguments
        ---------
        ex: Exception -- The exception to print.
        """
        linemaps = dict()
        if (
            self._workflow_api is not None
            and self._workflow_api._workflow_store is not None
        ):
            linemaps = self._workflow_api._workflow_store.linemaps
        print_exception(ex, linemaps)

    def setup_logger(
        self,
        stdout: bool = False,
        mode: ExecMode = ExecMode.DEFAULT,
        dryrun: bool = False,
    ):
        if not self.output_settings.keep_logger and not logger_manager.initialized:
            log_handlers = []
            for name, settings in self.output_settings.log_handler_settings.items():
                plugin = LoggerPluginRegistry().get_plugin(name)
                plugin.validate_settings(settings)
                log_handlers.append(plugin.log_handler(self.output_settings, settings))

            self.output_settings.dryrun = dryrun
            logger_manager.setup(
                mode=mode,
                handlers=log_handlers,
                settings=self.output_settings,
            )

    def _check_is_in_context(self):
        if not self._is_in_context:
            raise ApiError(
                "This method can only be called when SnakemakeApi is used within a with "
                "statement."
            )

    def _check_default_storage_provider(self, storage_settings: StorageSettings):
        if storage_settings.default_storage_provider is not None:
            plugin = StoragePluginRegistry().get_plugin(
                storage_settings.default_storage_provider
            )
            if not plugin.is_read_write():
                raise ApiError(
                    f"Default storage provider {storage_settings.default_storage_provider} "
                    "is not a read-write storage provider."
                )

    def __enter__(self):
        self._is_in_context = True
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self._is_in_context = False
        self._cleanup()


def _no_dag(method):
    @functools.wraps(method)
    def _handle_no_dag(self: "WorkflowApi", *args, **kwargs):
        self.snakemake_api.setup_logger()
        self.resource_settings.cores = 1
        return method(self, *args, **kwargs)

    return _handle_no_dag


@dataclass
class WorkflowApi(ApiBase):
    """The workflow API.

    Arguments
    ---------
    snakemake_api: SnakemakeApi -- The Snakemake API.
    snakefile: Path -- The path to the snakefile.
    config_settings: ConfigSettings -- The config settings for the workflow.
    resource_settings: ResourceSettings -- The resource settings for the workflow.
    """

    snakemake_api: SnakemakeApi
    snakefile: Path
    workdir: Optional[Path]
    config_settings: ConfigSettings
    resource_settings: ResourceSettings
    storage_settings: StorageSettings
    workflow_settings: WorkflowSettings
    deployment_settings: DeploymentSettings
    storage_provider_settings: Mapping[str, TaggedSettings]

    _workflow_store: Optional[Workflow] = field(init=False, default=None)
    _workdir_handler: Optional[WorkdirHandler] = field(init=False)

    def dag(
        self,
        dag_settings: Optional[DAGSettings] = None,
    ):
        """Create a DAG API.

        Arguments
        ---------
        dag_settings: DAGSettings -- The DAG settings for the DAG API.
        """
        if dag_settings is None:
            dag_settings = DAGSettings()

        return DAGApi(
            self.snakemake_api,
            self,
            dag_settings=dag_settings,
        )

    @_no_dag
    def lint(self, json: bool = False):
        """Lint the workflow.

        Arguments
        ---------
        json: bool -- Whether to print the linting results as JSON.

        Returns
        -------
        True if any lints were printed
        """
        workflow = self._get_workflow(check_envvars=False)
        self._workflow_store = workflow
        workflow.include(
            self.snakefile, overwrite_default_target=True, print_compilation=False
        )
        workflow.check()
        return workflow.lint(json=json)

    @_no_dag
    def list_rules(self, only_targets: bool = False):
        """List the rules of the workflow.

        Arguments
        ---------
        only_targets: bool -- Whether to only list target rules.
        """
        self._workflow.list_rules(only_targets=only_targets)

    @_no_dag
    def list_resources(self):
        """List the resources of the workflow."""
        self._workflow.list_resources()

    @_no_dag
    def print_compilation(self):
        """Print the pure python compilation of the workflow."""
        workflow = self._get_workflow()
        workflow.include(self.snakefile, print_compilation=True)

    @property
    def _workflow(self):
        if self._workflow_store is None:
            workflow = self._get_workflow()
            self._workflow_store = workflow
            workflow.include(
                self.snakefile, overwrite_default_target=True, print_compilation=False
            )
            workflow.check()
        return self._workflow_store

    def _get_workflow(self, **kwargs):
        from snakemake.workflow import Workflow

        if "group_settings" not in kwargs:
            # just init with defaults, can be overwritten later
            kwargs["group_settings"] = GroupSettings()

        return Workflow(
            config_settings=self.config_settings,
            resource_settings=self.resource_settings,
            workflow_settings=self.workflow_settings,
            deployment_settings=self.deployment_settings,
            storage_settings=self.storage_settings,
            output_settings=self.snakemake_api.output_settings,
            overwrite_workdir=self.workdir,
            storage_provider_settings=self.storage_provider_settings,
            **kwargs,
        )

    def __post_init__(self):
        self._workdir_handler = None
        super().__post_init__()
        self.snakefile = self.snakefile.absolute()
        self._workdir_handler = WorkdirHandler(self.workdir)
        self._workdir_handler.change_to()

    def _check(self):
        if not self.snakefile.exists():
            raise ApiError(f'Snakefile "{self.snakefile}" not found.')


@dataclass
class DAGApi(ApiBase):
    """The DAG API.

    Arguments
    ---------
    snakemake_api: SnakemakeApi -- The Snakemake API.
    workflow_api: WorkflowApi -- The workflow API.
    dag_settings: DAGSettings -- The DAG settings for the DAG API.
    """

    snakemake_api: SnakemakeApi
    workflow_api: WorkflowApi
    dag_settings: DAGSettings

    def __post_init__(self):
        self.workflow_api._workflow.dag_settings = self.dag_settings

    def execute_workflow(
        self,
        executor: str = "local",
        execution_settings: Optional[ExecutionSettings] = None,
        remote_execution_settings: Optional[RemoteExecutionSettings] = None,
        scheduling_settings: Optional[SchedulingSettings] = None,
        group_settings: Optional[GroupSettings] = None,
        executor_settings: Optional[ExecutorSettingsBase] = None,
        updated_files: Optional[List[str]] = None,
        scheduler_settings: Optional[SchedulerSettingsBase] = None,
        greedy_scheduler_settings: Optional[GreedySchedulerSettings] = None,
    ):
        """Execute the workflow.

        Arguments
        ---------
        executor: str -- The executor to use.
        execution_settings: ExecutionSettings -- The execution settings for the workflow.
        resource_settings: ResourceSettings -- The resource settings for the workflow.
        remote_execution_settings: RemoteExecutionSettings -- The remote execution settings for the workflow.
        executor_settings: Optional[ExecutorSettingsBase] -- The executor settings for the workflow.
        updated_files: Optional[List[str]] -- An optional list where Snakemake will put all updated files.
        """

        if execution_settings is None:
            execution_settings = ExecutionSettings()
        if remote_execution_settings is None:
            remote_execution_settings = RemoteExecutionSettings()
        if scheduling_settings is None:
            scheduling_settings = SchedulingSettings()
        if group_settings is None:
            group_settings = GroupSettings()

        if (
            remote_execution_settings.immediate_submit
            and not self.workflow_api.storage_settings.notemp
        ):
            raise ApiError(
                "immediate_submit has to be combined with notemp (it does not support temp file handling)"
            )

        executor_plugin_registry = ExecutorPluginRegistry()
        executor_plugin = executor_plugin_registry.get_plugin(executor)

        if executor_settings is not None:
            executor_plugin.validate_settings(executor_settings)

        if executor_plugin.common_settings.implies_no_shared_fs:
            # no shared FS at all
            self.workflow_api.storage_settings.shared_fs_usage = frozenset()

        self.snakemake_api.setup_logger(
            stdout=executor_plugin.common_settings.dryrun_exec,
            mode=self.workflow_api.workflow_settings.exec_mode,
            dryrun=executor_plugin.common_settings.dryrun_exec,
        )

        if (
            executor_plugin.common_settings.local_exec
            and not executor_plugin.common_settings.dryrun_exec
            and self.workflow_api.workflow_settings.exec_mode == ExecMode.DEFAULT
        ):
            logger.info("Assuming unrestricted shared filesystem usage.")
            self.workflow_api.storage_settings.shared_fs_usage = SharedFSUsage.all()
        if executor_plugin.common_settings.job_deploy_sources:
            remote_execution_settings.job_deploy_sources = True

        if (
            self.workflow_api.workflow_settings.exec_mode == ExecMode.DEFAULT
            and SharedFSUsage.INPUT_OUTPUT
            not in self.workflow_api.storage_settings.shared_fs_usage
            and (
                not self.workflow_api.storage_settings.default_storage_provider
                or self.workflow_api.storage_settings.default_storage_prefix is None
            )
            and executor_plugin.common_settings.can_transfer_local_files is False
        ):
            raise ApiError(
                "If no shared filesystem is assumed for input and output files, a "
                "default storage provider (--default-storage-provider) and "
                "default storage prefix (--default-storage-prefix) has to be set. "
                "See https://snakemake.github.io/snakemake-plugin-catalog for possible "
                "storage provider plugins."
            )
        if (
            executor_plugin.common_settings.local_exec
            and not executor_plugin.common_settings.dryrun_exec
            and self.workflow_api.workflow_settings.exec_mode == ExecMode.DEFAULT
            and self.workflow_api.storage_settings.shared_fs_usage
            != SharedFSUsage.all()
        ):
            raise ApiError(
                "For local execution, --shared-fs-usage has to be unrestricted."
            )

        if executor_plugin.common_settings.local_exec:
            if (
                not executor_plugin.common_settings.dryrun_exec
                and not executor_plugin.common_settings.touch_exec
            ):
                if self.workflow_api.resource_settings.cores is None:
                    raise ApiError(
                        "cores have to be specified for local execution "
                        "(use --cores N with N being a number >= 1 or 'all')"
                    )
                # clean up all previously recorded jobids.
                shell.cleanup()
            else:
                # set cores if that is not done yet
                if self.workflow_api.resource_settings.cores is None:
                    self.workflow_api.resource_settings.cores = 1
            if (
                execution_settings.debug
                and self.workflow_api.resource_settings.cores > 1
            ):
                raise ApiError(
                    "debug mode cannot be used with multi-core execution, "
                    "please enforce a single core by setting --cores 1"
                )
        else:
            if self.workflow_api.resource_settings.nodes is None:
                raise ApiError(
                    "maximum number of parallel jobs/used nodes has to be specified for remote execution "
                    "(use --jobs N with N being a number >= 1)"
                )
            # non local execution
            if self.workflow_api.resource_settings.default_resources is None:
                # use full default resources if in cluster or cloud mode
                self.workflow_api.resource_settings.default_resources = (
                    DefaultResources(mode="full")
                )
            if execution_settings.edit_notebook is not None:
                raise ApiError(
                    "notebook edit mode is only allowed with local execution."
                )
            if execution_settings.debug:
                raise ApiError("debug mode cannot be used with non-local execution")

        execution_settings.use_threads = (
            execution_settings.use_threads
            or (os.name not in ["posix"])
            or not executor_plugin.common_settings.local_exec
        )

        scheduler = scheduling_settings.scheduler

        if greedy_scheduler_settings is None:
            greedy_scheduler_settings = GreedySchedulerSettings()

        if (
            executor == "touch"
            or executor == "dryrun"
            or remote_execution_settings.immediate_submit
        ):
            greedy_scheduler_settings.omit_prioritize_by_temp_and_input = True
            scheduler = "greedy"
        if scheduling_settings.greediness is not None:
            greedy_scheduler_settings.greediness = scheduling_settings.greediness

        if scheduler == "ilp":
            if scheduler_settings is None:
                scheduler_settings = IlpSchedulerSettings()
            import pulp

            if pulp.apis.LpSolverDefault is None:
                logger.warning(
                    "Falling back to greedy scheduler because no default "
                    "ILP solver is found (you have to install either "
                    "coincbc or glpk)."
                )
                scheduler = "greedy"
                scheduler_settings = greedy_scheduler_settings

        scheduler_plugin = SchedulerPluginRegistry().get_plugin(scheduler)

        workflow = self.workflow_api._workflow
        workflow.execution_settings = execution_settings
        workflow.remote_execution_settings = remote_execution_settings
        workflow.scheduling_settings = scheduling_settings
        workflow.group_settings = group_settings
        logger.info(
            None,
            extra=dict(
                event=LogEvent.WORKFLOW_STARTED,
                workflow_id=uuid.uuid4(),
                snakefile=self.workflow_api.snakefile,
            ),
        )
        workflow.execute(
            executor_plugin=executor_plugin,
            executor_settings=executor_settings,
            scheduler_plugin=scheduler_plugin,
            scheduler_settings=scheduler_settings,
            greedy_scheduler_settings=greedy_scheduler_settings,
            updated_files=updated_files,
        )

    def _no_exec(method):
        @functools.wraps(method)
        def _handle_no_exec(self, *args, **kwargs):
            self.workflow_api.resource_settings.cores = 1
            self.snakemake_api.setup_logger()
            return method(self, *args, **kwargs)

        return _handle_no_exec

    @_no_exec
    def generate_unit_tests(self, path: Path):
        """Generate unit tests for the workflow.

        Arguments
        ---------
        path: Path -- The path to store the unit tests.
        """
        self.workflow_api._workflow.generate_unit_tests(path=path)

    @_no_exec
    def containerize(self):
        """Containerize the workflow."""
        self.workflow_api._workflow.containerize()

    @_no_exec
    def create_report(
        self,
        reporter: str = "html",
        report_settings: Optional[ReportSettingsBase] = None,
        global_report_settings: Optional[GlobalReportSettings] = None,
    ):
        """Create a report for the workflow.

        Arguments
        ---------
        report: Path -- The path to the report.
        report_settings: Optional[ReportSettingsBase] -- Report settings for the html report.
        global_report_settings: Optional[GlobalReportSettings] -- Report settings that apply to all report plugins.
        reporter: str -- report plugin to use (default: html)
        """

        report_plugin_registry = ReportPluginRegistry()
        report_plugin = report_plugin_registry.get_plugin(reporter)

        if report_settings is not None:
            report_plugin.validate_settings(report_settings)

        if global_report_settings is None:
            global_report_settings = GlobalReportSettings()

        self.workflow_api._workflow.create_report(
            report_plugin=report_plugin,
            report_settings=report_settings,
            global_report_settings=global_report_settings,
        )

    @_no_exec
    def printdag(self):
        """Print the DAG of the workflow."""
        self.workflow_api._workflow.printdag()

    @_no_exec
    def printrulegraph(self):
        """Print the rule graph of the workflow."""
        self.workflow_api._workflow.printrulegraph()

    @_no_exec
    def printfilegraph(self):
        """Print the file graph of the workflow."""
        self.workflow_api._workflow.printfilegraph()

    @_no_exec
    def printd3dag(self):
        """Print the DAG of the workflow in D3.js compatible JSON."""
        self.workflow_api._workflow.printd3dag()

    @_no_exec
    def unlock(self):
        """Unlock the workflow."""
        self.workflow_api._workflow.unlock()

    @_no_exec
    def cleanup_metadata(self, paths: List[Path]):
        """Cleanup the metadata of the workflow."""
        self.workflow_api._workflow.cleanup_metadata(paths)

    @_no_exec
    def conda_cleanup_envs(self):
        """Cleanup the conda environments of the workflow."""
        self.workflow_api.deployment_settings.imply_deployment_method(
            DeploymentMethod.CONDA
        )
        self.workflow_api._workflow.conda_cleanup_envs()

    @_no_exec
    def conda_create_envs(self):
        """Only create the conda environments of the workflow."""
        self.workflow_api.deployment_settings.imply_deployment_method(
            DeploymentMethod.CONDA
        )
        self.workflow_api._workflow.conda_create_envs()

    @_no_exec
    def conda_list_envs(self):
        """List the conda environments of the workflow."""
        self.workflow_api.deployment_settings.imply_deployment_method(
            DeploymentMethod.CONDA
        )
        self.workflow_api._workflow.conda_list_envs()

    @_no_exec
    def cleanup_shadow(self):
        """Cleanup the shadow directories of the workflow."""
        self.workflow_api._workflow.cleanup_shadow()

    @_no_exec
    def container_cleanup_images(self):
        """Cleanup the container images of the workflow."""
        self.workflow_api.deployment_settings.imply_deployment_method(
            DeploymentMethod.APPTAINER
        )
        self.workflow_api._workflow.container_cleanup_images()

    @_no_exec
    def list_changes(self, change_type: ChangeType):
        """List the changes of the workflow.

        Arguments
        ---------
        change_type: ChangeType -- The type of changes to list.
        """
        self.workflow_api._workflow.list_changes(change_type=change_type)

    @_no_exec
    def list_untracked(self):
        """List the untracked files of the workflow."""
        self.workflow_api._workflow.list_untracked()

    @_no_exec
    def summary(self, detailed: bool = False):
        """Summarize the workflow.

        Arguments
        ---------
        detailed: bool -- Whether to print a detailed summary.
        """
        self.workflow_api._workflow.summary(detailed=detailed)

    @_no_exec
    def archive(self, path: Path):
        """Archive the workflow.

        Arguments
        ---------
        path: Path -- The path to the archive.
        """
        self.workflow_api._workflow.archive(path=path)

    @_no_exec
    def delete_output(self, only_temp: bool = False, dryrun: bool = False):
        """Delete the output of the workflow.

        Arguments
        ---------
        only_temp: bool -- Whether to only delete temporary output.
        dryrun: bool -- Whether to only dry-run the deletion.
        """
        self.workflow_api._workflow.delete_output(only_temp=only_temp, dryrun=dryrun)

    @_no_exec
    def export_to_cwl(self, path: Path):
        """Export the workflow to CWL.

        Arguments
        ---------
        path: Path -- The path to the CWL file.
        """
        self.workflow_api._workflow.export_to_cwl(path=path)



================================================
FILE: src/snakemake/benchmark.py
================================================
__author__ = "Manuel Holtgrewe"
__copyright__ = "Copyright 2022, Manuel Holtgrewe"
__email__ = "manuel.holtgrewe@bihealth.de"
__license__ = "MIT"

import contextlib
import datetime
from itertools import chain
import os
import time
import threading
from pathlib import Path

from snakemake.logging import logger

#: Interval (in seconds) between measuring resource usage
BENCHMARK_INTERVAL = 30
#: Interval (in seconds) between measuring resource usage before
#: BENCHMARK_INTERVAL
BENCHMARK_INTERVAL_SHORT = 0.5


class BenchmarkRecord:
    """Record type for benchmark times"""

    @classmethod
    def get_header(klass, extended_fmt=False):
        header = [
            "s",
            "h:m:s",
            "max_rss",
            "max_vms",
            "max_uss",
            "max_pss",
            "io_in",
            "io_out",
            "mean_load",
            "cpu_time",
        ]

        if extended_fmt:
            header += [
                "jobid",
                "rule_name",
                "wildcards",
                "params",
                "threads",
                "cpu_usage",
                "resources",
                "input_size_mb",
            ]

        return header

    def __init__(
        self,
        jobid=None,
        rule_name=None,
        wildcards=None,
        params=None,
        running_time=None,
        max_rss=None,
        max_vms=None,
        max_uss=None,
        max_pss=None,
        io_in=None,
        io_out=None,
        cpu_usage=None,
        cpu_time=None,
        resources=None,
        threads=None,
        input=None,
    ):
        #: Job ID
        self.jobid = (jobid,)
        #: Rule name
        self.rule_name = (rule_name,)
        #: Job wildcards
        self.wildcards = (wildcards,)
        #: Job parameters
        self.params = (params,)
        #: Running time in seconds
        self.running_time = running_time
        #: Maximal RSS in MB
        self.max_rss = max_rss
        #: Maximal VMS in MB
        self.max_vms = max_vms
        #: Maximal USS in MB
        self.max_uss = max_uss
        #: Maximal PSS in MB
        self.max_pss = max_pss
        #: I/O read in bytes
        self.io_in = io_in
        #: I/O written in bytes
        self.io_out = io_out
        #: Count of CPU seconds, divide by running time to get mean load estimate
        self.cpu_usage = cpu_usage or 0
        #: CPU usage (user and system) in seconds
        self.cpu_time = cpu_time or 0
        #: Job resources
        self.resources = (resources,)
        #: Job threads
        self.threads = (threads,)
        #: Job input
        self.input = input
        #: First time when we measured CPU load, for estimating total running time
        self.first_time = None
        #: Previous point when measured CPU load, for estimating total running time
        self.prev_time = None
        #: Set with procs that has been skipped
        self.processed_procs = dict()
        #: Set with procs that has been saved
        self.skipped_procs = set()
        #: Track if data has been collected
        self.data_collected = False

    def timedelta_to_str(self, x):
        """Conversion of timedelta to str without fractions of seconds"""
        mm, ss = divmod(x.seconds, 60)
        hh, mm = divmod(mm, 60)
        s = "%d:%02d:%02d" % (hh, mm, ss)
        if x.days:

            def plural(n):
                return n, abs(n) != 1 and "s" or ""

            s = ("%d day%s, " % plural(x.days)) + s
        return s

    def mean_load(self):
        return self.cpu_usage / self.running_time

    def parse_wildcards(self):
        return {key: value for key, value in self.wildcards.items()}

    def parse_params(self):
        return {key: value for key, value in self.params.items()}

    def parse_resources(self):
        return {key: value for key, value in self.resources.items()}

    def input_size_mb(self):
        return {file: Path(file).stat().st_size / 1024 / 1024 for file in self.input}

    def get_benchmarks(self, extended_fmt=False):
        logger.debug(
            f"Stats included in benchmarks file: {self.get_header(extended_fmt)}"
        )
        if self.skipped_procs:
            logger.debug(
                "Benchmark: not collected for "
                "; ".join(
                    [
                        f"{{'pid': {record[0]}, 'name': '{record[1]}''}}"
                        for record in self.skipped_procs
                    ]
                )
            )
            logger.debug(
                "Benchmark: collected for "
                "; ".join(
                    [
                        f"{{'pid': {record[0]}, 'name': '{record[1]}'}}"
                        for record in self.processed_procs
                    ]
                )
            )

        # If no data has been collect mem and cpu statistics will be printed as NA
        # to make it possible to distinguish this case from processes that complete instantly
        if not self.data_collected:
            logger.warning(
                "Benchmark: unable to collect cpu and memory benchmark statistics"
            )
        record = [
            f"{self.running_time:.4f}",
            self.timedelta_to_str(datetime.timedelta(seconds=self.running_time)),
            self.max_rss if self.data_collected else "NA",
            self.max_vms if self.data_collected else "NA",
            self.max_uss if self.data_collected else "NA",
            self.max_pss if self.data_collected else "NA",
            self.io_in if self.data_collected else "NA",
            self.io_out if self.data_collected else "NA",
            self.mean_load() if self.data_collected else "NA",
            self.cpu_time if self.data_collected else "NA",
        ]
        if extended_fmt:
            record += [
                self.jobid,
                self.rule_name,
                self.parse_wildcards(),
                self.parse_params(),
                self.threads,
                self.cpu_usage if self.data_collected else "NA",
                self.parse_resources(),
                self.input_size_mb(),
            ]
        return record

    def to_tsv(self, extended_fmt):
        """Return ``str`` with the TSV representation of this record"""

        def to_tsv_str(x):
            """Conversion of value to str for TSV (None becomes "-")"""
            if x is None:
                return "-"
            elif isinstance(x, float):
                return f"{x:.2f}"
            else:
                return str(x)

        return "\t".join(map(to_tsv_str, self.get_benchmarks(extended_fmt)))

    def to_json(self, extended_fmt):
        """Return ``str`` with the JSON representation of this record"""
        import json

        return json.dumps(
            dict(zip(self.get_header(extended_fmt), self.get_benchmarks(extended_fmt))),
            sort_keys=True,
        )


class DaemonTimer(threading.Thread):
    """A variant of threading.The timer that is daemonized"""

    def __init__(self, interval, function, args=None, kwargs=None):
        threading.Thread.__init__(self, daemon=True)
        self.interval = interval
        self.function = function
        self.args = args if args is not None else []
        self.kwargs = kwargs if kwargs is not None else {}
        self.finished = threading.Event()

    def cancel(self):
        """Stop the timer if it hasn't finished yet."""
        self.finished.set()

    def run(self):
        self.finished.wait(self.interval)
        if not self.finished.is_set():
            self.function(*self.args, **self.kwargs)
        self.finished.set()


class ScheduledPeriodicTimer:
    """Scheduling of periodic events

    Up to self._interval, schedule actions per second, above schedule events
    in self._interval second gaps.
    """

    def __init__(self, interval):
        self._times_called = 0
        self._interval = interval
        self._timer = None
        self._stopped = True

    def start(self):
        """Start the intervalic timer"""
        self.work()
        self._times_called += 1
        self._stopped = False
        if self._times_called > self._interval:
            self._timer = DaemonTimer(self._interval, self._action)
        else:
            self._timer = DaemonTimer(BENCHMARK_INTERVAL_SHORT, self._action)
        self._timer.start()

    def _action(self):
        """Internally, called by timer"""
        self.work()
        self._times_called += 1
        if self._times_called > self._interval:
            self._timer = DaemonTimer(self._interval, self._action)
        else:
            self._timer = DaemonTimer(BENCHMARK_INTERVAL_SHORT, self._action)
        self._timer.start()

    def work(self):
        """Override to perform the action"""
        raise NotImplementedError("Override me!")

    def cancel(self):
        """Call to cancel any events"""
        self._timer.cancel()
        self._stopped = True


class BenchmarkTimer(ScheduledPeriodicTimer):
    """Allows easy observation of a given PID for resource usage"""

    def __init__(self, pid, bench_record, interval=BENCHMARK_INTERVAL):
        import psutil

        ScheduledPeriodicTimer.__init__(self, interval)
        #: PID of observed process
        self.pid = pid
        self.main = psutil.Process(self.pid)
        #: ``BenchmarkRecord`` to write results to
        self.bench_record = bench_record
        #: Cache of processes to keep track of cpu percent
        self.procs = {}

    def work(self):
        """Write statistics"""
        import psutil

        try:
            self._update_record()
        except psutil.NoSuchProcess:
            pass  # skip, process died in flight
        except AttributeError:
            pass  # skip, process died in flight

    def _update_record(self):
        """Perform the actual measurement"""
        import psutil

        # Memory measurements
        rss, vms, uss, pss = 0, 0, 0, 0
        # I/O measurements
        io_in, io_out = 0, 0
        check_io = True
        # CPU seconds
        cpu_usage = 0
        # CPU usage time
        cpu_time = 0

        data_collected = False
        # Iterate over process and all children
        try:
            this_time = time.time()
            for proc in chain((self.main,), self.main.children(recursive=True)):
                proc = self.procs.setdefault(proc.pid, proc)
                with proc.oneshot():
                    if self.bench_record.prev_time:
                        cpu_usage += proc.cpu_percent() * (
                            this_time - self.bench_record.prev_time
                        )
                    # Makes it possible to summarize information about the process even
                    # if the benchmark has tried to access a process that the user does
                    # not have access to.
                    try:
                        meminfo = proc.memory_full_info()
                    except psutil.Error:
                        # Continue to fetch information about the remaining processes
                        # save skipped processes pid and name for debugging
                        self.bench_record.skipped_procs.add((proc.pid, proc.name()))
                        continue
                    rss += meminfo.rss
                    vms += meminfo.vms
                    uss += meminfo.uss
                    pss += meminfo.pss

                    if check_io:
                        try:
                            ioinfo = proc.io_counters()
                            io_in += ioinfo.read_bytes
                            io_out += ioinfo.write_bytes
                        except NotImplementedError as nie:
                            # OS doesn't track IO
                            check_io = False

                    cpu_times = proc.cpu_times()
                    self.bench_record.processed_procs[(proc.pid, proc.name())] = (
                        cpu_times.user + cpu_times.system
                    )

            cpu_time = sum(self.bench_record.processed_procs.values())

            self.bench_record.prev_time = this_time
            if not self.bench_record.first_time:
                self.bench_record.prev_time = this_time

            rss /= 1024 * 1024
            vms /= 1024 * 1024
            uss /= 1024 * 1024
            pss /= 1024 * 1024

            if check_io:
                io_in /= 1024 * 1024
                io_out /= 1024 * 1024
            else:
                io_in = None
                io_out = None
            data_collected = True
        except psutil.Error as e:
            return

        # Update benchmark record's RSS and VMS
        if data_collected:
            self.bench_record.data_collected = True
            self.bench_record.max_rss = max(self.bench_record.max_rss or 0, rss)
            self.bench_record.max_vms = max(self.bench_record.max_vms or 0, vms)
            self.bench_record.max_uss = max(self.bench_record.max_uss or 0, uss)
            self.bench_record.max_pss = max(self.bench_record.max_pss or 0, pss)

            self.bench_record.io_in = io_in
            self.bench_record.io_out = io_out

            self.bench_record.cpu_usage += cpu_usage
            self.bench_record.cpu_time = cpu_time


@contextlib.contextmanager
def benchmarked(pid=None, benchmark_record=None, interval=BENCHMARK_INTERVAL):
    """Measure benchmark parameters while within the context manager

    Yields a ``BenchmarkRecord`` with the results (values are set after
    leaving context).

    If ``pid`` is ``None`` then the PID of the current process will be used.
    If ``benchmark_record`` is ``None`` then a new ``BenchmarkRecord`` is
    created and returned, otherwise, the object passed as this parameter is
    returned.

    Usage::

        with benchmarked() as bench_result:
            pass
    """
    result = benchmark_record or BenchmarkRecord()
    if pid is False:
        yield result
    else:
        start_time = time.time()
        bench_thread = BenchmarkTimer(int(pid or os.getpid()), result, interval)
        bench_thread.start()
        yield result
        bench_thread.cancel()
        result.running_time = time.time() - start_time


def print_benchmark_tsv(records, file_, extended_fmt):
    """Write benchmark records to file-like the object"""
    logger.debug("Benchmarks in TSV format")
    print("\t".join(BenchmarkRecord.get_header(extended_fmt)), file=file_)
    for r in records:
        print(r.to_tsv(extended_fmt), file=file_)


def print_benchmark_jsonl(records, file_, extended_fmt):
    """Write benchmark records to file-like the object"""
    logger.debug("Benchmarks in JSONL format")
    for r in records:
        print(r.to_json(extended_fmt), file=file_)


def write_benchmark_records(records, path, extended_fmt):
    """Write benchmark records to file at path"""
    with open(path, "wt") as f:
        if path.endswith(".jsonl"):
            print_benchmark_jsonl(records, f, extended_fmt)
        else:
            print_benchmark_tsv(records, f, extended_fmt)



================================================
FILE: src/snakemake/checkpoints.py
================================================
from typing import TYPE_CHECKING
from snakemake.exceptions import IncompleteCheckpointException, WorkflowError
from snakemake.io import checkpoint_target
from snakemake.logging import logger

if TYPE_CHECKING:
    from snakemake.rules import Rule


class Checkpoints:
    """A singleton object in a workflow.

    Created_output can be accessed by checkpoint rules in or out modules.
    This never go into snakefile, so no rules name will be set to it.
    """

    def __init__(self):
        self._created_output = set()

    @property
    def created_output(self):
        return self._created_output

    def spawn_new_namespace(self):
        """Make a new namespace for checkpoints in the module."""
        return CheckpointsProxy(self)


class CheckpointsProxy(Checkpoints):
    """A namespace for checkpoints so that they can be accessed via dot notation.

    It will be created once a module is created,
    and different module will have different checkpoint namespace,
    but share a single created_output set.
    """

    def __init__(self, parent: Checkpoints):
        self.parent = parent

    @property
    def created_output(self):
        return self.parent.created_output

    def register(self, rule: "Rule", fallback_name=None):
        checkpoint = Checkpoint(rule, self)
        if fallback_name:
            setattr(self, fallback_name, checkpoint)
        setattr(self, rule.name, checkpoint)


class Checkpoint:
    __slots__ = ["rule", "checkpoints"]

    def __init__(self, rule: "Rule", checkpoints: Checkpoints):
        self.rule = rule
        self.checkpoints = checkpoints

    def get(self, **wildcards):
        missing = self.rule.wildcard_names.difference(wildcards.keys())
        if missing:
            raise WorkflowError(
                "Missing wildcard values for {}".format(", ".join(missing))
            )

        output, _ = self.rule.expand_output(wildcards)
        if self.checkpoints.created_output:
            missing_output = set(output) - set(self.checkpoints.created_output)
            if not missing_output:
                return CheckpointJob(self.rule, output)
            else:
                logger.debug(
                    f"Missing checkpoint output for {self.rule.name} "
                    f"(wildcards: {wildcards}): {','.join(missing_output)} of {','.join(output)}"
                )

        raise IncompleteCheckpointException(self.rule, checkpoint_target(output[0]))


class CheckpointJob:
    __slots__ = ["rule", "output"]

    def __init__(self, rule: "Rule", output):
        self.output = output
        self.rule = rule



================================================
FILE: src/snakemake/cwl.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from urllib.request import pathname2url
import os
import tempfile
import json
import shutil
from itertools import chain

from snakemake.utils import format
from snakemake.exceptions import WorkflowError
from snakemake.shell import shell
from snakemake.common import get_container_image
from snakemake_interface_executor_plugins.settings import ExecMode


def cwl(
    path,
    basedir,
    input,
    output,
    params,
    wildcards,
    threads,
    resources,
    log,
    config,
    rulename,
    use_singularity,
    bench_record,
    jobid,
    sourcecache_path,
    runtime_sourcecache_path,
):
    """
    Load cwl from the given basedir + path and execute it.
    """
    if shutil.which("cwltool") is None:
        raise WorkflowError(
            "'cwltool' must be in PATH in order to execute cwl directive."
        )

    if not path.startswith("http"):
        if path.startswith("file://"):
            path = path[7:]
        elif path.startswith("file:"):
            path = path[5:]
        if not os.path.isabs(path):
            path = os.path.abspath(os.path.join(basedir, path))
        path = "file://" + path
    path = format(path, wildcards=wildcards)
    if path.startswith("file://"):
        sourceurl = "file:" + pathname2url(path[7:])
    else:
        sourceurl = path

    def file_spec(f):
        if isinstance(f, str):
            return {"path": os.path.abspath(f), "class": "File"}
        return [file_spec(f_) for f_ in f]

    inputs = dict()
    inputs.update({name: file_spec(f) for name, f in input.items()})
    inputs.update({name: p for name, p in params.items()})
    inputs.update({name: f for name, f in output.items()})
    inputs.update({name: f for name, f in log.items()})

    args = "--singularity" if use_singularity else ""

    with tempfile.NamedTemporaryFile(mode="w") as input_file:
        json.dump(inputs, input_file)
        input_file.flush()
        cmd = f"cwltool {args} {sourceurl} {input_file.name}"
        shell(cmd, bench_record=bench_record)


def job_to_cwl(job, dag, outputs, inputs):
    """Convert a job with its dependencies to a CWL workflow step."""
    for f in job.output:
        if os.path.isabs(f):
            raise WorkflowError(
                "All output files have to be relative to the working directory."
            )

    get_output_id = lambda job, i: f"#main/job-{job.jobid}/{i}"

    dep_ids = {
        o: get_output_id(dep, i)
        for dep, files in dag.dependencies[job].items()
        for i, o in enumerate(dep.output)
        if o in files
    }
    files = [f for f in job.input if f not in dep_ids]
    if job.conda_env_file:
        files.add(os.path.relpath(job.conda_env_file))

    out = [get_output_id(job, i) for i, _ in enumerate(job.output)]

    def workdir_entry(i, f):
        location = f"??inputs.input_files[{i}].location??"
        if f.is_directory:
            entry = {
                "class": "Directory",
                "basename": os.path.basename(f),
                "location": location,
            }
        else:
            entry = {
                "class": "File",
                "basename": os.path.basename(f),
                "location": location,
            }
        return "$({})".format(
            json.dumps(outer_entry(f, entry)).replace('"??', "").replace('??"', "")
        ).replace('"', "'")

    def outer_entry(f, entry):
        parent = os.path.dirname(f)
        if parent:
            return outer_entry(
                parent,
                {
                    "class": "Directory",
                    "basename": os.path.basename(parent),
                    "listing": [entry],
                },
            )
        else:
            return entry

    if job in dag.targetjobs:
        # TODO this maps output files into the cwd after the workflow is complete.
        # We need to find a way to define subdirectories though. Otherwise,
        # there can be name clashes, and it will also become very crowded.
        outputs.append(
            {
                "type": {"type": "array", "items": "File"},
                "outputSource": f"#main/job-{job.jobid}/output_files",
                "id": f"#main/output/job-{job.jobid}",
            }
        )

    cwl = {
        "run": "#snakemake-job",
        "requirements": {
            "InitialWorkDirRequirement": {
                "listing": [
                    {"writable": True, "entry": workdir_entry(i, f)}
                    for i, f in enumerate(
                        chain(
                            files,
                            (f for dep in dag.dependencies[job] for f in dep.output),
                        )
                    )
                ]
            }
        },
        "in": {
            "cores": {"default": job.threads},
            "target_files": {"default": job.output._plainstrings()},
            "rules": {"default": [job.rule.name]},
        },
        "out": ["output_files"],
        "id": f"#main/job-{job.jobid}",
    }
    if files:
        inputs.append(
            {
                "type": {"type": "array", "items": "File"},
                "default": [{"class": "File", "location": f} for f in files],
                "id": f"#main/input/job-{job.jobid}",
            }
        )

    input_files = []
    if files:
        input_files.append(f"#main/input/job-{job.jobid}")
    input_files.extend(
        f"#main/job-{dep.jobid}/output_files" for dep in dag.dependencies[job]
    )

    cwl["in"]["input_files"] = {"source": input_files, "linkMerge": "merge_flattened"}

    return cwl


def dag_to_cwl(dag):
    """Convert a given DAG to a CWL workflow, which is returned as a JSON object."""
    snakemake_cwl = {
        "class": "CommandLineTool",
        "id": "#snakemake-job",
        "label": "Snakemake job executor",
        "hints": [{"dockerPull": get_container_image(), "class": "DockerRequirement"}],
        "baseCommand": "snakemake",
        "requirements": {"ResourceRequirement": {"coresMin": "$(inputs.cores)"}},
        "arguments": [
            "--force",
            "--target-files-omit-workdir-adjustment",
            "--keep-remote",
            "--force-use-threads",
            "--wrapper-prefix",
            dag.workflow.workflow_settings.wrapper_prefix,
            "--notemp",
            "--quiet",
            "--use-conda",
            "--no-hooks",
            "--nolock",
            "--mode",
            str(ExecMode.SUBPROCESS.item_to_choice()),
        ],
        "inputs": {
            "snakefile": {
                "type": "File",
                "default": {
                    "class": "File",
                    "location": os.path.relpath(dag.workflow.main_snakefile),
                },
                "inputBinding": {"prefix": "--snakefile"},
            },
            "sources": {
                "type": "File[]",
                "default": [
                    {"class": "File", "location": f} for f in dag.get_sources()
                ],
            },
            "cores": {
                "type": "int",
                "default": 1,
                "inputBinding": {"prefix": "--cores"},
            },
            "rules": {
                "type": "string[]?",
                "inputBinding": {"prefix": "--allowed-rules"},
            },
            "input_files": {"type": "File[]", "default": []},
            "target_files": {"type": "string[]?", "inputBinding": {"position": 0}},
        },
        "outputs": {
            "output_files": {
                "type": {"type": "array", "items": "File"},
                "outputBinding": {"glob": "$(inputs.target_files)"},
            }
        },
    }
    groups = dag.get_jobs_or_groups()
    outputs = []
    inputs = []

    dag_cwl = [job_to_cwl(job, dag, outputs, inputs) for job in groups]

    return {
        "cwlVersion": "v1.0",
        "$graph": [
            snakemake_cwl,
            {
                "class": "Workflow",
                "requirements": {
                    "InlineJavascriptRequirement": {},
                    "MultipleInputFeatureRequirement": {},
                },
                "steps": dag_cwl,
                "inputs": inputs,
                "outputs": outputs,
                "id": "#main",
            },
        ],
    }



================================================
FILE: src/snakemake/decorators.py
================================================
__author__ = "Christopher Tomkins-Tinch"
__copyright__ = "Copyright 2022, Christopher Tomkins-Tinch"
__email__ = "tomkinsc@broadinstitute.org"
__license__ = "MIT"

import inspect


def dec_all_methods(decorator, prefix="test_"):
    def dec_class(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return dec_class



================================================
FILE: src/snakemake/exceptions.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import os
import traceback
from tokenize import TokenError
from snakemake_interface_common.exceptions import WorkflowError, ApiError
from snakemake_interface_logger_plugins.common import LogEvent


def format_error(
    ex, lineno, linemaps=None, snakefile=None, show_traceback=False, rule=None
):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]

    if isinstance(ex, SyntaxError):
        msg = ex.msg.split("(")[0]
        msg = f"{msg}:\n{ex.text}"

    location = ""
    if lineno and snakefile:
        location = f' in file "{snakefile}", line {lineno}'
        if rule:
            location = f" in rule {rule}{location}"

    tb = ""
    if show_traceback:
        tb = "\n".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return "{}{}{}{}".format(
        ex.__class__.__name__,
        location,
        ":\n" + msg if msg else ".",
        f"\n{tb}" if show_traceback and tb else "",
    )


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    lines = []
    snakemake_path = os.path.dirname(__file__)
    not_seen_snakemake = True
    for line in traceback.extract_tb(ex.__traceback__)[::-1]:
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "."
        is_snakemake_dir = lambda path: os.path.realpath(path).startswith(
            os.path.realpath(snakemake_path)
        )
        if is_snakemake_dir(dir):
            not_seen_snakemake = False
        if not os.path.isdir(dir) or not_seen_snakemake:
            lines.append(line)
    return lines[::-1]


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield f'  File "{file}", line {lineno}, in {function}'


def log_verbose_traceback(ex):
    from snakemake.logging import logger

    tb = "Full " + "".join(traceback.format_exception(type(ex), ex, ex.__traceback__))
    logger.debug(tb)


def format_exception_to_string(ex, linemaps=None):
    """
    Returns the error message for a given exception as a string.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        return format_error(
            ex,
            ex.lineno,
            linemaps=linemaps,
            snakefile=ex.filename,
            show_traceback=True,
        )

    origin = get_exception_origin(ex, linemaps) if linemaps is not None else None
    if origin is not None:
        lineno, file = origin
        return format_error(
            ex, lineno, linemaps=linemaps, snakefile=file, show_traceback=True
        )
    elif isinstance(ex, TokenError):
        return format_error(ex, None, show_traceback=False)
    elif isinstance(ex, MissingRuleException):
        return format_error(
            ex, None, linemaps=linemaps, snakefile=ex.filename, show_traceback=False
        )
    elif isinstance(ex, RuleException):
        error_string = ""
        for e in ex._include:
            if not e.omit:
                error_string += (
                    format_error(
                        e,
                        e.lineno,
                        linemaps=linemaps,
                        snakefile=e.filename,
                        show_traceback=True,
                    )
                    + "\n"
                )
        error_string += format_error(
            ex,
            ex.lineno,
            linemaps=linemaps,
            snakefile=ex.filename,
            show_traceback=True,
            rule=ex.rule,
        )
        return error_string
    elif isinstance(ex, WorkflowError):
        return format_error(
            ex,
            ex.lineno,
            linemaps=linemaps,
            snakefile=ex.snakefile,
            show_traceback=True,
            rule=ex.rule,
        )
    elif isinstance(ex, ApiError):
        return f"Error: {ex}"
    elif isinstance(ex, CliException):
        return f"Error: {ex}"
    elif isinstance(ex, KeyboardInterrupt):
        return "Cancelling snakemake on user request."
    else:
        return "\n".join(traceback.format_exception(ex))


def print_exception_warning(ex, linemaps=None, footer_message=""):
    """
    Print an error message for a given exception using logger warning.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """
    from snakemake.logging import logger

    log_verbose_traceback(ex)
    logger.warning(f"{format_exception_to_string(ex, linemaps)}\n{footer_message}")


def print_exception(ex, linemaps=None):
    """
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """
    from snakemake.logging import logger

    log_verbose_traceback(ex)
    logger.error(
        format_exception_to_string(ex, linemaps),
        extra=dict(event=LogEvent.ERROR, exception=ex.__class__.__name__),
    )


def update_lineno(ex: SyntaxError, linemaps):
    if ex.filename and ex.lineno:
        linemap = linemaps[ex.filename]
        try:
            ex.lineno = linemap[ex.lineno]
        except KeyError:
            # linemap does not yet contain the line, it must happen during parsing
            # such that no update is needed.
            pass
        return ex


class SourceFileError(WorkflowError):
    def __init__(self, msg):
        super().__init__(f"Error in source file definition: {msg}")


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """
    Base class for exception occurring within the
    execution or definition of rules.
    """

    def __init__(
        self, message=None, include=None, lineno=None, snakefile=None, rule=None
    ):
        """
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """
        super(RuleException, self).__init__(message)
        _include = set()
        if include:
            for ex in include:
                _include.add(ex)
                _include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(_include)
        self.rule = rule
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    def __init__(self, msg, wildcards=None, lineno=None, snakefile=None, rule=None):
        fmt_msg = (
            "Error:\n  "
            + self.format_arg(msg)
            + "\nWildcards:\n"
            + "\n".join(f"  {name}={value}" for name, value in wildcards.items())
        )
        if isinstance(msg, Exception):
            fmt_msg += "\nTraceback:\n" + "\n".join(
                format_traceback(cut_traceback(msg), rule.workflow.linemaps)
            )
        super().__init__(fmt_msg, lineno=lineno, snakefile=snakefile, rule=rule)


class ChildIOException(WorkflowError):
    def __init__(
        self,
        parent=None,
        child=None,
        wildcards=None,
        lineno=None,
        snakefile=None,
        rule=None,
    ):
        msg = "File/directory is a child to another output:\n" + "{}\n{}".format(
            parent, child
        )
        super().__init__(msg, lineno=lineno, snakefile=snakefile, rule=rule)


class IOException(RuleException):
    def __init__(self, prefix, job, files, include=None, lineno=None, snakefile=None):
        from snakemake.logging import format_wildcards
        from snakemake.io.fmt import fmt_iofile

        msg = ""
        if files:
            msg = f"{prefix} for rule {job.rule}:"
            if job.output:
                msg += "\n" + f"    output: {', '.join(map(fmt_iofile, job.output))}"
            if job.wildcards:
                msg += "\n" + f"    wildcards: {format_wildcards(job.wildcards)}"
            msg += "\n    affected files:\n        " + "\n        ".join(
                map(fmt_iofile, files)
            )
        super().__init__(
            message=msg,
            include=include,
            lineno=lineno,
            snakefile=snakefile,
            rule=job.rule,
        )


class MissingOutputException(RuleException):
    def __init__(
        self,
        message=None,
        include=None,
        lineno=None,
        snakefile=None,
        rule=None,
        jobid="",
    ):
        if jobid:
            jobid = f"{jobid} "
        message = f"Job {jobid} completed successfully, but some output files are missing. {message}"
        super().__init__(message, include, lineno, snakefile, rule)


class MissingInputException(IOException):
    def __init__(self, job, files, include=None, lineno=None, snakefile=None):
        msg = "Missing input files"

        if any(map(lambda f: f.startswith("~"), files)):
            msg += (
                "(Using '~' in your paths is not allowed as such platform "
                "specific syntax is not resolved by Snakemake. In general, "
                "try sticking to relative paths for everything inside the "
                "working directory.)"
            )
        super().__init__(msg, job, files, include, lineno=lineno, snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, job, files, include=None, lineno=None, snakefile=None):
        super().__init__(
            "Write-protected output files",
            job,
            files,
            include,
            lineno=lineno,
            snakefile=snakefile,
        )


class ImproperOutputException(IOException):
    def __init__(self, job, files, include=None, lineno=None, snakefile=None):
        super().__init__(
            "Outputs of incorrect type (directories when expecting files or vice versa). "
            "Output directories must be flagged with directory().",
            job,
            files,
            include,
            lineno=lineno,
            snakefile=snakefile,
        )


class ImproperShadowException(RuleException):
    def __init__(self, rule, lineno=None, snakefile=None):
        super().__init__(
            "Rule cannot shadow if using ThreadPoolExecutor",
            rule=rule,
            lineno=lineno,
            snakefile=snakefile,
        )


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        from snakemake import utils
        from snakemake.io.fmt import fmt_iofile

        filename = fmt_iofile(filename)

        wildcards_a = utils.format("{}", job_a._format_wildcards)
        wildcards_b = utils.format("{}", job_b._format_wildcards)
        super().__init__(
            "Rules {job_a} and {job_b} are ambiguous for the file {f}.\n"
            "Consider starting rule output with a unique prefix, constrain "
            "your wildcards, or use the ruleorder directive.\n"
            "Wildcards:\n"
            "\t{job_a}: {wildcards_a}\n"
            "\t{job_b}: {wildcards_b}\n"
            "Expected input files:\n"
            "\t{job_a}: {job_a.input}\n"
            "\t{job_b}: {job_b.input}\n"
            "Expected output files:\n"
            "\t{job_a}: {job_a.output}\n"
            "\t{job_b}: {job_b.output}".format(
                job_a=job_a,
                job_b=job_b,
                f=filename,
                wildcards_a=wildcards_a,
                wildcards_b=wildcards_b,
            ),
            lineno=lineno,
            snakefile=snakefile,
        )
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(f"Cyclic dependency on rule {repeatedrule}.", rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            f"No rule to produce {file} (if you use input "
            "functions make sure that they don't raise unexpected exceptions).",
            lineno=lineno,
            snakefile=snakefile,
        )


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="", lineno=None, snakefile=None):
        msg = f"There is no rule named {name}."
        if prefix:
            msg = f"{prefix} {msg}"
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(
            "There has to be at least one rule.", lineno=lineno, snakefile=snakefile
        )


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        from snakemake.io.fmt import fmt_iofile

        super().__init__(
            "The files below seem to be incomplete. "
            "If you are sure that certain files are not incomplete, "
            "mark them as complete with\n\n"
            "    snakemake --cleanup-metadata <filenames>\n\n"
            "To re-generate the files rerun your command with the "
            "--rerun-incomplete flag.\nIncomplete files:\n{}".format(
                "\n".join(map(fmt_iofile, files))
            )
        )


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class HTTPFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class FTPFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class AzureFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class SFTPFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class DropboxFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class XRootDFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NCBIFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class WebDAVFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class ZenodoFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class ClusterJobException(RuleException):
    def __init__(self, job_info, jobid):
        super().__init__(
            "Error executing rule {} on cluster (jobid: {}, external: {}, jobscript: {}). "
            "For detailed error see the cluster log.".format(
                job_info.job.rule.name, jobid, job_info.jobid, job_info.jobscript
            ),
            lineno=job_info.job.rule.lineno,
            snakefile=job_info.job.rule.snakefile,
        )


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass


class CreateCondaEnvironmentException(WorkflowError):
    pass


class SpawnedJobError(Exception):
    pass


class CheckSumMismatchException(WorkflowError):
    """ "should be called to indicate that checksum of a file compared to known
    hash does not match, typically done with large downloads, etc.
    """

    pass


class IncompleteCheckpointException(Exception):
    def __init__(self, rule, targetfile):
        super().__init__(
            "The requested checkpoint output is not yet created. "
            "If you see this error, you have likely tried to use "
            "checkpoint output outside of an input function, or "
            "you have tried to call an input function directly "
            "via <function_name>(). Please check the docs at "
            "https://snakemake.readthedocs.io/en/stable/"
            "snakefiles/rules.html#data-dependent-conditional-execution "
            "and note that the input function in the example rule "
            "'aggregate' is NOT called, but passed to the rule "
            "by name, such that Snakemake can call it internally "
            "once the checkpoint is finished."
        )
        self.rule = rule
        from snakemake.io import checkpoint_target

        self.targetfile = checkpoint_target(targetfile)


class InputOpenException(Exception):
    def __init__(self, iofile):
        self.iofile = iofile
        self.rule = None


class CacheMissException(Exception):
    pass


class LockException(WorkflowError):
    def __init__(self):
        super().__init__(
            "Error: Directory cannot be locked. Please make "
            "sure that no other Snakemake process is trying to create "
            "the same files in the following directory:\n{}\n"
            "If you are sure that no other "
            "instances of snakemake are running on this directory, "
            "the remaining lock was likely caused by a kill signal or "
            "a power loss. It can be removed with "
            "the --unlock argument.".format(os.getcwd())
        )


class ResourceScopesException(Exception):
    def __init__(self, msg, invalid_resources):
        super().__init__(msg, invalid_resources)
        self.msg = msg
        self.invalid_resources = invalid_resources


class CliException(Exception):
    def __init__(self, msg):
        super().__init__(msg)
        self.msg = msg


class LookupError(WorkflowError):
    def __init__(self, msg=None, exc=None, query=None, dpath=None):
        msg = f" {msg}" if msg is not None else ""
        expr = ""
        if query is not None:
            expr = f" with query: {repr(query)}"
        if dpath is not None:
            expr = f" with dpath: {repr(dpath)}"
        annotated_msg = f"Error in lookup function{expr}.{msg}"
        args = [annotated_msg]
        if exc is not None:
            args.append(exc)
        super().__init__(*args)


class MissingOutputFileCachePathException(Exception):
    pass


def is_file_not_found_error(exc, considered_files):
    # TODO find a better way to detect whether the input files are not present
    if isinstance(exc, FileNotFoundError) and exc.filename in considered_files:
        return True
    elif isinstance(exc, WorkflowError) and "FileNotFoundError" in str(exc):
        return True
    else:
        return False



================================================
FILE: src/snakemake/gui.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <title>Snakemake</title>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.4.6/d3.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.1.1/js/bootstrap.min.js"></script>
        <script type="text/javascript" src="https://cpettitt.github.io/project/dagre-d3/v0.1.5/dagre-d3.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-select/1.5.4/bootstrap-select.min.js"></script>
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.1.1/css/bootstrap.min.css"/>
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-select/1.5.4/bootstrap-select.min.css"/>
        <style>
        body {
          font-family: sans-serif;
          font-size: 10pt;
          color: #333333;
          background-color: #EEEEEE;
        }
        
        #header {
            position: fixed;
            top: 0px;
            left: 0px;
            width: 100%;
            background-color: #3498db;
            color: #FFFFFF;
            font-size: 150%;
            height: 70px;
        }
        
        #header #snakemake {
            float: left;
            padding: 5px;
            padding-right: 15px;
        }
        
        #header #workflow {
            float: left;
            padding: 5px;
        }
        
        #top {
            padding-top: 70px;
        }
        
        #bottom {
            width: 100%;
            border-width: 0px;
        }
        
        #bottom td {
            vertical-align: top;
        }
        
        #workflow-progress {
            width: 100%;
        }
        
        #workflow-progress #progress-text {
            width: 0%;
            transition: width 2s;
            min-width: 20em;
            text-align: right;
            color: #CCCCCC;
            height: 2em;
        }
        
        #workflow-progress #progress-bar {
            width: 0%;
            transition: width 2s;
            height: 0.5em;
            background-color: #CCCCCC;
        }
        
        #left-panel {
            width: 50%;
            background-color: #DDDDDD;
        }
        
        #left-panel #control {
            padding: 5px;
        }
        
        #log {
            width: 50%;
            background-color: #FFFFFF;
        }
        
        #log table {
            font-size: 10pt;
            margin: 0;
        }
        
        #log div {
             overflow: auto;
             padding: 5px;
        }
        
        #log .info {
            background-color: #f1c40f;
        }
        
        #log .info:nth-child(even) {
            background-color: #FFDD53;
        }
        
        #log .error {
            background-color: #e74c3c;
        }
        
        #log .error:nth-child(even) {
            background-color: #c0392b;
        }
        
        #log .job_info {
            background-color: #2ecc71;
        }
        
        #log .job_info:nth-child(even) {
            background-color: #27ae60;
        }

        #log .job_error {
            background-color: #e74c3c;
        }

        #log .job_error:nth-child(even) {
            background-color: #c0392b;
        }
        
        th.logentryitem {
            width: 10em;
            vertical-align: top;
        }

        #dag-container {
            width: 100%;
            max-height: 400px;
            overflow: auto;
            text-align: center;
            padding: 10px;
        }
        
        #dag .node rect {
            stroke-width: 3px;
            fill: #EEEEEE;
        }

        #dag .edgeLabel rect {
            fill: #fff;
        }

        #dag .edgePath {
            stroke: #333333;
            stroke-width: 3px;
            fill: none;
        }
        </style>
        <script>
        var dag = null;

        function render_dag() {
            if(!dag) {
                return;
            }
            
            var svg = d3.select("#dag");
            svg.select("g").remove();
            var view = svg.append("g");
            view.attr("transform", "translate(5,5)");
            
            var color = d3.scale.category20();
            
            var layout = dagreD3.layout().rankDir("LR");
            var g = dagreD3.json.decode(dag.nodes, dag.edges);
            var renderer = new dagreD3.Renderer().layout(layout);
            var oldDrawNodes = renderer.drawNodes();
            renderer.drawNodes(function(graph, svg) {
                var svgNodes = oldDrawNodes(graph, svg);
                svgNodes.select("rect")
                    .attr("id", function(u) { return "job_" + g.node(u).jobid; })
                    .attr("style", function(u) {
                        col = color(g.node(u).rule);
                        return "stroke: " + col + ";";
                    });
                return svgNodes;
            });
            
            var result = renderer.run(g, view);
            svg.attr("width", result.graph().width + 40)
               .attr("height", result.graph().height + 40);
        }

        function update_dag() {
            d3.json("dag", function(dag_json) {
                dag = dag_json;
                render_dag();
            });
        }

        var logid = 0;
        function update() {
            d3.json("status", function(status) {
                if(status["running"]) {
                    $("#control fieldset").attr("disabled", true);
                }
                else {
                    $("#control fieldset").attr("disabled", false);
                }
            });
        
            d3.json("progress", function(progress) {
                var done = progress.done;
                if(done) {
                    var total = progress.total;
                    var percent = done / total * 100;
                    $("#workflow-progress #progress-bar")
                        .css("width", percent + "%");
                    $("#workflow-progress #progress-text")
                        .css("width", percent + "%")
                        .text(done + " of " + total + " jobs finished (" + percent + "%)");
                }
            });
            
            $.getJSON("log/" + logid, function(entries) {
                $.each(entries, function(i, entry) {
                    if(entry.level == "job_info") {
                        var html = '<div class="job_info"><h4>'
                        + entry.name + '</h4><table>';
                        $.each(["input", "output"], function(i, item) {
                            if(entry[item].length) {
                                html += '<tr><th class="logentryitem">' + item + '</th><td>' + entry[item].join(", ") + '</td></tr>';
                            }
                        });
                        $.each(["threads", "priority"], function(item) {
                            if(entry[item] > 1) {
                                html += '<tr><th class="logentryitem">' + item + '</th><td>' + entry[item] + '</td></tr>';
                            }
                        });
                        html += '</table></div>';
                        $(html).appendTo("#log");
                    }
                    else if (entry.level == "job_error") {
                        console.log(entry);
                        var html = `<div class="job_error"><h4>Error in rule ${entry.name}</h4><table>`;
                        $.each(["output", "log"], function(i, item) {
                            if(entry[item].length) {
                                html += `<tr><th class="logentryitem">${item}</th><td>${entry[item].join(",")}</td></tr>`;
                            }
                        });
                        $.each(["conda_env", "shellcmd"], function(i, item) {
                            if(entry[item] != null) {
                                html += `<tr><th class="logentryitem">${item}</th><td>${entry[item]}</td></tr>`;
                            }
                        });
                        html += '</table></div>';
                        $(html).appendTo("#log");
                    }
                    else if (entry.level == "job_finished") {
                        // do nothing for now
                    }
                    else {
                        var type = "info"
                        switch(entry.level) {
                            case "info":
                                type="warning";
                                break;
                            case "error":
                                type="danger";
                                break;
                        }
                        $("#log").append(
                            '<div class="' + entry.level + '">'
                            + entry.msg +
                            '</div>'
                        );
                    }
                    logid++;
                });
            });
        }
        
        function set_args() {
            $.post("set_args", {"targets": $("#targets").val()});
        }
        
        $( document ).ready(function() {
            update_dag();
            setInterval(update, 2000);

            $('.selectpicker').selectpicker();

            $("#run-btn").click(function() {
                $("#log").empty();
                var cores = $("#cores").val();
                $.ajax(`run/${cores}`);
            });

            $("#dryrun-btn").click(function() {
                $("#log").empty();
                $.ajax("dryrun");
            });

            $("#targets").change(function() {
                set_args();
                update_dag();
            });
        });
        </script>
    </head>
    <body>
        <div id="header">
          <div id="snakemake">SNAKEMAKE {{version}}</div>
          <div id="workflow">
              WORKFLOW<br/>
             <span style="font-size: 50%; font-weight: normal;">{{snakefilepath}}</span>
          </div>
        </div>
        <div id="top">
            <div id="dag-container">
                <svg id="dag">
                </svg>
            </div>
            <div id="workflow-progress">
                <div id="progress-bar">
                </div>
                <div id="progress-text">
                </div>
            </div>
        </div>
        <table id="bottom">
            <tr>
                <td id="left-panel">
                    <form class="form-horizontal" id="control">
                        <fieldset>
                            <div class="form-group">
                                <div class="col-sm-offset-2 col-sm-10">
                                    <div class="btn-group">
                                        <button type="button" id="dryrun-btn" class="btn btn-default">Dry-Run</button>
                                        <button type="button" id="run-btn" class="btn btn-default">Run</button>
                                    </div>
                                </div>
                            </div>
                            <div class="form-group">
                                <label for="targets" class="col-sm-2 control-label">Targets</label>
                                <div class="col-sm-10">
                                    <select id="targets" class="form-control selectpicker" autocomplete="off" multiple>
                                        <option  selected="selected" value="{{targets[0]}}">{{targets[0]}}</option>
                                        {% for target in targets[1:] %}
                                        <option value="{{target}}">{{target}}</option>
                                        {% endfor %}
                                    </select>
                                </div>
                            </div>
                            <div class="form-group">
                                <div class="col-sm-offset-2 col-sm-10">
                                    <h4>Resources</h4>
                                </div>
                            </div>
                            <div class="form-group">
                                <label for="cores" class="col-sm-2 control-label">{{cores_label}}</label>
                                <div class="col-sm-10">
                                    <input type="text" class="col-sm-10 form-control" id="cores" value="1">
                                </div>
                            </div>
                            {% for resource in resources %}
                                <div class="form-group">
                                    <label for="{{resource}}" class="col-sm-2 control-label">{{resource}}</label>
                                    <div class="col-sm-10">
                                        <div class="input-group">
                                            <span class="input-group-addon"><input type="checkbox" id="{{resource}}"></span>
                                            <input type="text" class="form-control" id="{{resource}}_value">
                                        </div>
                                    </div>
                                </div>
                            {% endfor %}
                        </fieldset>
                    </form>
                </td>
                <td id="log">
                </td>
            </tr>
        </table>
    </body>
</html>



================================================
FILE: src/snakemake/gui.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import json
import os
import threading

from flask import Flask, render_template, request

from snakemake.common import __version__

LOCK = threading.Lock()

app = Flask("snakemake", template_folder=os.path.dirname(__file__))
# app.debug=True
app.extensions = {
    "dag": None,
    "run_snakemake": None,
    "progress": "",
    "log": [],
    "status": {"running": False},
    "args": None,
    "targets": [],
    "rule_info": [],
    "resources": [],
}


def register(run_snakemake, args):
    app.extensions["run_snakemake"] = run_snakemake
    app.extensions["args"] = dict(
        targets=args.target,
        cluster=args.cluster,
        workdir=args.directory,
        touch=args.touch,
        forcetargets=args.force,
        forceall=args.forceall,
        forcerun=args.forcerun,
        prioritytargets=args.prioritize,
        stats=args.stats,
        keepgoing=args.keep_going,
        jobname=args.jobname,
        immediate_submit=args.immediate_submit,
        ignore_ambiguity=args.allow_ambiguity,
        lock=not args.nolock,
        force_incomplete=args.rerun_incomplete,
        ignore_incomplete=args.ignore_incomplete,
        jobscript=args.jobscript,
        notemp=args.notemp,
        latency_wait=args.latency_wait,
    )

    target_rules = []

    def log_handler(msg):
        if msg["level"] == "rule_info":
            target_rules.append(msg["name"])

    run_snakemake(list_target_rules=True, log_handler=[log_handler])
    for target in args.target:
        target_rules.remove(target)
    app.extensions["targets"] = args.target + target_rules

    resources = []

    def log_handler(msg):
        if msg["level"] == "info":
            resources.append(msg["msg"])

    run_snakemake(list_resources=True, log_handler=[log_handler])
    app.extensions["resources"] = resources
    app.extensions["snakefilepath"] = os.path.abspath(args.snakefile)


def run_snakemake(**kwargs):
    args = dict(app.extensions["args"])
    args.update(kwargs)
    app.extensions["run_snakemake"](**args)


@app.route("/")
def index():
    args = app.extensions["args"]
    return render_template(
        "gui.html",
        targets=app.extensions["targets"],
        cores_label="Nodes" if args["cluster"] else "Cores",
        resources=app.extensions["resources"],
        snakefilepath=app.extensions["snakefilepath"],
        version=__version__,
        node_width=15,
        node_padding=10,
    )


@app.route("/dag")
def dag():
    if app.extensions["dag"] is None:

        def record(msg):
            if msg["level"] == "d3dag":
                app.extensions["dag"] = msg
            elif msg["level"] in ("error", "info"):
                app.extensions["log"].append(msg)

        run_snakemake(printd3dag=True, log_handler=[record])
    return json.dumps(app.extensions["dag"])


@app.route("/log/<int:id>")
def log(id):
    log = app.extensions["log"][id:]
    return json.dumps(log)


@app.route("/progress")
def progress():
    return json.dumps(app.extensions["progress"])


def _run(dryrun=False, cores=1):
    def log_handler(msg):
        level = msg["level"]
        if level == "progress":
            app.extensions["progress"] = msg
        elif level in ("info", "error", "job_info", "job_finished", "job_error"):
            app.extensions["log"].append(msg)

    with LOCK:
        app.extensions["status"]["running"] = True
    run_snakemake(log_handler=[log_handler], dryrun=dryrun, cores=cores)
    with LOCK:
        app.extensions["status"]["running"] = False
    return ""


@app.route("/run/<int:cores>")
def run(cores):
    return _run(cores=cores)


@app.route("/dryrun")
def dryrun():
    return _run(dryrun=True)


@app.route("/status")
def status():
    with LOCK:
        return json.dumps(app.extensions["status"])


@app.route("/targets")
def targets():
    return json.dumps(app.extensions["targets"])


@app.route("/get_args")
def get_args():
    return json.dumps(app.extensions["args"])


@app.route("/set_args", methods=["POST"])
def set_args():
    app.extensions["args"].update(
        {name: value for name, value in request.form.items() if not name.endswith("[]")}
    )
    targets = request.form.getlist("targets[]")
    if targets != app.extensions["args"]["targets"]:
        app.extensions["dag"] = None
    app.extensions["args"]["targets"] = targets
    return ""



================================================
FILE: src/snakemake/ioflags.py
================================================
from snakemake.io import flag, is_flagged


def update(value):
    """
    A flag for an output file that shall be updated instead of overwritten.
    """
    return flag(value, "update")


def before_update(value):
    """
    Flag an input file to be used as is in storage/on-disk before being updated
    in a later rule.
    This flag leads to the input file being considered as not being created by any other
    job.
    """
    return flag(value, "before_update")


def register_in_globals(_globals):
    _globals.update(
        {
            "update": update,
            "before_update": before_update,
        }
    )



================================================
FILE: src/snakemake/logging.py
================================================
from __future__ import annotations

__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"


import logging
import logging.handlers
import platform
import time
import datetime
import sys
import os
import json
import threading
from queue import Queue
from functools import partial
from typing import TYPE_CHECKING
import textwrap
from typing import List, Optional
from snakemake_interface_logger_plugins.base import LogHandlerBase
from snakemake_interface_logger_plugins.settings import OutputSettingsLoggerInterface
from snakemake_interface_logger_plugins.common import LogEvent

if TYPE_CHECKING:
    from snakemake_interface_executor_plugins.settings import ExecMode
    from snakemake.settings.enums import Quietness


def timestamp():
    """Helper method to format the timestamp."""
    return f"[{time.asctime()}]"


def show_logs(logs):
    """Helper method to show logs."""
    for f in logs:
        try:
            with open(f, "r") as log_file:
                content = log_file.read()
        except FileNotFoundError:
            yield f"Logfile {f} not found."
            return
        except UnicodeDecodeError:
            yield f"Logfile {f} is not a text file."
            return
        lines = content.splitlines()
        logfile_header = f"Logfile {f}:"
        if not lines:
            logfile_header += " empty file"
            yield logfile_header
            return
        yield logfile_header
        max_len = min(max(max(len(line) for line in lines), len(logfile_header)), 80)
        yield "=" * max_len
        yield from lines
        yield "=" * max_len


def format_dict(dict_like, omit_keys=None, omit_values=None):
    from snakemake.io import Namedlist

    omit_keys = omit_keys or []
    omit_values = omit_values or []

    if isinstance(dict_like, (Namedlist, dict)):
        items = dict_like.items()

    else:
        raise ValueError(
            "bug: format_dict applied to something neither a dict nor a Namedlist"
        )
    return ", ".join(
        f"{name}={value}"
        for name, value in items
        if name not in omit_keys and value not in omit_values
    )


format_resources = partial(format_dict, omit_keys={"_cores", "_nodes"})
format_wildcards = format_dict


def format_resource_names(resources, omit_resources="_cores _nodes".split()):
    return ", ".join(name for name in resources if name not in omit_resources)


def format_percentage(done, total):
    """Format percentage from given fraction while avoiding superfluous precision."""
    if done == total:
        return "100%"
    if done == 0:
        return "0%"
    precision = 0
    fraction = done / total
    fmt_precision = "{{:.{}%}}".format

    def fmt(fraction):
        return fmt_precision(precision).format(fraction)

    while fmt(fraction) == "100%" or fmt(fraction) == "0%":
        precision += 1
    return fmt(fraction)


def get_event_level(record: logging.LogRecord) -> tuple[LogEvent, str]:
    """
    Gets snakemake log level from a log record. If there is no snakemake log level,
    returns the log record's level name.

    Args:
        record (logging.LogRecord)
    Returns:
        tuple[LogEvent, str]

    """
    event = record.__dict__.get("event", None)

    return (event, record.levelname)


def is_quiet_about(quiet: "Quietness", msg_type: str):
    from snakemake.settings.enums import Quietness

    parsed = Quietness.parse_choice(msg_type)

    return Quietness.ALL in quiet or parsed in quiet


class DefaultFormatter(logging.Formatter):
    def __init__(
        self,
        quiet: "Quietness",
        show_failed_logs: bool = False,
    ):
        self.quiet = set() if quiet is None else quiet
        self.show_failed_logs = show_failed_logs
        self.last_msg_was_job_info = False

    def format(self, record):
        """
        Override format method to format Snakemake-specific log messages.
        """
        event, level = get_event_level(record)
        record_dict = record.__dict__.copy()

        def default_formatter(rd):
            return rd["msg"]

        formatters = {
            None: default_formatter,
            LogEvent.JOB_INFO: self.format_job_info,
            LogEvent.JOB_ERROR: self.format_job_error,
            LogEvent.JOB_FINISHED: self.format_job_finished,
            LogEvent.GROUP_INFO: self.format_group_info,
            LogEvent.GROUP_ERROR: self.format_group_error,
            LogEvent.SHELLCMD: self.format_shellcmd,
            LogEvent.RUN_INFO: self.format_run_info,
            LogEvent.DEBUG_DAG: self.format_dag_debug,
            LogEvent.PROGRESS: self.format_progress,
        }

        formatter = formatters.get(event, default_formatter)
        return formatter(record_dict)

    def format_info(self, msg):
        """
        Format 'info' level messages.
        """
        output = []

        # Check if 'indent' is specified
        indent = "    " if msg.get("indent", False) else ""

        # Split the message by lines in case it's multiline
        lines = msg["msg"].split("\n")

        # Apply indentation to each line
        for line in lines:
            output.append(f"{indent}{line}")

        # Return the formatted message as a single string with newlines
        return "\n".join(output)

    def format_run_info(self, msg):
        """Format the run_info log messages."""
        return msg["msg"]  # Log the message directly

    def format_host(self, msg):
        """Format for host log."""
        return f"host: {platform.node()}"

    def format_job_info(self, msg):
        """Format for job_info log."""
        output = []

        output.append(timestamp())
        if msg["rule_msg"]:
            output.append(f"Job {msg['jobid']}: {msg['rule_msg']}")
            if not is_quiet_about(self.quiet, "reason"):
                output.append(f"Reason: {msg['reason']}")
        else:
            output.append("\n".join(self._format_job_info(msg)))

        if msg.get("indent", False):
            return textwrap.indent("\n".join(output), "    ")
        return "\n".join(output)

    def format_group_info(self, msg):
        """Format for group_info log."""
        msg = f"{timestamp()} {msg['msg']}"

        return msg

    def format_job_error(self, msg):
        """Format for job_error log."""
        output = []
        output.append(timestamp())
        output.append("\n".join(self._format_job_error(msg)))

        if msg.get("indent", False):
            return textwrap.indent("\n".join(output), "    ")
        return "\n".join(output)

    def format_group_error(self, msg):
        """Format for group_error log."""
        output = []
        output.append(timestamp())
        output.append("\n".join(self._format_group_error(msg)))
        return "\n".join(output)

    def format_progress(self, msg):
        """Format for progress log."""
        done = msg["done"]
        total = msg["total"]
        return f"{done} of {total} steps ({format_percentage(done, total)}) done"

    def format_job_finished(self, msg):
        """Format for job_finished log."""
        return f"{timestamp()}\n{msg['msg']}"

    def format_shellcmd(self, msg):
        """Format for shellcmd log."""
        return msg["msg"]

    def format_d3dag(self, msg):
        """Format for d3dag log."""

        return json.dumps({"nodes": msg["nodes"], "links": msg["edges"]})

    def format_dag_debug(self, msg):
        """Format for dag_debug log."""
        output = []

        if "file" in msg:
            output.append(
                f"file {msg['file']}:\n    {msg['msg']}\n{textwrap.indent(str(msg['exception']), '    ')}"
            )
        else:
            job = msg["job"]
            output.append(
                f"{msg['status']} job {job.rule.name}\n    wildcards: {format_wildcards(job.wildcards)}"
            )
        return "\n".join(output)

    def _format_job_info(self, msg):
        """Helper method to format job info details."""

        def format_item(item, omit=None, valueformat=str):
            value = msg[item]
            if value != omit:
                return f"    {item}: {valueformat(value)}"

        output = [
            f"{'local' if msg['local'] else ''}{'checkpoint' if msg['is_checkpoint'] else 'rule'} {msg['rule_name']}:"
        ]
        for item in ["input", "output", "log"]:
            fmt = format_item(item, omit=[], valueformat=", ".join)
            if fmt:
                output.append(fmt)

        singleitems = ["jobid", "benchmark"]
        if not is_quiet_about(self.quiet, "reason"):
            singleitems.append("reason")
        for item in singleitems:
            fmt = format_item(item, omit=None)
            if fmt:
                output.append(fmt)

        wildcards = format_wildcards(msg["wildcards"])
        if wildcards:
            output.append(f"    wildcards: {wildcards}")

        for item, omit in zip("priority threads".split(), [0, 1]):
            fmt = format_item(item, omit=omit)
            if fmt:
                output.append(fmt)

        resources = format_resources(msg["resources"])
        if resources:
            output.append(f"    resources: {resources}")

        return output

    def _format_job_error(self, msg):
        """Helper method to format job error details."""
        output = [f"Error in rule {msg['rule_name']}:"]

        if msg["msg"]:
            output.append(f"    message: {msg['rule_msg']}")
        output.append(f"    jobid: {msg['jobid']}")
        if msg["input"]:
            output.append(f"    input: {', '.join(msg['input'])}")
        if msg["output"]:
            output.append(f"    output: {', '.join(msg['output'])}")
        if msg["log"]:
            output.append(
                f"    log: {', '.join(msg['log'])} (check log file(s) for error details)"
            )
        if msg["conda_env"]:
            output.append(f"    conda-env: {msg['conda_env']}")
        if msg["shellcmd"]:
            output.append(
                f"    shell:\n        {msg['shellcmd']}\n        (command exited with non-zero exit code)"
            )

        for item in msg["aux"].items():
            output.append(f"    {item[0]}: {item[1]}")

        if self.show_failed_logs and msg["log"]:
            output.extend(show_logs(msg["log"]))

        return output

    def _format_group_error(self, msg):
        """Helper method to format group error details."""
        output = []

        if msg["msg"]:
            output.append(f"    message: {msg['msg']}")
        if msg["aux_logs"]:
            output.append(
                f"    log: {', '.join(msg['aux_logs'])} (check log file(s) for error details)"
            )

        output.append("    jobs:")
        for info in msg["job_error_info"]:
            output.append(f"        rule {info['name']}:")
            output.append(f"            jobid: {info['jobid']}")
            if info["output"]:
                output.append(f"            output: {', '.join(info['output'])}")
            if info["log"]:
                output.append(
                    f"            log: {', '.join(info['log'])} (check log file(s) for error details)"
                )

        logs = msg["aux_logs"] + [
            f for info in msg["job_error_info"] for f in info["log"]
        ]
        if self.show_failed_logs and logs:
            output.extend(show_logs(logs))

        return output


class DefaultFilter:
    def __init__(self, quiet, debug_dag, dryrun, printshellcmds) -> None:
        if quiet is None:
            quiet = set()
        self.quiet = quiet
        self.debug_dag = debug_dag
        self.dryrun = dryrun
        self.printshellcmds = printshellcmds

    def filter(self, record):
        from snakemake.settings.enums import Quietness

        event, level = get_event_level(record)
        if self.dryrun and level == "run_info":
            return True

        if Quietness.ALL in self.quiet and not self.dryrun:
            return False

        if hasattr(record, "quietness"):
            if record.quietness in self.quiet:
                return False

        quietness_map = {
            LogEvent.JOB_INFO: Quietness.RULES,
            LogEvent.GROUP_INFO: Quietness.RULES,
            LogEvent.JOB_ERROR: Quietness.RULES,
            LogEvent.GROUP_ERROR: Quietness.RULES,
            LogEvent.PROGRESS: Quietness.PROGRESS,
            LogEvent.SHELLCMD: Quietness.RULES,
            LogEvent.JOB_FINISHED: Quietness.PROGRESS,
            LogEvent.RESOURCES_INFO: Quietness.PROGRESS,
            LogEvent.RUN_INFO: Quietness.PROGRESS,
        }

        # Handle shell commands
        if event == LogEvent.SHELLCMD and not self.printshellcmds:
            return False

        # Check quietness for specific levels
        if event in quietness_map:
            if quietness_map[event] in self.quiet:
                return False

        # Handle dag_debug specifically
        if event == LogEvent.DEBUG_DAG and not self.debug_dag:
            return False

        return True


class ColorizingTextHandler(logging.StreamHandler):
    """
    Custom handler that combines colorization and Snakemake-specific formatting.
    """

    BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE = range(8)
    RESET_SEQ = "\033[0m"
    COLOR_SEQ = "\033[%dm"
    BOLD_SEQ = "\033[1m"

    colors = {
        "WARNING": YELLOW,
        "INFO": GREEN,
        "DEBUG": BLUE,
        "CRITICAL": MAGENTA,
        "ERROR": RED,
    }

    yellow_info_events = [
        LogEvent.RUN_INFO,
        LogEvent.SHELLCMD,
        LogEvent.JOB_STARTED,
        None,  # To mimic old coloring where log.info was mapped to log.warn
    ]

    def __init__(
        self,
        nocolor=False,
        stream=sys.stderr,
        mode=None,
        formatter: Optional[logging.Formatter] = None,
        filter: Optional[logging.Filter] = None,
    ):
        super().__init__(stream=stream)
        self.last_msg_was_job_info = False
        self._output_lock = threading.Lock()
        self.nocolor = nocolor or not self.can_color_tty(mode)
        self.mode = mode

        if formatter:
            self.setFormatter(formatter)
        if filter:
            self.addFilter(filter)

    def can_color_tty(self, mode):
        """
        Colors are supported when:
        1. Terminal is not "dumb"
        2. Running in subprocess mode
        3. Using a TTY on non-Windows systems
        """
        from snakemake_interface_executor_plugins.settings import ExecMode

        # Case 1: Check if terminal is "dumb"
        if os.environ.get("TERM") == "dumb":
            return False

        # Case 2: Always support colors in subprocess mode
        if mode == ExecMode.SUBPROCESS:
            return True

        # Case 3: Support colors on TTY except for Windows
        is_windows = platform.system() == "Windows"
        has_tty = self.is_tty

        if has_tty and not is_windows:
            return True

        return False

    @property
    def is_tty(self):
        isatty = getattr(self.stream, "isatty", None)
        return isatty and isatty()

    def emit(self, record):
        """
        Emit a log message with custom formatting and color.
        """

        with self._output_lock:
            try:
                event, level = get_event_level(record)

                if event == LogEvent.JOB_INFO:
                    if not self.last_msg_was_job_info:
                        self.stream.write(
                            "\n"
                        )  # Add a blank line before a new job_info message
                    self.last_msg_was_job_info = True
                else:
                    # Reset flag if the message is not a 'job_info'
                    self.last_msg_was_job_info = False
                formatted_message = self.format(record)
                if formatted_message == "None" or formatted_message == "":
                    return
                # Apply color to the formatted message
                self.stream.write(self.decorate(record, formatted_message))
                self.stream.write(getattr(self, "terminator", "\n"))
                self.flush()
            except BrokenPipeError:
                raise
            except (KeyboardInterrupt, SystemExit):
                pass  # Ignore exceptions for these cases, all errors have been handled before.
            except Exception:
                self.handleError(record)

    def decorate(self, record, message):
        """
        Add color to the log message based on its level.
        """
        message = [message]

        event, level = get_event_level(record)

        if not self.nocolor and record.levelname in self.colors:
            if level == "INFO" and event in self.yellow_info_events:
                color = self.colors["WARNING"]
            else:
                color = self.colors[record.levelname]

            message.insert(0, self.COLOR_SEQ % (30 + color))
            message.append(self.RESET_SEQ)

        return "".join(message)


class LoggerManager:
    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.initialized = False
        self.queue_listener = None
        self.mode = None
        self.needs_rulegraph = False
        self.logfile_handlers = {}
        self.settings: OutputSettingsLoggerInterface = None

    def setup(
        self,
        mode: "ExecMode",
        handlers: List[LogHandlerBase],
        settings: OutputSettingsLoggerInterface,
    ):
        from snakemake_interface_executor_plugins.settings import ExecMode

        self.mode = mode
        self.settings = settings
        self.initialized = True

        stream_handlers = []
        other_handlers = []

        if self.mode == ExecMode.SUBPROCESS:
            handler = self._default_streamhandler()
            handler.setLevel(logging.ERROR)
            stream_handlers.append(handler)
        elif self.mode == ExecMode.REMOTE:
            stream_handlers.append(self._default_streamhandler())
        elif handlers:
            for handler in handlers:
                if handler.needs_rulegraph:
                    self.needs_rulegraph = True
                configured_handler = self._configure_plugin_handler(handler)
                if configured_handler.writes_to_file:
                    self.logfile_handlers[configured_handler] = (
                        configured_handler.baseFilename
                    )
                elif configured_handler.writes_to_stream:
                    stream_handlers.append(configured_handler)
                else:
                    other_handlers.append(configured_handler)

        if len(stream_handlers) > 1:
            raise ValueError("More than 1 stream log handler specified!")
        elif len(stream_handlers) == 0:
            # we dont have any stream_handlers from plugin(s) so give us the default one
            stream_handlers.append(self._default_streamhandler())

        self.setup_logfile()

        all_handlers = (
            stream_handlers + other_handlers + list(self.logfile_handlers.keys())
        )

        q = Queue(-1)
        self.queue_listener = logging.handlers.QueueListener(
            q,
            *all_handlers,
            respect_handler_level=True,
        )
        self.queue_listener.start()
        self.logger.setLevel(logging.DEBUG if settings.verbose else logging.INFO)
        self.logger.addHandler(logging.handlers.QueueHandler(q))

    def _configure_plugin_handler(self, plugin):
        if not plugin.has_filter:
            plugin.addFilter(self._default_filter())
        if not plugin.has_formatter:
            plugin.setFormatter(self._default_formatter())
        return plugin

    def _default_filter(self):
        return DefaultFilter(
            self.settings.quiet,
            self.settings.debug_dag,
            self.settings.dryrun,
            self.settings.printshellcmds,
        )

    def _default_formatter(self):
        return DefaultFormatter(
            self.settings.quiet,
            self.settings.show_failed_logs,
        )

    def _default_filehandler(self, logfile):
        logfile_handler = logging.FileHandler(logfile)
        logfile_handler.setFormatter(self._default_formatter())
        logfile_handler.addFilter(self._default_filter())
        logfile_handler.setLevel(
            logging.DEBUG if self.settings.verbose else logging.INFO
        )
        logfile_handler.name = "DefaultLogFileHandler"
        return logfile_handler

    def _default_streamhandler(self):
        stream_handler = ColorizingTextHandler(
            nocolor=self.settings.nocolor,
            stream=sys.stdout if self.settings.stdout else sys.stderr,
            mode=self.mode,
        )
        stream_handler.addFilter(self._default_filter())
        stream_handler.setFormatter(self._default_formatter())
        stream_handler.name = "DefaultStreamHandler"
        return stream_handler

    def get_logfile(self) -> List[str]:
        return list(self.logfile_handlers.values())

    def logfile_hint(self):
        from snakemake_interface_executor_plugins.settings import ExecMode

        """Log the logfile location if applicable."""
        logfiles = self.logfile_handlers.values()
        if self.mode == ExecMode.DEFAULT and not self.settings.dryrun and logfiles:
            log_paths = ", ".join([os.path.abspath(p) for p in logfiles])
            self.logger.info(f"Complete log(s): {log_paths}")
        return logfiles

    def cleanup_logfile(self):
        from snakemake_interface_executor_plugins.settings import ExecMode

        if self.mode == ExecMode.DEFAULT:
            for handler in self.logfile_handlers.keys():
                self.logger.removeHandler(handler)
                handler.close()

    def setup_logfile(self):
        from snakemake_interface_executor_plugins.settings import ExecMode

        if self.mode == ExecMode.DEFAULT and not self.settings.dryrun:
            try:
                os.makedirs(os.path.join(".snakemake", "log"), exist_ok=True)
                logfile = os.path.abspath(
                    os.path.join(
                        ".snakemake",
                        "log",
                        datetime.datetime.now().isoformat().replace(":", "")
                        + ".snakemake.log",
                    )
                )
                handler = self._default_filehandler(logfile)
                self.logfile_handlers[handler] = logfile

            except OSError as e:
                self.logger.error(f"Failed to setup log file: {e}")

    def stop(self):
        if self.queue_listener is not None and self.queue_listener._thread is not None:
            self.queue_listener.stop()


# Global logger instance
logger = logging.getLogger(__name__)
logger_manager = LoggerManager(logger)



================================================
FILE: src/snakemake/modules.py
================================================
__authors__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from pathlib import Path
import types
import re
from typing import Callable, List
from snakemake.common import Rules

from snakemake.exceptions import WorkflowError
from snakemake.io.flags import DefaultFlags
from snakemake.path_modifier import PathModifier
from snakemake import wrapper


def get_name_modifier_func(rules=None, name_modifier=None, parent_modifier=None):
    if name_modifier is None:
        return None
    else:
        if parent_modifier is None:
            parent_modifier_func = lambda rulename: rulename
        else:
            parent_modifier_func = parent_modifier.modify_rulename
        if "*" in name_modifier:
            return lambda rulename: parent_modifier_func(
                name_modifier.replace("*", rulename)
            )
        elif name_modifier is not None:
            if len(rules) > 1:
                raise SyntaxError(
                    "Multiple rules in 'use rule' statement but name modification ('as' statement) does not contain a wildcard '*'."
                )
            return lambda rulename: parent_modifier_func(name_modifier)


class ModuleInfo:
    def __init__(
        self,
        workflow,
        name,
        snakefile=None,
        meta_wrapper=None,
        config=None,
        skip_validation=False,
        replace_prefix=None,
        prefix=None,
    ):
        self.workflow = workflow
        self.name = name
        self.snakefile = snakefile
        self.meta_wrapper = meta_wrapper
        self.config = config
        self.skip_validation = skip_validation
        self.parent_modifier = self.workflow.modifier
        self.rule_proxies = Rules()

        if prefix is not None:
            if isinstance(prefix, Path):
                prefix = str(prefix)
            if not isinstance(prefix, str):
                raise WorkflowError(
                    "Prefix definition in module statement must be string or Path."
                )
            if replace_prefix is not None:
                raise WorkflowError(
                    "Module definition contains both prefix and replace_prefix. "
                    "Only one at a time is allowed."
                )

        self.replace_prefix = replace_prefix
        self.prefix = prefix

    def use_rules(
        self,
        rules=None,
        name_modifier=None,
        exclude_rules=None,
        ruleinfo=None,
        skip_global_report_caption=False,
    ):
        snakefile = self.get_snakefile()
        modifier = WorkflowModifier(
            self.workflow,
            config=self.config,
            base_snakefile=snakefile,
            skip_configfile=self.config is not None,
            skip_validation=self.skip_validation,
            skip_global_report_caption=skip_global_report_caption,
            rule_exclude_list=exclude_rules,
            rule_whitelist=self.get_rule_whitelist(rules),
            resolved_rulename_modifier=get_name_modifier_func(
                rules, name_modifier, parent_modifier=self.parent_modifier
            ),
            local_rulename_modifier=get_name_modifier_func(rules, name_modifier),
            ruleinfo_overwrite=ruleinfo,
            allow_rule_overwrite=True,
            namespace=self.name,
            replace_prefix=self.replace_prefix,
            prefix=self.prefix,
            replace_wrapper_tag=self.get_wrapper_tag(),
            rule_proxies=self.rule_proxies,
        )
        with modifier:
            self.workflow.include(snakefile, overwrite_default_target=True)
            self.parent_modifier.inherit_rule_proxies(modifier)

    def get_snakefile(self):
        if self.meta_wrapper:
            return wrapper.get_path(
                self.meta_wrapper + "/test/Snakefile",
                self.workflow.workflow_settings.wrapper_prefix,
            )
        elif self.snakefile:
            return self.snakefile
        else:
            raise WorkflowError(
                "Module statement must either define snakefile or meta_wrapper to use."
            )

    def get_wrapper_tag(self):
        if self.meta_wrapper:
            if wrapper.is_url(self.meta_wrapper):
                raise WorkflowError(
                    "meta_wrapper directive of module statement currently does not support full URLs."
                )
            return self.meta_wrapper.split("/", 1)[0]
        return None

    def get_rule_whitelist(self, rules):
        if "*" in rules:
            if len(rules) != 1:
                raise SyntaxError(
                    "The 'use rule' statement uses a wildcard '*' but lists multiple additional rules."
                )
            else:
                return None
        return set(rules)


class WorkflowModifier:
    def __init__(
        self,
        workflow,
        parent_modifier=None,
        globals=None,
        config=None,
        base_snakefile=None,
        skip_configfile=False,
        skip_validation=False,
        skip_global_report_caption=False,
        resolved_rulename_modifier=None,
        local_rulename_modifier=None,
        rule_whitelist=None,
        rule_exclude_list=None,
        ruleinfo_overwrite=None,
        allow_rule_overwrite=False,
        replace_prefix=None,
        prefix=None,
        replace_wrapper_tag=None,
        namespace=None,
        rule_proxies=None,
    ):
        if parent_modifier is not None:
            # init with values from parent modifier
            self.base_snakefile = parent_modifier.base_snakefile
            self.globals = parent_modifier.globals
            self.skip_configfile = parent_modifier.skip_configfile
            self.resolved_rulename_modifier = parent_modifier.resolved_rulename_modifier
            self.local_rulename_modifier = parent_modifier.local_rulename_modifier
            self.skip_validation = parent_modifier.skip_validation
            self.skip_global_report_caption = parent_modifier.skip_global_report_caption
            self.rule_whitelist = parent_modifier.rule_whitelist
            self.rule_exclude_list = parent_modifier.rule_exclude_list
            self.ruleinfo_overwrite = parent_modifier.ruleinfo_overwrite
            self.allow_rule_overwrite = parent_modifier.allow_rule_overwrite
            self.path_modifier = parent_modifier.path_modifier
            self.replace_wrapper_tag = parent_modifier.replace_wrapper_tag
            self.namespace = parent_modifier.namespace
            self.wildcard_constraints = parent_modifier.wildcard_constraints
            self.rules = parent_modifier.rules
            self.rule_proxies = parent_modifier.rule_proxies
        else:
            # default settings for globals if not inheriting from parent
            self.globals = (
                globals if globals is not None else dict(workflow.vanilla_globals)
            )
            self.wildcard_constraints = dict()
            self.rules = set()
            self.rule_proxies = rule_proxies or Rules()
            self.globals["rules"] = self.rule_proxies
            self.globals["checkpoints"] = self.globals[
                "checkpoints"
            ].spawn_new_namespace()

        self.workflow = workflow
        self.base_snakefile = base_snakefile

        if config is not None:
            self.globals["config"] = config

        self.skip_configfile = skip_configfile
        self.resolved_rulename_modifier = resolved_rulename_modifier
        self.local_rulename_modifier = local_rulename_modifier
        self.skip_validation = skip_validation
        self.skip_global_report_caption = skip_global_report_caption
        self.rule_whitelist = rule_whitelist
        self.rule_exclude_list = rule_exclude_list
        self.ruleinfo_overwrite = ruleinfo_overwrite
        self.allow_rule_overwrite = allow_rule_overwrite
        self.path_modifier = PathModifier(replace_prefix, prefix, workflow)
        self.replace_wrapper_tag = replace_wrapper_tag
        self.namespace = namespace
        self.default_input_flags: DefaultFlags = DefaultFlags()
        self.default_output_flags: DefaultFlags = DefaultFlags()

    def inherit_rule_proxies(self, child_modifier):
        for name, rule in child_modifier.rule_proxies._rules.items():
            if child_modifier.local_rulename_modifier is not None:
                name = child_modifier.local_rulename_modifier(name)
            self.rule_proxies._register_rule(name, rule)

    def skip_rule(self, rulename):
        return (
            self.rule_whitelist is not None and rulename not in self.rule_whitelist
        ) or (self.rule_exclude_list is not None and rulename in self.rule_exclude_list)

    def modify_rulename(self, rulename):
        if self.resolved_rulename_modifier is not None:
            return self.resolved_rulename_modifier(rulename)
        return rulename

    def modify_path(self, path, property=None):
        return self.path_modifier.modify(path, property)

    def modify_wrapper_uri(self, wrapper_uri, pattern=re.compile("^master/")):
        if self.replace_wrapper_tag is None or wrapper.is_url(wrapper_uri):
            return wrapper_uri
        else:
            return pattern.sub(self.replace_wrapper_tag + "/", wrapper_uri)

    def __enter__(self):
        # put this modifier on the stack, it becomes the currently valid modifier
        self.workflow.modifier_stack.append(self)

    def __exit__(self, type, value, traceback):
        # remove this modifier from the stack
        self.workflow.modifier_stack.pop()
        if self.namespace:
            namespace = types.ModuleType(self.namespace)
            namespace.__dict__.update(self.globals)
            self.workflow.globals[self.namespace] = namespace



================================================
FILE: src/snakemake/notebook.py
================================================
from abc import abstractmethod
import os
from pathlib import Path
import subprocess as sp
import shutil
import tempfile
import re

from snakemake.exceptions import WorkflowError
from snakemake.script import get_source, ScriptBase, PythonScript, RScript
from snakemake.logging import logger
from snakemake.common import is_local_file
from snakemake.common import ON_WINDOWS
from snakemake.sourcecache import SourceCache, infer_source_file
from snakemake.utils import format

KERNEL_STARTED_RE = re.compile(r"Kernel started: (?P<kernel_id>\S+)")
KERNEL_SHUTDOWN_RE = re.compile(r"Kernel shutdown: (?P<kernel_id>\S+)")


def get_cell_sources(source):
    import nbformat

    nb = nbformat.reads(source, as_version=nbformat.NO_CONVERT)

    return [cell["source"] for cell in nb["cells"]]


class JupyterNotebook(ScriptBase):
    editable = True

    def draft(self):
        import nbformat

        preamble = self.get_preamble()
        nb = nbformat.v4.new_notebook()
        self.insert_preamble_cell(preamble, nb)

        nb["cells"].append(nbformat.v4.new_code_cell("# start coding here"))
        nb["metadata"] = {"language_info": {"name": self.get_language_name()}}

        os.makedirs(os.path.dirname(self.local_path), exist_ok=True)

        with open(self.local_path, "wb") as out:
            out.write(nbformat.writes(nb).encode())

    def draft_and_edit(self, listen):
        self.draft()

        self.source = open(self.local_path).read()

        self.evaluate(edit=listen)

    def write_script(self, preamble, fd):
        import nbformat

        nb = nbformat.reads(self.source, as_version=nbformat.NO_CONVERT)

        self.remove_preamble_cell(nb)
        self.insert_preamble_cell(preamble, nb)

        fd.write(nbformat.writes(nb).encode())

    def execute_script(self, fname, edit=None):
        import nbformat

        fname_out = self.log.get("notebook", None)

        with tempfile.TemporaryDirectory() as tmp:
            try:
                self._execute_cmd("papermill --version", read=True)
                has_papermill = True
            except sp.CalledProcessError:
                has_papermill = False

            if edit is not None:
                assert not edit.draft_only
                logger.info(f"Opening notebook for editing at {edit.ip}:{edit.port}")
                cmd = (
                    "jupyter notebook --browser ':' --no-browser --log-level ERROR --ip {edit.ip} --port {edit.port} "
                    "--ServerApp.quit_button=True {{fname:q}}".format(edit=edit)
                )
            elif has_papermill:
                if fname_out is None:
                    output_parameter = fname
                else:
                    output_parameter = "{fname_out}"
                cmd = (
                    "papermill --log-level ERROR {{fname:q}} "
                    "{output_parameter}".format(output_parameter=output_parameter)
                )
            else:
                if fname_out is None:
                    output_parameter = f"--output '{tmp}/notebook.ipynb'"
                else:
                    fname_out = os.path.abspath(fname_out)
                    output_parameter = "--output {fname_out:q}"

                cmd = (
                    "jupyter-nbconvert --log-level ERROR --execute {output_parameter} "
                    "--to notebook --ExecutePreprocessor.timeout=-1 {{fname:q}}".format(
                        output_parameter=output_parameter
                    )
                )

            if ON_WINDOWS:
                fname = fname.replace("\\", "/")
                fname_out = fname_out.replace("\\", "/") if fname_out else fname_out

            self._execute_cmd(
                cmd,
                fname_out=fname_out,
                fname=fname,
                additional_envvars={"IPYTHONDIR": tmp},
                is_python_script=True,
            )

            if edit:
                if fname_out is not None:
                    # store log file (executed notebook) in requested path
                    shutil.copyfile(fname, fname_out)

                logger.info("Saving modified notebook.")
                nb = nbformat.read(fname, as_version=4)

                self.remove_preamble_cell(nb)

                # clean up all outputs
                for cell in nb["cells"]:
                    if "outputs" in cell:
                        cell["outputs"] = []
                    if "execution_count" in cell:
                        cell["execution_count"] = None

                nbformat.write(nb, self.local_path)

    def insert_preamble_cell(self, preamble, notebook):
        import nbformat

        preamble_cell = nbformat.v4.new_code_cell(preamble)
        preamble_cell["metadata"]["tags"] = ["snakemake-job-properties"]
        notebook["cells"].insert(0, preamble_cell)

    def remove_preamble_cell(self, notebook):
        preambles = [
            i
            for i, cell in enumerate(notebook["cells"])
            if "snakemake-job-properties" in cell["metadata"].get("tags", [])
        ]
        if len(preambles) > 1:
            raise WorkflowError(
                "More than one snakemake preamble cell found in notebook. "
                "Please clean up the notebook first, by removing all or all but one of them."
            )
        elif len(preambles) == 1:
            preamble = preambles[0]
            # remove old preamble
            del notebook["cells"][preamble]

    @abstractmethod
    def get_language_name(self): ...

    @abstractmethod
    def get_interpreter_exec(self): ...


class PythonJupyterNotebook(JupyterNotebook):
    def get_preamble(self):
        preamble_addendum = f"import os; os.chdir(r'{os.getcwd()}');"

        return PythonScript.generate_preamble(
            self.path,
            self.cache_path,
            self.source,
            self.basedir,
            self.input,
            self.output,
            self.params,
            self.wildcards,
            self.threads,
            self.resources,
            self.log,
            self.config,
            self.rulename,
            self.conda_env,
            self.container_img,
            self.singularity_args,
            self.env_modules,
            self.bench_record,
            self.jobid,
            self.bench_iteration,
            self.cleanup_scripts,
            self.shadow_dir,
            self.is_local,
            preamble_addendum=preamble_addendum,
        )

    def get_language_name(self):
        return "python"

    def get_interpreter_exec(self):
        return "python"


class RJupyterNotebook(JupyterNotebook):
    def get_preamble(self):
        preamble_addendum = f"setwd('{os.getcwd()}');"

        return RScript.generate_preamble(
            self.path,
            self.source,
            self.basedir,
            self.input,
            self.output,
            self.params,
            self.wildcards,
            self.threads,
            self.resources,
            self.log,
            self.config,
            self.rulename,
            self.conda_env,
            self.container_img,
            self.singularity_args,
            self.env_modules,
            self.bench_record,
            self.jobid,
            self.bench_iteration,
            self.cleanup_scripts,
            self.shadow_dir,
            preamble_addendum=preamble_addendum,
        )

    def get_language_name(self):
        return "r"

    def get_interpreter_exec(self):
        return "RScript"


def get_exec_class(language):
    exec_class = {
        "jupyter_python": PythonJupyterNotebook,
        "jupyter_r": RJupyterNotebook,
    }.get(language, None)
    if exec_class is None:
        raise ValueError("Unsupported notebook: Expecting Jupyter Notebook (.ipynb).")
    return exec_class


def notebook(
    path,
    basedir,
    input,
    output,
    params,
    wildcards,
    threads,
    resources,
    log,
    config,
    rulename,
    conda_env,
    conda_base_path,
    container_img,
    singularity_args,
    env_modules,
    bench_record,
    jobid,
    bench_iteration,
    cleanup_scripts,
    shadow_dir,
    edit,
    sourcecache_path,
    runtime_sourcecache_path,
):
    """
    Load a script from the given basedir + path and execute it.
    """
    draft = False
    path = format(path, wildcards=wildcards, params=params)
    if edit is not None:
        if is_local_file(path):
            if not os.path.isabs(path):
                local_path = os.path.join(basedir, path)
            else:
                local_path = path
            if not os.path.exists(local_path):
                # draft the notebook, it does not exist yet
                language = None
                draft = True
                path = f"file://{os.path.abspath(local_path)}"
                if path.endswith(".py.ipynb"):
                    language = "jupyter_python"
                elif path.endswith(".r.ipynb"):
                    language = "jupyter_r"
                else:
                    raise WorkflowError(
                        "Notebook to edit has to end on .py.ipynb or .r.ipynb in order "
                        "to decide which programming language shall be used."
                    )
        else:
            raise WorkflowError(
                "Notebook {} is not local, but edit mode is only allowed for "
                "local notebooks.".format(path)
            )

    if not draft:
        path, source, language, is_local, cache_path = get_source(
            path,
            SourceCache(sourcecache_path, runtime_sourcecache_path),
            basedir,
            wildcards,
            params,
        )
    else:
        source = None
        cache_path = None
        is_local = True
        path = infer_source_file(path)

    exec_class = get_exec_class(language)

    executor = exec_class(
        path,
        cache_path,
        source,
        basedir,
        input,
        output,
        params,
        wildcards,
        threads,
        resources,
        log,
        config,
        rulename,
        conda_env,
        conda_base_path,
        container_img,
        singularity_args,
        env_modules,
        bench_record,
        jobid,
        bench_iteration,
        cleanup_scripts,
        shadow_dir,
        is_local,
    )

    if edit is None:
        executor.evaluate(edit=edit)
    elif edit.draft_only:
        executor.draft()
        msg = f"Generated skeleton notebook:\n{path} "
        if conda_env and not container_img:
            msg += (
                "\n\nEditing with VSCode:\nOpen notebook, run command 'Select notebook kernel' (Ctrl+Shift+P or Cmd+Shift+P), and choose:"
                "\n{}\n".format(
                    str(Path(conda_env) / "bin" / executor.get_interpreter_exec())
                )
            )
            msg += (
                "\nEditing with Jupyter CLI:"
                "\nconda activate {}\njupyter notebook {}\n".format(conda_env, path)
            )
        logger.info(msg)
    elif draft:
        executor.draft_and_edit(listen=edit)
    else:
        executor.evaluate(edit=edit)



================================================
FILE: src/snakemake/output_index.py
================================================
from __future__ import annotations

__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@protonmail.com"
__license__ = "MIT"

from typing import TYPE_CHECKING

from snakemake.common.prefix_lookup import PrefixLookup

if TYPE_CHECKING:
    from snakemake.rules import Rule


class OutputIndex:
    """Look up structure for rules, that can be queried by the output products which they create."""

    def __init__(self, rules: list[Rule]) -> None:
        entries = []
        for rule in rules:
            for product in rule.products():
                prefix = str(product.constant_prefix())
                suffix = str(product.constant_suffix())
                entries.append((prefix, (rule, suffix)))
        self._lookup = PrefixLookup(entries=entries)

    def match(self, targetfile: str) -> set[Rule]:
        """Returns all rules that match the given target file, considering only the prefix and suffix up to the
        first wildcard.

        To further verify the match, the returned rules should be checked with ``Rule.is_producer(targetfile)``.
        """
        return {
            rule
            for prefix, (rule, suffix) in self._lookup.match_iter(targetfile)
            if targetfile.endswith(suffix)
        }

    def match_producers(self, targetfile: str) -> set[Rule]:
        """Returns all rules that match and produce the given target file."""
        return {rule for rule in self.match(targetfile) if rule.is_producer(targetfile)}



================================================
FILE: src/snakemake/parser.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import sys
import textwrap
import tokenize
from typing import Any, Callable, Dict, Generator, List, Optional, TYPE_CHECKING

from snakemake import common

if TYPE_CHECKING:
    from snakemake import sourcecache, workflow

dd = textwrap.dedent

INDENT = "\t"


def is_newline(token, newline_tokens=set((tokenize.NEWLINE, tokenize.NL))):
    return token.type in newline_tokens


def is_line_start(token):
    prefix = token.line[: token.start[1]]
    return not prefix or prefix.isspace()


def is_indent(token):
    return token.type == tokenize.INDENT


def is_dedent(token):
    return token.type == tokenize.DEDENT


def is_op(token):
    return token.type == tokenize.OP


def is_greater(token):
    return is_op(token) and token.string == ">"


def is_comma(token):
    return is_op(token) and token.string == ","


def is_name(token):
    return token.type == tokenize.NAME


def is_colon(token):
    return is_op(token) and token.string == ":"


def is_comment(token):
    return token.type == tokenize.COMMENT


def is_string(token):
    return token.type == tokenize.STRING


def is_fstring_start(token):
    return sys.version_info >= (3, 12) and token.type == tokenize.FSTRING_START


def is_eof(token):
    return token.type == tokenize.ENDMARKER


def lineno(token: tokenize.TokenInfo):
    return token.start[0]


class StopAutomaton(Exception):
    def __init__(self, token):
        self.token = token


class TokenAutomaton:
    subautomata: Dict[str, Any] = {}
    deprecated: Dict[str, str] = {}

    def __init__(self, snakefile: "Snakefile", base_indent=0, dedent=0, root=True):
        self.root = root
        self.snakefile = snakefile
        self.state: Callable[[tokenize.TokenInfo], Generator] = None  # type: ignore
        self.base_indent = base_indent
        self.line = 0
        self.indent = 0
        self.was_indented = False
        self.lasttoken = None
        self._dedent = dedent

    @property
    def dedent(self):
        return self._dedent

    @property
    def effective_indent(self):
        return self.base_indent + self.indent - self.dedent

    def indentation(self, token):
        if is_indent(token) or is_dedent(token):
            self.indent = token.end[1] - self.base_indent
            self.was_indented |= self.indent > 0

    def parse_fstring(self, token: tokenize.TokenInfo):
        """
        only for python >= 3.12, since then python changed the
        parsing manner of f-string, see
        [pep-0701](https://peps.python.org/pep-0701)

        Here, we just read where the f-string start and end from tokens.
        Luckily, each token records the content of the line,
        and we can just take what we want there.
        """
        related_lines = token.start[0]
        s = token.line
        isin_fstring = 1
        for t1 in self.snakefile:
            if related_lines < t1.start[0]:
                # go to the next line
                related_lines = t1.start[0]
                s += t1.line
            if t1.type == tokenize.FSTRING_START:
                isin_fstring += 1
            elif t1.type == tokenize.FSTRING_END:
                isin_fstring -= 1
            if isin_fstring == 0:
                break
        # trim those around the f-string
        t = s[token.start[1] : t1.end[1] - len(t1.line)]
        if hasattr(self, "cmd") and self.cmd[-1][1] == token:
            self.cmd[-1] = t, token
        return t

    def consume(self):
        for token in self.snakefile:
            self.indentation(token)
            try:
                for t, orig in self.state(token):
                    # python >= 3.12 only
                    if is_fstring_start(token):
                        t = self.parse_fstring(token)
                    if self.lasttoken == "\n" and not t.isspace():
                        yield INDENT * self.effective_indent, orig
                    yield t, orig
                    self.lasttoken = t
            except tokenize.TokenError as e:
                self.error(
                    str(e).split(",")[0].strip("()''"), token
                )  # TODO the inferred line number seems to be wrong sometimes

    def error(self, msg, token, naming_hint=None):
        if naming_hint is not None:
            msg += (
                f" The keyword {naming_hint} has a special meaning in Snakemake. "
                "If you named a variable or function like this, please rename it to "
                "avoid the conflict."
            )
        raise SyntaxError(msg, (self.snakefile.path, lineno(token), None, None))

    def subautomaton(self, automaton, *args, token=None, **kwargs):
        if automaton in self.deprecated:
            assert (
                token is not None
            ), "bug: deprecation encountered but subautomaton not called with a token"
            self.error(
                f"Keyword {automaton} is deprecated. {self.deprecated[automaton]}",
                token,
            )
        return self.subautomata[automaton](
            self.snakefile,
            *args,
            base_indent=self.base_indent + self.indent,
            dedent=self.dedent,
            root=False,
            **kwargs,
        )


class KeywordState(TokenAutomaton):
    prefix = ""
    start: Callable[[], Generator[str, None, None]]

    def __init__(self, snakefile, base_indent=0, dedent=0, root=True):
        super().__init__(snakefile, base_indent=base_indent, dedent=dedent, root=root)
        self.line = 0
        self.state = self.colon

    @property
    def keyword(self):
        return self.__class__.__name__.lower()[len(self.prefix) :]

    def end(self):
        # Add newline to prevent https://github.com/snakemake/snakemake/issues/1943
        yield "\n"
        yield ")"

    def decorate_end(self, token):
        for t in self.end():
            if isinstance(t, tuple):
                yield t
            else:
                yield t, token

    def colon(self, token):
        if is_colon(token):
            self.state = self.block
            for t in self.start():
                yield t, token
        else:
            self.error(f"Colon expected after keyword {self.keyword}.", token)

    def is_block_end(self, token):
        return (self.line and self.indent <= 0) or is_eof(token)

    def block(self, token, force_block_end=False):
        if self.lasttoken == "\n" and is_comment(token):
            # ignore lines containing only comments
            self.line -= 1
        if force_block_end or self.is_block_end(token):
            yield from self.decorate_end(token)
            yield "\n", token
            raise StopAutomaton(token)

        if is_newline(token):
            self.line += 1
            yield token.string, token
        elif not (is_indent(token) or is_dedent(token)):
            if is_comment(token):
                yield token.string, token
            else:
                yield from self.block_content(token)

    def yield_indent(self, token):
        return token.string, token

    def block_content(self, token):
        yield token.string, token


class GlobalKeywordState(KeywordState):
    def start(self):
        yield f"workflow.{self.keyword}("


class DecoratorKeywordState(KeywordState):
    decorator: Optional[str] = None
    args: List[str] = []

    def start(self):
        yield f"@workflow.{self.decorator}"
        yield "\n"
        yield "def __{}({}):".format(self.decorator, ", ".join(self.args))

    def end(self):
        yield ""


class RuleKeywordState(KeywordState):
    def __init__(self, snakefile, base_indent=0, dedent=0, root=True, rulename=None):
        super().__init__(snakefile, base_indent=base_indent, dedent=dedent, root=root)
        self.rulename = rulename

    def start(self):
        yield "\n"
        yield f"@workflow.{self.keyword}("


class SectionKeywordState(KeywordState):
    def start(self):
        yield f", {self.keyword}="

    def end(self):
        # no end needed
        return list()


# Global keyword states


class Envvars(GlobalKeywordState):
    @property
    def keyword(self):
        return "register_envvars"


class Include(GlobalKeywordState):
    pass


class Workdir(GlobalKeywordState):
    pass


class Configfile(GlobalKeywordState):
    pass


# PEPs


class Pepfile(GlobalKeywordState):
    @property
    def keyword(self):
        return "set_pepfile"


class Pepschema(GlobalKeywordState):
    pass


class Report(GlobalKeywordState):
    pass


class Scattergather(GlobalKeywordState):
    pass


class Storage(GlobalKeywordState):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.tag = None
        self.state = self.register_tag

    def start(self):
        yield f"workflow.storage_registry.register_storage(tag={self.tag!r}, "

    def register_tag(self, token):
        if is_name(token):
            self.tag = token.string
        elif is_colon(token):
            self.state = self.block
            for t in self.start():
                yield t, token
        else:
            self.error(
                "Expected name or colon after storage keyword.",
                token,
                naming_hint="storage",
            )


class ResourceScope(GlobalKeywordState):
    err_msg = (
        "Invalid scope: {resource}={scope}. Scope must be set to either 'local' or "
        "'global'"
    )
    current_resource = ""

    def block_content(self, token):
        if is_name(token):
            self.current_resource = token.string
        if is_string(token) and self.lasttoken == "=":
            if token.string[1:][:-1] not in ["local", "global"]:
                self.error(
                    self.err_msg.format(
                        resource=self.current_resource, scope=token.string
                    ),
                    token,
                )
        yield token.string, token


class Ruleorder(GlobalKeywordState):
    def block_content(self, token):
        if is_greater(token):
            yield ",", token
        elif is_name(token):
            yield repr(token.string), token
        else:
            self.error(
                "Expected a descending order of rule names, "
                "e.g. rule1 > rule2 > rule3 ...",
                token,
            )


class GlobalWildcardConstraints(GlobalKeywordState):
    @property
    def keyword(self):
        return "global_wildcard_constraints"


class GlobalSingularity(GlobalKeywordState):
    @property
    def keyword(self):
        return "global_container"


class GlobalContainer(GlobalKeywordState):
    @property
    def keyword(self):
        return "global_container"


class GlobalContainerized(GlobalKeywordState):
    @property
    def keyword(self):
        return "global_containerized"


class GlobalConda(GlobalKeywordState):
    @property
    def keyword(self):
        return "global_conda"


class DefaultInputFlags(GlobalKeywordState):
    @property
    def keyword(self):
        return "set_default_input_flags"


class DefaultOutputFlags(GlobalKeywordState):
    @property
    def keyword(self):
        return "set_default_output_flags"


class Localrules(GlobalKeywordState):
    def block_content(self, token):
        if is_comma(token):
            yield ",", token
        elif is_name(token):
            yield repr(token.string), token
        else:
            self.error(
                "Expected a comma separated list of rules that shall "
                "not be executed by the cluster command.",
                token,
            )


# Rule keyword states


class Name(RuleKeywordState):
    pass


class Input(RuleKeywordState):
    pass


class Output(RuleKeywordState):
    pass


class Params(RuleKeywordState):
    pass


class Threads(RuleKeywordState):
    pass


class Retries(RuleKeywordState):
    pass


class Shadow(RuleKeywordState):
    pass


class Resources(RuleKeywordState):
    pass


class Priority(RuleKeywordState):
    pass


class Version(RuleKeywordState):
    pass


class Log(RuleKeywordState):
    pass


class Message(RuleKeywordState):
    pass


class Benchmark(RuleKeywordState):
    pass


class Conda(RuleKeywordState):
    pass


class Singularity(RuleKeywordState):
    @property
    def keyword(self):
        return "container"


class Container(RuleKeywordState):
    pass


class Containerized(RuleKeywordState):
    pass


class EnvModules(RuleKeywordState):
    pass


class Group(RuleKeywordState):
    pass


class Cache(RuleKeywordState):
    @property
    def keyword(self):
        return "cache_rule"


class DefaultTarget(RuleKeywordState):
    @property
    def keyword(self):
        return "default_target_rule"


class Handover(RuleKeywordState):
    pass


class WildcardConstraints(RuleKeywordState):
    @property
    def keyword(self):
        return "register_wildcard_constraints"


class LocalRule(RuleKeywordState):
    pass


class Run(RuleKeywordState):
    def __init__(self, snakefile, rulename, base_indent=0, dedent=0, root=True):
        super().__init__(snakefile, base_indent=base_indent, dedent=dedent, root=root)
        self.rulename = rulename
        self.content = 0

    def start(self):
        yield "@workflow.run"
        yield "\n"
        yield (
            "def __rule_{rulename}(input, output, params, wildcards, threads, "
            "resources, log, rule, conda_env, container_img, "
            "singularity_args, use_singularity, env_modules, bench_record, jobid, "
            "is_shell, bench_iteration, cleanup_scripts, shadow_dir, edit_notebook, "
            "conda_base_path, basedir, sourcecache_path, runtime_sourcecache_path, {rule_func_marker}=True):".format(
                rulename=(
                    self.rulename
                    if self.rulename is not None
                    else self.snakefile.rulecount
                ),
                rule_func_marker=common.RULEFUNC_CONTEXT_MARKER,
            )
        )

    def end(self):
        yield ""

    def block_content(self, token):
        self.content += 1
        yield token.string, token

    def is_block_end(self, token):
        return (self.content and self.line and self.indent <= 0) or is_eof(token)


class AbstractCmd(Run):
    overwrite_cmd: Optional[str] = None
    start_func: Optional[str] = None
    end_func: Optional[str] = None

    def __init__(self, snakefile, rulename, base_indent=0, dedent=0, root=True):
        super().__init__(
            snakefile, rulename, base_indent=base_indent, dedent=dedent, root=root
        )
        self.cmd: list[tuple[str, tokenize.TokenInfo]] = []
        self.token = None
        if self.overwrite_cmd is not None:
            self.block_content = self.overwrite_block_content  # type: ignore

    def is_block_end(self, token):
        return (self.line and self.indent <= 0) or is_eof(token)

    def start(self):
        if self.start_func is not None:
            yield self.start_func
            yield "("

    def args(self):
        yield from []

    def end(self):
        # the end is detected. So we can safely reset the indent to zero here
        self.indent = 0
        yield "\n"
        yield ")"
        yield "\n"
        for t in super().start():
            yield t
        yield "\n"
        yield INDENT * (self.effective_indent + 1)
        yield self.end_func
        yield "("
        yield from self.cmd
        yield from self.args()
        yield "\n"
        yield ")"
        for t in super().end():
            yield t

    def decorate_end(self, token):
        if self.token is None:
            # no block after shell keyword
            self.error(
                "Command must be given as string after the shell keyword.", token
            )
        yield from super().decorate_end(self.token)

    def block_content(self, token):
        self.token = token
        self.cmd.append((token.string, token))
        yield token.string, token

    def overwrite_block_content(self, token):
        if self.token is None:
            self.token = token
            cmd = repr(self.overwrite_cmd)
            self.cmd.append((cmd, token))
            yield cmd, token


class Shell(AbstractCmd):
    start_func = "@workflow.shellcmd"
    end_func = "shell"

    def args(self):
        yield ", bench_record=bench_record, bench_iteration=bench_iteration"


class Script(AbstractCmd):
    start_func = "@workflow.script"
    end_func = "script"

    def args(self):
        yield (
            ", basedir, input, output, params, wildcards, threads, resources, log, "
            "config, rule, conda_env, conda_base_path, container_img, singularity_args, env_modules, "
            "bench_record, jobid, bench_iteration, cleanup_scripts, shadow_dir, sourcecache_path, "
            "runtime_sourcecache_path"
        )


class Notebook(Script):
    start_func = "@workflow.notebook"
    end_func = "notebook"

    def args(self):
        yield (
            ", basedir, input, output, params, wildcards, threads, resources, log, "
            "config, rule, conda_env, conda_base_path, container_img, singularity_args, env_modules, "
            "bench_record, jobid, bench_iteration, cleanup_scripts, shadow_dir, "
            "edit_notebook, sourcecache_path, runtime_sourcecache_path"
        )


class Wrapper(Script):
    start_func = "@workflow.wrapper"
    end_func = "wrapper"

    def args(self):
        yield (
            ", input, output, params, wildcards, threads, resources, log, "
            "config, rule, conda_env, conda_base_path, container_img, singularity_args, env_modules, "
            "bench_record, workflow.workflow_settings.wrapper_prefix, jobid, bench_iteration, "
            "cleanup_scripts, shadow_dir, sourcecache_path, runtime_sourcecache_path"
        )


class TemplateEngine(Script):
    start_func = "@workflow.template_engine"
    end_func = "render_template"

    def args(self):
        yield (", input, output, params, wildcards, config, rule")


class CWL(Script):
    start_func = "@workflow.cwl"
    end_func = "cwl"

    def args(self):
        yield (
            ", basedir, input, output, params, wildcards, threads, resources, log, "
            "config, rule, use_singularity, bench_record, jobid, sourcecache_path, "
            "runtime_sourcecache_path"
        )


rule_property_subautomata = dict(
    name=Name,
    input=Input,
    output=Output,
    params=Params,
    threads=Threads,
    resources=Resources,
    retries=Retries,
    priority=Priority,
    log=Log,
    message=Message,
    benchmark=Benchmark,
    conda=Conda,
    singularity=Singularity,
    container=Container,
    containerized=Containerized,
    envmodules=EnvModules,
    wildcard_constraints=WildcardConstraints,
    shadow=Shadow,
    group=Group,
    cache=Cache,
    handover=Handover,
    default_target=DefaultTarget,
    localrule=LocalRule,
)
rule_property_deprecated = dict(
    version="Use conda or container directive instead (see docs)."
)


class Rule(GlobalKeywordState):
    subautomata = dict(
        run=Run,
        shell=Shell,
        script=Script,
        notebook=Notebook,
        wrapper=Wrapper,
        template_engine=TemplateEngine,
        cwl=CWL,
        **rule_property_subautomata,
    )
    deprecated = rule_property_deprecated

    def __init__(self, snakefile, base_indent=0, dedent=0, root=True):
        super().__init__(snakefile, base_indent=base_indent, dedent=dedent, root=root)
        self.state = self.name
        self.lineno = None
        self.rulename = None
        self.run = False
        self.snakefile.rulecount += 1

    def start(self, aux=""):
        yield (
            f"@workflow.rule(name={self.rulename!r}, lineno={self.lineno}, "
            f"snakefile={self.snakefile.path!r}{aux})"
        )

    def end(self):
        if not self.run:
            yield "@workflow.norun()"
            yield "\n"
            for t in self.subautomaton("run", rulename=self.rulename).start():
                yield t
            # the end is detected.
            # So we can safely reset the indent to zero here
            self.indent = 0
            yield "\n"
            yield INDENT * (self.effective_indent + 1)
            yield "pass"

    def name(self, token):
        if is_name(token):
            self.rulename = token.string
        elif is_colon(token):
            self.lineno = self.snakefile.lines + 1
            self.state = self.block
            for t in self.start():
                yield t, token
        else:
            self.error(
                "Expected name or colon after rule or checkpoint keyword.",
                token,
                naming_hint="rule",
            )

    def block_content(self, token):
        if is_name(token):
            try:
                if (
                    token.string == "run"
                    or token.string == "shell"
                    or token.string == "script"
                    or token.string == "wrapper"
                    or token.string == "notebook"
                    or token.string == "template_engine"
                    or token.string == "cwl"
                ):
                    if self.run:
                        raise self.error(
                            "Multiple run/shell/script/notebook/wrapper/template_engine/cwl "
                            "keywords in rule {}.".format(self.rulename),
                            token,
                        )
                    self.run = True
                elif self.run:
                    raise self.error(
                        "No rule keywords allowed after "
                        "run/shell/script/notebook/wrapper/template_engine/cwl in "
                        "rule {}.".format(self.rulename),
                        token,
                    )
                for t in self.subautomaton(
                    token.string, token=token, rulename=self.rulename
                ).consume():
                    yield t
            except KeyError:
                self.error(
                    f"Unexpected keyword {token.string} in rule definition",
                    token,
                )
            except StopAutomaton as e:
                self.indentation(e.token)
                for t in self.block(e.token):
                    yield t
        elif is_comment(token):
            yield "\n", token
            yield token.string, token
        elif is_string(token):
            yield "\n", token
            yield f"@workflow.docstring({token.string})", token
        else:
            self.error(
                "Expecting rule keyword, comment or docstrings "
                "inside a rule definition.",
                token,
            )

    @property
    def dedent(self):
        return self.indent


class Checkpoint(Rule):
    def start(self):
        yield from super().start(aux=", checkpoint=True")


class OnSuccess(DecoratorKeywordState):
    decorator = "onsuccess"
    args = ["log"]


class OnError(DecoratorKeywordState):
    decorator = "onerror"
    args = ["log"]


class OnStart(DecoratorKeywordState):
    decorator = "onstart"
    args = ["log"]


# modules


class ModuleKeywordState(SectionKeywordState):
    prefix = "Module"

    def start(self):
        yield f"{self.keyword}="

    def end(self):
        yield ","


class ModuleName(ModuleKeywordState):
    pass


class ModuleSnakefile(ModuleKeywordState):
    pass


class ModulePrefix(ModuleKeywordState):
    pass


class ModuleMetaWrapper(ModuleKeywordState):
    @property
    def keyword(self):
        return "meta_wrapper"


class ModuleConfig(ModuleKeywordState):
    pass


class ModuleSkipValidation(ModuleKeywordState):
    @property
    def keyword(self):
        return "skip_validation"


class ModuleReplacePrefix(ModuleKeywordState):
    @property
    def keyword(self):
        return "replace_prefix"


class Module(GlobalKeywordState):
    subautomata = dict(
        name=ModuleName,
        snakefile=ModuleSnakefile,
        meta_wrapper=ModuleMetaWrapper,
        config=ModuleConfig,
        skip_validation=ModuleSkipValidation,
        replace_prefix=ModuleReplacePrefix,
        prefix=ModulePrefix,
    )

    def __init__(self, snakefile, base_indent=0, dedent=0, root=True):
        super().__init__(snakefile, base_indent=base_indent, dedent=dedent, root=root)
        self.state = self.name
        self.has_snakefile = False
        self.has_meta_wrapper = False
        self.modulename = None
        self.has_name = False
        self.primary_token = None

    def end(self):
        if self.modulename is not None:
            yield f"name={self.modulename!r}\n"
        elif not self.has_name:
            self.error(
                "Missing module name. "
                "A module name must be provided either after the module keyword or "
                "inside the module definition after the name keyword.",
                self.primary_token,
            )

        if not (self.has_snakefile or self.has_meta_wrapper):
            self.error(
                "A module needs either a path to a Snakefile or a meta wrapper URL.",
                self.primary_token,
            )
        yield ")"

    def name(self, token):
        if is_name(token):
            self.modulename = token.string
            self.has_name = True
        elif is_colon(token):
            self.primary_token = token
            self.state = self.block
            yield "workflow.module(", token
        else:
            self.error(
                "Expected name or colon after module keyword.",
                token,
                naming_hint="module",
            )

    def block_content(self, token):
        if is_name(token):
            try:
                if token.string == "snakefile":
                    self.has_snakefile = True
                if token.string == "meta_wrapper":
                    self.has_meta_wrapper = True
                if token.string == "name":
                    if self.has_name:
                        raise self.error(
                            "Ambiguous module name. "
                            "A module name was provided directly after the module keyword. "
                            "Another module name was provided by the name keyword.",
                            token,
                            naming_hint="module",
                        )
                    self.has_name = True
                for t in self.subautomaton(token.string, token=token).consume():
                    yield t
            except KeyError:
                self.error(
                    "Unexpected keyword {} in "
                    "module definition".format(token.string),
                    token,
                )
            except StopAutomaton as e:
                self.indentation(e.token)
                for t in self.block(e.token):
                    yield t
        elif is_comment(token):
            yield "\n", token
            yield token.string, token
        elif is_string(token):
            # ignore docstring
            pass
        else:
            self.error(
                "Expecting module keyword, comment or docstrings "
                "inside a module definition.",
                token,
            )


class UseRule(GlobalKeywordState):
    subautomata = rule_property_subautomata
    deprecated = rule_property_deprecated

    def __init__(self, snakefile, base_indent=0, dedent=0, root=True):
        super().__init__(snakefile, base_indent=base_indent, dedent=dedent, root=root)
        self.state = self.state_keyword_rule
        self.rules = []
        self.exclude_rules = []
        self.has_with = False
        self.name_modifier = []
        self.from_module = None
        self._with_block = []
        self.lineno = self.snakefile.lines + 1

    def end(self):
        name_modifier = "".join(self.name_modifier) if self.name_modifier else None
        yield "@workflow.userule(rules={!r}, from_module={!r}, exclude_rules={!r}, name_modifier={!r}, lineno={})".format(
            self.rules, self.from_module, self.exclude_rules, name_modifier, self.lineno
        )
        yield "\n"

        if self._with_block:
            # yield with block
            yield from self._with_block

            yield "@workflow.run"
            yield "\n"

        rulename = self.rules[0]
        if rulename == "*":
            rulename = "__allrules__"
        yield f"def __userule_{self.from_module}_{rulename}():"
        # the end is detected.
        # So we can safely reset the indent to zero here
        self.indent = 0
        yield "\n"
        yield INDENT * (self.effective_indent + 1)
        yield "pass"

    def state_keyword_rule(self, token):
        if is_name(token) and token.string == "rule":
            self.state = self.state_rules_rule
            yield from ()
        else:
            self.error("Expecting keyword 'rule' after keyword 'use'", token)

    def state_rules_rule(self, token):
        if is_name(token):
            if token.string == "from" or token.string == "as" and not self.rules:
                self.error("Expecting rule names after 'use rule' statement.", token)

            self.rules.append(token.string)
            self.state = self.state_rules_comma_or_end
            yield from ()
        elif is_op(token):
            if token.string == "*":
                self.rules.append(token.string)
                self.state = self.state_rules_end
                yield from ()
            else:
                self.error(
                    "Expecting rule name or '*' after 'use rule' statement.", token
                )
        else:
            self.error(
                "Expecting rule listing (comma separated) after 'use rule' statement.",
                token,
            )
        # TODO newline and parentheses handling

    def state_rules_end(self, token):
        if is_name(token) and token.string == "from":
            self.state = self.state_from
            yield from ()
        else:
            self.error(
                "Expecting list of rules in 'use rule' statement to end with keyword 'from'.",
                token,
            )

    def state_rules_comma_or_end(self, token):
        if is_name(token):
            if token.string == "from" or token.string == "as":
                if not self.rules:
                    self.error(
                        "Expecting rule names after 'use rule' statement.", token
                    )
                if token.string == "from":
                    self.state = self.state_from
                else:
                    self.state = self.state_as
                yield from ()
            else:
                self.error(
                    "Expecting list of rules in 'use rule' statement to end with keyword 'from'.",
                    token,
                )
        elif is_comma(token):
            self.state = self.state_rules_rule
            yield from ()
        else:
            self.error(
                "Unexpected token in list of rules within 'use rule' statement.", token
            )

    def state_from(self, token):
        if is_name(token):
            self.state = self.state_modifier
            self.from_module = token.string
            yield from ()
        else:
            self.error(
                "Expecting module name after 'from' keyword in 'use rule' statement.",
                token,
            )

    def state_modifier(self, token):
        if is_name(token):
            if token.string == "as" and not self.name_modifier:
                self.state = self.state_as
                yield from ()
            elif token.string == "exclude":
                self.state = self.state_exclude
                yield from ()
            elif token.string == "with":
                yield from self.handle_with(token)
            else:
                self.error(
                    "Expecting at most one 'as' or 'with' statement, or the end of the line.",
                    token,
                )
        elif is_newline(token) or is_comment(token) or is_eof(token):
            # end of the statement, close block manually
            yield from self.block(token, force_block_end=True)
        else:
            self.error(
                "Expecting either 'as', 'with' or end of line in 'use rule' statement.",
                token,
            )

    def handle_with(self, token):
        if "*" in self.rules:
            self.error(
                "Keyword 'with' in 'use rule' statement is not allowed in combination with rule pattern '*'.",
                token,
            )
        self.has_with = True
        self.state = self.state_with
        yield from ()

    def state_as(self, token):
        if is_name(token):
            if token.string != "with":
                self.name_modifier.append(token.string)
                yield from ()
            else:
                yield from self.handle_with(token)
        elif is_op(token) and token.string == "*":
            self.name_modifier.append(token.string)
            yield from ()
        elif is_newline(token) or is_comment(token) or is_eof(token):
            # end of the statement, close block manually
            yield from self.block(token, force_block_end=True)
        else:
            self.error(
                "Expecting rulename modifying pattern (e.g. modulename_*) after 'as' keyword.",
                token,
            )

    def state_with(self, token):
        if is_colon(token):
            self.state = self.block
            yield from ()
        else:
            self.error(
                "Expecting colon after 'with' keyword in 'use rule' statement.", token
            )

    def state_exclude(self, token):
        if is_name(token):
            self.exclude_rules.append(token.string)
            self.state = self.state_exclude_comma_or_end
            yield from ()
        else:
            self.error(
                "Expecting rule name(s) after 'exclude' keyword in 'use rule' statement.",
                token,
            )

    def state_exclude_comma_or_end(self, token):
        if is_name(token):
            if token.string == "from" or token.string == "as":
                if not self.exclude_rules:
                    self.error("Expecting rule names after 'exclude' statement.", token)
                if token.string == "from":
                    self.state = self.state_from
                else:
                    self.state = self.state_as
                yield from ()
            else:
                yield from ()
        elif is_comma(token):
            self.state = self.state_exclude
            yield from ()
        else:
            self.state = self.state_modifier
            yield from ()

    def block_content(self, token):
        if is_comment(token):
            yield "\n", token
            yield token.string, token
        elif is_name(token):
            try:
                self._with_block.extend(
                    self.subautomaton(token.string, token=token).consume()
                )
                yield from ()
            except KeyError:
                self.error(
                    f"Unexpected keyword {token.string} in rule definition",
                    token,
                )
            except StopAutomaton as e:
                self.indentation(e.token)
                yield from self.block(e.token)
        else:
            self.error(
                "Expecting a keyword or comment "
                "inside a 'use rule ... with:' statement.",
                token,
            )

    @property
    def dedent(self):
        return self.indent


class Python(TokenAutomaton):
    subautomata = dict(
        envvars=Envvars,
        include=Include,
        workdir=Workdir,
        configfile=Configfile,
        pepfile=Pepfile,
        pepschema=Pepschema,
        report=Report,
        ruleorder=Ruleorder,
        rule=Rule,
        checkpoint=Checkpoint,
        localrules=Localrules,
        onsuccess=OnSuccess,
        onerror=OnError,
        onstart=OnStart,
        wildcard_constraints=GlobalWildcardConstraints,
        singularity=GlobalSingularity,
        container=GlobalContainer,
        containerized=GlobalContainerized,
        conda=GlobalConda,
        scattergather=Scattergather,
        inputflags=DefaultInputFlags,
        outputflags=DefaultOutputFlags,
        storage=Storage,
        resource_scopes=ResourceScope,
        module=Module,
        use=UseRule,
    )
    deprecated = dict(subworkflow="Use module directive instead (see docs).")

    def __init__(self, snakefile, base_indent=0, dedent=0, root=True):
        super().__init__(snakefile, base_indent=base_indent, dedent=dedent, root=root)
        self.state = self.python

    def python(self, token: tokenize.TokenInfo):
        if not (is_indent(token) or is_dedent(token)):
            if (
                self.lasttoken is None
                or self.lasttoken.isspace()
                and is_line_start(token)
            ):
                try:
                    for t in self.subautomaton(token.string, token=token).consume():
                        yield t
                except KeyError:
                    yield token.string, token
                except StopAutomaton as e:
                    self.indentation(e.token)
                    for t in self.python(e.token):
                        yield t
            else:
                yield token.string, token


class Snakefile:
    def __init__(
        self,
        path: "sourcecache.SourceFile",
        workflow: "workflow.Workflow",
        rulecount=0,
    ):
        self.path = path.get_path_or_uri()
        self.file = workflow.sourcecache.open(path)
        self.tokens = tokenize.generate_tokens(self.file.readline)
        self.rulecount = rulecount
        self.lines = 0

    def __next__(self):
        return next(self.tokens)

    def __iter__(self):
        return self

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.file.close()


def format_tokens(tokens) -> Generator[str, None, None]:
    t_: Optional[str] = None
    for t in tokens:
        if t_ and not t.isspace() and not t_.isspace():
            yield " "
        yield t
        t_ = t


def parse(
    path,
    workflow: "workflow.Workflow",
    linemap: Dict[int, int],
    overwrite_shellcmd=None,
    rulecount=0,
):
    Shell.overwrite_cmd = overwrite_shellcmd
    with Snakefile(path, workflow, rulecount=rulecount) as snakefile:
        automaton = Python(snakefile)
        compilation = list()
        for t, orig_token in automaton.consume():
            line_number = lineno(orig_token)
            linemap |= {
                i: line_number
                for i in range(snakefile.lines + 1, snakefile.lines + t.count("\n") + 1)
            }
            snakefile.lines += t.count("\n")
            compilation.append(t)
    join_compilation = "".join(format_tokens(compilation))
    if linemap:
        last = max(linemap)
        linemap[last + 1] = linemap[last]
    return join_compilation, snakefile.rulecount



================================================
FILE: src/snakemake/path_modifier.py
================================================
__authors__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import os

from snakemake.common.prefix_lookup import PrefixLookup
from snakemake.exceptions import WorkflowError
from snakemake.io import (
    is_callable,
    is_flagged,
    AnnotatedString,
    flag,
    get_flag_value,
)
from snakemake.logging import logger

PATH_MODIFIER_FLAG = "path_modified"


class PathModifier:
    def __init__(self, replace_prefix: dict, prefix: str, workflow):
        self.skip_properties = set()
        self.workflow = workflow

        self.prefix = None
        assert not (prefix and replace_prefix)
        if prefix:
            if not prefix.endswith("/"):
                prefix += "/"
            self.prefix = prefix

        self._prefix_replacements = (
            PrefixLookup(entries=list(replace_prefix.items()))
            if replace_prefix
            else None
        )

    def modify(self, path, property=None):
        if get_flag_value(path, PATH_MODIFIER_FLAG):
            # Path has been modified before and is reused now, no need to modify again.
            return path

        if get_flag_value(path, "local"):
            logger.debug(f"Not modifying path of file {path}, as it is local")
            # File is local
            return path

        modified_path = self.apply_default_storage(self.replace_prefix(path, property))
        if modified_path == path:
            # nothing has changed
            return path

        # Important, update with previous flags in case of AnnotatedString #596
        if hasattr(path, "flags"):
            if not hasattr(modified_path, "flags"):
                modified_path = AnnotatedString(modified_path)
            modified_path.flags.update(path.flags)
            if is_flagged(modified_path, "multiext"):
                multiext_value = modified_path.flags["multiext"]
                multiext_value.prefix = self.apply_default_storage(
                    self.replace_prefix(multiext_value.prefix, property)
                )
        # Flag the path as modified and return.
        modified_path = flag(modified_path, PATH_MODIFIER_FLAG)
        return modified_path

    def replace_prefix(self, path, property=None):
        if (self._prefix_replacements is None and self.prefix is None) or (
            property in self.skip_properties
            or os.path.isabs(path)
            or path.startswith("..")
            or is_flagged(path, "storage_object")
            or is_callable(path)
        ):
            # no replacement
            return path

        if self._prefix_replacements is not None:
            prefixes = list(self._prefix_replacements.match_iter(path))

            if len(prefixes) > 1:
                # ambiguous prefixes
                raise WorkflowError(
                    "Multiple prefixes ({}) match the path {}. Make sure that the replace_prefix statement "
                    "in your module definition does not yield ambiguous matches.".format(
                        ", ".join(prefix[0] for prefix in prefixes), path
                    )
                )
            elif prefixes:
                prefix, replacement = prefixes[0]
                return replacement + path[len(prefix) :]
            else:
                # no matching prefix
                return path
        else:
            # prefix case
            return self.prefix + path

    def apply_default_storage(self, path):
        """Apply the defined default remote provider to the given path and return the updated _IOFile.
        Asserts that default remote provider is defined.
        """
        from snakemake.storage import flag_with_storage_object

        def is_annotated_callable(value):
            if isinstance(value, AnnotatedString):
                return bool(value.callable)

        provider = self.workflow.storage_settings.default_storage_provider

        if (
            provider is None
            or is_flagged(path, "storage_object")
            or is_flagged(path, "local")
            or is_flagged(path, "sourcecache_entry")
            or is_annotated_callable(path)
        ):
            # no default remote needed
            return path

        # This will convert any AnnotatedString to str
        prefix = self.workflow.storage_settings.default_storage_prefix
        if prefix and not prefix.endswith("/"):
            prefix = f"{prefix}/"
        query = f"{prefix}{os.path.normpath(path)}"
        storage_object = self.workflow.storage_registry.default_storage_provider.object(
            query
        )
        validation_res = storage_object.is_valid_query()
        if not validation_res:
            raise WorkflowError(
                f"Error applying default storage provider {provider}. "
                "Make sure to provide a valid --default-storage-prefix "
                "(see https://snakemake.github.io/snakemake-plugin-catalog/plugins/"
                f"storage/{provider}.html). "
                "Usually, the storage provider requires a scheme in the prefix, "
                f"like 's3://' in case of the s3 storage provider. {validation_res}",
            )
        return flag_with_storage_object(path, storage_object)

    @property
    def modifies_prefixes(self) -> bool:
        return self._prefix_replacements is not None



================================================
FILE: src/snakemake/persistence.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import asyncio
from dataclasses import dataclass, field
import hashlib
import os
import shutil
import json
import stat
import tempfile
import time
from base64 import urlsafe_b64encode, b64encode
from functools import lru_cache
from itertools import count
from pathlib import Path
from contextlib import contextmanager
from typing import Any, Optional, Set

from snakemake_interface_executor_plugins.persistence import (
    PersistenceExecutorInterface,
)
from snakemake_interface_executor_plugins.settings import ExecMode

from snakemake.common.tbdstring import TBDString
import snakemake.exceptions
from snakemake.logging import logger
from snakemake.jobs import jobfiles, Job
from snakemake.utils import listfiles
from snakemake.io import _IOFile, is_flagged, get_flag_value, IOCache
from snakemake_interface_common.exceptions import WorkflowError
from snakemake.settings.types import DeploymentMethod

UNREPRESENTABLE = object()
RECORD_FORMAT_VERSION = 6


class Persistence(PersistenceExecutorInterface):
    def __init__(
        self,
        nolock=False,
        dag=None,
        conda_prefix=None,
        singularity_prefix=None,
        shadow_prefix=None,
        warn_only=False,
        path: Path | None = None,
    ):
        import importlib.util

        self._serialize_param = (
            self._serialize_param_pandas
            if importlib.util.find_spec("pandas") is not None
            else self._serialize_param_builtin
        )

        self._max_len = None

        if path is None:
            self._path = Path(os.path.abspath(".snakemake"))
        else:
            self._path = path
        os.makedirs(self.path, exist_ok=True)

        self._lockdir = os.path.join(self.path, "locks")
        os.makedirs(self._lockdir, exist_ok=True)

        self.dag = dag
        self._lockfile = dict()

        self._metadata_path = os.path.join(self.path, "metadata")
        self._incomplete_path = os.path.join(self.path, "incomplete")

        self.conda_env_archive_path = os.path.join(self.path, "conda-archive")
        self.benchmark_path = os.path.join(self.path, "benchmarks")

        self.source_cache = os.path.join(self.path, "source_cache")

        self.iocache_path = os.path.join(self.path, "iocache")

        if conda_prefix is None:
            self.conda_env_path = os.path.join(self.path, "conda")
        else:
            self.conda_env_path = os.path.abspath(conda_prefix)
        if singularity_prefix is None:
            self.container_img_path = os.path.join(self.path, "singularity")
        else:
            self.container_img_path = os.path.abspath(singularity_prefix)
        if shadow_prefix is None:
            self.shadow_path = os.path.join(self.path, "shadow")
        else:
            self.shadow_path = os.path.join(shadow_prefix, "shadow")

        # place to store any auxiliary information needed during a run (e.g. source tarballs)
        self._aux_path = os.path.join(self.path, "auxiliary")

        # migration of .snakemake folder structure
        migration_indicator = Path(
            os.path.join(self._incomplete_path, "migration_underway")
        )
        if (
            os.path.exists(self._metadata_path)
            and not os.path.exists(self._incomplete_path)
        ) or migration_indicator.exists():
            os.makedirs(self._incomplete_path, exist_ok=True)

            migration_indicator.touch()

            self.migrate_v1_to_v2()

            migration_indicator.unlink()

        self._incomplete_cache = None

        for d in (
            self._metadata_path,
            self._incomplete_path,
            self.shadow_path,
            self.conda_env_archive_path,
            self.conda_env_path,
            self.container_img_path,
            self.aux_path,
            self.iocache_path,
        ):
            os.makedirs(d, exist_ok=True)

        if nolock:
            self.lock = self.noop
            self.unlock = self.noop
        if warn_only:
            self.lock = self.lock_warn_only
            self.unlock = self.noop

        self._read_record = self._read_record_cached
        self.max_checksum_file_size = (
            self.dag.workflow.dag_settings.max_checksum_file_size
        )

    @property
    def path(self) -> Path:
        return Path(self._path)

    @property
    def aux_path(self) -> Path:
        return Path(self._aux_path)

    def migrate_v1_to_v2(self):
        logger.info("Migrating .snakemake folder to new format...")
        i = 0
        for path, _, filenames in os.walk(self._metadata_path):
            path = Path(path)
            for filename in filenames:
                with open(path / filename, "r") as f:
                    try:
                        record = json.load(f)
                    except json.JSONDecodeError:
                        continue  # not a properly formatted JSON file

                    if record.get("incomplete", False):
                        target_path = Path(self._incomplete_path) / path.relative_to(
                            self._metadata_path
                        )
                        os.makedirs(target_path, exist_ok=True)
                        shutil.copyfile(path / filename, target_path / filename)
                i += 1
                # this can take a while for large folders...
                if (i % 10000) == 0 and i > 0:
                    logger.info(f"{i} files migrated")

        logger.info("Migration complete")

    @property
    def files(self):
        if self._files is None:
            self._files = set(self.dag.output_files)
        return self._files

    @property
    def locked(self):
        inputfiles = set(self.all_inputfiles())
        outputfiles = set(self.all_outputfiles())
        if os.path.exists(self._lockdir):
            for lockfile in self._locks("input"):
                with open(lockfile) as lock:
                    for f in lock:
                        f = f.strip()
                        if f in outputfiles:
                            return True
            for lockfile in self._locks("output"):
                with open(lockfile) as lock:
                    for f in lock:
                        f = f.strip()
                        if f in outputfiles or f in inputfiles:
                            return True
        return False

    @contextmanager
    def lock_warn_only(self):
        if self.locked:
            logger.info(
                "Error: Directory cannot be locked. This usually "
                "means that another Snakemake instance is running on this directory. "
                "Another possibility is that a previous run exited unexpectedly."
            )
        yield

    @contextmanager
    def lock(self):
        if self.locked:
            raise snakemake.exceptions.LockException()
        try:
            self._lock(self.all_inputfiles(), "input")
            self._lock(self.all_outputfiles(), "output")
            yield
        finally:
            self.unlock()

    def unlock(self):
        logger.debug("unlocking")
        for lockfile in self._lockfile.values():
            try:
                logger.debug("removing lock")
                os.remove(lockfile)
            except OSError as e:
                if e.errno != 2:  # missing file
                    raise e
        logger.debug("removed all locks")

    def cleanup_locks(self):
        shutil.rmtree(self._lockdir)

    def cleanup_metadata(self, path):
        return self._delete_record(self._incomplete_path, path) or self._delete_record(
            self._metadata_path, path
        )

    def cleanup_shadow(self):
        if os.path.exists(self.shadow_path):
            shutil.rmtree(self.shadow_path)
            os.mkdir(self.shadow_path)

    def cleanup_containers(self):
        from humanfriendly import format_size

        required_imgs = {Path(img.path) for img in self.dag.container_imgs.values()}
        img_dir = Path(self.container_img_path)
        total_size_cleaned_up = 0
        num_containers_removed = 0
        for pulled_img in img_dir.glob("*.simg"):
            if pulled_img in required_imgs:
                continue
            size_bytes = pulled_img.stat().st_size
            total_size_cleaned_up += size_bytes
            filesize = format_size(size_bytes)
            pulled_img.unlink()
            logger.debug(f"Removed unrequired container {pulled_img} ({filesize})")
            num_containers_removed += 1

        if num_containers_removed == 0:
            logger.info("No containers require cleaning up")
        else:
            logger.info(
                f"Cleaned up {num_containers_removed} containers, saving {format_size(total_size_cleaned_up)}"
            )

    def conda_cleanup_envs(self):
        # cleanup envs
        for address in set(
            env.address
            for env in self.dag.conda_envs.values()
            if not env.is_externally_managed
        ):
            removed = False
            if os.path.exists(address):
                try:
                    shutil.rmtree(address)
                except Exception as e:
                    raise WorkflowError(f"Failed to remove conda env {address}: {e}")
                removed = True
            yaml_path = Path(address).with_suffix(".yaml")
            if yaml_path.exists():
                try:
                    yaml_path.unlink()
                except Exception as e:
                    raise WorkflowError(
                        f"Failed to remove conda env yaml {yaml_path}: {e}"
                    )

                removed = True
            if removed:
                logger.info(f"Removed conda env {address}")

        # cleanup env archives
        in_use = set(env.content_hash for env in self.dag.conda_envs.values())
        for d in os.listdir(self.conda_env_archive_path):
            if d not in in_use:
                shutil.rmtree(os.path.join(self.conda_env_archive_path, d))

    def started(self, job, external_jobid: Optional[str] = None):
        for f in job.output:
            self._record(self._incomplete_path, {"external_jobid": external_jobid}, f)

    def _remove_incomplete_marker(self, job):
        for f in job.output:
            self._delete_record(self._incomplete_path, f)

    async def finished(self, job):
        if not self.dag.workflow.execution_settings.keep_metadata:
            self._remove_incomplete_marker(job)
            # do not store metadata if not requested
            return

        if (
            self.dag.workflow.exec_mode == ExecMode.DEFAULT
            or self.dag.workflow.remote_execution_settings.immediate_submit
        ):
            code = self._code(job.rule)
            input = self._input(job)
            log = self._log(job)
            params = self._params(job)
            shellcmd = job.shellcmd
            conda_env = self._conda_env(job)
            software_stack_hash = self._software_stack_hash(job)
            fallback_time = time.time()
            for f in job.output:
                rec_path = self._record_path(self._incomplete_path, f)
                starttime = (
                    os.path.getmtime(rec_path) if os.path.exists(rec_path) else None
                )
                # Sometimes finished is called twice, if so, lookup the previous starttime
                if not os.path.exists(rec_path):
                    starttime = self._read_record(self._metadata_path, f).get(
                        "starttime", None
                    )

                endtime = (
                    (await f.mtime()).local_or_storage()
                    if await f.exists()
                    else fallback_time
                )

                checksums = (
                    (infile, await infile.checksum(self.max_checksum_file_size))
                    for infile in job.input
                )
                self._record(
                    self._metadata_path,
                    {
                        "record_format_version": RECORD_FORMAT_VERSION,
                        "code": code,
                        "rule": job.rule.name,
                        "input": input,
                        "log": log,
                        "params": params,
                        "shellcmd": shellcmd,
                        "incomplete": False,
                        "starttime": starttime,
                        "endtime": endtime,
                        "job_hash": hash(job),
                        "conda_env": conda_env,
                        "software_stack_hash": software_stack_hash,
                        "container_img_url": job.container_img_url,
                        "input_checksums": {
                            infile: checksum
                            async for infile, checksum in checksums
                            if checksum is not None
                        },
                    },
                    f,
                )
        # remove incomplete marker only after creation of metadata record.
        # otherwise the job starttime will be missing.
        self._remove_incomplete_marker(job)

    def cleanup(self, job):
        for f in job.output:
            self.cleanup_metadata(f)

    async def incomplete(self, job):
        if self._incomplete_cache is None:
            self._cache_incomplete_folder()

        if self._incomplete_cache is False:  # cache deactivated

            def marked_incomplete(f):
                return self._exists_record(self._incomplete_path, f)

        else:

            def marked_incomplete(f):
                rec_path = self._record_path(self._incomplete_path, f)
                return rec_path in self._incomplete_cache

        async def is_incomplete(f):
            exists = await f.exists()
            marked = marked_incomplete(f)
            return f if exists and marked else None

        async with asyncio.TaskGroup() as tg:
            tasks = [tg.create_task(is_incomplete(f)) for f in job.output]

        return [task.result() for task in tasks]

    def _cache_incomplete_folder(self):
        self._incomplete_cache = {
            os.path.join(path, f)
            for path, dirnames, filenames in os.walk(self._incomplete_path)
            for f in filenames
        }

    def external_jobids(self, job):
        return list(
            set(
                self._read_record(self._incomplete_path, f).get("external_jobid", None)
                for f in job.output
            )
        )

    def has_metadata(self, job: Job) -> bool:
        return all(self.metadata(path) for path in job.output)

    def has_outdated_metadata(self, job: Job) -> bool:
        return any(
            self.metadata(path).get("record_format_version", 0) < RECORD_FORMAT_VERSION
            for path in job.output
        )

    def metadata(self, path):
        return self._read_record(self._metadata_path, path)

    def rule(self, path):
        return self.metadata(path).get("rule")

    def input(self, path):
        return self.metadata(path).get("input")

    def log(self, path):
        return self.metadata(path).get("log")

    def shellcmd(self, path):
        return self.metadata(path).get("shellcmd")

    def params(self, path):
        return self.metadata(path).get("params")

    def code(self, path):
        return self.metadata(path).get("code")

    def record_format_version(self, path):
        return self.metadata(path).get("record_format_version")

    def conda_env(self, path):
        return self.metadata(path).get("conda_env")

    def container_img_url(self, path):
        return self.metadata(path).get("container_img_url")

    def input_checksums(self, job, input_path):
        """Return all checksums of the given input file
        recorded for the output of the given job.
        """
        return set(
            self.metadata(output_path).get("input_checksums", {}).get(input_path)
            for output_path in job.output
        )

    def code_changed(self, job, file=None):
        """Yields output files with changed code or bool if file given."""
        return _bool_or_gen(self._code_changed, job, file=file)

    def input_changed(self, job, file=None):
        """Yields output files with changed input or bool if file given."""
        return _bool_or_gen(self._input_changed, job, file=file)

    def params_changed(self, job, file=None):
        """Yields output files with changed params or bool if file given."""
        files = [file] if file is not None else job.output

        changes = NO_PARAMS_CHANGE

        new = set(self._params(job))

        for outfile in files:
            fmt_version = self.record_format_version(outfile)
            if fmt_version is None or fmt_version < 6:
                # no reliable params stored (version 4 refactored params storage
                # and version 6 fixed a bug in determination of whether params are
                # derived from e.g. input or output files). If they are,
                # there is the risk to store storage paths here. Derived param
                # changes will also be captured by input changes.
                continue
            recorded = self.params(outfile)
            if recorded is not None:
                old = set(recorded)
                changes |= ParamsChange(
                    only_old=old - new, only_new=new - old, files={outfile}
                )
        return changes

    def software_stack_changed(self, job, file=None):
        """Yields output files with changed software env or bool if file given."""
        return _bool_or_gen(self._software_stack_changed, job, file=file)

    def _code_changed(self, job, file=None):
        assert file is not None
        fmt_version = self.record_format_version(file)
        if fmt_version is None or fmt_version < 3:
            # no reliable code stored
            return False
        recorded = self.code(file)
        return recorded is not None and recorded != self._code(job.rule)

    def _input_changed(self, job, file=None):
        assert file is not None
        fmt_version = self.record_format_version(file)
        if fmt_version is None or fmt_version < 4:
            # no reliable input stored
            return False
        recorded = self.input(file)
        return recorded is not None and recorded != self._input(job)

    def _software_stack_changed(self, job, file=None):
        assert file is not None
        fmt_version = self.record_format_version(file)
        if fmt_version is None or fmt_version < 5:
            # no reliable software stack hash stored (previous storage ignored pin files
            # and aux deploy files of conda envs as well as env modules)
            return False

        recorded = self.software_stack_hash(file)
        return recorded is not None and recorded != self._software_stack_hash(job)

    def software_stack_hash(self, path):
        return self.metadata(path).get("software_stack_hash")

    def _software_stack_hash(self, job):
        # TODO move code for retrieval into software deployment plugin interface once
        # available
        md5hash = hashlib.md5(usedforsecurity=False)
        if (
            DeploymentMethod.CONDA
            in self.dag.workflow.deployment_settings.deployment_method
            and job.conda_env
        ):
            md5hash.update(job.conda_env.hash.encode())
        if (
            DeploymentMethod.APPTAINER
            in self.dag.workflow.deployment_settings.deployment_method
            and job.container_img_url
        ):
            md5hash.update(job.container_img_url.encode())
        if job.env_modules:
            md5hash.update(job.env_modules.hash.encode())
        return md5hash.hexdigest()

    @contextmanager
    def noop(self, *args):
        yield

    def _b64id(self, s):
        return urlsafe_b64encode(str(s).encode()).decode()

    @lru_cache()
    def _code(self, rule):
        # We only consider shell commands for now.
        # Plain python code rules are hard to capture because the pickling of the code
        # can change with different python versions.
        # Scripts and notebooks are triggered by changes in the script mtime.
        return rule.shellcmd if rule.shellcmd is not None else None

    @lru_cache()
    def _conda_env(self, job):
        if job.conda_env:
            return b64encode(job.conda_env.content).decode()

    @lru_cache()
    def _input(self, job):
        def get_paths():
            for f in job.input:
                if f.is_storage:
                    yield f.storage_object.query
                elif is_flagged(f, "pipe"):
                    yield "<pipe>"
                elif is_flagged(f, "service"):
                    yield "<service>"
                else:
                    yield (
                        # get the true path instead of the cache path
                        get_flag_value(f, "sourcecache_entry")
                        if is_flagged(f, "sourcecache_entry")
                        else f
                    )

        return sorted(get_paths())

    @lru_cache()
    def _log(self, job):
        return sorted(job.log)

    def _serialize_param_builtin(self, value: Any):
        if (
            value is None
            or isinstance(
                value,
                (
                    int,
                    float,
                    bool,
                    str,
                    complex,
                    range,
                    list,
                    tuple,
                    dict,
                    set,
                    frozenset,
                    bytes,
                    bytearray,
                ),
            )
            and value is not TBDString
        ):
            return repr(value)
        else:
            return UNREPRESENTABLE

    def _serialize_param_pandas(self, value: Any):
        import pandas as pd

        if isinstance(value, (pd.DataFrame, pd.Series, pd.Index)):
            return repr(pd.util.hash_pandas_object(value).tolist())
        return self._serialize_param_builtin(value)

    @lru_cache()
    def _params(self, job: Job):
        return sorted(
            filter(
                lambda p: p is not UNREPRESENTABLE,
                (self._serialize_param(value) for value in job.non_derived_params),
            )
        )

    @lru_cache()
    def _output(self, job):
        return sorted(job.output)

    def _record(
        self,
        subject,
        json_value,
        id,
        mode=stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IWGRP,
    ):
        recpath = self._record_path(subject, id)
        try:
            recpath_stat = os.stat(recpath)
        except FileNotFoundError:
            recpath_stat = None
            recdir = os.path.dirname(recpath)
            os.makedirs(recdir, exist_ok=True)

        with open(recpath, "w") as recfile:
            json.dump(json_value, recfile)

        # ensure read and write permissions for user and group if they don't include the required mode
        if recpath_stat is None:
            os.chmod(recpath, mode)
        else:
            existing = stat.S_IMODE(recpath_stat.st_mode)
            new_mode = existing | mode
            if existing != new_mode:
                os.chmod(recpath, new_mode)

    def _delete_record(self, subject, id):
        try:
            recpath = self._record_path(subject, id)
            os.remove(recpath)
            recdirs = os.path.relpath(os.path.dirname(recpath), start=subject)
            if recdirs != ".":
                os.removedirs(recdirs)
            return True
        except OSError as e:
            if e.errno != 2:
                # not missing
                raise e
            else:
                # file is missing, report failure
                return False

    @lru_cache()
    def _read_record_cached(self, subject, id):
        return self._read_record_uncached(subject, id)

    def _read_record_uncached(self, subject, id):
        if not self._exists_record(subject, id):
            return dict()
        path = self._record_path(subject, id)
        with open(path, "r") as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                # Since record writing cannot be reliably made atomic (some network
                # filesystems, e.g. gluster have issues with writing to a temp file
                # and then moving) we ignore corrupted or incompletely written records
                # here.
                # They can only occur if a snakemake process is running and one does a
                # dry-run (or intentionally disables locking) at the same time.
                logger.warning(
                    f"Ignore corrupted or currently written metadata record {path}."
                )
                return dict()

    def _exists_record(self, subject, id):
        return os.path.exists(self._record_path(subject, id))

    def _locks(self, type):
        return (
            f
            for f, _ in listfiles(
                os.path.join(self._lockdir, f"{{n,[0-9]+}}.{type}.lock")
            )
            if not os.path.isdir(f)
        )

    def _lock(self, files, type):
        for i in count(0):
            lockfile = os.path.join(self._lockdir, f"{i}.{type}.lock")
            if not os.path.exists(lockfile):
                self._lockfile[type] = lockfile
                with open(lockfile, "w") as lock:
                    print(*files, sep="\n", file=lock)
                return

    def _fetch_max_len(self, subject):
        if self._max_len is None:
            self._max_len = os.pathconf(subject, "PC_NAME_MAX")
        return self._max_len

    def _record_path(self, subject, id: _IOFile):
        assert isinstance(id, _IOFile)
        id = id.storage_object.query if id.is_storage else id

        max_len = (
            self._fetch_max_len(subject) if os.name == "posix" else 255
        )  # maximum NTFS and FAT32 filename length
        if max_len == 0:
            max_len = 255

        b64id = self._b64id(id)
        # split into chunks of proper length
        b64id = [b64id[i : i + max_len - 1] for i in range(0, len(b64id), max_len - 1)]
        # prepend dirs with @ (does not occur in b64) to avoid conflict with b64-named files in the same dir
        b64id = ["@" + s for s in b64id[:-1]] + [b64id[-1]]
        path = os.path.join(subject, *b64id)
        return path

    def all_outputfiles(self):
        # we only look at output files that will be updated
        return jobfiles(self.dag.needrun_jobs(), "output")

    def all_inputfiles(self):
        # we consider all input files, also of not running jobs
        return jobfiles(self.dag.jobs, "input")

    def deactivate_cache(self):
        self._read_record_cached.cache_clear()
        self._read_record = self._read_record_uncached
        self._incomplete_cache = False

    @property
    def _iocache_filename(self):
        return os.path.join(self.iocache_path, "latest.pkl")

    def save_iocache(self):
        filepath = self._iocache_filename
        with open(filepath, "wb") as handle:
            self.dag.workflow.iocache.save(handle)

    def load_iocache(self):
        filepath = self._iocache_filename
        if os.path.exists(filepath):
            logger.info("Loading trusted IOCache from latest dry-run.")
            with open(filepath, "rb") as handle:
                self.dag.workflow.iocache = IOCache.load(handle)

    def drop_iocache(self):
        filepath = self._iocache_filename
        if os.path.exists(filepath):
            os.remove(filepath)


def _bool_or_gen(func, job, file=None):
    if file is None:
        return (f for f in job.output if func(job, file=f))
    else:
        return func(job, file=file)


@dataclass
class ParamsChange:
    only_old: Set[Any] = field(default_factory=set)
    only_new: Set[Any] = field(default_factory=set)
    files: Set[str] = field(default_factory=set)

    def __post_init__(self):
        if not self:
            self.files = set()

    def __bool__(self):
        return bool(self.only_old or self.only_new)

    def __or__(self, other):
        if not self:
            return other
        if not other:
            return self
        return ParamsChange(
            only_old=self.only_old | other.only_old,
            only_new=self.only_new | other.only_new,
            files=self.files | other.files,
        )

    def __iter__(self):
        return iter(self.files)

    def __str__(self):
        if not self:
            return "No params change"
        else:

            def fmt_set(s, label):
                if s:
                    return f"{label}: {','.join(s)}"
                else:
                    return f"{label}: <nothing exclusive>"

            return (
                "Union of exclusive params before and now across all output: "
                f"{fmt_set(self.only_old, 'before')} "
                f"{fmt_set(self.only_new, 'now')} "
            )


NO_PARAMS_CHANGE = ParamsChange()



================================================
FILE: src/snakemake/profiles.py
================================================
from collections import OrderedDict
import os
from pathlib import Path
from configargparse import YAMLConfigFileParser, ConfigFileParserException

from snakemake_interface_common.exceptions import WorkflowError


class ProfileConfigFileParser(YAMLConfigFileParser):
    def parse(self, stream):
        # taken from configargparse and modified to add special handling for key-value pairs
        import yte

        profile_dir = Path(stream.name).parent
        try:
            parsed_obj = yte.process_yaml(stream, require_use_yte=True)
        except Exception as e:
            raise ConfigFileParserException("Couldn't parse config file: %s" % e)

        if not isinstance(parsed_obj, dict):
            raise ConfigFileParserException(
                "The config file doesn't appear to "
                "contain 'key: value' pairs (aka. a YAML mapping). "
                "yaml.load('%s') returned type '%s' instead of 'dict'."
                % (getattr(stream, "name", "stream"), type(parsed_obj).__name__)
            )

        def format_one_level_dict(d):
            return [f"{key}={os.path.expandvars(str(val))}" for key, val in d.items()]

        def format_two_level_dict(d, item: str):
            if not all(isinstance(val, dict) for val in d.values()):
                raise WorkflowError(
                    f"Invalid {item} format in profile. Expected two-level mapping, got {d}"
                )
            return [
                f"{key}:{key2}={os.path.expandvars(str(val2))}"
                for key, val in d.items()
                for key2, val2 in val.items()
            ]

        result = OrderedDict()
        for key, value in parsed_obj.items():
            if isinstance(value, list):
                result[key] = value
            elif value is None:
                continue
            else:
                # special handling for simplified pure YAML syntax for key-value CLI arguments like --resources
                if isinstance(value, (dict, OrderedDict)):
                    if key in (
                        "set-threads",
                        "resources",
                        "set-scatter",
                        "batch",
                        "set-resource-scopes",
                        "default-resources",
                        "config",
                        "groups",
                        "group-components",
                        "consider-ancient",
                    ):
                        result[key] = format_one_level_dict(value)
                    elif key == "set-resources":
                        result[key] = format_two_level_dict(value, "set-resources")
                else:
                    value = os.path.expandvars(str(value))

                    # Adjust path if it exists in the profile dir.
                    # Otherwise value is not a file or not existing in the profile dir.
                    if (profile_dir / value).exists():
                        value = str(profile_dir / value)

                    result[key] = value

        return result



================================================
FILE: src/snakemake/report.css
================================================
/**
Credits for the colors and font selection go to the Twitter Bootstrap framework.
*/


body {
    color: rgb(51, 51, 51);
    font-size: 10pt;
    padding-top: 10px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
}

h1 {
    font-size: 150%;
}

h2 {
    font-size: 140%;
}

h3 {
    font-size: 130%;
}

h4 {
    font-size: 120%;
}

h5 {
    font-size: 110%;
}

h6 {
    font-size: 100%;
}

div#attachments {
    display: inline-block;
    color: gray;
    border-width: 1px;
    border-style: solid;
    border-color: white;
    border-radius: 4px 4px 4px 4px;
    padding: 0px;
}

div#attachments dt {
    margin-top: 2px;
    margin-bottom: 2px;
}

div#attachments dd p {
    margin-top: 2px;
    margin-bottom: 2px;
}

div#attachments :target dt {
    font-weight: bold;
}

div#attachments :target a {
    color: rgb(70, 136, 71);
}


h1.title {
    text-align: center;
    font-size: 180%;
}

div.document {
    position: relative;
    background: white;
    max-width: 800px;
    margin: auto;
    padding: 20px;
    border: 1px solid rgb(221, 221, 221);
    border-radius: 4px 4px 4px 4px;
}

div.document:after {
    content: "snakemake report";
    position: absolute;
    top: -1px;
    right: -1px;
    padding: 3px 7px;
    background-color: #f5f5f5;
    border: 1px solid rgb(221, 221, 221);
    color: #9da0a4;
    font-weight: bold;
    font-size: 12pt;
    border-radius: 0 0 0 4px;
}

div.document p {
    text-align: justify;
}

div#metadata {
    text-align: right;
}

table.docutils {
    border: none;
    border-collapse: collapse;
    border-top: 2px solid gray;
    border-bottom: 2px solid gray;
    text-align: center;
}

table.docutils th {
    border: none;
    border-top: 2px solid gray;
    border-bottom: 2px solid gray;
    padding: 5px;
}

table.docutils td {
    border: none;
    padding: 5px;
}

table.docutils th:last-child, td:last-child {
    text-align: left;
}

table.docutils th:first-child, td:first-child {
    text-align: right;
}

table.docutils th:only-child, td:only-child {
    text-align: center;
}

table.docutils.footnote {
    border: none;
    text-align: left;
}

a {
    color: rgb(0, 136, 204);
    text-decoration: none;
}

a:hover {
    color: rgb(0, 85, 128);
    text-decoration: underline;
}


div.figure {
    margin-left: 2em;
    margin-right: 2em;
}

img {
    max-width: 100%;
}

p.caption {
    font-style: italic;
}



================================================
FILE: src/snakemake/resources.py
================================================
from collections import UserDict, defaultdict
from dataclasses import dataclass
import itertools as it
import operator as op
import re
import shutil
import tempfile
import math
from typing import Any

from snakemake.exceptions import (
    ResourceScopesException,
    WorkflowError,
    is_file_not_found_error,
)
from snakemake.common.tbdstring import TBDString


@dataclass
class ParsedResource:
    orig_arg: str
    value: Any


class DefaultResources:
    defaults = {
        "mem_mb": "min(max(2*input.size_mb, 1000), 8000)",
        "disk_mb": "max(2*input.size_mb, 1000) if input else 50000",
        "tmpdir": "system_tmpdir",
    }

    bare_defaults = {"tmpdir": "system_tmpdir"}

    @classmethod
    def decode_arg(cls, arg):
        try:
            return arg.split("=", maxsplit=1)
        except ValueError:
            raise ValueError("Resources have to be defined as name=value pairs.")

    @classmethod
    def encode_arg(cls, name, value):
        return f"{name}={value}"

    def __init__(self, args=None, from_other=None, mode="full"):
        if mode == "full":
            self._args = dict(DefaultResources.defaults)
        elif mode == "bare":
            self._args = dict(DefaultResources.bare_defaults)
        else:
            raise ValueError(f"Unexpected mode for DefaultResources: {mode}")

        if from_other is not None:
            self._args = dict(from_other._args)
            self.parsed = dict(from_other.parsed)
        else:
            if args is None:
                args = []

            self._args.update(
                {name: value for name, value in map(self.decode_arg, args)}
            )

            self.parsed = dict(_cores=1, _nodes=1)
            self.parsed.update(
                parse_resources(self._args, fallback=eval_resource_expression)
            )

    def set_resource(self, name, value):
        self._args[name] = f"{value}"
        self.parsed[name] = value

    @property
    def args(self):
        return [self.encode_arg(name, value) for name, value in self._args.items()]

    def __bool__(self):
        return bool(self.parsed)


class ResourceScopes(UserDict):
    """Index of resource scopes, where each entry is 'RESOURCE': 'SCOPE'

    Each resource may be scoped as local, global, or excluded. Any resources not
    specified are considered global.
    """

    def __init__(self, *args, **kwargs):
        self.data = dict(*args, **kwargs)
        valid_scopes = {"local", "global", "excluded"}
        if set(self.data.values()) - valid_scopes:
            invalid_res = [
                res for res, scope in self.data.items() if scope not in valid_scopes
            ]
            invalid_pairs = {res: self.data[res] for res in invalid_res}

            # For now, we don't want excluded in the documentation
            raise ResourceScopesException(
                "Invalid resource scopes: entries must be defined as RESOURCE=SCOPE "
                "pairs, where SCOPE is either 'local' or 'global'",
                invalid_pairs,
            )

    @classmethod
    def defaults(cls):
        return cls(mem_mb="local", disk_mb="local", runtime="excluded")

    @property
    def locals(self):
        """Resources are not tallied by the global scheduler when submitting jobs

        Each submitted job or group gets its own pool of the resource, as
        specified under --resources.


        Returns
        -------
        set
        """
        return set(res for res, scope in self.data.items() if scope == "local")

    @property
    def globals(self):
        """Resources tallied across all job and group submissions.

        Returns
        -------
        set
        """
        return set(res for res, scope in self.data.items() if scope == "global")

    @property
    def excluded(self):
        """Resources not submitted to cluster jobs

        These resources are used exclusively by the global scheduler. The primary case
        is for additive resources in GroupJobs such as runtime, which would not be
        properly handled by the scheduler in the sub-snakemake instance. This scope is
        not currently intended for use by end-users and is thus not documented

        Returns
        -------
        set
        """
        return set(res for res, scope in self.data.items() if scope == "excluded")


class GroupResources:
    @classmethod
    def basic_layered(
        cls,
        toposorted_jobs,
        constraints,
        run_local,
        additive_resources=None,
        sortby=None,
    ):
        """Basic implementation of group job resources calculation

        Each toposort level is individually sorted into a series of layers, where each
        layer fits within the constraints. Resource constraints represent a "width" into
        which the layer must fit. For instance, with a mem_mb constraint of 5G, all the
        jobs in a single layer must together not consume more than 5G of memory. Any
        jobs that would exceed this constraint are pushed into a new layer. The overall
        width for the entire group job is equal to the width of the widest layer.

        Additive resources (by default, "runtime") represent the "height" of the layer.
        They are not directly constrained, but their value will be determined by the
        sorting of jobs based on other constraints. Each layer's height is equal to the
        height of its tallest job. For instance, a layer containing a 3hr job will have
        a runtime height of 3 hr. The total height of the entire group job will be the
        sum of the heights of all the layers.

        Note that both height and width are multidimensial, so layer widths will be
        calculated with respect to every constraint created by the user.

        In this implementation, there is no mixing of layers, which may lead to "voids".
        For instance, a layer containing a tall, skinny job of 3hr length and 1G mem
        combined with a short, fat job of 10min length and 20G memory would have a 2hr
        50min period where 19G of memory are not used. In practice, this void will be
        filled by the actual snakemake subprocess, which performs real-time scheduling
        of jobs as resources become available.  But it may lead to overestimation of
        resource requirements.

        To help mitigate against voids, this implementation sorts the jobs within a
        toposort level before assignment to layers. Jobs are first sorted by their
        overall width relative to the available constraints. So the fattest jobs will
        grouped together on the same layer. Jobs are then sorted by the resources
        specified in ``sortby``, by default "runtime". So jobs of similar length will be
        grouped on the same layer.

        Users can help mitigate against voids by grouping jobs of similar resource
        dimensions.  Eclectic groups of various runtimes and resource consumptions will
        not be estimated as efficiently as groups of homogeneous consumptions.

        Parameters
        ----------
        toposorted_jobs : list of lists of jobs
            Jobs sorted into toposort levels: the jobs in each level only depend on jobs
            in previous levels.
        constraints : dict of str -> int
            Upper limit of resource allowed. Resources without constraints will be
            treated as infinite
        run_local : bool
            True if the group is being run in the local process, rather than being
            submitted. Relevant for Pipe groups and Service groups
        additive_resources : list of str, optional
            Resources to be treated as the "height" of each layer, i.e. to be summed
            across layers.
        sortby : list of str, optional
            Resources by which to sort jobs prior to layer assignment.

        Returns
        -------
        Dict of str -> int,str
            Total resource requirements of the group job

        Raises
        ------
        WorkflowError
            Raised if an individual job requires more resources than the constraints
            allow (chiefly relevant for pipe groups)
        """
        additive_resources = (
            additive_resources if additive_resources is not None else ["runtime"]
        )
        sortby = sortby if sortby is not None else ["runtime"]

        total_resources = defaultdict(int)
        total_resources["_nodes"] = 1
        blocks = []
        # iterate over siblings that can be executed in parallel
        for siblings in toposorted_jobs:
            # Total resource requirements for this toposort layer
            block_resources = {}

            job_resources = []
            pipe_resources = defaultdict(list)
            for job in siblings:
                # Get resources, filtering out FileNotFoundErrors. List items will
                # be job resources objects with (resource: value)
                # [
                #   { "runtime": 5, "threads": 2, "tmpdir": "/tmp" },
                #   { "runtime": 15, "tmpdir": "/tmp"},
                #   ...
                # ]
                # Pipe jobs and regular jobs are put in separate lists.
                try:
                    # Remove any TBDStrings from values. These will typically arise
                    # here because the default mem_mb and disk_mb are based off of
                    # input file size, and intermediate files in the group are not yet
                    # generated. Thus rules consuming such files must explicitly
                    # specify their resources
                    res = {
                        k: res
                        for k, res, in job.resources.items()
                        if not isinstance(res, TBDString)
                    }
                    if job.pipe_group:
                        pipe_resources[job.pipe_group].append(res)
                    else:
                        job_resources.append(res)
                except FileNotFoundError:
                    # Skip job if resource evaluation leads to a file not found error.
                    # This will be caused by an inner job, which needs files created by
                    # the same group. All we can do is to ignore such jobs for now.
                    continue

            # Jobs in pipe groups must be run simultaneously, so we merge all the
            # resources of each pipe group into one big "job". Resources are combined
            # as "intralayer", so additives (like runtime) are maxed, and the rest are
            # summed
            for pipe in pipe_resources.values():
                job_resources.append(
                    cls._merge_resource_dict(
                        pipe,
                        methods={res: max for res in additive_resources},
                        default_method=sum,
                    )
                )

            # Set of resource types requested in at least one job
            resource_types = list(set(it.chain(*job_resources)))
            int_resources = {}
            # Sort all integer resources in job_resources into int_resources. Resources
            # defined as a string are placed immediately into block_resources.
            for res in resource_types:
                if res == "_nodes":
                    continue

                values = [resources.get(res, 0) for resources in job_resources]

                if cls._is_string_resource(res, values):
                    block_resources[res] = values[0]
                else:
                    int_resources[res] = values

            # Collect values from global_resources to use as constraints.
            sorted_constraints = {
                name: constraints.get(name, None) for name in int_resources
            }

            # For now, we are unable to handle a constraint on runtime, so ignore.
            # Jobs requesting too much runtime will still get flagged by the
            # scheduler
            for res in additive_resources:
                if res in sorted_constraints:
                    sorted_constraints[res] = None

            # Get layers
            try:
                layers = cls._get_layers(
                    int_resources, sorted_constraints.values(), sortby
                )
            except WorkflowError as err:
                raise cls._get_saturated_resource_error(additive_resources, err.args[0])

            # Merge jobs within layers
            intralayer_merge_methods = [
                max if res in additive_resources else sum for res in int_resources
            ]
            merged = [
                cls._merge_resource_layer(layer, intralayer_merge_methods)
                for layer in layers
            ]

            # Combine layers
            interlayer_merge_methods = [
                sum if res in additive_resources else max for res in int_resources
            ]
            combined = cls._merge_resource_layer(merged, interlayer_merge_methods)

            # Reassign the combined values from each layer to their resource names
            block_resources.update(dict(zip(int_resources, combined)))

            blocks.append(block_resources)

        if run_local:
            return {**cls._merge_resource_dict(blocks, default_method=sum), "_nodes": 1}

        return {
            **cls._merge_resource_dict(
                blocks,
                default_method=max,
                methods={res: sum for res in additive_resources},
            ),
            "_nodes": 1,
        }

    @classmethod
    def _get_saturated_resource_error(cls, additive_resources, excess_resources):
        isare = "is" if len(additive_resources) == 1 else "are"
        additive_clause = (
            (f", except for {additive_resources}, which {isare} calculated via max(). ")
            if additive_resources
            else ". "
        )
        return WorkflowError(
            "Not enough resources were provided. This error is typically "
            "caused by a Pipe group requiring too many resources. Note "
            "that resources are summed across every member of the pipe "
            f"group{additive_clause}"
            f"Excess Resources:\n{excess_resources}"
        )

    @classmethod
    def _is_string_resource(cls, name, values):
        # If any one of the values provided for a resource is not an int, we
        # can't process it in any way. So we constrain all such resource to be
        # the same
        if all([isinstance(val, int) for val in values]):
            return False
        else:
            unique = set(values)
            if len(unique) > 1:
                raise WorkflowError(
                    "Resource {name} is a string but not all group jobs require the "
                    "same value. Observed values: {values}.".format(
                        name=name, values=unique
                    )
                )
            return True

    @classmethod
    def _merge_resource_dict(cls, resources, skip=[], methods={}, default_method=max):
        grouped = {}
        for job in resources:
            # Wrap every value in job with a list so that lists can be merged later
            job_l = {k: [v] for k, v in job.items()}

            # Merge two dicts together, merging key-values found in both into a
            # list. Code adapted from
            # https://stackoverflow.com/a/11012181/16980632
            grouped = {
                **grouped,
                **job_l,
                **{k: grouped[k] + job_l[k] for k in grouped.keys() & job_l},
            }

        ret = {}
        for res, values in grouped.items():
            if res in skip:
                continue

            if cls._is_string_resource(res, values):
                ret[res] = values[0]
            elif res in methods:
                ret[res] = methods[res](values)
            else:
                ret[res] = default_method(values)
        return ret

    @classmethod
    def _merge_resource_layer(cls, resources, methods):
        """
        Sum or max across all resource types within a layer, similar to
        summing along axis 0 in numpy:
        [
          ( 3 ^ , 4 ^ , 1 ),
          ( 2 | , 1 | , 6 ),
          ( 1 | , 4 | , 0 ),
        ]
        The method for each column is specified in methods, which should be an array
        with one index per column
        """
        return [method(r) for method, r in zip(methods, zip(*resources))]

    @staticmethod
    def _check_constraint(resources, constraints):
        sums = [sum(res) for res in zip(*resources)]
        for s, constraint in zip(sums, constraints):
            if constraint:
                layers, mod = divmod(s, constraint)
            else:
                layers = 1
                mod = 0

            # If mod not 0, we add 1 to the number of layers. We then subtract
            # 1, so that if everything fits within the constraint we have 0,
            # otherwise, some number higher than 0. Finally, we convert to bool.
            # If the result is 0 or negative, it fits. If greater, it doesn't
            # fit so we return False
            if bool(max(0, layers + int(bool(mod)) - 1)):
                return False
        return True

    @classmethod
    def _get_layers(cls, resources, constraints, sortby=None):
        """Calculate required consecutive job layers.

        Layers are used to keep resource requirements within given
        constraint. For instance, if the jobs together require 50 threads,
        but only 12 are available, we will use 5 layers. If multiple constraints are
        used, all will be considered and met. Any constraints given as None will be
        treated as infinite.
        """

        # Calculates the ratio of resource to constraint. E.g, if the resource is 12
        #  cores, and the constraint is 16, it will return 0.75. This is done for
        # every resource type in the group, returning the result in a list
        def _proportion(group):
            return [r / c if c else 0 for r, c in zip(group, constraints)]

        # Return the highest _proportion item in the list
        def _highest_proportion(group):
            return max(_proportion(group))

        rows = [[]]

        # By zipping, we combine the vals into tuples based on job, 1 tuple per
        # job: [ (val1, 1_val1, 2_val1), ...]. In each tuple, the resources
        # will remain in the same order as the original dict, so their identity
        # can be extracted later.
        resource_groups = zip(*resources.values())

        # Sort by _proportion highest to lowest
        pre_sorted = sorted(resource_groups, key=_highest_proportion, reverse=True)

        # If a key is provided (e.g. runtime), we sort again by that key from
        # highest to lowest
        for res in sortby or []:
            if res in resources:
                # Find the position of the key in the job tuple
                i = list(resources).index(res)
                pre_sorted = sorted(pre_sorted, key=op.itemgetter(i), reverse=True)

        for group in pre_sorted:
            appended = False

            # Check each row for space, starting with the first.
            for row in rows:
                if not appended and cls._check_constraint(row + [group], constraints):
                    row.append(group)
                    appended = True

            # If the final "row" in rows has something, we add a new empty
            # row. That way, we guarantee we have a row with space
            if len(rows[-1]) > 0:
                rows.append([])

            # If not appended, that means a rule required more resource
            # than allowed by the constraint. This should only be possible for pipe
            # jobs, which must be run simultaneously.
            if not appended:
                too_high = []
                for i, val in enumerate(_proportion(group)):
                    if val > 1:
                        too_high.append(
                            (list(resources)[i], group[i], list(constraints)[i])
                        )

                error_text = [
                    f"\t{res}: {amount}/{constraint}"
                    for res, amount, constraint in too_high
                ]
                raise WorkflowError("\n".join(error_text))

        # Remove final empty row. (The above loop ends each cycle by ensuring
        # there's an empty row)
        rows.pop()
        return rows


def eval_resource_expression(val, threads_arg=True):
    def generic_callable(val, threads_arg, **kwargs):
        args = {
            "input": kwargs["input"],
            "attempt": kwargs["attempt"],
            "system_tmpdir": tempfile.gettempdir(),
            "shutil": shutil,
        }
        if threads_arg:
            args["threads"] = kwargs["threads"]
        try:
            value = eval(
                val,
                args,
            )
        # Triggers for string arguments like n1-standard-4
        except (NameError, SyntaxError):
            return val
        except Exception as e:
            if is_humanfriendly_resource(val):
                return val
            if not is_file_not_found_error(e, kwargs["input"]):
                # Missing input files are handled by the caller
                raise WorkflowError(
                    "Failed to evaluate resources value "
                    f"'{val}'.\n"
                    "    String arguments may need additional "
                    "quoting. E.g.: --default-resources "
                    "\"tmpdir='/home/user/tmp'\" or "
                    "--set-resources \"somerule:someresource='--nice=100'\". "
                    "This also holds for setting resources inside of a profile, where "
                    "you might have to enclose them in single and double quotes, "
                    "i.e. someresource: \"'--nice=100'\".",
                    e,
                )
            raise e
        return value

    if threads_arg:

        def callable(wildcards, input, attempt, threads, rulename):
            return generic_callable(
                val,
                threads_arg=threads_arg,
                wildcards=wildcards,
                input=input,
                attempt=attempt,
                threads=threads,
                rulename=rulename,
            )

    else:

        def callable(wildcards, input, attempt, rulename):
            return generic_callable(
                val,
                threads_arg=threads_arg,
                wildcards=wildcards,
                input=input,
                attempt=attempt,
                rulename=rulename,
            )

    return callable


def parse_resources(resources_args, fallback=None):
    """Parse resources from args."""
    resources = dict()

    if resources_args is not None:
        valid = re.compile(r"[a-zA-Z_]\w*$")

        if isinstance(resources_args, list):
            resources_args = map(DefaultResources.decode_arg, resources_args)
        else:
            resources_args = resources_args.items()

        for res, val in resources_args:
            if not valid.match(res):
                raise ValueError(
                    "Resource definition must start with a valid identifier, but found "
                    "{}.".format(res)
                )

            try:
                val = int(val)
            except ValueError:
                if fallback is not None:
                    val = fallback(val)
                else:
                    raise ValueError(
                        "Resource definition must contain an integer, string or python expression after the identifier."
                    )
            if res == "_cores":
                raise ValueError(
                    "Resource _cores is already defined internally. Use a different "
                    "name."
                )
            resources[res] = val
    return resources


def infer_resources(name, value, resources: dict):
    """Infer resources from a given one, if possible."""
    from humanfriendly import parse_size, parse_timespan, InvalidTimespan, InvalidSize

    if isinstance(value, str):
        value = value.strip("'\"")

    if (
        (name == "mem" or name == "disk")
        and isinstance(value, str)
        and not isinstance(value, TBDString)
    ):
        inferred_name = f"{name}_mb"
        try:
            in_bytes = parse_size(value)
        except InvalidSize:
            raise WorkflowError(
                f"Cannot parse mem or disk value into size in MB for setting {inferred_name} resource: {value}"
            )
        resources[inferred_name] = max(int(math.ceil(in_bytes / 1e6)), 1)
    elif (
        name == "runtime"
        and isinstance(value, str)
        and not isinstance(value, TBDString)
    ):
        try:
            parsed = max(int(round(parse_timespan(value) / 60)), 1)
        except InvalidTimespan:
            raise WorkflowError(
                f"Cannot parse runtime value into minutes for setting runtime resource: {value}"
            )
        resources["runtime"] = parsed


def is_humanfriendly_resource(value):
    from humanfriendly import parse_size, parse_timespan, InvalidTimespan, InvalidSize

    try:
        parse_size(value)
        return True
    except InvalidSize:
        pass

    try:
        parse_timespan(value)
        return True
    except InvalidTimespan:
        pass

    return False



================================================
FILE: src/snakemake/ruleinfo.py
================================================
__authors__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from collections import namedtuple
from copy import copy
from snakemake.logging import logger


InOutput = namedtuple("InOutput", ["paths", "kwpaths", "modifier"])


class RuleInfo:
    ref_attributes = {"func", "path_modifier"}

    def __init__(self, func=None):
        self.func = func
        self.shellcmd = None
        self.name = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.conda_env = None
        self.container_img = None
        self.is_containerized = False
        self.env_modules = None
        self.wildcard_constraints = None
        self.threads = None
        self.shadow_depth = None
        self.resources = None
        self.priority = None
        self.retries = None
        self.log = None
        self.docstring = None
        self.group = None
        self.script = None
        self.notebook = None
        self.wrapper = None
        self.template_engine = None
        self.cwl = None
        self.cache = False
        self.path_modifier = None
        self.handover = False
        self.default_target = False
        self.localrule = False

    def __copy__(self):
        """Return a copy of this ruleinfo."""
        ruleinfo = RuleInfo(self.func)
        for attribute in self.__dict__:
            if attribute in self.ref_attributes:
                setattr(ruleinfo, attribute, getattr(self, attribute))
            else:
                # shallow copies are enough
                setattr(ruleinfo, attribute, copy(getattr(self, attribute)))
        return ruleinfo

    def apply_modifier(
        self,
        modifier,
        rulename,
        prefix_replacables={"input", "output", "log", "benchmark"},
    ):
        """Update this ruleinfo with the given one (used for 'use rule' overrides)."""
        path_modifier = modifier.path_modifier
        skips = set()

        if modifier.ruleinfo_overwrite:
            for key, value in modifier.ruleinfo_overwrite.__dict__.items():
                if key != "func" and value is not None:
                    if key == "params" and self.params is not None:
                        # if positional arguments are used after the 'with' statement
                        # overwrite all positional arguments of the original rule
                        # for keyword arguments replace only the ones defined after 'with'
                        original_positional, original_keyword = self.__dict__["params"]
                        modifier_positional, modifier_keyword = value
                        positional = original_positional
                        if modifier_positional:
                            if original_positional and (
                                len(original_positional) != len(modifier_positional)
                            ):
                                logger.warning(
                                    f"Overwriting positional arguments {original_positional} "
                                    f"with {modifier_positional} in rule {rulename}"
                                )
                            positional = modifier_positional
                        self.__dict__[key] = (
                            positional,
                            {**original_keyword, **modifier_keyword},
                        )
                    else:
                        self.__dict__[key] = value
                    if key in prefix_replacables:
                        skips.add(key)

        if path_modifier.modifies_prefixes and skips:
            # use a specialized copy of the path modifier
            path_modifier = copy(path_modifier)
            path_modifier.skip_properties = skips
        # add path modifier
        self.path_modifier = path_modifier

        # modify wrapper if requested
        self.wrapper = modifier.modify_wrapper_uri(self.wrapper)



================================================
FILE: src/snakemake/rules.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import copy
import os
import types
import typing
from snakemake.path_modifier import PATH_MODIFIER_FLAG
import collections
from pathlib import Path
from itertools import chain
from functools import partial

try:
    import re._constants as sre_constants
except ImportError:  # python < 3.11
    import sre_constants

from snakemake_interface_executor_plugins.settings import ExecMode

from snakemake.io import (
    IOFile,
    _IOFile,
    Namedlist,
    AnnotatedString,
    contains_wildcard,
    contains_wildcard_constraints,
    get_flag_store_keys,
    is_multiext_items,
    remove_flag,
    update_wildcard_constraints,
    flag,
    get_flag_value,
    expand,
    InputFiles,
    OutputFiles,
    Wildcards,
    Params,
    Log,
    Resources,
    strip_wildcard_constraints,
    apply_wildcards,
    is_flagged,
    flag,
    is_callable,
    ReportObject,
)
from snakemake.exceptions import (
    InputOpenException,
    RuleException,
    IOFileException,
    WildcardError,
    InputFunctionException,
    WorkflowError,
    IncompleteCheckpointException,
    is_file_not_found_error,
)
from snakemake.logging import logger
from snakemake.common import (
    ON_WINDOWS,
    get_function_params,
    get_input_function_aux_params,
    mb_to_mib,
)
from snakemake.common.tbdstring import TBDString
from snakemake.resources import infer_resources
from snakemake_interface_common.utils import not_iterable, lazy_property
from snakemake_interface_common.rules import RuleInterface


_NOT_CACHED = object()


class Rule(RuleInterface):
    def __init__(self, name, workflow, lineno=None, snakefile=None):
        """
        Create a rule

        Arguments
        name -- the name of the rule
        """
        self._name = name
        self.workflow = workflow
        self.docstring = None
        self.message = None
        self._input = InputFiles()
        self._output = OutputFiles()
        self._params = Params()
        self._wildcard_constraints = dict()
        self.dependencies = dict()
        self.temp_output = set()
        self.protected_output = set()
        self.touch_output = set()
        self.shadow_depth = None
        self.resources = None
        self.priority = 0
        self._log = Log()
        self._benchmark = None
        self._conda_env = None
        self._expanded_conda_env = _NOT_CACHED
        self._container_img = None
        self.is_containerized = False
        self.env_modules = None
        self._group = None
        self._wildcard_names = None
        self._lineno = lineno
        self._snakefile = snakefile
        self.run_func = None
        self.shellcmd = None
        self.script = None
        self.notebook = None
        self.wrapper = None
        self.template_engine = None
        self.cwl = None
        self.norun = False
        self.is_handover = False
        self.is_checkpoint = False
        self._restart_times = 0
        self.basedir = None
        self.input_modifier = None
        self.output_modifier = None
        self.log_modifier = None
        self.benchmark_modifier = None
        self.ruleinfo = None
        self.module_globals = None

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, name):
        self._name = name

    @property
    def lineno(self):
        return self._lineno

    @property
    def snakefile(self):
        return self._snakefile

    @property
    def restart_times(self):
        if self.workflow.remote_execution_settings.preemptible_rules.is_preemptible(
            self.name
        ):
            return self.workflow.remote_execution_settings.preemptible_retries
        if self._restart_times is None:
            return self.workflow.execution_settings.retries
        return self._restart_times

    @restart_times.setter
    def restart_times(self, restart_times):
        self._restart_times = restart_times

    @property
    def group(self):
        if not self.workflow.non_local_exec_or_dryrun:
            return None
        else:
            overwrite_group = self.workflow.group_settings.overwrite_groups.get(
                self.name
            )
            if overwrite_group is not None:
                return overwrite_group
            return self._group

    @group.setter
    def group(self, group):
        self._group = group

    @property
    def is_shell(self):
        return self.shellcmd is not None

    @property
    def is_script(self):
        return self.script is not None

    @property
    def is_notebook(self):
        return self.notebook is not None

    @property
    def is_wrapper(self):
        return self.wrapper is not None

    @property
    def is_template_engine(self):
        return self.template_engine is not None

    @property
    def is_cwl(self):
        return self.cwl is not None

    @property
    def is_run(self):
        return not (
            self.is_shell
            or self.norun
            or self.is_script
            or self.is_notebook
            or self.is_wrapper
            or self.is_cwl
        )

    def check_caching(self):
        if self.workflow.cache_rules.get(self.name):
            if len(self.output) == 0:
                raise RuleException(
                    "Rules without output files cannot be cached.", rule=self
                )
            if len(self.output) > 1:
                prefixes = set(out.multiext_prefix for out in self.output)
                if None in prefixes or len(prefixes) > 1:
                    raise RuleException(
                        "Rules marked as eligible for caching that have with multiple "
                        "output files must define them as a single multiext() "
                        '(e.g. multiext("path/to/index", ".bwt", ".ann")). '
                        "The rationale is that multiple output files can only be unambiously resolved "
                        "if they can be distinguished by a fixed set of extensions (i.e. mime types).",
                        rule=self,
                    )

    def has_wildcards(self):
        """
        Return True if rule contains wildcards.
        """
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        if isinstance(benchmark, Path):
            benchmark = str(benchmark)
        if not callable(benchmark):
            benchmark = self.apply_path_modifier(
                benchmark, self.benchmark_modifier, property="benchmark"
            )
            benchmark = self._update_item_wildcard_constraints(benchmark)

        self._benchmark = IOFile(benchmark, rule=self)
        self.register_wildcards(self._benchmark.get_wildcard_names())

    @property
    def conda_env(self):
        return self._conda_env

    @conda_env.setter
    def conda_env(self, conda_env):
        self._conda_env = conda_env

    @property
    def container_img(self):
        return self._container_img

    @container_img.setter
    def container_img(self, container_img):
        self._container_img = container_img

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """

        consider_ancient = self.workflow.workflow_settings.consider_ancient.get(
            self.name, frozenset()
        )
        for i, item in enumerate(input):
            if is_multiext_items(item):
                for ifile in item:
                    self._set_inoutput_item(
                        ifile,
                        name=get_flag_value(ifile, "multiext").name,
                        mark_ancient=i in consider_ancient,
                    )
            else:
                self._set_inoutput_item(
                    item,
                    mark_ancient=i in consider_ancient,
                )

        for name, item in kwinput.items():
            self._set_inoutput_item(
                item, name=name, mark_ancient=name in consider_ancient
            )

    @property
    def output(self):
        return self._output

    def products(self, include_logfiles=True):
        products = [self.output]
        if include_logfiles:
            products.append(self.log)
        if self.benchmark:
            products.append([self.benchmark])
        return chain(*products)

    def get_some_product(self):
        for product in self.products():
            return product
        return None

    def has_products(self):
        return self.get_some_product() is not None

    def register_wildcards(self, wildcard_names):
        if self._wildcard_names is None:
            self._wildcard_names = wildcard_names
        else:
            if self.wildcard_names != wildcard_names:
                raise RuleException(
                    "Not all output, log and benchmark files of "
                    "rule {} contain the same wildcards. "
                    "This is crucial though, in order to "
                    "avoid that two or more jobs write to the "
                    "same file.".format(self.name),
                    rule=self,
                )

    @property
    def wildcard_names(self):
        if self._wildcard_names is None:
            return set()
        return self._wildcard_names

    def set_output(self, *output, **kwoutput):
        """
        Add a list of output files. Recursive lists are flattened.

        After creating the output files, they are checked for duplicates.

        Arguments
        output -- the list of output files
        """
        for item in output:
            # Named multiext have their name set under the flag (MultiextValue), if the first one is named, all of them are named.
            # Any of the output files in item can be multiext, so we do need to check all of them.
            if is_multiext_items(item):
                for ofile in item:
                    self._set_inoutput_item(
                        ofile,
                        name=get_flag_value(ofile, "multiext").name,
                        output=True,
                    )
            else:
                self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            self.register_wildcards(item.get_wildcard_names())
        # Check output file name list for duplicates
        self.check_output_duplicates()
        self.check_caching()

    def check_output_duplicates(self):
        """Check ``Namedlist`` for duplicate entries and raise a ``WorkflowError``
        on problems. Does not raise if the entry is empty.
        """
        seen = dict()
        idx = None
        for name, value in self.output._allitems():
            if name is None:
                if idx is None:
                    idx = 0
                else:
                    idx += 1
            if value and value in seen:
                raise WorkflowError(
                    "Duplicate output file pattern in rule {}. First two "
                    "duplicate for entries {} and {}.".format(
                        self.name, seen[value], name or idx
                    )
                )
            seen[value] = name or idx

    def apply_path_modifier(self, item, path_modifier, property=None):
        assert path_modifier is not None
        apply = partial(path_modifier.modify, property=property)

        assert not callable(item)
        if isinstance(item, dict):
            return {k: apply(v) for k, v in item.items()}
        elif isinstance(item, collections.abc.Iterable) and not isinstance(item, str):
            return [apply(e) for e in item]
        else:
            return apply(item)

    def update_wildcard_constraints(self):
        for i in range(len(self.output)):
            item = self.output[i]

            newitem = None
            if item.is_storage:
                storage_object = copy.copy(item.storage_object)
                storage_object.query = self._update_item_wildcard_constraints(
                    storage_object.query
                )
                newitem = IOFile(storage_object.local_path(), rule=self)
                newitem.clone_flags(item, skip_storage_object=True)
                newitem.flags["storage_object"] = storage_object
            else:
                newitem = IOFile(
                    self._update_item_wildcard_constraints(self.output[i]), rule=self
                )
                newitem.clone_flags(item)
            self.output[i] = newitem

    def _update_item_wildcard_constraints(self, item):
        if not (self.wildcard_constraints or self.workflow.wildcard_constraints):
            return item
        try:
            return update_wildcard_constraints(
                item, self.wildcard_constraints, self.workflow.wildcard_constraints
            )
        except ValueError as e:
            raise WorkflowError(e, snakefile=self.snakefile, lineno=self.lineno)

    def _set_inoutput_item(self, item, output=False, name=None, mark_ancient=False):
        """
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- a Namedlist of either input or output items
        name     -- an optional name for the item
        """

        inoutput = self.output if output else self.input

        default_flags = (
            self.workflow.modifier.default_output_flags
            if output
            else self.workflow.modifier.default_input_flags
        )

        # Check to see if the item is a path, if so, just make it a string
        if isinstance(item, Path):
            item = str(item.as_posix())

        if isinstance(item, str):
            if ON_WINDOWS:
                if isinstance(item, (_IOFile, AnnotatedString)):
                    item = item.new_from(item.replace(os.sep, os.altsep))
                else:
                    item = item.replace(os.sep, os.altsep)

            rule_dependency = None
            if isinstance(item, _IOFile) and item.rule and item in item.rule.output:
                rule_dependency = item.rule

            if output:
                path_modifier = self.output_modifier
                property = "output"
            else:
                path_modifier = self.input_modifier
                property = "input"

            item = self.apply_path_modifier(item, path_modifier, property=property)

            item = default_flags.apply(item)

            for flag_name in self.workflow.storage_settings.omit_flags:
                item = remove_flag(item, flag_name)

            # Check to see that all flags are valid
            # Note that "storage", and "expand" are valid for both inputs and outputs.
            if isinstance(item, AnnotatedString):
                for item_flag in item.flags:
                    if not output and item_flag in [
                        "protected",
                        "temp",
                        "nodelocal",
                        "temporary",
                        "directory",
                        "touch",
                        "pipe",
                        "service",
                        "ensure",
                        "update",
                    ]:
                        logger.warning(
                            "The flag '{}' used in rule {} is only valid for outputs, not inputs.".format(
                                item_flag, self
                            )
                        )
                    if output and item_flag in ["ancient", "before_update"]:
                        logger.warning(
                            "The flag '{}' used in rule {} is only valid for inputs, not outputs.".format(
                                item_flag, self
                            )
                        )

            if rule_dependency is not None:
                # add the rule to the dependencies
                self.dependencies[item] = rule_dependency

            if output:
                item = self._update_item_wildcard_constraints(item)
                if self.workflow.storage_settings.all_temp:
                    # mark as temp if all output files shall be marked as temp
                    item = flag(item, "temp")
            else:
                # input
                if (
                    contains_wildcard_constraints(item)
                    and self.workflow.exec_mode != ExecMode.SUBPROCESS
                ):
                    logger.warning(
                        "Wildcard constraints in inputs are ignored. (rule: {})".format(
                            self
                        )
                    )
                if mark_ancient:
                    item = flag(item, "ancient")

            # record rule if this is an output file output
            _item = IOFile(item, rule=self)

            if is_flagged(item, "temp"):
                if output:
                    self.temp_output.add(_item)
            if is_flagged(item, "protected"):
                if output:
                    self.protected_output.add(_item)
            if is_flagged(item, "touch"):
                if output:
                    self.touch_output.add(_item)
            if is_flagged(item, "report"):
                report_obj = item.flags["report"]
                if report_obj.caption is not None:
                    r = ReportObject(
                        self.workflow.current_basedir.join(report_obj.caption),
                        report_obj.category,
                        report_obj.subcategory,
                        report_obj.labels,
                        report_obj.patterns,
                        report_obj.htmlindex,
                    )
                    item.flags["report"] = r
            inoutput.append(_item)
            if name:
                inoutput._add_name(name)
        elif callable(item):
            if output:
                raise RuleException(
                    "Only input files can be specified as functions", rule=self
                )

            item = default_flags.apply(item)

            inoutput.append(item)
            if name:
                inoutput._add_name(name)
        else:
            try:
                start = len(inoutput)
                for subitem in item:
                    self._set_inoutput_item(subitem, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput._set_name(name, start, end=len(inoutput))
            except TypeError:
                raise RuleException(
                    "Input and output files have to be specified as strings or lists of strings.",
                    rule=self,
                )

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        self.params.append(item)
        if name:
            self.params._add_name(name)

    @property
    def wildcard_constraints(self):
        return self._wildcard_constraints

    def set_wildcard_constraints(self, **kwwildcard_constraints):
        self._wildcard_constraints.update(kwwildcard_constraints)

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

        for item in self.log:
            self.register_wildcards(item.get_wildcard_names())

    def _set_log_item(self, item, name=None):
        # Pathlib compatibility
        if isinstance(item, Path):
            item = str(item)
        if isinstance(item, str) or callable(item):
            if not callable(item):
                item = self.apply_path_modifier(item, self.log_modifier, property="log")
                item = self._update_item_wildcard_constraints(item)

            self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item)
            if name:
                self.log._add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log._set_name(name, start, end=len(self.log))
            except TypeError:
                raise RuleException(
                    "Log files have to be specified as strings.", rule=self
                )

    def check_wildcards(self, wildcards):
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                "Could not resolve wildcards:\n{}".format(
                    "\n".join(self.wildcard_names)
                ),
                lineno=self.lineno,
                snakefile=self.snakefile,
            )

    def apply_input_function(
        self,
        func,
        wildcards,
        incomplete_checkpoint_func=lambda e: None,
        raw_exceptions=False,
        groupid=None,
        **aux_params,
    ):
        if isinstance(func, _IOFile):
            func = func._file.callable
        elif isinstance(func, AnnotatedString):
            func = func.callable

        if "groupid" in get_function_params(func):
            if groupid is not None:
                aux_params["groupid"] = groupid
            else:
                # Return empty list of files and incomplete marker
                # the job will be reevaluated once groupids have been determined
                return [], True

        _aux_params = get_input_function_aux_params(func, aux_params)

        # call any callables in _aux_params
        # This way, we enable to delay the evaluation of expensive
        # aux params until they are actually needed.
        for name, value in list(_aux_params.items()):
            if callable(value):
                _aux_params[name] = value()

        wildcards_arg = Wildcards(fromdict=wildcards)

        def apply_func(func):
            incomplete = False
            try:
                value = func(wildcards_arg, **_aux_params)
                if isinstance(value, types.GeneratorType):
                    # generators should be immediately collected here,
                    # otherwise we would miss any exceptions and
                    # would have to capture them again later.
                    value = list(value)
            except IncompleteCheckpointException as e:
                value = incomplete_checkpoint_func(e)
                incomplete = True
            except InputOpenException as e:
                e.rule = self
                raise e
            except Exception as e:
                if "input" in aux_params and is_file_not_found_error(
                    e, aux_params["input"]
                ):
                    # Function evaluation can depend on input files. Since expansion can happen during dryrun,
                    # where input files are not yet present, we need to skip such cases and
                    # mark them as <TBD>.
                    value = TBDString()
                elif raw_exceptions:
                    raise e
                else:
                    raise InputFunctionException(e, rule=self, wildcards=wildcards)
            return value, incomplete

        res = func
        tries = 0
        while (callable(res) or tries == 0) and tries < 10:
            res, incomplete = apply_func(res)
            tries += 1
        if tries == 10:
            raise WorkflowError(
                "Evaluated 10 nested input functions (i.e. input functions that "
                "themselves return an input function.). More than 10 such nested "
                "evaluations are not allowed. Does the workflow accidentally return a "
                "function instead of calling it in the input function?",
                rule=self,
            )

        return res, incomplete

    def _apply_wildcards(
        self,
        newitems,
        olditems,
        wildcards,
        concretize=None,
        check_return_type=True,
        omit_callable=False,
        mapping=None,
        no_flattening=False,
        aux_params=None,
        path_modifier=None,
        property=None,
        incomplete_checkpoint_func=lambda e: None,
        allow_unpack=True,
        groupid=None,
        non_derived_items: typing.List[typing.Any] = None,
    ):
        incomplete = False
        if aux_params is None:
            aux_params = dict()
        for name, item in olditems._allitems():
            olditem = item
            start = len(newitems)
            is_unpack = is_flagged(item, "unpack")
            _is_callable = is_callable(item)
            is_derived = False

            if _is_callable:
                if omit_callable:
                    continue
                if non_derived_items is not None:
                    is_derived = self._is_deriving_function(item)
                item, incomplete = self.apply_input_function(
                    item,
                    wildcards,
                    incomplete_checkpoint_func=incomplete_checkpoint_func,
                    is_unpack=is_unpack,
                    groupid=groupid,
                    **aux_params,
                )

            if is_unpack and not incomplete:
                if not allow_unpack:
                    raise WorkflowError(
                        "unpack() is not allowed with params. "
                        "Simply return a dictionary which can be directly ."
                        "used, e.g. via {params[mykey]}.",
                        rule=self,
                    )
                # Sanity checks before interpreting unpack()
                if not isinstance(item, (list, dict)):
                    raise WorkflowError(
                        f"Can only use unpack() on list and dict, but {item} was returned.",
                        rule=self,
                    )
                if name:
                    raise WorkflowError(
                        f"Cannot combine named input file (name {name}) with unpack()",
                        rule=self,
                    )
                # Allow streamlined code with/without unpack
                if isinstance(item, list):
                    apply_results = zip(
                        [None] * len(item),
                        item,
                        [_is_callable] * len(item),
                        [is_derived] * len(item),
                    )
                else:
                    assert isinstance(item, dict)
                    apply_results = [
                        (name, item, _is_callable, is_derived)
                        for name, item in item.items()
                    ]
            else:
                apply_results = [
                    (name, item, olditem if _is_callable else None, is_derived)
                ]

            for name, item, from_callable, is_derived in apply_results:
                is_iterable = True
                if not_iterable(item) or no_flattening:
                    item = [item]
                    is_iterable = False
                for item_ in item:
                    if (
                        check_return_type
                        and not isinstance(item_, str)
                        and not isinstance(item_, Path)
                    ):
                        raise InputFunctionException(
                            f"Function did not return str or iterable of str. Encountered: {item} ({type(item)})",
                            rule=self,
                            wildcards=wildcards,
                        )

                    if (
                        from_callable is not None
                        and not incomplete
                        and path_modifier is not None
                    ):
                        item_ = self.apply_path_modifier(
                            item_, path_modifier, property=property
                        )

                    concrete = concretize(item_, wildcards, from_callable)
                    newitems.append(concrete)
                    if not is_derived and non_derived_items is not None:
                        non_derived_items.append(concrete)
                    if mapping is not None:
                        mapping[concrete] = olditem

                if name:
                    newitems._set_name(
                        name, start, end=len(newitems) if is_iterable else None
                    )
                    start = len(newitems)
        return incomplete

    def expand_input(self, wildcards, groupid=None):
        def concretize_iofile(f, wildcards, from_callable):
            if from_callable is not None:
                if isinstance(f, Path):
                    f = str(f)
                iofile = IOFile(f, rule=self).apply_wildcards(wildcards)

                # inherit flags from callable
                if hasattr(from_callable, "flags"):
                    for key, value in from_callable.flags.items():
                        if key in iofile.flags:
                            continue
                        iofile.flags[key] = value

                return self.workflow.modifier.default_input_flags.apply(iofile)
            else:
                return f.apply_wildcards(wildcards)

        def handle_incomplete_checkpoint(exception):
            """If checkpoint is incomplete, target it such that it is completed
            before this rule gets executed."""
            return exception.targetfile

        input = InputFiles()
        mapping = dict()
        try:
            incomplete = self._apply_wildcards(
                input,
                self.input,
                wildcards,
                concretize=concretize_iofile,
                mapping=mapping,
                incomplete_checkpoint_func=handle_incomplete_checkpoint,
                path_modifier=self.input_modifier,
                property="input",
                groupid=groupid,
            )
        except WildcardError as e:
            raise WildcardError(
                "Wildcards in input files cannot be determined from output files:",
                str(e),
                rule=self,
            )

        if self.dependencies:
            dependencies = {
                f: self.dependencies[f_]
                for f, f_ in mapping.items()
                if f_ in self.dependencies
            }
            if None in self.dependencies:
                dependencies[None] = self.dependencies[None]
        else:
            dependencies = self.dependencies

        for f in input:
            f.check()

        return input, mapping, dependencies, incomplete

    @classmethod
    def _is_deriving_function(cls, func):
        if is_callable(func):
            func_params = get_function_params(func)
            return (
                "input" in func_params
                or "output" in func_params
                or "threads" in func_params
                or "resources" in func_params
            )
        return False

    def expand_params(self, wildcards, input, output, job, omit_callable=False):
        def concretize_param(p, wildcards, is_from_callable):
            if not is_from_callable:
                if isinstance(p, str):
                    return apply_wildcards(p, wildcards)
                if isinstance(p, list):
                    return [
                        (apply_wildcards(v, wildcards) if isinstance(v, str) else v)
                        for v in p
                    ]
            return p

        def handle_incomplete_checkpoint(exception):
            """If checkpoint is incomplete, target it such that it is completed
            before this rule gets executed."""
            if exception.targetfile in input:
                return TBDString()
            else:
                raise WorkflowError(
                    "Rule parameter depends on checkpoint but checkpoint output is not "
                    "defined as input file for the rule. Please add the output of the "
                    "respective checkpoint to the rule inputs. "
                    f"Input: {','.join(input)} "
                    f"Checkpoint file: {exception.targetfile}",
                    rule=self,
                )

        # We make sure that resources are only evaluated if a param function
        # actually needs them by turning them into callables and delegating their
        # evaluation to a later stage that only happens if the param function
        # requests access to resources or threads.
        resources = lambda: job.resources
        threads = lambda: job.resources._cores

        params = Params()
        non_derived_params = []
        try:
            # When applying wildcards to params, the return type need not be
            # a string, so the check is disabled.
            self._apply_wildcards(
                params,
                self.params,
                wildcards,
                concretize=concretize_param,
                check_return_type=False,
                omit_callable=omit_callable,
                allow_unpack=False,
                no_flattening=True,
                property="params",
                aux_params={
                    "input": input._plainstrings(),
                    "resources": resources,
                    "output": output._plainstrings(),
                    "threads": threads,
                },
                incomplete_checkpoint_func=handle_incomplete_checkpoint,
                non_derived_items=non_derived_params,
            )
        except WildcardError as e:
            raise WildcardError(
                "Wildcards in params cannot be "
                "determined from output files. Note that you have "
                "to use a function to deactivate automatic wildcard expansion "
                "in params strings, e.g., `lambda wildcards: '{test}'`. Also "
                "see https://snakemake.readthedocs.io/en/stable/snakefiles/"
                "rules.html#non-file-parameters-for-rules:",
                str(e),
                rule=self,
            )
        return params, non_derived_params

    def expand_output(self, wildcards):
        output = OutputFiles(o.apply_wildcards(wildcards) for o in self.output)
        output._take_names(self.output._get_names())
        mapping = {f: f_ for f, f_ in zip(output, self.output)}

        for f in output:
            f.check()

        # Note that we do not need to check for duplicate file names after
        # expansion as all output patterns have contain all wildcards anyway.

        return output, mapping

    def expand_log(self, wildcards):
        def concretize_logfile(f, wildcards, is_from_callable):
            if is_from_callable:
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards)

        log = Log()

        try:
            self._apply_wildcards(
                log,
                self.log,
                wildcards,
                concretize=concretize_logfile,
                path_modifier=self.log_modifier,
                property="log",
            )
        except WildcardError as e:
            raise WildcardError(
                "Wildcards in log files cannot be determined from output files:",
                str(e),
                rule=self,
            )

        for f in log:
            f.check()

        return log

    def expand_benchmark(self, wildcards):
        try:
            benchmark = (
                self.benchmark.apply_wildcards(wildcards) if self.benchmark else None
            )
        except WildcardError as e:
            raise WildcardError(
                "Wildcards in benchmark file cannot be "
                "determined from output files:",
                str(e),
                rule=self,
            )

        if benchmark is not None:
            benchmark.check()

        return benchmark

    def expand_resources(
        self, wildcards, input, attempt, skip_evaluation: typing.Optional[set] = None
    ):
        resources = dict()

        def apply(name, res, threads=None):
            if skip_evaluation is not None and name in skip_evaluation:
                res = TBDString()
            else:
                if isinstance(res, AnnotatedString) and res.callable:
                    res = res.callable
                if callable(res):
                    aux = dict(rulename=self.name)
                    if threads is not None:
                        aux["threads"] = threads
                    try:
                        res, _ = self.apply_input_function(
                            res,
                            wildcards,
                            input=input,
                            attempt=attempt,
                            incomplete_checkpoint_func=lambda e: 0,
                            raw_exceptions=True,
                            **aux,
                        )
                    except BaseException as e:
                        raise InputFunctionException(e, rule=self, wildcards=wildcards)

                if isinstance(res, float):
                    # round to integer
                    res = int(round(res))

                if (
                    not isinstance(res, int)
                    and not isinstance(res, str)
                    and res is not None
                ):
                    raise WorkflowError(
                        f"Resource {name} is neither int, float(would be rounded to nearest int), str, or None.",
                        rule=self,
                    )

            global_res = self.workflow.global_resources.get(name)
            if global_res is not None and res is not None:
                if not isinstance(res, TBDString) and type(res) != type(global_res):
                    global_type = (
                        "an int" if isinstance(global_res, int) else type(global_res)
                    )
                    raise WorkflowError(
                        f"Resource {name} is of type {type(res).__name__} but global resource constraint "
                        f"defines {global_type} with value {global_res}. "
                        "Resources with the same name need to have the same types (int, float, or str are allowed).",
                        rule=self,
                    )
                if isinstance(res, int):
                    res = min(global_res, res)
            return res

        threads = apply("_cores", self.resources["_cores"])
        if threads is None:
            raise WorkflowError("Threads must be given as an int", rule=self)
        if self.workflow.resource_settings.max_threads is not None and not isinstance(
            threads, TBDString
        ):
            threads = min(threads, self.workflow.resource_settings.max_threads)
        resources["_cores"] = threads

        for name, res in list(self.resources.items()):
            if name != "_cores":
                value = apply(name, res, threads=threads)

                if value is not None:
                    resources[name] = value

                    if not isinstance(value, TBDString):
                        # Infer standard resources from eventual human readable forms.
                        infer_resources(name, value, resources)
                        value = resources[name]

                    # infer additional resources
                    for mb_item, mib_item in (
                        ("mem_mb", "mem_mib"),
                        ("disk_mb", "disk_mib"),
                    ):
                        if (
                            name == mb_item
                            and mib_item not in self.resources.keys()
                            and isinstance(value, int)
                        ):
                            # infer mem_mib (memory in Mebibytes) as additional resource
                            resources[mib_item] = mb_to_mib(value)

        resources = Resources(fromdict=resources)
        return resources

    def expand_group(self, wildcards):
        """Expand the group given wildcards."""
        if callable(self.group):
            item, _ = self.apply_input_function(self.group, wildcards)
            return item
        elif isinstance(self.group, str):
            resolved = apply_wildcards(self.group, wildcards)
            if resolved != self.group:
                self.workflow.parent_groupids[resolved] = self.group
            return resolved
        else:
            return self.group

    def expand_conda_env(self, wildcards, params=None, input=None):
        if self._expanded_conda_env is not _NOT_CACHED:
            return self._expanded_conda_env

        from snakemake.common import is_local_file
        from snakemake.sourcecache import SourceFile, infer_source_file
        from snakemake.deployment.conda import (
            CondaEnvFileSpec,
            CondaEnvNameSpec,
            CondaEnvDirSpec,
            CondaEnvSpecType,
        )

        conda_env = self._conda_env
        if conda_env is not None:
            if not callable(conda_env):
                cacheable = not contains_wildcard(conda_env)
            else:
                conda_env, _ = self.apply_input_function(
                    conda_env, wildcards=wildcards, params=params, input=input
                )
                cacheable = False
                if conda_env is None:
                    return None
        else:
            self._expanded_conda_env = None
            return None

        spec_type = CondaEnvSpecType.from_spec(conda_env)

        if spec_type is CondaEnvSpecType.FILE:
            if not isinstance(conda_env, SourceFile):
                if is_local_file(conda_env) and not os.path.isabs(conda_env):
                    # Conda env file paths are considered to be relative to the directory of the Snakefile
                    # hence we adjust the path accordingly.
                    # This is not necessary in case of receiving a SourceFile.
                    conda_env = self.basedir.join(conda_env)
                else:
                    # infer source file from unmodified uri or path
                    conda_env = infer_source_file(conda_env)

            conda_env = CondaEnvFileSpec(conda_env, rule=self)
        elif spec_type is CondaEnvSpecType.NAME:
            conda_env = CondaEnvNameSpec(conda_env)
        elif spec_type is CondaEnvSpecType.DIR:
            conda_env = CondaEnvDirSpec(conda_env, rule=self)

        conda_env = conda_env.apply_wildcards(wildcards, self)
        conda_env.check()

        if cacheable:
            self._expanded_conda_env = conda_env

        return conda_env

    def expand_container_img(self, wildcards):
        """
        Expand the given container wildcards
        """
        if callable(self.container_img):
            container_url, _ = self.apply_input_function(
                self.container_img, wildcards=wildcards
            )
            return container_url

        elif isinstance(self.container_img, str):
            resolved_url = apply_wildcards(self.container_img, wildcards)
            return resolved_url

        return self.container_img

    def is_producer(self, requested_output):
        """
        Returns True if this rule is a producer of the requested output.
        """
        try:
            for o in self.products():
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(
                f"{ex} in wildcard statement",
                snakefile=self.snakefile,
                lineno=self.lineno,
            )
        except ValueError as ex:
            raise IOFileException(f"{ex}", snakefile=self.snakefile, lineno=self.lineno)

    def get_wildcards(self, requested_output, wildcards_dict=None):
        """
        Return wildcard dictionary by
        1. trying to format the output with the given wildcards and comparing with the requested output
        2. matching regular expression output files to the requested concrete ones.

        Arguments
        requested_output -- a concrete filepath
        """
        if requested_output is None:
            return dict()

        # first try to match the output with the given wildcards
        if wildcards_dict is not None:
            if self.wildcard_names <= wildcards_dict.keys():
                wildcards_dict = {
                    wildcard: value
                    for wildcard, value in wildcards_dict.items()
                    if wildcard in self.wildcard_names
                }
                for o in self.products():
                    try:
                        applied = o.apply_wildcards(wildcards_dict)
                        # if the output formatted with the wildcards matches the requested output,
                        if applied == requested_output:
                            # we check whether the wildcards satisfy the constraints
                            constraints = o.wildcard_constraints()

                            def check_constraint(wildcard, value):
                                constraint = constraints.get(wildcard)
                                return constraint is None or constraint.match(value)

                            if all(
                                check_constraint(name, value)
                                for name, value in wildcards_dict.items()
                            ):
                                # and then just return the given wildcards_dict limited to the wildcards that are actually used
                                return wildcards_dict
                    except WildcardError:
                        continue

        bestmatchlen = 0
        bestmatch = None

        for o in self.products():
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        self.check_wildcards(bestmatch)
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        if isinstance(other, Rule):
            return self.name == other.name and self.output == other.output
        else:
            return False


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """
        Return whether rule2 has a higher priority than rule1.
        """
        if rule1.name != rule2.name:
            # try the last clause first,
            # i.e. clauses added later overwrite those before.
            for clause in reversed(self.order):
                try:
                    i = clause.index(rule1.name)
                    j = clause.index(rule2.name)
                    # rules with higher priority should have a smaller index
                    comp = j - i
                    if comp < 0:
                        comp = -1
                    elif comp > 0:
                        comp = 1
                    return comp
                except ValueError:
                    pass

        # if no ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()


class RuleProxy:
    def __init__(self, rule):
        self.rule = rule

    @lazy_property
    def output(self):
        return self._to_iofile(self.rule.output)

    @lazy_property
    def input(self):
        def modify_callable(item):
            if is_callable(item):
                # For callables ensure that the rule's original path modifier is applied as well.

                def inner(wildcards):
                    return self.rule.apply_path_modifier(
                        item(wildcards), self.rule.input_modifier, property="input"
                    )

                return inner
            else:
                # For strings, the path modifier has been already applied.
                return item

        return InputFiles(
            toclone=self.rule.input, strip_constraints=True, custom_map=modify_callable
        )

    @lazy_property
    def params(self):
        return self.rule.params._clone()

    @property
    def benchmark(self):
        return IOFile(strip_wildcard_constraints(self.rule.benchmark), rule=self.rule)

    @lazy_property
    def log(self):
        return self._to_iofile(self.rule.log)

    def _to_iofile(self, files):
        def cleanup(f):
            prefix = self.rule.workflow.storage_settings.default_storage_prefix
            # remove constraints and turn this into a plain string
            cleaned = strip_wildcard_constraints(f)

            modified_by = get_flag_value(f, PATH_MODIFIER_FLAG)

            if (
                self.rule.workflow.storage_settings.default_storage_provider is not None
                and f.startswith(prefix)
                and not is_flagged(f, "local")
            ):
                start = len(prefix) + 1 if prefix else 0
                cleaned = f.storage_object.query[start:]
                cleaned = IOFile(cleaned, rule=self.rule)
            else:
                cleaned = IOFile(AnnotatedString(cleaned), rule=self.rule)
                cleaned.clone_storage_object(f)

            if modified_by is not None:
                cleaned.flags[PATH_MODIFIER_FLAG] = modified_by

            return cleaned

        files = Namedlist(files, custom_map=cleanup)

        return files



================================================
FILE: src/snakemake/shell.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from pathlib import Path
import _io
import sys
import os
import subprocess as sp
import inspect
import shutil
import stat
import tempfile
import threading

from snakemake.utils import format, argvquote, cmd_exe_quote
from snakemake.common import ON_WINDOWS, RULEFUNC_CONTEXT_MARKER
from snakemake.logging import logger
from snakemake_interface_logger_plugins.common import LogEvent
from snakemake.deployment import singularity
from snakemake.deployment.conda import Conda
from snakemake.exceptions import WorkflowError


__author__ = "Johannes Köster"

STDOUT = sys.stdout
if not isinstance(sys.stdout, _io.TextIOWrapper):
    # workaround for nosetest since it overwrites sys.stdout
    # in a strange way that does not work with Popen
    STDOUT = None


# There is a max length for a command executed as well as a maximum
# length for each argument passed to a command. The latter impacts us
# especially when doing `sh -c 'long script from user'`. On Linux, it's
# hardcoded in the kernel as 32 pages, or 128kB. On OSX it appears to be
# close to `getconf ARG_MAX`, about 253kb.
MAX_ARG_LEN = 16 * 4096 - 1


class shell:
    _process_args = {}
    _process_prefix = None
    _process_suffix = ""
    _win_command_prefix = ""
    _lock = threading.Lock()
    _processes = {}
    conda_block_conflicting_envvars = True

    @classmethod
    def get_executable(cls):
        return cls._process_args.get("executable", None)

    @classmethod
    def check_output(cls, cmd, **kwargs):
        executable = cls.get_executable()
        if ON_WINDOWS and executable:
            win_prefix = cls._get_win_command_prefix()
            cmd = f'"{executable}" {win_prefix} {argvquote(cmd)}'
            logger.debug(f"Executing: {cmd}")
            return sp.check_output(cmd, shell=False, executable=executable, **kwargs)
        else:
            return sp.check_output(cmd, shell=True, executable=executable, **kwargs)

    @classmethod
    def executable(cls, cmd):
        if isinstance(cmd, Path):
            cmd = str(cmd)
        if cmd and not os.path.isabs(cmd):
            # always enforce absolute path
            cmd = shutil.which(cmd)
            if not cmd:
                raise WorkflowError(
                    f"Cannot set default shell {cmd} because it is not available in your PATH."
                )
        cls._process_args["executable"] = cmd
        logger.debug(f"Setting shell executable to {cmd}.")

    @classmethod
    def _get_process_prefix(cls, shell_exec=None):
        shell_exec = cls._get_executable_name(shell_exec)
        if (
            shell_exec == "bash" or (ON_WINDOWS and shell_exec == "bash.exe")
        ) and cls._process_prefix is None:
            return "set -euo pipefail; "
        else:
            return cls._process_prefix or ""

    @classmethod
    def _get_win_command_prefix(cls, use_default=False, shell_exec=None):
        assert ON_WINDOWS
        if use_default or (cls._win_command_prefix and shell_exec is None):
            # use whatever is the default
            return cls._win_command_prefix
        shell_exec = cls._get_executable_name(shell_exec)
        if shell_exec == "bash" or shell_exec == "bash.exe":
            return "-c"
        else:
            return ""

    @classmethod
    def _check_executable(cls, shell_exec=None):
        shell_exec = shell_exec or cls.get_executable()
        if shell_exec is not None:
            if ON_WINDOWS and shell_exec == r"C:\Windows\System32\bash.exe":
                raise WorkflowError(
                    "Cannot use WSL bash.exe on Windows. Ensure that you have "
                    "a usable bash.exe available on your path."
                )
            if not os.path.isabs(shell_exec):
                path = shutil.which(shell_exec)
                if not path:
                    raise WorkflowError(
                        f"Cannot set shell to {shell_exec} because it is not "
                        "available in your PATH."
                    )
            elif not os.path.exists(shell_exec):
                raise WorkflowError(
                    f"Cannot set shell to {shell_exec} because it does not exist."
                )

    @classmethod
    def _get_executable_name(cls, shell_exec=None):
        shell_exec = shell_exec or cls.get_executable()
        if shell_exec:
            return os.path.split(shell_exec)[-1].lower()
        else:
            return None

    @classmethod
    def prefix(cls, prefix):
        cls._process_prefix = format(prefix, stepout=2)

    @classmethod
    def suffix(cls, suffix):
        cls._process_suffix = format(suffix, stepout=2)

    @classmethod
    def win_command_prefix(cls, cmd):
        """The command prefix used on windows when specifying a explicit
        shell executable. This would be "-c" for bash.
        Note: that if no explicit executable is set commands are executed
        with Popen(..., shell=True) which uses COMSPEC on windows where this
        is not needed.
        """
        cls._win_command_prefix = cmd

    @classmethod
    def kill(cls, jobid):
        with cls._lock:
            if jobid in cls._processes:
                cls._processes[jobid].kill()
                del cls._processes[jobid]

    @classmethod
    def terminate(cls, jobid):
        with cls._lock:
            if jobid in cls._processes:
                cls._processes[jobid].terminate()
                del cls._processes[jobid]

    @classmethod
    def cleanup(cls):
        with cls._lock:
            cls._processes.clear()

    def __new__(
        cls, cmd, *args, iterable=False, read=False, bench_record=None, **kwargs
    ):
        if "stepout" in kwargs:
            raise KeyError("Argument stepout is not allowed in shell command.")

        if ON_WINDOWS and not cls.get_executable():
            # If bash is not used on Windows quoting must be handled in a special way
            kwargs["quote_func"] = cmd_exe_quote

        cmd = format(cmd, *args, stepout=2, **kwargs)

        stdout = sp.PIPE if iterable or read else STDOUT

        close_fds = sys.platform != "win32"

        func_context = inspect.currentframe().f_back.f_locals

        if func_context.get(RULEFUNC_CONTEXT_MARKER):
            # If this comes from a rule, we expect certain information to be passed
            # implicitly via the rule func context, which is added here.
            context = func_context
        else:
            # Otherwise, context is just filled via kwargs.
            context = dict()
        # add kwargs to context (overwriting the locals of the caller)
        context.update(kwargs)

        jobid = context.get("jobid")
        if not context.get("is_shell") and jobid is not None:
            logger.info(None, extra=dict(event=LogEvent.SHELLCMD, cmd=cmd))

        conda_env = context.get("conda_env", None)
        conda_base_path = context.get("conda_base_path", None)
        container_img = context.get("container_img", None)
        env_modules = context.get("env_modules", None)
        shadow_dir = context.get("shadow_dir", None)
        resources = context.get("resources", {})
        singularity_args = context.get("singularity_args", "")
        threads = context.get("threads", 1)

        shell_executable = resources.get("shell_exec")
        if shell_executable is not None:
            process_args = dict(cls._process_args)
            process_args["executable"] = shell_executable
        else:
            shell_executable = cls.get_executable()
            process_args = cls._process_args

        cls._check_executable(shell_executable)

        cmd = " ".join(
            (cls._get_process_prefix(shell_executable), cmd, cls._process_suffix)
        ).strip()

        # If the executor is the submit executor or the jobstep executor for the SLURM
        # backend, we do not want the environment modules to be activated:
        # if the rule requires a Python module, snakemake's environment might be
        # incompatible with the module's environment.
        if env_modules and "slurm" not in (item.filename for item in inspect.stack()):
            cmd = env_modules.shellcmd(cmd)
            logger.info(f"Activating environment modules: {env_modules}")

        if conda_env:
            if ON_WINDOWS and not cls.get_executable():
                # If we use cmd.exe directly on windows we need to prepend batch activation script.
                cmd = Conda(
                    container_img=container_img, prefix_path=conda_base_path
                ).shellcmd_win(conda_env, cmd)
            else:
                cmd = Conda(
                    container_img=container_img, prefix_path=conda_base_path
                ).shellcmd(conda_env, cmd)

        tmpdir = None
        if len(cmd.replace("'", r"'\''")) + 2 > MAX_ARG_LEN:
            tmpdir = tempfile.mkdtemp(dir=".snakemake", prefix="shell_tmp.")
            script = os.path.join(os.path.abspath(tmpdir), "script.sh")
            with open(script, "w") as script_fd:
                print(cmd, file=script_fd)
            os.chmod(script, os.stat(script).st_mode | stat.S_IXUSR | stat.S_IRUSR)
            cmd = '"{}" "{}"'.format(cls.get_executable() or "/bin/sh", script)

        if container_img:
            cmd = singularity.shellcmd(
                container_img,
                cmd,
                singularity_args,
                envvars=None,
                shell_executable=shell_executable,
                container_workdir=shadow_dir,
                is_python_script=context.get("is_python_script", False),
            )
            logger.info(f"Activating singularity image {container_img}")
        if conda_env:
            logger.info(f"Activating conda environment: {os.path.relpath(conda_env)}")

        tmpdir_resource = resources.get("tmpdir", None)
        # environment variable lists for linear algebra libraries taken from:
        # https://stackoverflow.com/a/53224849/2352071
        # https://github.com/xianyi/OpenBLAS/tree/59243d49ab8e958bb3872f16a7c0ef8c04067c0a#setting-the-number-of-threads-using-environment-variables
        envvars = dict(os.environ)
        threads = str(threads)
        envvars["OMP_NUM_THREADS"] = threads
        envvars["GOTO_NUM_THREADS"] = threads
        envvars["OPENBLAS_NUM_THREADS"] = threads
        envvars["MKL_NUM_THREADS"] = threads
        envvars["VECLIB_MAXIMUM_THREADS"] = threads
        envvars["NUMEXPR_NUM_THREADS"] = threads

        if tmpdir_resource:
            envvars["TMPDIR"] = tmpdir_resource
            envvars["TMP"] = tmpdir_resource
            envvars["TEMPDIR"] = tmpdir_resource
            envvars["TEMP"] = tmpdir_resource

        if "additional_envvars" in kwargs:
            env = kwargs["additional_envvars"]
            if not isinstance(env, dict) or not all(
                isinstance(v, str) for v in env.values()
            ):
                raise WorkflowError(
                    "Given environment variables for shell command have to be a dict of strings, "
                    "but the following was provided instead:\n{}".format(env)
                )
            envvars.update(env)

        if conda_env and cls.conda_block_conflicting_envvars:
            # remove envvars that conflict with conda
            for var in ["R_LIBS", "PYTHONPATH", "PERLLIB", "PERL5LIB"]:
                try:
                    del envvars[var]
                except KeyError:
                    pass

        use_shell = True
        if ON_WINDOWS and shell_executable:
            # If executable is set on Windows shell mode can not be used
            # and the executable should be prepended the command together
            # with a command prefix (e.g. -c for bash).
            use_shell = False
            win_prefix = cls._get_win_command_prefix(
                use_default=False, shell_exec=shell_executable
            )
            cmd = '"{}" {} {}'.format(
                shell_executable,
                win_prefix,
                argvquote(cmd),
            )

        proc = sp.Popen(
            cmd,
            bufsize=-1,
            shell=use_shell,
            stdout=stdout,
            universal_newlines=iterable or read or None,
            close_fds=close_fds,
            **process_args,
            env=envvars,
        )

        if jobid is not None:
            with cls._lock:
                cls._processes[jobid] = proc

        ret = None
        if iterable:
            return cls.iter_stdout(proc, cmd, tmpdir)
        if read:
            ret = proc.stdout.read()
        if bench_record is not None:
            from snakemake.benchmark import benchmarked

            with benchmarked(proc.pid, bench_record):
                retcode = proc.wait()
        else:
            retcode = proc.wait()

        if tmpdir:
            shutil.rmtree(tmpdir)

        if jobid is not None:
            with cls._lock:
                try:
                    del cls._processes[jobid]
                except KeyError:
                    pass

        if retcode:
            raise sp.CalledProcessError(retcode, cmd)
        return ret

    @staticmethod
    def iter_stdout(proc, cmd, tmpdir):
        for l in proc.stdout:
            yield l[:-1]
        retcode = proc.wait()
        if tmpdir:
            shutil.rmtree(tmpdir)
        if retcode:
            raise sp.CalledProcessError(retcode, cmd)


# set bash as default shell on posix compatible OS
if os.name == "posix":
    if not shutil.which("bash"):
        logger.warning(
            "Cannot set bash as default shell because it is not "
            "available in your PATH. Falling back to sh."
        )
        if not shutil.which("sh"):
            logger.warning(
                "Cannot fall back to sh since it seems to be not "
                "available on this system. Using whatever is "
                "defined as default."
            )
        else:
            shell.executable("sh")
    else:
        shell.executable("bash")
elif ON_WINDOWS:
    shell.executable(None)



================================================
FILE: src/snakemake/snakemake.code-workspace
================================================
{
	"folders": [
		{
			"path": ".."
		}
	],
	"settings": {}
}


================================================
FILE: src/snakemake/sourcecache.py
================================================
__authors__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from pathlib import Path
import posixpath
import re
import os
import shutil
import stat
from typing import Optional
from snakemake import utils
import tempfile
import io
from abc import ABC, abstractmethod
from urllib.parse import unquote

from snakemake_interface_executor_plugins.settings import ExecMode
from snakemake.common import (
    ON_WINDOWS,
    is_local_file,
    get_appdirs,
    parse_uri,
    smart_join,
)
from snakemake.exceptions import WorkflowError, SourceFileError
from snakemake.common.git import split_git_path
from snakemake.logging import logger


def _check_git_args(tag: str = None, branch: str = None, commit: str = None):
    n_refs = sum(1 for ref in (tag, branch, commit) if ref is not None)
    if n_refs != 1:
        raise SourceFileError(
            "exactly one of tag, branch, or commit must be specified."
        )


class SourceFile(ABC):
    @abstractmethod
    def get_path_or_uri(self) -> str: ...

    @abstractmethod
    def is_persistently_cacheable(self): ...

    def get_cache_path(self):
        uri = parse_uri(self.get_path_or_uri())
        return os.path.join(uri.scheme, unquote(uri.uri_path.lstrip("/")))

    def get_basedir(self):
        path = os.path.dirname(self.get_path_or_uri())
        return self.__class__(path)

    @abstractmethod
    def get_filename(self): ...

    def join(self, path):
        if isinstance(path, SourceFile):
            path = path.get_path_or_uri()
        return self.__class__(smart_join(self.get_path_or_uri(), path))

    def mtime(self):
        """If possible, return mtime of the file. Otherwise, return None."""
        return None

    @property
    @abstractmethod
    def is_local(self): ...

    def __hash__(self):
        return self.get_path_or_uri().__hash__()

    def __eq__(self, other):
        if isinstance(other, SourceFile):
            return self.get_path_or_uri() == other.get_path_or_uri()
        return False

    def __str__(self):
        return self.get_path_or_uri()

    def simplify_path(self):
        return self


class GenericSourceFile(SourceFile):
    def __init__(self, path_or_uri):
        self.path_or_uri = path_or_uri

    def get_path_or_uri(self) -> str:
        return self.path_or_uri

    def get_filename(self):
        return os.path.basename(self.path_or_uri)

    def is_persistently_cacheable(self):
        return False

    @property
    def is_local(self):
        return False


class LocalSourceFile(SourceFile):
    def __init__(self, path):
        self.path = path

    def get_path_or_uri(self) -> str:
        return self.path

    def is_persistently_cacheable(self):
        return False

    def get_filename(self):
        return os.path.basename(self.path)

    def abspath(self):
        return LocalSourceFile(os.path.abspath(self.path))

    def isabs(self):
        return os.path.isabs(self.path)

    def simplify_path(self):
        return utils.simplify_path(self.path)

    def mtime(self):
        return os.stat(self.path).st_mtime

    def __fspath__(self):
        return self.path

    @property
    def is_local(self):
        return True


class LocalGitFile(SourceFile):
    def __init__(
        self, repo_path, path: str, tag: str = None, ref: str = None, commit: str = None
    ):
        _check_git_args(tag, ref, commit)
        self.tag = tag
        self.commit = commit
        self._ref = ref
        self.repo_path = repo_path
        self.path = path

    def get_path_or_uri(self) -> str:
        return "git+file://{}/{}@{}".format(
            os.path.abspath(self.repo_path), self.path, self.ref
        )

    def join(self, path):
        path = os.path.normpath("/".join((self.path, path)))
        if ON_WINDOWS:
            # convert back to URL separators
            # (win specific separators are introduced by normpath above)
            path = path.replace("\\", "/")
        return LocalGitFile(
            self.repo_path, path, tag=self.tag, ref=self._ref, commit=self.commit
        )

    def get_basedir(self):
        return self.__class__(
            repo_path=self.repo_path,
            path=os.path.dirname(self.path),
            tag=self.tag,
            commit=self.commit,
            ref=self._ref,
        )

    def is_persistently_cacheable(self):
        return False

    def get_filename(self):
        return posixpath.basename(self.path)

    @property
    def ref(self):
        return self.tag or self.commit or self._ref

    @property
    def is_local(self):
        return True


class HostingProviderFile(SourceFile):
    """Marker for denoting github source files from releases."""

    valid_repo = re.compile("^.+/.+$")

    def __init__(
        self,
        repo: str = None,
        path: str = None,
        tag: str = None,
        branch: str = None,
        commit: str = None,
        host: str = None,
    ):
        if repo is None:
            raise SourceFileError("repo must be given")
        if not self.__class__.valid_repo.match(repo):
            raise SourceFileError(
                "repo {} is not a valid repo specification (must be given as owner/name)."
            )

        _check_git_args(tag, branch, commit)

        if path is None:
            raise SourceFileError("path must be given")

        if not all(
            isinstance(item, str)
            for item in (repo, path, tag, branch, commit)
            if item is not None
        ):
            raise SourceFileError("arguments must be given as str.")

        self.repo = repo
        self.tag = tag
        self.commit = commit
        self.branch = branch
        self.path = path.strip("/")
        self.token = ""
        self.host = host

        # Via __post_init__ implementing subclasses can do additional things without
        # replicating the constructor args.
        self.__post_init__()

    def __post_init__(self):
        pass

    def mtime(self) -> Optional[float]:
        # Intentionally None, hence causing any caching to generate an updated mtime.
        # Switching commits/branches/refs in the same repo should cause rerun triggers
        # if those files are used as input files for jobs and have changed checksums.
        return None

    def is_persistently_cacheable(self):
        return bool(self.tag or self.commit)

    def get_filename(self):
        return os.path.basename(self.path)

    @property
    def ref(self):
        return self.tag or self.commit or self.branch

    def get_basedir(self):
        return self.__class__(
            repo=self.repo,
            path=os.path.dirname(self.path),
            tag=self.tag,
            commit=self.commit,
            branch=self.branch,
            host=self.host,
        )

    def join(self, path):
        path = os.path.normpath(f"{self.path}/{path}")
        if ON_WINDOWS:
            # convert back to URL separators
            # (win specific separators are introduced by normpath above)
            path = path.replace("\\", "/")
        return self.__class__(
            repo=self.repo,
            path=path,
            tag=self.tag,
            commit=self.commit,
            branch=self.branch,
            host=self.host,
        )

    @property
    def is_local(self):
        return False


class GithubFile(HostingProviderFile):
    def __post_init__(self):
        if self.host is not None:
            raise WorkflowError(
                "host keyword argument is not yet supported by GithubFile."
            )
        self.token = os.environ.get("GITHUB_TOKEN", "")

    def get_path_or_uri(self) -> str:
        auth = f":{self.token}@" if self.token else ""
        # TODO find out how this URL looks like with Github enterprise server and support
        # self.host being not none by removing the check in __post_init__
        return f"https://{auth}raw.githubusercontent.com/{self.repo}/{self.ref}/{self.path}"


class GitlabFile(HostingProviderFile):
    def __post_init__(self):
        if self.host is None:
            self.host = "gitlab.com"
        self.token = os.environ.get("GITLAB_TOKEN", "")

    def get_path_or_uri(self) -> str:
        from urllib.parse import quote

        auth = f"&private_token={self.token}" if self.token else ""
        return "https://{}/api/v4/projects/{}/repository/files/{}/raw?ref={}{}".format(
            self.host,
            quote(self.repo, safe=""),
            quote(self.path, safe=""),
            self.ref,
            auth,
        )


def infer_source_file(path_or_uri, basedir: Optional[SourceFile] = None) -> SourceFile:
    if isinstance(path_or_uri, SourceFile):
        if basedir is None or isinstance(path_or_uri, HostingProviderFile):
            return path_or_uri
        else:
            path_or_uri = path_or_uri.get_path_or_uri()
    if isinstance(path_or_uri, Path):
        path_or_uri = str(path_or_uri)
    if not isinstance(path_or_uri, str):
        raise SourceFileError(
            "must be given as Python string or one of the predefined source file marker types (see docs)"
        )
    if is_local_file(path_or_uri):
        # either local file or relative to some remote basedir
        for schema in ("file://", "file:"):
            if path_or_uri.startswith(schema):
                path_or_uri = path_or_uri[len(schema) :]
                break
        if not os.path.isabs(path_or_uri) and basedir is not None:
            return basedir.join(path_or_uri)
        return LocalSourceFile(path_or_uri)
    if path_or_uri.startswith("git+file:"):
        try:
            root_path, file_path, ref = split_git_path(path_or_uri)
        except Exception as e:
            raise WorkflowError(
                f"Failed to read source {path_or_uri} from git repo.", e
            )
        return LocalGitFile(root_path, file_path, ref=ref)
    # something else
    return GenericSourceFile(path_or_uri)


class SourceCache:
    cache_whitelist = [
        r"https://raw.githubusercontent.com/snakemake/snakemake-wrappers/\d+\.\d+.\d+"
    ]  # TODO add more prefixes for uris that are save to be cached

    def __init__(self, cache_path: Path, runtime_cache_path: Path = None):
        self.cache_path = cache_path
        os.makedirs(self.cache_path, exist_ok=True)
        if runtime_cache_path is None:
            runtime_cache_parent = self.cache_path / "runtime-cache"
            os.makedirs(runtime_cache_parent, exist_ok=True)
            self.runtime_cache = tempfile.TemporaryDirectory(
                dir=runtime_cache_parent, ignore_cleanup_errors=True
            )
            self._runtime_cache_path = None
        else:
            self._runtime_cache_path = runtime_cache_path
            self.runtime_cache = None
        self.cacheable_prefixes = re.compile("|".join(self.cache_whitelist))

    @property
    def runtime_cache_path(self):
        return self._runtime_cache_path or self.runtime_cache.name

    def open(self, source_file, mode="r"):
        cache_entry = self._cache(source_file)
        return self._open_local_or_remote(
            LocalSourceFile(cache_entry), mode, encoding="utf-8"
        )

    def exists(self, source_file):
        try:
            self._cache(source_file, retries=1)
        except Exception:
            return False
        return True

    def get_path(self, source_file):
        cache_entry = self._cache(source_file)
        return str(cache_entry)

    def _cache_entry(self, source_file: SourceFile) -> Path:
        file_cache_path = source_file.get_cache_path()
        assert file_cache_path

        if source_file.is_persistently_cacheable():
            # check cache
            return self.cache_path / file_cache_path
        else:
            # check runtime cache
            return Path(self.runtime_cache_path) / file_cache_path

    def _cache(self, source_file: SourceFile, retries: int = 3):
        cache_entry = self._cache_entry(source_file)
        if not cache_entry.exists():
            self._do_cache(source_file, cache_entry, retries=retries)
        return cache_entry

    def _do_cache(self, source_file, cache_entry: Path, retries: int = 3):
        # open from origin
        with self._open_local_or_remote(source_file, "rb", retries=retries) as source:
            cache_entry.parent.mkdir(parents=True, exist_ok=True)
            tmp_source = tempfile.NamedTemporaryFile(
                prefix=str(cache_entry),
                delete=False,  # no need to delete since we move it below
            )
            tmp_source.write(source.read())
            tmp_source.close()
            # ensure read and write permissions for owner and group
            os.chmod(
                tmp_source.name,
                stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IWGRP,
            )
            # Atomic move to right name.
            # This way we avoid the need to lock.
            shutil.move(tmp_source.name, cache_entry)

        mtime = source_file.mtime()
        if mtime is not None:
            # Set to mtime of original file
            # In case we don't have that mtime, it is fine
            # to just keep the time at the time of caching
            # as mtime.
            os.utime(cache_entry, times=(mtime, mtime))

    def _open_local_or_remote(
        self, source_file: SourceFile, mode, encoding=None, retries: int = 3
    ):
        from reretry.api import retry_call

        if source_file.is_local:
            return self._open(source_file, mode, encoding=encoding)
        else:
            return retry_call(
                self._open,
                [source_file, mode, encoding],
                tries=retries,
                delay=3,
                backoff=2,
                logger=logger,
            )

    def _open(self, source_file: SourceFile, mode, encoding=None):
        from smart_open import open

        if isinstance(source_file, LocalGitFile):
            import git

            return io.BytesIO(
                git.Repo(source_file.repo_path)
                .git.show(f"{source_file.ref}:{source_file.path}")
                .encode()
            )

        path_or_uri = source_file.get_path_or_uri()

        try:
            return open(path_or_uri, mode, encoding=None if "b" in mode else encoding)
        except Exception as e:
            raise WorkflowError(f"Failed to open source file {path_or_uri}", e)



================================================
FILE: src/snakemake/spawn_jobs.py
================================================
from dataclasses import dataclass, fields
import hashlib
from itertools import chain
import os
import sys
from typing import Callable, Mapping, TypeVar, TYPE_CHECKING, Any
from snakemake_interface_executor_plugins.utils import format_cli_arg, join_cli_args
from snakemake_interface_executor_plugins.settings import CommonSettings
from snakemake.resources import ParsedResource
from snakemake_interface_storage_plugins.registry import StoragePluginRegistry

from snakemake import PIP_DEPLOYMENTS_PATH
from snakemake.io import get_flag_value, is_flagged
from snakemake.settings.types import SharedFSUsage
from snakemake_interface_common.exceptions import WorkflowError

if TYPE_CHECKING:
    from snakemake.workflow import Workflow

    TWorkflow = TypeVar("TWorkflow", bound="Workflow")
else:
    TWorkflow = Any


@dataclass
class SpawnedJobArgsFactory:
    workflow: TWorkflow

    def get_default_storage_provider_args(self) -> str:
        has_default_storage_provider = (
            self.workflow.storage_registry.default_storage_provider is not None
        )
        if has_default_storage_provider:
            return join_cli_args(
                [
                    format_cli_arg(
                        "--default-storage-prefix",
                        self.workflow.storage_settings.default_storage_prefix,
                    ),
                    format_cli_arg(
                        "--default-storage-provider",
                        self.workflow.storage_settings.default_storage_provider,
                    ),
                ]
            )
        else:
            return ""

    def _get_storage_provider_setting_items(self):
        for (
            plugin_name,
            tagged_settings,
        ) in self.workflow.storage_provider_settings.items():
            plugin = StoragePluginRegistry().get_plugin(plugin_name)
            if plugin.settings_cls:
                for field in fields(plugin.settings_cls):
                    unparse = field.metadata.get("unparse", lambda value: value)

                    def fmt_value(tag, value):
                        if callable(value):
                            raise WorkflowError(
                                f"Invalid setting for plugin {plugin_name}. Unable "
                                "to pass callable value as a setting to spawned jobs."
                            )
                        value = unparse(value)
                        if tag is not None:
                            return f"{tag}:{value}"
                        else:
                            return value

                    field_settings = [
                        fmt_value(tag, value)
                        for tag, value in tagged_settings.get_field_settings(
                            field.name
                        ).items()
                        if value is not None
                    ]
                    if field_settings:
                        yield plugin, field, field_settings

    def get_storage_provider_args(self):
        for plugin, field, field_settings in self._get_storage_provider_setting_items():
            if not field.metadata.get("env_var", False):
                cli_arg = plugin.get_cli_arg(field.name)
                if isinstance(field_settings[0], bool) and field.default is not True:
                    # so far no tagged settings for flags with default value False
                    assert len(field_settings) == 1
                    field_settings = field_settings[0]
                yield format_cli_arg(cli_arg, field_settings)

    def get_storage_provider_envvars(self):
        return {
            plugin.get_envvar(field.name): " ".join(map(str, field_settings))
            for plugin, field, field_settings in self._get_storage_provider_setting_items()
            if field.metadata.get("env_var")
        }

    def get_set_resources_args(self):
        def get_orig_arg(value):
            if isinstance(value, ParsedResource):
                return value.orig_arg
            else:
                return value

        return [
            format_cli_arg(
                "--set-resources",
                [
                    f"{rule}:{name}={get_orig_arg(value)}"
                    for rule, res in self.workflow.resource_settings.overwrite_resources.items()
                    for name, value in res.items()
                ],
                skip=not self.workflow.resource_settings.overwrite_resources,
                base64_encode=True,
            ),
            format_cli_arg(
                "--set-threads",
                [
                    f"{rule}={get_orig_arg(value)}"
                    for rule, value in self.workflow.resource_settings.overwrite_threads.items()
                ],
                skip=not self.workflow.resource_settings.overwrite_threads,
                base64_encode=True,
            ),
        ]

    def get_resource_scopes_arg(self):
        return format_cli_arg(
            "--set-resource-scopes",
            self.workflow.resource_settings.overwrite_resource_scopes,
        )

    def get_shared_fs_usage_arg(self, executor_common_settings: CommonSettings):
        if executor_common_settings.spawned_jobs_assume_shared_fs:
            usage = SharedFSUsage.all()
        else:
            usage = self.workflow.storage_settings.shared_fs_usage
        return format_cli_arg(
            "--shared-fs-usage",
            usage if usage else "none",
        )

    def get_group_args(self):
        group_settings = self.workflow.group_settings
        groups = format_cli_arg(
            "--groups",
            [
                f"{rule}={group}"
                for rule, group in group_settings.overwrite_groups.items()
            ],
            skip=not group_settings.overwrite_groups,
        )
        group_components = format_cli_arg(
            "--group-components",
            [f"{group}={n}" for group, n in group_settings.group_components.items()],
            skip=not group_settings.group_components,
        )
        return join_cli_args([groups, group_components])

    def workflow_property_to_arg(
        self,
        property,
        flag=None,
        base64_encode=False,
        skip=False,
        force=False,
        invert=False,
        attr=None,
        convert_value: Callable = None,
    ):
        if skip:
            return ""

        # Get the value of the property. If property is nested, follow the hierarchy until
        # reaching the final value.
        query = property.split(".")
        base = self.workflow
        for prop in query[:-1]:
            base = getattr(base, prop)
        value = getattr(base, query[-1])

        if value is not None and attr is not None:
            value = getattr(value, attr)

        if flag is None:
            flag = f"--{query[-1].replace('_', '-')}"

        if invert and isinstance(value, bool):
            value = not value

        if force:
            assert isinstance(value, bool)
            value = True

        if convert_value is not None and value is not None:
            value = convert_value(value)

        return format_cli_arg(flag, value, base64_encode=base64_encode)

    def envvars(self) -> Mapping[str, str]:
        assert self.workflow.remote_execution_settings is not None
        envvars = {
            var: os.environ[var]
            for var in chain(
                self.workflow.remote_execution_settings.envvars, self.workflow.envvars
            )
        }
        envvars.update(self.get_storage_provider_envvars())
        return envvars

    def precommand(
        self,
        executor_common_settings: CommonSettings,
        python_executable: str = "python",
    ) -> str:
        precommand = []
        if self.workflow.remote_execution_settings.precommand:
            precommand.append(self.workflow.remote_execution_settings.precommand)
        if (
            executor_common_settings.auto_deploy_default_storage_provider
            and self.workflow.storage_settings.default_storage_provider is not None
        ):
            packages_to_install = set(
                StoragePluginRegistry().get_plugin_package_name(pkg)
                for pkg in self.workflow.storage_provider_settings.keys()
            )
            pkgs = " ".join(packages_to_install)

            precommand.append(f"pip install --target '{PIP_DEPLOYMENTS_PATH}' {pkgs}")

        if (
            SharedFSUsage.SOURCES not in self.workflow.storage_settings.shared_fs_usage
            and self.workflow.remote_execution_settings.job_deploy_sources
            and not executor_common_settings.can_transfer_local_files
        ):
            archive = self.workflow.source_archive
            default_storage_provider_args = self.get_default_storage_provider_args()
            storage_provider_args = " ".join(self.get_storage_provider_args())
            precommand.append(
                f"{python_executable} -m snakemake --deploy-sources "
                f"{archive.query} {archive.checksum} {default_storage_provider_args} "
                f"{storage_provider_args}"
            )

        return " && ".join(precommand)

    def get_configfiles_arg(self):
        # If not shared FS, use relpath for the configfiles (deployed via source archive)
        # Source archive creation ensures that configfiles are in a subdir of the CWD
        # and errors otherwise.
        if SharedFSUsage.SOURCES not in self.workflow.storage_settings.shared_fs_usage:
            configfiles = [
                os.path.relpath(f) for f in self.workflow.overwrite_configfiles
            ]
        else:
            configfiles = self.workflow.overwrite_configfiles
        if configfiles:
            return format_cli_arg("--configfiles", configfiles)
        else:
            return ""

    def general_args(
        self,
        executor_common_settings: CommonSettings,
    ) -> str:
        """Return a string to add to self.exec_job that includes additional
        arguments from the command line. This is currently used in the
        ClusterExecutor and CPUExecutor, as both were using the same
        code. Both have base class of the RealExecutor.
        """
        w2a = self.workflow_property_to_arg

        shared_deployment = (
            SharedFSUsage.SOFTWARE_DEPLOYMENT
            in self.workflow.storage_settings.shared_fs_usage
        )

        # base64 encode the prefix to ensure that eventually unexpanded env vars
        # are not replaced with values (or become empty if missing) by the shell
        if executor_common_settings.non_local_exec and not self.workflow.remote_exec:
            # this is used when the main process submits a job via a remote executor
            local_storage_prefix = w2a(
                "storage_settings.remote_job_local_storage_prefix",
                flag="--local-storage-prefix",
                base64_encode=True,
            )
        else:
            # this is used in the main process when submitting jobs to a local executor
            # or when a remote executor spawns an inner executor
            local_storage_prefix = w2a(
                "storage_settings.local_storage_prefix", base64_encode=True
            )

        args = [
            "--force",
            "--target-files-omit-workdir-adjustment",
            w2a(
                "storage_settings.keep_storage_local",
                flag="--keep-storage-local-copies",
            ),
            "--max-inventory-time 0",
            "--nocolor",
            "--notemp",
            "--no-hooks",
            "--nolock",
            "--ignore-incomplete",
            w2a("execution_settings.keep_incomplete"),
            w2a("output_settings.verbose"),
            w2a("rerun_triggers"),
            w2a(
                "storage_settings.wait_for_free_local_storage",
                convert_value="{}s".format,
            ),
            w2a(
                "execution_settings.cleanup_scripts",
                invert=True,
                flag="--skip-script-cleanup",
            ),
            w2a("execution_settings.shadow_prefix"),
            w2a("deployment_settings.deployment_method"),
            w2a("deployment_settings.conda_frontend"),
            w2a("deployment_settings.conda_prefix"),
            w2a(
                "conda_base_path",
                skip=not shared_deployment,
            ),
            w2a("deployment_settings.apptainer_prefix"),
            w2a("deployment_settings.apptainer_args", base64_encode=True),
            w2a("resource_settings.max_threads"),
            self.get_shared_fs_usage_arg(executor_common_settings),
            w2a(
                "execution_settings.keep_metadata", flag="--drop-metadata", invert=True
            ),
            w2a("workflow_settings.wrapper_prefix"),
            w2a("resource_settings.overwrite_scatter", flag="--set-scatter"),
            w2a("deployment_settings.conda_not_block_search_path_envvars"),
            w2a("config_settings.config_args", flag="--config"),
            w2a("output_settings.printshellcmds"),
            w2a("output_settings.benchmark_extended"),
            w2a("execution_settings.latency_wait"),
            w2a("scheduling_settings.scheduler", flag="--scheduler"),
            w2a("workflow_settings.cache"),
            local_storage_prefix,
            format_cli_arg(
                "--scheduler-solver-path",
                os.path.dirname(sys.executable),
                skip=not shared_deployment,
            ),
            w2a(
                "overwrite_workdir",
                flag="--directory",
                skip=self.workflow.storage_settings.assume_common_workdir,
            ),
            self.get_resource_scopes_arg(),
            self.get_configfiles_arg(),
        ]
        args.extend(self.get_storage_provider_args())
        args.extend(self.get_set_resources_args())
        if executor_common_settings.pass_default_storage_provider_args:
            args.append(self.get_default_storage_provider_args())
        if executor_common_settings.pass_default_resources_args:
            args.append(
                w2a(
                    "resource_settings.default_resources",
                    attr="args",
                    base64_encode=True,
                )
            )
        if executor_common_settings.pass_group_args:
            args.append(self.get_group_args())

        from snakemake.logging import logger

        logger.debug(f"General args: {args}")

        return join_cli_args(args)



================================================
FILE: src/snakemake/storage.py
================================================
import copy
from typing import Any, List, Mapping, Optional, Union
from snakemake.io import flag
from snakemake.workflow import Workflow
from snakemake_interface_common.exceptions import WorkflowError
from snakemake_interface_storage_plugins.registry import StoragePluginRegistry
from snakemake_interface_storage_plugins.storage_provider import StorageProviderBase
from snakemake_interface_storage_plugins.storage_object import (
    StorageObjectWrite,
    StorageObjectRead,
)
from snakemake.io import MaybeAnnotated
from snakemake.common import __version__
from snakemake.logging import logger


def flag_with_storage_object(path: MaybeAnnotated, storage_object):
    modified = flag(storage_object.local_path(), "storage_object", storage_object)
    modified.flags.update(getattr(path, "flags", {}).copy())

    return modified


class StorageRegistry:
    attrs = {
        "workflow",
        "_storages",
        "_default_storage_provider",
        "default_storage_provider",
        "_register_default_storage",
        "register_storage",
        "infer_provider",
        "_storage_object",
        "__getattribute__",
        "attrs",
    }

    def __init__(self, workflow: Workflow):
        self.workflow = workflow
        self._storages: Mapping[str, StorageProviderBase] = dict()
        self._default_storage_provider = None

        if self.workflow.storage_settings.default_storage_provider is not None:
            self._default_storage_provider = self.register_storage(
                self.workflow.storage_settings.default_storage_provider,
                is_default=True,
            )

    @property
    def default_storage_provider(self):
        return self._default_storage_provider

    def register_storage(
        self,
        provider: Optional[str] = None,
        tag: Optional[str] = None,
        is_default: bool = False,
        **settings,
    ):
        if provider is None:
            raise WorkflowError('Storage provider must be specified (provider="...").')
        if provider != provider.lower():
            raise WorkflowError("Storage provider must be lowercase.")

        # First retrieve plugin in order to ensure that it is there.
        plugin = StoragePluginRegistry().get_plugin(provider)

        tagged_settings = self.workflow.storage_provider_settings.get(provider)
        if tagged_settings is None:
            final_settings = None
        else:
            final_settings = tagged_settings.get_settings(tag)
        if final_settings is None and plugin.settings_cls is not None:
            final_settings = plugin.settings_cls()

        if final_settings is not None:
            final_settings = copy.copy(final_settings)
            final_settings.__dict__.update(**settings)

            plugin.validate_settings(final_settings)

        name = tag if tag else plugin.name

        local_prefix = self.workflow.storage_settings.local_storage_prefix / name

        if is_default and not (
            issubclass(
                plugin.storage_provider.get_storage_object_cls(), StorageObjectWrite
            )
            and issubclass(
                plugin.storage_provider.get_storage_object_cls(), StorageObjectRead
            )
        ):
            raise WorkflowError(
                "Default storage provider must be a read-write storage provider, but "
                f"{plugin.name} is not."
            )

        keep_local = settings.get(
            "keep_local", self.workflow.storage_settings.keep_storage_local
        )
        retrieve = settings.get(
            "retrieve", self.workflow.storage_settings.retrieve_storage
        )
        provider_instance = plugin.storage_provider(
            logger=logger,
            local_prefix=local_prefix,
            settings=final_settings,
            keep_local=keep_local,
            retrieve=retrieve,
            is_default=is_default,
            wait_for_free_local_storage=self.workflow.storage_settings.wait_for_free_local_storage,
        )
        self._storages[name] = provider_instance
        # if a tagged storage provider is registered before the untagged then the
        # untagged provider is registered later without the settings
        # prevent the settings loss by registering it here
        if tag is not None and plugin.name not in self._storages:
            local_prefix = (
                self.workflow.storage_settings.local_storage_prefix / plugin.name
            )
            provider_instance = plugin.storage_provider(
                logger=logger,
                local_prefix=local_prefix,
                settings=final_settings,
                keep_local=keep_local,
                retrieve=retrieve,
                is_default=is_default,
                wait_for_free_local_storage=self.workflow.storage_settings.wait_for_free_local_storage,
            )
            self._storages[plugin.name] = provider_instance
        return provider_instance

    def infer_provider(self, query: str):
        plugins = [
            plugin.name
            for plugin in StoragePluginRegistry().plugins.values()
            if plugin.storage_provider.is_valid_query(query)
        ]
        if len(plugins) == 1:
            return plugins[0]
        elif len(plugins) == 0:
            raise WorkflowError(
                f"No storage provider found for query {query}. "
                "Either install the required storage plugin or check your query. "
                "Also consider to explicitly specify the storage provider to get a more "
                "informative error message."
            )
        else:
            raise WorkflowError(
                f"Multiple suitable storage providers found for query {query}: {', '.join(plugins)}. "
                "Explicitly specify the storage provider."
            )

    def __getattribute__(self, name: str) -> Any:
        if name == "attrs":
            return super().__getattribute__(name)
        elif name in self.attrs:
            return super().__getattribute__(name)
        else:
            return StorageProviderProxy(registry=self, name=name)

    def __call__(
        self,
        query: str,
        retrieve: Optional[bool] = None,
        keep_local: Optional[bool] = None,
        **kwargs,
    ):
        return self._storage_object(
            query, provider=None, retrieve=retrieve, keep_local=keep_local, **kwargs
        )

    def _storage_object(
        self,
        query: Union[str, List[str]],
        provider: Optional[str] = None,
        retrieve: Optional[bool] = None,
        keep_local: Optional[bool] = None,
        **kwargs,
    ):
        if isinstance(query, list):
            return [
                self._storage_object(
                    q,
                    provider=provider,
                    retrieve=retrieve,
                    keep_local=keep_local,
                    **kwargs,
                )
                for q in query
            ]

        provider_name = provider

        if provider_name is None:
            provider_name = self.infer_provider(query)

        provider = self._storages.get(provider_name)
        if provider is None:
            provider = self.register_storage(provider_name, **kwargs)

        query_validity = provider.is_valid_query(query)
        if not query_validity:
            raise WorkflowError(
                f"Error applying storage provider {provider_name} "
                "(see https://snakemake.github.io/snakemake-plugin-catalog/plugins/"
                f"storage/{provider}.html). {query_validity}"
            )

        storage_object = provider.object(
            query, retrieve=retrieve, keep_local=keep_local
        )

        return flag_with_storage_object(storage_object.query, storage_object)


class StorageProviderProxy:
    def __init__(self, registry: StorageRegistry, name: str):
        self.registry = registry
        self.name = name

    def __call__(
        self,
        query: str,
        retrieve: Optional[bool] = None,
        keep_local: Optional[bool] = None,
        **kwargs,
    ):
        return self.registry._storage_object(
            query,
            provider=self.name,
            retrieve=retrieve,
            keep_local=keep_local,
            **kwargs,
        )



================================================
FILE: src/snakemake/target_jobs.py
================================================
from collections import namedtuple
import typing

from snakemake_interface_executor_plugins.utils import TargetSpec

from snakemake.common import parse_key_value_arg


def parse_target_jobs_cli_args(target_jobs_args):
    errmsg = "Invalid target wildcards definition: entries have to be defined as WILDCARD=VALUE pairs"
    if target_jobs_args is not None:
        target_jobs = list()
        for entry in target_jobs_args:
            rulename, wildcards = entry.split(":", 1)
            if wildcards:

                def parse_wildcard(entry):
                    return parse_key_value_arg(entry, errmsg)

                wildcards = dict(
                    parse_wildcard(entry) for entry in wildcards.split(",")
                )
                target_jobs.append(TargetSpec(rulename, wildcards))
            else:
                target_jobs.append(TargetSpec(rulename, dict()))
        return target_jobs



================================================
FILE: src/snakemake/utils.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import os
import json
import re
import inspect
import textwrap
from itertools import chain
import collections
import multiprocessing
import string
import shlex
import sys

from snakemake.io import Namedlist, Wildcards
from snakemake.common.configfile import _load_configfile
from snakemake.logging import logger
from snakemake.common import ON_WINDOWS
from snakemake.exceptions import WorkflowError
from snakemake.io import regex_from_filepattern


def validate(data, schema, set_default=True):
    """Validate data with JSON schema at given path.

    Args:
        data (object): data to validate. Can be a config dict or a pandas data frame.
        schema (str): Path to JSON schema used for validation. The schema can also be
            in YAML format. If validating a pandas data frame, the schema has to
            describe a row record (i.e., a dict with column names as keys pointing
            to row values). See https://json-schema.org. The path is interpreted
            relative to the Snakefile when this function is called.
        set_default (bool): set default values defined in schema. See
            https://python-jsonschema.readthedocs.io/en/latest/faq/ for more
            information
    """
    frame = inspect.currentframe().f_back
    workflow = frame.f_globals.get("workflow")

    if workflow and (workflow.modifier.skip_validation or workflow.remote_exec):
        # skip if a corresponding modifier has been defined or if this is a
        # remote job. In the latter case, the schema has been already  validated by the
        # main process.
        return

    from snakemake.sourcecache import LocalSourceFile, infer_source_file
    import jsonschema
    from jsonschema import Draft202012Validator, validators
    from referencing import Registry, Resource

    schemafile = infer_source_file(schema)

    if isinstance(schemafile, LocalSourceFile) and not schemafile.isabs() and workflow:
        # if workflow object is not available this has not been started from a workflow
        schemafile = workflow.current_basedir.join(schemafile.get_path_or_uri())

    source = (
        workflow.sourcecache.open(schemafile)
        if workflow
        else schemafile.get_path_or_uri()
    )

    schema = _load_configfile(source, filetype="Schema")

    def retrieve_uri(uri):
        return Resource.from_contents(contents=_load_configfile(uri, filetype="Schema"))

    resource = Resource.from_contents(contents=schema)
    registry = Registry(retrieve=retrieve_uri).with_resource(
        uri=schemafile.get_path_or_uri(), resource=resource
    )
    Validator = Draft202012Validator(schema, registry=registry)

    # Taken from https://python-jsonschema.readthedocs.io/en/latest/faq/
    def extend_with_default(validator_class):
        validate_properties = validator_class.VALIDATORS["properties"]

        def set_defaults(validator, properties, instance, schema):
            for property, subschema in properties.items():
                if "default" in subschema:
                    instance.setdefault(property, subschema["default"])

            for error in validate_properties(
                validator,
                properties,
                instance,
                schema,
            ):
                yield error

        return validators.extend(
            validator_class,
            {"properties": set_defaults},
        )

    if Validator.META_SCHEMA["$schema"] != schema["$schema"]:
        logger.warning(
            f"No validator found for JSON Schema version identifier '{schema['$schema']}'"
        )
        logger.warning(
            f"Defaulting to validator for JSON Schema version '{Validator.META_SCHEMA['$schema']}'"
        )
        logger.warning("Note that schema file may not be validated correctly.")
    Defaultvalidator = extend_with_default(Validator)

    def _validate_record(record):
        if set_default:
            Defaultvalidator(schema, registry=registry).validate(record)
            return record
        else:
            Validator.validate(record)

    def _validate_pandas(data):
        try:
            import pandas as pd

            if isinstance(data, pd.DataFrame):
                logger.debug("Validating pandas DataFrame")

                recordlist = []
                for i, record in enumerate(data.to_dict("records")):
                    # Exclude NULL values
                    record = {k: v for k, v in record.items() if pd.notnull(v)}
                    try:
                        recordlist.append(_validate_record(record))
                    except jsonschema.exceptions.ValidationError as e:
                        raise WorkflowError(
                            f"Error validating row {i} of data frame.", e
                        )

                if set_default:
                    newdata = pd.DataFrame(recordlist, data.index)
                    # Add missing columns
                    newcol = newdata.columns[~newdata.columns.isin(data.columns)]
                    data[newcol] = None
                    # Fill in None values with values from newdata
                    data.update(newdata)

            else:
                return False
        except ImportError:
            return False
        return True

    def _validate_polars(data):
        try:
            import polars as pl

            if isinstance(data, pl.DataFrame):
                logger.debug("Validating polars DataFrame")

                recordlist = []
                for i, record in enumerate(data.iter_rows(named=True)):
                    # Exclude NULL values
                    record = {
                        k: v
                        for k, v in record.items()
                        if pl.Series(k, [v]).is_not_null().all()
                    }
                    try:
                        recordlist.append(_validate_record(record))
                    except jsonschema.exceptions.ValidationError as e:
                        raise WorkflowError(
                            f"Error validating row {i} of data frame.", e
                        )

                if set_default:
                    newdata = pl.DataFrame(recordlist)
                    # Add missing columns
                    newcol = [col for col in newdata.columns if col not in data.columns]
                    [
                        data.insert_column(
                            len(data.columns),
                            pl.lit(None, newdata[col].dtype).alias(col),
                        )
                        for col in newcol
                    ]
                    # Fill in None values with values from newdata
                    for i in range(data.shape[0]):
                        for j in range(data.shape[1]):
                            if data[i, j] is None:
                                data[i, j] = newdata[i, j]

            elif isinstance(data, pl.LazyFrame):
                # If a LazyFrame is being used, probably it is a large dataframe (so check only first 1000 records)
                logger.debug("Validating first 1000 rows of polars LazyFrame")

                recordlist = []
                for i, record in enumerate(
                    data.head(1000).collect().iter_rows(named=True)
                ):
                    # Exclude NULL values
                    record = {
                        k: v
                        for k, v in record.items()
                        if pl.Series(k, [v]).is_not_null().all()
                    }
                    try:
                        recordlist.append(_validate_record(record))
                    except jsonschema.exceptions.ValidationError as e:
                        raise WorkflowError(
                            f"Error validating row {i} of data frame.", e
                        )

                if set_default:
                    logger.warning("LazyFrame does not support setting default values.")

            else:
                return False
        except ImportError:
            return False
        return True

    if isinstance(data, dict):
        logger.debug("Validating dictionary")
        try:
            _validate_record(data)
        except jsonschema.exceptions.ValidationError as e:
            raise WorkflowError("Error validating config file.", e)
        logger.debug("Dictionary validated!")
    else:
        if _validate_pandas(data):
            logger.debug("Pandas dataframe validated!")
        elif _validate_polars(data):
            logger.debug("Polars dataframe validated!")


def simplify_path(path):
    """Return a simplified version of the given path."""
    relpath = os.path.relpath(path)
    if relpath.startswith("../../"):
        return path
    else:
        return relpath


def linecount(filename):
    """Return the number of lines of the given file.

    Args:
        filename (str): the path to the file
    """
    with open(filename) as f:
        return sum(1 for l in f)


def listfiles(pattern, restriction=None, omit_value=None):
    """Yield a tuple of existing filepaths for the given pattern.

    Wildcard values are yielded as the second tuple item.

    Args:
        pattern (str):       a filepattern. Wildcards are specified in snakemake syntax, e.g. "{id}.txt"
        restriction (dict):  restrict to wildcard values given in this dictionary
        omit_value (str):    wildcard value to omit

    Yields:
        tuple: The next file matching the pattern, and the corresponding wildcards object
    """
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search("{[^{]", pattern)
    if first_wildcard:
        dirname = os.path.dirname(pattern[: first_wildcard.start()])
        if not dirname:
            dirname = "."
    else:
        dirname = os.path.dirname(pattern)
    pattern = re.compile(regex_from_filepattern(pattern))

    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ".":
                f = os.path.normpath(os.path.join(dirpath, f))
            match = re.match(pattern, f)
            if match:
                wildcards = Namedlist(fromdict=match.groupdict())
                if restriction is not None:
                    invalid = any(
                        omit_value not in v and v != wildcards[k]
                        for k, v in restriction.items()
                    )
                    if not invalid:
                        yield f, wildcards
                else:
                    yield f, wildcards


def makedirs(dirnames):
    """Recursively create the given directory or directories without
    reporting errors if they are present.
    """
    if isinstance(dirnames, str):
        dirnames = [dirnames]
    for dirname in dirnames:
        os.makedirs(dirname, exist_ok=True)


def report(
    text,
    path,
    stylesheet=None,
    defaultenc="utf8",
    template=None,
    metadata=None,
    **files,
):
    """Create an HTML report using python docutils.

    This is deprecated in favor of the --report flag.

    Attention: This function needs Python docutils to be installed for the
    python installation you use with Snakemake.

    All keywords not listed below are interpreted as paths to files that shall
    be embedded into the document. The keywords will be available as link
    targets in the text. E.g. append a file as keyword arg via F1=input[0]
    and put a download link in the text like this:

    .. code:: python

        report('''
        ==============
        Report for ...
        ==============

        Some text. A link to an embedded file: F1_.

        Further text.
        ''', outputpath, F1=input[0])

        Instead of specifying each file as a keyword arg, you can also expand
        the input of your rule if it is completely named, e.g.:

        report('''
        Some text...
        ''', outputpath, **input)

    Args:
        text (str):         The "restructured text" as it is expected by python docutils.
        path (str):         The path to the desired output file
        stylesheet (str):   An optional path to a CSS file that defines the style of the document. This defaults to <your snakemake install>/report.css. Use the default to get a hint on how to create your own.
        defaultenc (str):   The encoding that is reported to the browser for embedded text files, defaults to utf8.
        template (str):     An optional path to a docutils HTML template.
        metadata (str):     E.g. an optional author name or email address.

    """
    if stylesheet is None:
        os.path.join(os.path.dirname(__file__), "report.css")
    try:
        import snakemake.report
    except ImportError:
        raise WorkflowError(
            "Python 3 package docutils needs to be installed to use the report function."
        )
    snakemake.report.report(
        text,
        path,
        stylesheet=stylesheet,
        defaultenc=defaultenc,
        template=template,
        metadata=metadata,
        **files,
    )


def R(code):
    """Execute R code.

    This is deprecated in favor of the ``script`` directive.
    This function executes the R code given as a string.
    The function requires rpy2 to be installed.

    Args:
        code (str): R code to be executed
    """
    try:
        import rpy2.robjects as robjects
    except ImportError:
        raise ValueError(
            "Python 3 package rpy2 needs to be installed to use the R function."
        )
    robjects.r(format(textwrap.dedent(code), stepout=2))


class SequenceFormatter(string.Formatter):
    """string.Formatter subclass with special behavior for sequences.

    This class delegates the formatting of individual elements to another
    formatter object. Non-list objects are formatted by calling the
    delegate formatter's "format_field" method. List-like objects
    (list, tuple, set, frozenset) are formatted by formatting each
    element of the list according to the specified format spec using
    the delegate formatter and then joining the resulting strings with
    a separator (space by default).

    """

    def __init__(
        self, separator=" ", element_formatter=string.Formatter(), *args, **kwargs
    ):
        self.separator = separator
        self.element_formatter = element_formatter

    def format_element(self, elem, format_spec):
        """Format a single element

        For sequences, this is called once for each element in a
        sequence. For anything else, it is called on the entire
        object. It is intended to be overridden in subclases.

        """
        return self.element_formatter.format_field(elem, format_spec)

    def format_field(self, value, format_spec):
        if isinstance(value, Wildcards):
            return ",".join(
                f"{name}={value}"
                for name, value in sorted(value.items(), key=lambda item: item[0])
            )
        if isinstance(value, (list, tuple, set, frozenset)):
            return self.separator.join(
                self.format_element(v, format_spec) for v in value
            )
        else:
            return self.format_element(value, format_spec)


class QuotedFormatter(string.Formatter):
    """Subclass of string.Formatter that supports quoting.

    Using this formatter, any field can be quoted after formatting by
    appending "q" to its format string. By default, shell quoting is
    performed using "shlex.quote", but you can pass a different
    quote_func to the constructor. The quote_func simply has to take a
    string argument and return a new string representing the quoted
    form of the input string.

    Note that if an element after formatting is the empty string, it
    will not be quoted.

    """

    def __init__(self, quote_func=None, *args, **kwargs):
        if quote_func is None:
            quote_func = shlex.quote
        self.quote_func = quote_func
        super().__init__(*args, **kwargs)

    def format_field(self, value, format_spec):
        if format_spec.endswith("u"):
            format_spec = format_spec[:-1]
            do_quote = False
        else:
            do_quote = format_spec.endswith("q")
            if do_quote:
                format_spec = format_spec[:-1]
        formatted = super().format_field(value, format_spec)
        if do_quote and formatted != "":
            formatted = self.quote_func(formatted)
        return formatted


class AlwaysQuotedFormatter(QuotedFormatter):
    """Subclass of QuotedFormatter that always quotes.

    Usage is identical to QuotedFormatter, except that it *always*
    acts like "q" was appended to the format spec, unless u (for unquoted) is appended.

    """

    def format_field(self, value, format_spec):
        if not format_spec.endswith("q") and not format_spec.endswith("u"):
            format_spec += "q"
        return super().format_field(value, format_spec)


def format(_pattern, *args, stepout=1, _quote_all=False, quote_func=None, **kwargs):
    """Format a pattern in Snakemake style.

    This means that keywords embedded in braces are replaced by any variable
    values that are available in the current namespace.
    """

    frame = inspect.currentframe().f_back
    while stepout > 1:
        if not frame.f_back:
            break
        frame = frame.f_back
        stepout -= 1

    variables = dict(frame.f_globals)
    # add local variables from calling rule/function
    variables.update(frame.f_locals)
    if "self" in variables and sys.version_info < (3, 5):
        # self is the first arg of fmt.format as well. Not removing it would
        # cause a multiple values error on Python <=3.4.2.
        del variables["self"]
    variables.update(kwargs)
    fmt = SequenceFormatter(separator=" ")
    if _quote_all:
        fmt.element_formatter = AlwaysQuotedFormatter(quote_func)
    else:
        fmt.element_formatter = QuotedFormatter(quote_func)
    try:
        return fmt.format(_pattern, *args, **variables)
    except KeyError as ex:
        if (
            "wildcards" in variables
            and str(ex).strip("'") in variables["wildcards"].keys()
        ):
            raise NameError(
                "The name '{0}' is unknown in this context. "
                "Did you mean 'wildcards.{0}'?".format(str(ex).strip("'"))
            )
        raise NameError(
            "The name {} is unknown in this context. Please "
            "make sure that you defined that variable. "
            "Also note that braces not used for variable access "
            "have to be escaped by repeating them, "
            "i.e. {{{{print $1}}}}".format(str(ex))
        )


class Unformattable:
    def __init__(self, errormsg="This cannot be used for formatting"):
        self.errormsg = errormsg

    def __str__(self):
        raise ValueError(self.errormsg)


def read_job_properties(
    jobscript, prefix="# properties", pattern=re.compile("# properties = (.*)")
):
    """Read the job properties defined in a snakemake jobscript.

    This function is a helper for writing custom wrappers for the
    snakemake --cluster functionality. Applying this function to a
    jobscript will return a dict containing information about the job.
    """
    with open(jobscript) as jobscript:
        for m in map(pattern.match, jobscript):
            if m:
                return json.loads(m.group(1))


def min_version(version):
    """Require minimum snakemake version, raise workflow error if not met."""
    from packaging.version import parse
    from snakemake.common import __version__

    if parse(__version__) < parse(version):
        raise WorkflowError(
            "Expecting Snakemake version {} or higher (you are currently using {}).".format(
                version, __version__
            )
        )


def update_config(config, overwrite_config):
    """Recursively update dictionary config with overwrite_config in-place.

    See
    https://stackoverflow.com/questions/3232943/update-value-of-a-nested-dictionary-of-varying-depth
    for details.

    Args:
      config (dict): dictionary to update
      overwrite_config (dict): dictionary whose items will overwrite those in config
    """

    def _update_config(config, overwrite_config):
        """Necessary as recursive calls require a return value,
        but `update_config()` has no return value.
        """
        for key, value in overwrite_config.items():
            if not isinstance(config, collections.abc.Mapping):
                # the config cannot be updated as it is no dict
                # -> just overwrite it with the new value
                config = {}
            if isinstance(value, collections.abc.Mapping):
                sub_config = config.get(key, {})
                config[key] = _update_config(sub_config, value)
            else:
                config[key] = value
        return config

    _update_config(config, overwrite_config)


def available_cpu_count():
    """
    Return the number of available virtual or physical CPUs on this system.
    The number of available CPUs can be smaller than the total number of CPUs
    when the cpuset(7) mechanism is in use, as is the case on some cluster
    systems.

    Adapted from https://stackoverflow.com/a/1006301/715090
    """
    try:
        with open("/proc/self/status") as f:
            status = f.read()
        m = re.search(r"(?m)^Cpus_allowed:\s*(.*)$", status)
        if m:
            res = bin(int(m.group(1).replace(",", ""), 16)).count("1")
            if res > 0:
                return min(res, multiprocessing.cpu_count())
    except IOError:
        pass

    return multiprocessing.cpu_count()


def argvquote(arg, force=True):
    """Returns an argument quoted in such a way that CommandLineToArgvW
    on Windows will return the argument string unchanged.
    This is the same thing Popen does when supplied with a list of arguments.
    Arguments in a command line should be separated by spaces; this
    function does not add these spaces. This implementation follows the
    suggestions outlined here:
    https://blogs.msdn.microsoft.com/twistylittlepassagesallalike/2011/04/23/everyone-quotes-command-line-arguments-the-wrong-way/
    """
    if not force and len(arg) != 0 and not any([c in arg for c in ' \t\n\v"']):
        return arg
    else:
        n_backslashes = 0
        cmdline = '"'
        for c in arg:
            if c == "\\":
                # first count the number of current backslashes
                n_backslashes += 1
                continue
            if c == '"':
                # Escape all backslashes and the following double quotation mark
                cmdline += (n_backslashes * 2 + 1) * "\\"
            else:
                # backslashes are not special here
                cmdline += n_backslashes * "\\"
            n_backslashes = 0
            cmdline += c
        # Escape all backslashes, but let the terminating
        # double quotation mark we add below be interpreted
        # as a metacharacter
        cmdline += +n_backslashes * 2 * "\\" + '"'
        return cmdline


def cmd_exe_quote(arg):
    """Quotes an argument in a cmd.exe compliant way."""
    arg = argvquote(arg)
    cmd_exe_metachars = '^()%!"<>&|'
    for char in cmd_exe_metachars:
        arg.replace(char, "^" + char)
    return arg


def os_sync():
    """Ensure flush to disk"""
    if not ON_WINDOWS:
        os.sync()


def find_bash_on_windows():
    """
    Find the path to a usable bash on windows.
    The first attempt is to look for a bash installed with a git conda package.
    Alternatively, try bash installed with 'Git for Windows'.
    """
    if not ON_WINDOWS:
        return None
    # First look for bash in git's conda package
    bashcmd = os.path.join(os.path.dirname(sys.executable), r"Library\bin\bash.exe")
    if not os.path.exists(bashcmd):
        # Otherwise try bash installed with "Git for Windows".
        import winreg

        try:
            key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, "SOFTWARE\\GitForWindows")
            gfwp, _ = winreg.QueryValueEx(key, "InstallPath")
            bashcmd = os.path.join(gfwp, "bin\\bash.exe")
        except FileNotFoundError:
            bashcmd = ""
    return bashcmd if os.path.exists(bashcmd) else None


class Paramspace:
    """A wrapper for pandas dataframes that provides helpers for using them as a parameter
    space in Snakemake.

    This is heavily inspired by @soumitrakp work on JUDI (https://github.com/ncbi/JUDI).

    By default, a directory structure with on folder level per parameter is created
    (e.g. column1~{column1}/column2~{column2}/***).

    The exact behavior can be tweaked with four parameters:

      - ``filename_params`` takes a list of column names of the passed dataframe.
        These names are used to build the filename (separated by '_') in the order
        in which they are passed.
        All remaining parameters will be used to generate a directory structure.
        Example for a data frame with four columns named column1 to column4:

        | ``Paramspace(df, filename_params=["column3", "column2"])`` ->
        | column1~{value1}/column4~{value4}/column3~{value3}_column2~{value2}

        If ``filename_params="*"``, all columns of the dataframe are encoded into
        the filename instead of parent directories.

      - ``param_sep`` takes a string which is used to join the column name and
        column value in the generated paths (Default: '~'). Example:

        | ``Paramspace(df, param_sep=":")`` ->
        | column1:{value1}/column2:{value2}/column3:{value3}/column4:{value4}

      - ``filename_sep`` takes a string which is used to join the parameter
        entries listed in ``filename_params`` in the generated paths
        (Default: '_'). Example:

        | ``Paramspace(df, filename_params="*", filename_sep="-")`` ->
        | column1~{value1}-column2~{value2}-column3~{value3}-column4~{value4}

      - ``single_wildcard`` takes a string which is used to replace the
        default behavior of using a wildcard for each column in the dataframe
        with a single wildcard that is used to encode all column values.
        The given string is the name of that wildcard. The value of the wildcard
        for individual instances of the paramspace is still controlled by above
        other arguments. The single_wildcard mechanism can be handy if you want
        to define a rule that shall be used for multiple paramspaces with different
        columns.
    """

    def __init__(
        self,
        dataframe,
        filename_params=None,
        param_sep="~",
        filename_sep="_",
        single_wildcard=None,
    ):
        self.dataframe = dataframe
        self.param_sep = param_sep
        self.filename_sep = filename_sep
        self.single_wildcard = single_wildcard
        if filename_params is None or not filename_params:
            # create a pattern of the form {}/{}/{} with one entry for each
            # column in the dataframe
            self.pattern = "/".join([r"{}"] * len(self.dataframe.columns))
            self.ordered_columns = self.dataframe.columns
        else:
            if isinstance(filename_params, str) and filename_params == "*":
                filename_params = dataframe.columns

            if any((param not in dataframe.columns for param in filename_params)):
                raise KeyError(
                    "One or more entries of filename_params are not valid column names for the param file."
                )
            elif len(set(filename_params)) != len(filename_params):
                raise ValueError("filename_params must be unique")
            # create a pattern of the form {}/{}_{} with one entry for each
            # column in the dataframe. The number of underscore-separated
            # fields is equal to the number filename_params
            self.pattern = "/".join(
                [r"{}"] * (len(self.dataframe.columns) - len(filename_params) + 1)
            )
            self.pattern = self.filename_sep.join(
                [self.pattern] + [r"{}"] * (len(filename_params) - 1)
            )
            self.ordered_columns = [
                param
                for param in self.dataframe.columns
                if param not in filename_params
            ]
            self.ordered_columns.extend(list(filename_params))
        self.dataframe = self.dataframe[self.ordered_columns]

    @property
    def wildcard_pattern(self):
        """Wildcard pattern over all columns of the underlying dataframe of the form
        column1~{column1}/column2~{column2}/*** or of the provided custom pattern.
        """
        if self.single_wildcard:
            return f"{{{self.single_wildcard}}}"
        else:
            return self.pattern.format(
                *map(
                    self.param_sep.join(("{0}", "{{{0}}}")).format, self.ordered_columns
                )
            )

    @property
    def instance_patterns(self):
        """Iterator over all instances of the parameter space (dataframe rows),
        formatted as file patterns of the form column1~{value1}/column2~{value2}/...
        or of the provided custom pattern.
        """
        import pandas as pd

        fmt_value = lambda value: "NA" if pd.isna(value) else value
        return (
            self.pattern.format(
                *(
                    self.param_sep.join(("{}", "{}")).format(name, fmt_value(value))
                    for name, value in row._asdict().items()
                )
            )
            for row in self.dataframe.itertuples(index=False)
        )

    def instance(self, wildcards):
        """Obtain instance (dataframe row) with the given wildcard values."""
        import pandas as pd
        from snakemake.io import regex_from_filepattern

        def convert_value_dtype(name, value):
            if self.dataframe.dtypes[name] == bool and value == "False":
                # handle problematic case when boolean False is returned as
                # boolean True because the string "False" is misinterpreted
                return False
            if value == "NA":
                return pd.NA
            else:
                return pd.Series([value]).astype(self.dataframe.dtypes[name])[0]

        if self.single_wildcard:
            wildcard_value = wildcards.get(self.single_wildcard)
            if wildcard_value is None:
                raise WorkflowError(
                    f"Error processing paramspace: wildcard {self.single_wildcard} is not used in rule."
                )

            pattern = self.pattern.format(
                *(
                    f"{name}{self.param_sep}{{{name}}}"
                    for name in self.dataframe.columns
                )
            )
            rexp = re.compile(regex_from_filepattern(pattern))
            match = rexp.match(wildcard_value)
            if not match:
                raise WorkflowError(
                    f"Error processing paramspace: wildcard {self.single_wildcard}={wildcards.get(self.single_wildcard)} does not match pattern {pattern}."
                )
            return {
                name: convert_value_dtype(name, value)
                for name, value in match.groupdict().items()
            }
        else:
            return {
                name: convert_value_dtype(name, value)
                for name, value in wildcards.items()
                if name in self.ordered_columns
            }

    def __getattr__(self, name):
        import pandas as pd

        ret = getattr(self.dataframe, name)
        if isinstance(ret, pd.DataFrame):
            return Paramspace(ret)
        return ret

    def __getitem__(self, key):
        import pandas as pd

        ret = self.dataframe[key]
        if isinstance(ret, pd.DataFrame):
            return Paramspace(ret)
        return ret



================================================
FILE: src/snakemake/wrapper.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"


from snakemake.exceptions import WorkflowError
from snakemake.script import script
from snakemake.sourcecache import SourceCache, infer_source_file


PREFIX = "https://github.com/snakemake/snakemake-wrappers/raw/"

EXTENSIONS = [".py", ".R", ".Rmd", ".jl"]


def is_script(source_file):
    filename = source_file.get_filename()
    return (
        filename.endswith("wrapper.py")
        or filename.endswith("wrapper.R")
        or filename.endswith("wrapper.jl")
    )


def get_path(path, prefix=None):
    if not is_url(path):
        if prefix is None:
            prefix = PREFIX
        elif prefix.startswith("git+file"):
            parts = path.split("/")
            path = "/" + "/".join(parts[1:]) + "@" + parts[0]
        path = prefix + path
    return infer_source_file(path)


def is_url(path):
    return (
        path.startswith("http")
        or path.startswith("file:")
        or path.startswith("git+file")
    )


def find_extension(source_file, sourcecache: SourceCache):
    for ext in EXTENSIONS:
        if source_file.get_filename().endswith(f"wrapper{ext}"):
            return source_file

    for ext in EXTENSIONS:
        script = source_file.join(f"wrapper{ext}")

        if sourcecache.exists(script):
            return script


def get_script(path, sourcecache: SourceCache, prefix=None):
    path = get_path(path, prefix=prefix)
    return find_extension(path, sourcecache)


def get_conda_env(path, prefix=None):
    path = get_path(path, prefix=prefix)
    if is_script(path):
        # URLs and posixpaths share the same separator. Hence use posixpath here.
        path = path.get_basedir()
    return path.join("environment.yaml")


def wrapper(
    path,
    input,
    output,
    params,
    wildcards,
    threads,
    resources,
    log,
    config,
    rulename,
    conda_env,
    conda_base_path,
    container_img,
    singularity_args,
    env_modules,
    bench_record,
    prefix,
    jobid,
    bench_iteration,
    cleanup_scripts,
    shadow_dir,
    sourcecache_path,
    runtime_sourcecache_path,
):
    """
    Load a wrapper from https://github.com/snakemake/snakemake-wrappers under
    the given path + wrapper.(py|R|Rmd) and execute it.
    """
    assert path is not None
    script_source = get_script(
        path,
        SourceCache(sourcecache_path, runtime_cache_path=runtime_sourcecache_path),
        prefix=prefix,
    )
    if script_source is None:
        raise WorkflowError(
            f"Unable to locate wrapper script for wrapper {path}. "
            "This can be a network issue or a mistake in the wrapper URL."
        )
    script(
        script_source.get_path_or_uri(),
        "",
        input,
        output,
        params,
        wildcards,
        threads,
        resources,
        log,
        config,
        rulename,
        conda_env,
        conda_base_path,
        container_img,
        singularity_args,
        env_modules,
        bench_record,
        jobid,
        bench_iteration,
        cleanup_scripts,
        shadow_dir,
        sourcecache_path,
        runtime_sourcecache_path,
    )



================================================
FILE: src/snakemake/assets/__init__.py
================================================
# This module handles the download of non python assets.
# It should not use any modules that are not part of the standard library because it will
# be called before the setup (and dependency deployment) of the snakemake package.

from dataclasses import dataclass
import hashlib
import importlib.resources
from pathlib import Path
from typing import Dict, Optional
import urllib.request
import urllib.error


class AssetDownloadError(Exception):
    pass


@dataclass
class Asset:
    url: str
    version: Optional[str] = None
    sha256: Optional[str] = None

    def get_content(self) -> bytes:
        """Get and validate asset content."""
        url = self.url.format(version=self.version) if self.version else self.url
        req = urllib.request.Request(url, headers={"User-Agent": "snakemake"})
        err = None
        for _ in range(6):
            try:
                resp = urllib.request.urlopen(req)
                content = resp.read()
            except urllib.error.URLError as e:
                err = AssetDownloadError(f"Failed to download asset {url}: {e}")
                continue
            if self.sha256 is not None:
                content_sha = hashlib.sha256(content).hexdigest()
                if self.sha256 != content_sha:
                    err = AssetDownloadError(
                        f"Checksum mismatch when downloading asset {self.url} "
                        f"(sha: {content_sha}). First 100 bytes:\n{content[:100].decode()}"
                    )
                    continue
            return content
        assert err is not None
        raise err


class Assets:
    _base_path: Optional[Path] = None
    spec: Dict[str, Asset] = {
        "snakemake/LICENSE.md": Asset(
            url="https://raw.githubusercontent.com/snakemake/snakemake/main/LICENSE.md",
            sha256="84a1a82b05c80637744d3fe8257235c15380efa6cc32608adf4b21f17af5d2b8",
        ),
        "pygments/LICENSE": Asset(
            url="https://raw.githubusercontent.com/pygments/pygments/master/LICENSE",
            sha256="a9d66f1d526df02e29dce73436d34e56e8632f46c275bbdffc70569e882f9f17",
        ),
        "tailwindcss/LICENSE": Asset(
            url="https://raw.githubusercontent.com/tailwindlabs/tailwindcss/refs/tags/v{version}/LICENSE",
            sha256="60e0b68c0f35c078eef3a5d29419d0b03ff84ec1df9c3f9d6e39a519a5ae7985",
            version="3.4.16",
        ),
        "tailwindcss/tailwind.css": Asset(
            url="https://cdn.tailwindcss.com/{version}?plugins=forms@0.5.9,typography@0.5.2",
            # The tailwindcss cdn checksum is not stable. Since this is only included
            # as CSS styles, the risk is low.
            version="3.4.16",
        ),
        "react/LICENSE": Asset(
            url="https://raw.githubusercontent.com/facebook/react/refs/tags/v{version}/LICENSE",
            sha256="52412d7bc7ce4157ea628bbaacb8829e0a9cb3c58f57f99176126bc8cf2bfc85",
            version="18.2.0",
        ),
        "react/react.production.min.js": Asset(
            url="https://cdnjs.cloudflare.com/ajax/libs/react/{version}/umd/react.production.min.js",
            sha256="4b4969fa4ef3594324da2c6d78ce8766fbbc2fd121fff395aedf997db0a99a06",
            version="18.2.0",
        ),
        "react/react-dom.production.min.js": Asset(
            url="https://cdnjs.cloudflare.com/ajax/libs/react-dom/{version}/umd/react-dom.production.min.js",
            sha256="21758ed084cd0e37e735722ee4f3957ea960628a29dfa6c3ce1a1d47a2d6e4f7",
            version="18.2.0",
        ),
        "vega/vega.js": Asset(
            url="https://cdnjs.cloudflare.com/ajax/libs/vega/{version}/vega.js",
            sha256="b34c43055ef5d39a093e937522955dc359fbaec6c5b0259ae2de4c9da698e9fe",
            version="5.21.0",
        ),
        "vega/LICENSE": Asset(
            url="https://raw.githubusercontent.com/vega/vega/refs/tags/v{version}/LICENSE",
            sha256="b75f7ed0af20dedadf92c52bc236161bcf0d294ff2e6e34ca76403203349f71d",
            version="5.21.0",
        ),
        # Begin dependencies for vega, included in vega/vega.js
        # Versions from https://github.com/vega/vega/blob/v5.21.0/yarn.lock.
        # Via vega-crossfilter, vega-encode, vega-format,
        # vega-functions, vega-geo, vega-regression, vega-scale, vega-statistics,
        # vega-time, vega-transforms, vega-view; and via d3-geo (via vega-functions,
        # vega-geo, vega-projection); and via d3-geo-projection (via vega-projection)
        #
        # This has its own dependency, internmap (1.0.1); however, while we can
        # find code from vega-crossfilter and d3-array in vega.js, there does
        # not appear to be any code from internmap after tree-shaking.
        "d3-array/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-array/refs/tags/v{version}/LICENSE",
            sha256="0747bebeb06b10954913149be9b9a8bdf6fad3e6fdcbd9f9524e7a94c13d2cea",
            version="2.12.1",
        ),
        # Via vega-format
        "d3-format/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-format/refs/tags/v{version}/LICENSE",
            sha256="7eea8533ea92bd8c32a901e89ecb0305890905bb12711449565ddff96891146d",
            version="2.0.0",
        ),
        # Via vega-format
        "d3-time-format/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-time-format/refs/tags/v{version}/LICENSE",
            sha256="7a3cb0e5055874e67db9aa2d5fe26de23204fa994ffbad198901ffe9c812a717",
            version="3.0.0",
        ),
        # Via vega-time, vega-view; and via d3-time-format (via vega-format)
        "d3-time/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-time/refs/tags/v{version}/LICENSE",
            sha256="e1211892da0b0e0585b7aebe8f98c1274fba15bafe47fa1f4ee8a7a502c06304",
            version="2.1.1",
        ),
        # Via vega-encode, vega-scale
        "d3-interpolate/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-interpolate/refs/tags/v{version}/LICENSE",
            sha256="e1211892da0b0e0585b7aebe8f98c1274fba15bafe47fa1f4ee8a7a502c06304",
            version="2.0.1",
        ),
        # Via vega-functions, vega-geo; and via d3-interpolate (via vega-encode)
        "d3-color/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-color/refs/tags/v{version}/LICENSE",
            sha256="e1211892da0b0e0585b7aebe8f98c1274fba15bafe47fa1f4ee8a7a502c06304",
            version="2.0.0",
        ),
        # Via vega-scale
        "d3-scale/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-scale/refs/tags/v{version}/LICENSE",
            sha256="7eea8533ea92bd8c32a901e89ecb0305890905bb12711449565ddff96891146d",
            version="3.3.0",
        ),
        # Via vega-expression (vega-lite also depends on vega-expression)
        # The license file included in the NPM package does not exist directly
        # in https://github.com/DefinitelyTyped/DefinitelyTyped, so we use an
        # unpkg URL to reference the contents of the NPM package instead.
        "@types-estree/LICENSE": Asset(
            url="https://unpkg.com/@types/estree@{version}/LICENSE",
            sha256="c2cfccb812fe482101a8f04597dfc5a9991a6b2748266c47ac91b6a5aae15383",
            version="0.0.50",
        ),
        # Via vega-force
        "d3-force/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-force/refs/tags/v{version}/LICENSE",
            sha256="e1211892da0b0e0585b7aebe8f98c1274fba15bafe47fa1f4ee8a7a502c06304",
            version="2.1.1",
        ),
        # Via d3-force (via vega-force)
        "d3-dispatch/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-dispatch/refs/tags/v{version}/LICENSE",
            sha256="e1211892da0b0e0585b7aebe8f98c1274fba15bafe47fa1f4ee8a7a502c06304",
            version="2.0.0",
        ),
        # Via d3-force (via vega-force)
        "d3-quadtree/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-quadtree/refs/tags/v{version}/LICENSE",
            sha256="e1211892da0b0e0585b7aebe8f98c1274fba15bafe47fa1f4ee8a7a502c06304",
            version="2.0.0",
        ),
        # Via vega-view; and via d3-force (via vega-force)
        "d3-timer/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-timer/refs/tags/v{version}/LICENSE",
            sha256="e1211892da0b0e0585b7aebe8f98c1274fba15bafe47fa1f4ee8a7a502c06304",
            version="2.0.0",
        ),
        # Via vega-functions, vega-geo, vega-projection (directly and via
        # d3-geo-projection)
        "d3-geo/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-geo/refs/tags/v{version}/LICENSE",
            sha256="c3c9d41a75b64963748657932167ec4c56404b8fd557d4f89c9eeda6e2fdf39a",
            version="2.0.2",
        ),
        # Via vega-hierarchy
        "d3-hierarchy/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-hierarchy/refs/tags/v{version}/LICENSE",
            sha256="e1211892da0b0e0585b7aebe8f98c1274fba15bafe47fa1f4ee8a7a502c06304",
            version="2.0.0",
        ),
        # Via vega-loader
        # This has its own dependencies (commander 2.20.3, iconv-lite 0.4.24,
        # rw 1.3.3), but they are all used only in the command-line tools
        # rather than in the library that is bundled in vega.js.
        "d3-dsv/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-dsv/refs/tags/v{version}/LICENSE",
            sha256="8b5304265ccedbd17567aa14a0dc6b9bf4013fee44964c46aa54fdf8527d6a68",
            version="2.0.0",
        ),
        # Via vega-loader; also from vega-geo/src/util/contours.js, "Based on
        # https://github.com/topojson/topojson-client/blob/v3.0.0/src/stitch.js"
        # This has its own dependency (commander 2.20.3), but it is used only
        # in the command-line tools rather than in the library that is bundled
        # in vega.js. Dependency node-fetch 2.6.4, via vega-loader, is not
        # used for browser builds.
        "topojson-client/LICENSE": Asset(
            url="https://raw.githubusercontent.com/topojson/topojson-client/refs/tags/v{version}/LICENSE",
            sha256="4c4d15b635e04e691825a76db7d33f7f2033b55669a7430011694f31e6c65999",
            version="3.1.0",
        ),
        # Via vega-projection
        # This has its own dependencies (commander 2.20.3, resolve 1.20.0), but
        # they are all used only in the command-line tools rather than in the
        # library that is bundled in vega.js.
        "d3-geo-projection/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-geo-projection/refs/tags/v{version}/LICENSE",
            sha256="4108a126a74cc35d4d5ae39ca5d0cc926fa7c8ec40e459a0c6d3481c69decbd8",
            version="3.0.0",
        ),
        # Via vega-scenegraph (directly and via d3-shape)
        "d3-path/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-path/refs/tags/v{version}/LICENSE",
            sha256="b8265327ab678f554800e71188b1cc6b1ff57522e292d2b1c0be66f27cf328b6",
            version="2.0.0",
        ),
        # Via vega-scenegraph
        "d3-shape/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-shape/refs/tags/v{version}/LICENSE",
            sha256="7eea8533ea92bd8c32a901e89ecb0305890905bb12711449565ddff96891146d",
            version="2.1.0",
        ),
        # Via vega-voronoi
        "d3-delaunay/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-delaunay/refs/tags/v{version}/LICENSE",
            sha256="d19bf35c4080eea32e098a9c8d62540ffd425611ac8dfc856a233f0357d8b02e",
            version="5.2.0",
        ),
        # Via d3-delaunay (via vega-voronoi)
        "delaunator/LICENSE": Asset(
            url="https://raw.githubusercontent.com/mapbox/delaunator/refs/tags/v{version}/LICENSE",
            sha256="27043d1a6a0e1985fde12660decbbd3b23c67de900b00609c90d4f0aa492f425",
            version="4.0.1",
        ),
        # End dependencies for vega, included in vega/vega.js
        # Begin copied/derived/adapted code in vega, included in vega/vega.js
        # From vega-util/src/lruCache.js, "adapted from
        # https://github.com/dominictarr/hashlru/ (MIT License)". Version 1.0.4
        # was the current version when the function was added to vega, in
        # commit f153863f3575bff1b42294c9cb065d80afb757ff on 2020-02-28.
        "hashlru/LICENSE": Asset(
            url="https://raw.githubusercontent.com/dominictarr/hashlru/refs/tags/v{version}/LICENSE",
            sha256="08e4bd7a280eaaf1fbcaad9dad1fb94412477fcdd1cf81660988303297e5e1d1",
            version="1.0.4",
        ),
        # From
        # vega-statistics/src/regression/{poly,r-squared,pow,log,linear}.js,
        # "Adapted from d3-regression by Harry Stevens". Version 1.2.1 was the
        # current version when the functions were added to vega, in commit
        # 71610e4456a3a4145435d83f8458748ba137a2a3 2019-05-10. That release was
        # not tagged in git, so we use the corresponding commit hash.
        "d3-regression/LICENSE": Asset(
            url="https://raw.githubusercontent.com/HarryStevens/d3-regression/e23d40a663dffba14b92cb42d9989de3a32894b5/LICENSE",
            sha256="d210203f922101502894baf700b9a392e323a26e4b603ab166c57e09a6e773b5",
            version="1.2.1",
        ),
        # From vega-statistics/src/regression/poly.js, "Adapted from
        # d3-regression by Harry Stevens [...] which was adapted from
        # regression-js by Tom Alexander" Version 2.0.1 was the current version
        # when the functions were added to vega, in commit
        # 71610e4456a3a4145435d83f8458748ba137a2a3 2019-05-10.
        "regression/LICENSE": Asset(
            url="https://raw.githubusercontent.com/Tom-Alexander/regression-js/refs/tags/{version}/LICENSE",
            sha256="2f932f5cfb9b042cc6c0089ee8004b33e3746ffeab879341dbd453c150524307",
            version="2.0.1",
        ),
        # From vega-statistics/src/regression/loess.js, "Adapted from
        # science.js by Jason Davies". Version 1.9.3 was the current version
        # when the functions were added to vega, in commit
        # 71610e4456a3a4145435d83f8458748ba137a2a3 on 2019-05-10.
        "science/LICENSE": Asset(
            url="https://raw.githubusercontent.com/jasondavies/science.js/refs/tags/v{version}/LICENSE",
            sha256="3bd1fdf686ffcad175daddcb36ee28599ac8f090b6cec2c7654118c8a6f3d4c9",
            version="1.9.3",
        ),
        # From src/quickselect.js in d3-array, "Based on
        # https://github.com/mourner/quickselect". Version 2.0.0 was the
        # current version when the file was added to d3-array, in commit
        # d447c2a31cd6aacf54a40b22c29620f7e17bbd7e on 2018-11-10.
        "quickselect/LICENSE": Asset(
            url="https://raw.githubusercontent.com/mourner/quickselect/refs/tags/v{version}/LICENSE",
            sha256="597034cb7c11c916ad407344ea99a0b08e3c443a6b4421460f1d23c411c69707",
            version="2.0.0",
        ),
        # From vega-statistics/src/normal.js, "Ported from Apache Commons
        # Math". Version 3.6.1 was the current version when this implementation
        # of erfinv was added to vega, in commit
        # c26050f21b0c95620f8f1a3094056716ec6c5aaa on 2019-09-26.
        "commons-math/LICENSE.txt": Asset(
            url="https://raw.githubusercontent.com/apache/commons-math/refs/tags/MATH_3_6_1/LICENSE.txt",
            sha256="64f23963615950bad9ddd31569a7f09afbbe11b81d94873ffd9b1cac6081a11d",
            version="3.6.1",
        ),
        "commons-math/NOTICE.txt": Asset(
            url="https://raw.githubusercontent.com/apache/commons-math/refs/tags/MATH_3_6_1/NOTICE.txt",
            sha256="5495442a32bfc2b93b4a8f2c34c5c218d16cca434aa5684fb953d9419120e3fa",
            version="3.6.1",
        ),
        # From vega-expression/src/parser.js, "The following expression parser
        # is based on Esprima (http://esprima.org/)." Unusually, the original
        # license text is reproduced in the source file, so we could get by
        # without a separate license file (and in fact the license file lacks
        # the necessary copyright statement present in the code) but we add it
        # for consistency. Version 2.2.0 was the last version of Esprima that
        # had a copyright statement in esprima.js attributing specific
        # individual contributors, as reproduced in the vega source.
        "esprima/LICENSE.BSD": Asset(
            url="https://raw.githubusercontent.com/jquery/esprima/refs/tags/{version}/LICENSE.BSD",
            sha256="0e74697a68cebdcd61502c30fe80ab7f9e341d995dcd452023654d57133534b1",
            version="2.2.0",
        ),
        # From vega-scenegraph/src/path/arc.js, "Copied from Inkscape svgtopdf,
        # thanks!" We cannot find the original implementation in Inkscape or
        # anywhere else, so we do not add a license file for this snippet.
        # From vega-scenegraph/src/path/parse.js, "Path parsing and rendering
        # code adapted from fabric.js -- Thanks!" This was added in commit
        # 82932143de7cef4187a34026689df12abaa25959 on 2018-12-20, and the
        # current release of fabric.js at that time was 2.4.5.
        "fabric/LICENSE": Asset(
            url="https://raw.githubusercontent.com/fabricjs/fabric.js/refs/tags/v{version}/LICENSE",
            sha256="9f6c2cc99aa9c618df93fed7d1cf7279d4e329d92dd2ce5e96173c73ce305055",
            version="2.4.5",
        ),
        # From vega-geo/src/util/{contours,density2D}.js, "Implementation
        # adapted from d3/d3-contour. Thanks!" When these routines were added
        # in 0ab6b730a7e576d33d00e12063855bb132194191 on 2019-11-11, the latest
        # version was 1.3.2.
        "d3-contour/LICENSE": Asset(
            url="https://raw.githubusercontent.com/d3/d3-contour/refs/tags/v{version}/LICENSE",
            sha256="5f5dcce265668080a60fbdc513f6f8ef21466780bcaa331e64ee39df19e63b30",
            version="1.3.2",
        ),
        # From vega-geo/src/util/contours.js, "Based on
        # https://github.com/mbostock/shapefile/blob/v0.6.2/shp/polygon.js"
        "shapefile/LICENSE": Asset(
            url="https://raw.githubusercontent.com/mbostock/shapefile/refs/tags/v{version}/LICENSE.txt",
            sha256="c16529a9d5b8802982abd714a6823344e24b0cb5131596bc343927ead605d708",
            version="0.6.2",
        ),
        # End copied/derived/adapted code in vega, included in vega/vega.js
        "vega-lite/vega-lite.js": Asset(
            url="https://cdnjs.cloudflare.com/ajax/libs/vega-lite/{version}/vega-lite.js",
            sha256="6eb7f93121cd9f44cf8640244f87c5e143f87c7a0b6cd113da4a9e41e3adf0aa",
            version="5.2.0",
        ),
        "vega-lite/LICENSE": Asset(
            url="https://raw.githubusercontent.com/vega/vega-lite/refs/tags/v{version}/LICENSE",
            sha256="f618900fd0d64046963b29f40590cdd1e341a2f41449f99110d82fd81fea808c",
            version="5.2.0",
        ),
        # Begin dependencies for vega-lite, included in vega-lite/vega-lite.js
        # Versions from https://github.com/vega/vega-lite/blob/v5.2.0/yarn.lock.
        "@types-clone/LICENSE": Asset(
            url="https://unpkg.com/@types/clone@{version}/LICENSE",
            sha256="c2cfccb812fe482101a8f04597dfc5a9991a6b2748266c47ac91b6a5aae15383",
            version="2.1.1",
        ),
        "array-flat-polyfill/LICENSE": Asset(
            # Releases are not tagged in git; we use the commit hash
            # corresponding to the 1.0.1 release
            url="https://raw.githubusercontent.com/jonathantneal/array-flat-polyfill/362d855cb6ea2ef12f0676b116c5f9e4233b6f31/LICENSE.md",
            sha256="597756adcb51f243ef4fb386920377f61d012ace0904364e1a8ee9aaec6afc84",
            version="1.0.1",
        ),
        "clone/LICENSE": Asset(
            url="https://raw.githubusercontent.com/pvorb/clone/refs/tags/v{version}/LICENSE",
            sha256="3fb0857ef0133928cf72c88dfc464e931486e88778961eedec25585e2321507f",
            version="2.1.2",
        ),
        "fast-deep-equal/LICENSE": Asset(
            url="https://raw.githubusercontent.com/epoberezkin/fast-deep-equal/refs/tags/v{version}/LICENSE",
            sha256="7bf9b2de73a6b356761c948d0e9eeb4be6c1270bd04c79cd489c1e400ffdfc1a",
            version="3.1.3",
        ),
        "fast-json-stable-stringify/LICENSE": Asset(
            url="https://raw.githubusercontent.com/epoberezkin/fast-json-stable-stringify/refs/tags/v{version}/LICENSE",
            sha256="a833d366242c298cf1b10263516572fb8dcbe68eb5072cdcac2b4546e2b4eb36",
            version="2.1.0",
        ),
        "json-stringify-pretty-compact/LICENSE": Asset(
            url="https://raw.githubusercontent.com/lydell/json-stringify-pretty-compact/refs/tags/v{version}/LICENSE",
            sha256="d0800a7c9b5f723f7fe029f75d3a9ae44173178b9729d1521e18371ab24ae97a",
            version="3.0.0",
        ),
        # The tslib package is among vega-lite's direct dependencies, and the
        # yarn.lock file shows three versions could be included: 2.2.0, 2.1.0
        # (both MIT-licensed), and 1.1.0 (Apache-2.0-licensed). However, we
        # found no trace of any of these in the actual vega-lite.js bundle.
        # ----
        # The following packages are part of vega, and are already covered by
        # vega/LICENSE.
        # - vega-event-selector: no dependencies
        # - vega-expression: depends on vega-util, @types/estree (already
        #   covered as one of vega's dependencies)
        # - vega-util (no dependencies; includes code from hashlru, already
        #   covered as one of vega's dependencies)
        # ----
        # End dependencies for vega-lite, included in vega-lite/vega-lite.js
        # Begin copied/derived/adapted code in vega-lite, included in vega-lite/vega-lite.js
        # - hashlru is included in vega-util; already covered as one of vega's dependencies
        # End copied/derived/adapted code in vega-lite, included in vega-lite/vega-lite.js
        "vega-embed/vega-embed.js": Asset(
            url="https://cdnjs.cloudflare.com/ajax/libs/vega-embed/{version}/vega-embed.js",
            sha256="4e546c1f86eb200333606440e92f76e2940b905757018d9672cd1708e4e6ff0a",
            version="6.20.8",
        ),
        "vega-embed/LICENSE": Asset(
            url="https://raw.githubusercontent.com/vega/vega-embed/refs/tags/v{version}/LICENSE",
            sha256="32df67148f0fc3db0eb9e263a7b75d07f1eb14c61955005a4a39c6918d10d137",
            version="6.20.8",
        ),
        # Begin dependencies for vega-embed, included in vega-embed/vega-embed.js
        # Versions from https://github.com/vega/vega-embed/blob/v6.20.8/yarn.lock.
        "fast-json-patch/LICENSE": Asset(
            url="https://raw.githubusercontent.com/Starcounter-Jack/JSON-Patch/refs/tags/v{version}/LICENSE.txt",
            sha256="26593d78926902a82eb9bc7e40d1811fc381055f23516b92fe48ca8399bf02df",
            version="3.1.0",
        ),
        # json-stringify-pretty-compact 3.0.0 is already covered as one of
        # vega's dependencies
        # ----
        # There are four versions of the semver package in yarn.lock (5.7.1,
        # 6.3.0, 7.0.0, 7.3.5), but it appears that only one copy remains in
        # the vega-embed.js bundle. The bundle has a compareBuild function,
        # added in 6.1.0. It does not export a "tokens" list, added in 6.3.0
        # and present in 7.0.0, but that is not conclusive because the package
        # was split into separate source files to improve tree-shaking. By
        # 7.3.5, the first line of the SemVer class constructor changes to
        # "options = parseOptions(options)", and this does match what we see in
        # the bundle, so we conclude that we have semver 7.3.5. In any case,
        # the ISC license was unchanged across all four versions.
        "semver/LICENSE": Asset(
            url="https://raw.githubusercontent.com/npm/node-semver/refs/tags/v{version}/LICENSE",
            sha256="4ec3d4c66cd87f5c8d8ad911b10f99bf27cb00cdfcff82621956e379186b016b",
            version="7.3.5",
        ),
        # The tslib package is among vega-embed's direct dependencies, and the
        # yarn.lock file shows four versions could be included: 2.3.1, 2.1.0,
        # 1.14.1 (all MIT-licensed), and 1.1.0 (Apache-2.0-licensed). However,
        # we found no trace of any of these in the actual vega-embed.js bundle.
        # ----
        # The following packages are part of vega, and are already covered by
        # vega/LICENSE.
        # - vega-interpreter: no dependencies
        # ----
        # This is not part of https://github.com/vega/vega, as far as we can
        # tell, so we handle it separately. The repository
        # https://github.com/vega/schema linked from NPM appears to contain
        # only JSON schemas, not JavaScript source code, so we use an unpkg URL
        # to reference the contents of the NPM package instead.
        "vega-schema-url-parser/LICENSE": Asset(
            url="https://unpkg.com/vega-schema-url-parser@{version}/LICENSE",
            sha256="eeb1606fda238f623c36d1eb5e27c33b986a446445340b220c141097424af045",
            version="2.2.0",
        ),
        "vega-themes/LICENSE": Asset(
            url="https://raw.githubusercontent.com/vega/vega-themes/refs/tags/v{version}/LICENSE",
            sha256="ceb75fa4fbebbc381c0465442a502ffc13b73f36ccb385a66bcd5f55869e3979",
            version="2.10.0",
        ),
        # Depends on vega-util, which is already covered in vega
        "vega-tooltip/LICENSE": Asset(
            url="https://raw.githubusercontent.com/vega/vega-tooltip/refs/tags/v{version}/LICENSE",
            sha256="93cd20d9cee8c77c080d1a585e3b07644ad2c8e9ccdfc86e350136884d62d935",
            version="0.28.0",
        ),
        # End dependencies for vega-embed, included in vega-embed/vega-embed.js
        # Begin copied/derived/adapted code in vega-embed, included in vega-embed/vega-embed.js
        # - hashlru is included in vega-util (via vega-tooltip); already
        #   covered as one of vega's dependencies
        # - _areEquals() is based on fast-deep-equal; already covered as one of
        #   vega-lite's dependencies
        # - googlecharts theme in vega-themes carries its own copyright statement,
        #     Copyright 2020 Google LLC.
        #
        #     Use of this source code is governed by a BSD-style
        #     license that can be found in the LICENSE file or at
        #     https://developers.google.com/open-source/licenses/bsd
        #   No plain-text version of that URL is available, and the license has
        #   the same BSD-3-Clause permission statement as vega-themes itself,
        #   so we consider the combination of vega-themes/LICENSE and the
        #   copyright statement in the source code sufficient.
        # End copied/derived/adapted code in vega-embed, included in vega-embed/vega-embed.js
        "heroicons/LICENSE": Asset(
            url="https://raw.githubusercontent.com/tailwindlabs/heroicons/refs/tags/v{version}/LICENSE",
            sha256="75523ddd65d9620bea09f84e89d0c373b4205a3708b8a1e9f9598a5438a3e641",
            version="1.0.3",
        ),
        "prop-types/prop-types.min.js": Asset(
            url="https://cdnjs.cloudflare.com/ajax/libs/prop-types/{version}/prop-types.min.js",
            sha256="4c88350517ee82aa4f3368e67ef1a453ca6636dcfa6449b4e3d6faa5c877066e",
            version="15.7.2",
        ),
        "prop-types/LICENSE": Asset(
            url="https://raw.githubusercontent.com/facebook/prop-types/refs/tags/v{version}/LICENSE",
            sha256="f657f99d3fb9647db92628e96007aabb46e5f04f33e49999075aab8e250ca7ce",
            version="15.7.2",
        ),
        # Begin dependencies for prop-types, included in prop-types/prop-types.min.js
        # Versions from https://github.com/facebook/prop-types/blob/v15.7.2/yarn.lock
        # ----
        # The loose-envify package is among prop-types's direct dependencies,
        # and the yarn.lock file shows two versions could be included: 1.4.0
        # and 1.3.1 (both MIT-licensed). However, we found no trace of
        # loose-envify code in the actual prop-types.min.js bundle, so it
        # appears this dependency was removed in tree-shaking, along with its
        # dependency js-tokens (3.0.0 and/or 4.0.0).
        # ----
        # The object-assign package (version 4.1.1) is among prop-types's
        # direct dependencies, but we found no trace of it in the actual
        # prop-types.min.js bundle, so it appears this dependency (a polyfill)
        # was removed in tree-shaking.
        # ----
        # The react-is package (version 16.8.1) is among prop-types's
        # direct dependencies, but we found no trace of it in the actual
        # prop-types.min.js bundle, so it appears this dependency was removed
        # in tree-shaking.
        # ----
        # End dependencies for prop-types, included in prop-types/prop-types.min.js
    }

    @classmethod
    def deploy(cls) -> None:
        # this has to work from setup.py without being able to load the snakemake
        # modules.
        base_path = Path(__file__).parent / "data"
        for asset_path, asset in cls.spec.items():
            target_path = base_path / asset_path

            if target_path.exists():
                with open(target_path, "rb") as fin:
                    # file is already present, check if it is up to date
                    if (asset.sha256 is None) or (
                        asset.sha256 == hashlib.sha256(fin.read()).hexdigest()
                    ):
                        continue

            target_path.parent.mkdir(parents=True, exist_ok=True)
            with open(target_path, "wb") as fout:
                fout.write(asset.get_content())

    @classmethod
    def get_content(cls, asset_path: str) -> str:
        try:
            return (cls.base_path() / asset_path).read_text(encoding="utf-8")
        except FileNotFoundError:
            from snakemake.logging import logger

            logger.warning(
                f"Asset {asset_path} not found (development setup?), downloading..."
            )
            return cls.spec[asset_path].get_content().decode("utf-8")

    @classmethod
    def get_version(cls, asset_path: str) -> Optional[str]:
        if asset_path in cls.spec:
            return cls.spec[asset_path].version
        else:
            return None

    @classmethod
    def base_path(cls) -> Path:
        # this is called from within snakemake, so we can use importlib.resources
        if cls._base_path is None:
            cls._base_path = importlib.resources.files("snakemake.assets") / "data"
        return cls._base_path



================================================
FILE: src/snakemake/caching/__init__.py
================================================
__authors__ = "Johannes Köster, Sven Nahnsen"
__copyright__ = "Copyright 2022, Johannes Köster, Sven Nahnsen"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from abc import ABCMeta, abstractmethod
import os
from pathlib import Path

from snakemake.jobs import Job
from snakemake.io import apply_wildcards
from snakemake.exceptions import (
    MissingOutputFileCachePathException,
    WorkflowError,
    CacheMissException,
)
from snakemake.caching.hash import ProvenanceHashMap
from snakemake.logging import logger

LOCATION_ENVVAR = "SNAKEMAKE_OUTPUT_CACHE"


class AbstractOutputFileCache:
    __metaclass__ = ABCMeta

    def __init__(self):
        try:
            self.cache_location = os.environ[LOCATION_ENVVAR]
        except KeyError:
            raise MissingOutputFileCachePathException()
        self.provenance_hash_map = ProvenanceHashMap()

    @abstractmethod
    async def store(self, job: Job, cache_mode):
        pass

    @abstractmethod
    async def fetch(self, job: Job, cache_mode):
        pass

    @abstractmethod
    async def exists(self, job: Job):
        pass

    def get_outputfiles(self, job: Job):
        if job.rule.output[0].is_multiext:
            prefix_len = len(
                apply_wildcards(job.rule.output[0].multiext_prefix, job.wildcards)
            )
            yield from ((f, f[prefix_len:]) for f in job.output)
        else:
            assert (
                len(job.output) == 1
            ), "bug: multiple output files in cacheable job but multiext not used for declaring them"
            # It is crucial to distinguish cacheable objects by the file extension.
            # Otherwise, for rules that generate different output based on the provided
            # extension a wrong cache entry can be returned.
            # Another nice side effect is that the cached files become more accessible
            # because their extension is presented in the cache dir.
            ext = Path(job.output[0]).suffix
            yield (job.output[0], ext)

    def raise_write_error(self, entry, exception=None):
        raise WorkflowError(
            "Given output cache entry {} ($SNAKEMAKE_OUTPUT_CACHE={}) is not writeable.".format(
                entry, self.cache_location
            ),
            *[exception],
        )

    def raise_read_error(self, entry, exception=None):
        raise WorkflowError(
            "Given output cache entry {} ($SNAKEMAKE_OUTPUT_CACHE={}) is not readable.".format(
                entry, self.cache_location
            ),
            *[exception],
        )

    def raise_cache_miss_exception(self, job):
        raise CacheMissException(f"Job {job} not yet cached.")



================================================
FILE: src/snakemake/caching/hash.py
================================================
__authors__ = "Johannes Köster, Sven Nahnsen"
__copyright__ = "Copyright 2022, Johannes Köster, Sven Nahnsen"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"


import hashlib
import json

from snakemake.jobs import Job
from snakemake import script
from snakemake import wrapper
from snakemake.exceptions import WorkflowError
from snakemake.settings.types import DeploymentMethod

# ATTENTION: increase version number whenever the hashing algorithm below changes!
__version__ = "0.1"


class ProvenanceHashMap:
    def __init__(self):
        self._hashes = dict()

    def get_provenance_hash(self, job: Job, cache_mode: str):
        versioned_hash = hashlib.sha256()
        # Ensure that semantic version changes in this module
        versioned_hash.update(self._get_provenance_hash(job, cache_mode).encode())
        versioned_hash.update(__version__.encode())
        return versioned_hash.hexdigest()

    def _get_provenance_hash(self, job: Job, cache_mode: str):
        """
        Recursively calculate hash for the output of the given job
        and all upstream jobs in a blockchain fashion.

        This is based on an idea of Sven Nahnsen.
        Fails if job has more than one output file. The reason is that there
        is no way to generate a per-output file hash without generating the files.
        This hash, however, shall work without having to generate the files,
        just by describing all steps down to a given job.
        """
        assert (cache_mode == "omit-software") or (cache_mode == "all")

        if job in self._hashes:
            return self._hashes[job]

        workflow = job.dag.workflow
        h = hashlib.sha256()

        # Hash shell command or script.
        if job.is_shell:
            # We cannot use the formatted shell command, because it also contains threads,
            # resources, and filenames (which shall be irrelevant for the hash).
            h.update(job.rule.shellcmd.encode())
        elif job.is_script:
            _, source, _, _, _ = script.get_source(
                job.rule.script,
                job.rule.workflow.sourcecache,
                basedir=job.rule.basedir,
                wildcards=job.wildcards,
                params=job.params,
            )
            h.update(source.encode())
        elif job.is_notebook:
            _, source, _, _, _ = script.get_source(
                job.rule.notebook,
                job.rule.workflow.sourcecache,
                basedir=job.rule.basedir,
                wildcards=job.wildcards,
                params=job.params,
            )
            h.update(source.encode())
        elif job.is_wrapper:
            _, source, _, _, _ = script.get_source(
                wrapper.get_script(
                    job.rule.wrapper,
                    sourcecache=job.rule.workflow.sourcecache,
                    prefix=workflow.workflow_settings.wrapper_prefix,
                ),
                job.rule.workflow.sourcecache,
                basedir=job.rule.basedir,
                wildcards=job.wildcards,
                params=job.params,
            )
            h.update(source.encode())

        # Hash params.
        for key, value in sorted(job.params._allitems()):
            if key is not None:
                h.update(key.encode())
            # If this raises a TypeError, we cannot calculate a reliable hash.
            try:
                h.update(json.dumps(value, sort_keys=True).encode())
            except TypeError as e:
                raise WorkflowError(
                    "Rule {} cannot be cached, because params "
                    "are not JSON serializable. "
                    "Consider converting them into a suitable format "
                    "if you are sure that caching is necessary. "
                    "Otherwise, deactivate caching for this rule "
                    "by removing it from the --cache command line argument "
                    "or removing the cache: true directive from the rule itself.".format(
                        job.rule.name
                    ),
                    e,
                )

        # Hash input files that are not generated by other jobs (sorted by hash value).
        for file_hash in sorted(
            hash_file(f)
            for f in job.input
            if not any(f in depfiles for depfiles in job.dag.dependencies[job].values())
        ):
            h.update(file_hash.encode())

        # Hash used containers or conda environments.
        if cache_mode != "omit-software":
            if (
                DeploymentMethod.CONDA in workflow.deployment_settings.deployment_method
                and job.conda_env
            ):
                if (
                    DeploymentMethod.APPTAINER
                    in workflow.deployment_settings.deployment_method
                    and job.conda_env.container_img_url
                ):
                    h.update(job.conda_env.container_img_url.encode())
                h.update(job.conda_env.content)
            elif (
                DeploymentMethod.APPTAINER
                in workflow.deployment_settings.deployment_method
                and job.container_img_url
            ):
                h.update(job.container_img_url.encode())

        # Generate hashes of dependencies, and add them in a blockchain fashion (as input to the current hash, sorted by hash value).
        for dep_hash in sorted(
            self._get_provenance_hash(dep, cache_mode)
            for dep in set(job.dag.dependencies[job].keys())
        ):
            h.update(dep_hash.encode())

        provenance_hash = h.hexdigest()

        # Store for reuse.
        self._hashes[job] = provenance_hash

        return provenance_hash


def hash_file(f):
    h = hashlib.sha256()
    with open(f, "rb") as f:
        # Read and update hash string value in blocks of 4K
        for byte_block in iter(lambda: f.read(4096), b""):
            h.update(byte_block)
    return h.hexdigest()



================================================
FILE: src/snakemake/caching/local.py
================================================
__authors__ = "Johannes Köster, Sven Nahnsen"
__copyright__ = "Copyright 2022, Johannes Köster, Sven Nahnsen"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from tempfile import TemporaryDirectory
from pathlib import Path
import os
import shutil
import stat

from snakemake.logging import logger
from snakemake.jobs import Job
from snakemake.exceptions import WorkflowError
from snakemake.caching import AbstractOutputFileCache


class OutputFileCache(AbstractOutputFileCache):
    """
    A cache for output files that uses a provenance hash value that
    describes all steps, parameters, and software needed to generate
    each output file.
    """

    def __init__(self):
        super().__init__()
        self.path = Path(self.cache_location)
        # make readable/writeable for all
        self.file_permissions = (
            stat.S_IRUSR
            | stat.S_IWUSR
            | stat.S_IRGRP
            | stat.S_IWGRP
            | stat.S_IROTH
            | stat.S_IWOTH
        )
        # directories need to have exec permission as well (for opening)
        self.dir_permissions = (
            self.file_permissions | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH
        )

    def check_writeable(self, cachefile):
        if not (os.access(cachefile.parent, os.W_OK) or os.access(cachefile, os.W_OK)):
            self.raise_write_error(cachefile)

    def check_readable(self, cachefile):
        if not os.access(cachefile, os.R_OK):
            self.raise_read_error(cachefile)

    async def store(self, job: Job, cache_mode: str):
        """
        Store generated job output in the cache.
        """

        if not os.access(self.path, os.W_OK):
            raise WorkflowError(
                "Cannot access cache location {}. Please ensure that "
                "it is present and writeable.".format(self.path)
            )
        with TemporaryDirectory(dir=self.path) as tmpdirname:
            tmpdir = Path(tmpdirname)

            for outputfile, cachefile in self.get_outputfiles_and_cachefiles(
                job, cache_mode
            ):
                if not os.path.exists(outputfile):
                    raise WorkflowError(
                        f"Cannot move output file {outputfile} to cache. It does not exist "
                        "(maybe it was not created by the job?)."
                    )
                self.check_writeable(cachefile)
                logger.info(f"Moving output file {outputfile} to cache.")

                tmp = tmpdir / cachefile.name
                # First move is performed into a tempdir (it might involve a copy if not on the same FS).
                # This is important, such that network filesystem latency
                # does not lead to concurrent writes to the same file.
                # We can use the plain copy method of shutil, because we do not care about the metadata.
                shutil.move(outputfile, tmp, copy_function=shutil.copy)

                self.set_permissions(tmp)

                # Move to the actual path (now we are on the same FS, hence move is atomic).
                # Here we use the default copy function, also copying metadata (which is important here).
                # It will always work, because we are guaranteed to be in the same FS.
                shutil.move(tmp, cachefile)
                # now restore the outputfile via a symlink
                self.symlink(cachefile, outputfile, utime=False)

    async def fetch(self, job: Job, cache_mode: str):
        """
        Retrieve cached output file and symlink to the place where the job expects it's output.
        """
        for outputfile, cachefile in self.get_outputfiles_and_cachefiles(
            job, cache_mode
        ):
            if not cachefile.exists():
                self.raise_cache_miss_exception(job)

            logger.debug(
                "Output file {} exists as {} in the cache.".format(
                    outputfile, cachefile
                )
            )

            self.check_readable(cachefile)
            if cachefile.is_dir():
                # For directories, create a new one and symlink each entry.
                # Then, the .snakemake_timestamp of the new dir is touched
                # by the executor.
                outputfile.mkdir(parents=True, exist_ok=True)
                for f in cachefile.iterdir():
                    self.symlink(f, outputfile / f.name)
            else:
                self.symlink(cachefile, outputfile)

    async def exists(self, job: Job, cache_mode: str):
        """
        Return True if job is already cached
        """
        for outputfile, cachefile in self.get_outputfiles_and_cachefiles(
            job, cache_mode
        ):
            if not cachefile.exists():
                return False

            logger.debug(
                "Output file {} exists as {} in the cache.".format(
                    outputfile, cachefile
                )
            )

            self.check_readable(cachefile)
        return True

    def get_outputfiles_and_cachefiles(self, job: Job, cache_mode: str):
        provenance_hash = self.provenance_hash_map.get_provenance_hash(job, cache_mode)
        base_path = self.path / provenance_hash

        return (
            (Path(outputfile), Path(f"{base_path}{ext}"))
            for outputfile, ext in self.get_outputfiles(job)
        )

    def symlink(self, path, outputfile, utime=True):
        if os.utime in os.supports_follow_symlinks or not utime:
            logger.info(f"Symlinking output file {outputfile} from cache.")
            os.symlink(path, outputfile)
            if utime:
                os.utime(outputfile, follow_symlinks=False)
        else:
            logger.info(
                "Copying output file {} from cache (OS does not support updating the modification date of symlinks).".format(
                    outputfile
                )
            )
            shutil.copyfile(path, outputfile)

    def set_permissions(self, entry):
        # make readable/writeable for all
        if entry.is_dir():
            # recursively apply permissions for all contained files
            for root, dirs, files in os.walk(entry):
                root = Path(root)
                for d in dirs:
                    os.chmod(root / d, self.dir_permissions)
                for f in files:
                    os.chmod(root / f, self.file_permissions)
            os.chmod(entry, self.dir_permissions)
        else:
            os.chmod(entry, self.file_permissions)



================================================
FILE: src/snakemake/caching/storage.py
================================================
__authors__ = "Johannes Köster, Sven Nahnsen"
__copyright__ = "Copyright 2022, Johannes Köster, Sven Nahnsen"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import os
from pathlib import Path

from snakemake.caching import AbstractOutputFileCache
from snakemake.exceptions import WorkflowError
from snakemake.jobs import Job
from snakemake_interface_storage_plugins.storage_provider import StorageProviderBase


class OutputFileCache(AbstractOutputFileCache):
    """
    A cache for output files that uses a provenance hash value that
    describes all steps, parameters, and software needed to generate
    each output file. This is the remote version.
    """

    def __init__(self, storage_provider: StorageProviderBase):
        super().__init__()
        self.storage_provider = storage_provider

    async def store(self, job: Job, cache_mode: str):
        for entry in self._get_storage_objects(
            job, cache_mode, check_output_exists=True
        ):
            # upload to remote
            try:
                await entry.managed_store()
            except Exception as e:
                self.raise_write_error(entry, exception=e)

    async def fetch(self, job: Job, cache_mode: str):
        for entry in self._get_storage_objects(job, cache_mode):
            if not await entry.managed_exists():
                self.raise_cache_miss_exception(job)

            # download to outputfile
            try:
                await entry.managed_retrieve()
            except Exception as e:
                self.raise_read_error(entry, exception=e)

    async def exists(self, job: Job, cache_mode: str):
        for entry in self._get_storage_objects(job, cache_mode):
            try:
                return await entry.managed_exists()
            except Exception as e:
                self.raise_read_error(entry, exception=e)

    def _get_storage_objects(
        self, job: Job, cache_mode: str, check_output_exists=False
    ):
        provenance_hash = self.provenance_hash_map.get_provenance_hash(job, cache_mode)

        for outputfile, ext in self.get_outputfiles(job):
            if check_output_exists and not os.path.exists(outputfile):
                raise WorkflowError(
                    "Cannot move output file {} to cache. It does not exist "
                    "(maybe it was not created by the job?)."
                )

            storage_object = self.storage_provider.object(
                f"{self.cache_location}/{provenance_hash}{ext}"
            )
            storage_object.set_local_path(Path(outputfile))
            yield storage_object



================================================
FILE: src/snakemake/common/__init__.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2023, Johannes Köster"
__email__ = "johannes.koester@protonmail.com"
__license__ = "MIT"

import contextlib
import itertools
import math
import operator
import platform
import hashlib
import inspect
import shutil
import sys
from typing import Callable, List, Tuple
import uuid
import os
import asyncio
import collections
from pathlib import Path
from typing import Union

from snakemake import __version__
from snakemake_interface_common.exceptions import WorkflowError


MIN_PY_VERSION: Tuple[int, int] = (3, 7)
UUID_NAMESPACE = uuid.uuid5(uuid.NAMESPACE_URL, "https://snakemake.readthedocs.io")
NOTHING_TO_BE_DONE_MSG = (
    "Nothing to be done (all requested files are present and up to date)."
)

ON_WINDOWS = platform.system() == "Windows"
# limit the number of input/output files list in job properties
# see https://github.com/snakemake/snakemake/issues/2097
IO_PROP_LIMIT = 100
SNAKEFILE_CHOICES = list(
    map(
        Path,
        (
            "Snakefile",
            "snakefile",
            "workflow/Snakefile",
            "workflow/snakefile",
        ),
    )
)


def get_snakemake_searchpaths():
    paths = [str(Path(__file__).parent.parent.parent)] + [
        path for path in sys.path if os.path.isdir(path)
    ]
    return list(unique_justseen(paths))


def get_report_id(path: Union[str, Path]) -> str:
    h = hashlib.sha256()
    h.update(str(path).encode())

    return h.hexdigest()


def mb_to_mib(mb):
    return int(math.ceil(mb * 0.95367431640625))


def parse_key_value_arg(arg, errmsg, strip_quotes=True):
    try:
        key, val = arg.split("=", 1)
    except ValueError:
        raise ValueError(errmsg + f" (Unparsable value: {repr(arg)})")
    if strip_quotes:
        val = val.strip("'\"")
    return key, val


def dict_to_key_value_args(
    some_dict: dict, quote_str: bool = True, repr_obj: bool = False
):
    items = []
    for key, value in some_dict.items():
        if repr_obj and not isinstance(value, str):
            encoded = repr(value)
        else:
            encoded = f"'{value}'" if quote_str and isinstance(value, str) else value
        items.append(f"{key}={encoded}")
    return items


def async_run(coroutine):
    """Attaches to running event loop or creates a new one to execute a
    coroutine.
    .. seealso::
         https://github.com/snakemake/snakemake/issues/1105
         https://stackoverflow.com/a/65696398
    """
    try:
        return asyncio.run(coroutine)
    except RuntimeError as e:
        coroutine.close()
        raise WorkflowError(
            "Error running coroutine in event loop. Snakemake currently does not "
            "support being executed from an already running event loop. "
            "If you run Snakemake e.g. from a Jupyter notebook, make sure to spawn a "
            "separate process for Snakemake.",
            e,
        )


APPDIRS = None


RULEFUNC_CONTEXT_MARKER = "__is_snakemake_rule_func"


def get_appdirs():
    global APPDIRS
    if APPDIRS is None:
        from appdirs import AppDirs

        APPDIRS = AppDirs("snakemake", "snakemake")
    return APPDIRS


def is_local_file(path_or_uri):
    return parse_uri(path_or_uri).scheme == "file"


def parse_uri(path_or_uri):
    from smart_open import parse_uri

    try:
        return parse_uri(path_or_uri)
    except NotImplementedError as e:
        # Snakemake sees a lot of URIs which are not supported by smart_open yet
        # "docker", "git+file", "shub", "ncbi","root","roots","rootk", "gsiftp",
        # "srm","ega","ab","dropbox"
        # Fall back to a simple split if we encounter something which isn't supported.
        scheme, _, uri_path = path_or_uri.partition("://")
        if scheme and uri_path:
            uri = collections.namedtuple("Uri", ["scheme", "uri_path"])
            return uri(scheme, uri_path)
        else:
            raise e


def smart_join(base, path, abspath=False):
    if is_local_file(base):
        full = os.path.join(base, path)
        if abspath:
            return os.path.abspath(full)
        return full
    else:
        from smart_open import parse_uri

        uri = parse_uri(f"{base}/{path}")
        if not ON_WINDOWS:
            # Norm the path such that it does not contain any ../,
            # which is invalid in an URL.
            assert uri.uri_path[0] == "/"
            uri_path = os.path.normpath(uri.uri_path)
        else:
            uri_path = uri.uri_path
        return f"{uri.scheme}:/{uri_path}"


def num_if_possible(s):
    """Convert string to number if possible, otherwise return string."""
    try:
        return int(s)
    except ValueError:
        try:
            return float(s)
        except ValueError:
            return s


def get_last_stable_version():
    return __version__.split("+")[0]


def get_container_image():
    return f"snakemake/snakemake:v{get_last_stable_version()}"


def get_uuid(name):
    return uuid.uuid5(UUID_NAMESPACE, name)


def get_file_hash(filename, algorithm="sha256"):
    """find the SHA256 hash string of a file. We use this so that the
    user can choose to cache working directories in storage.
    """
    from snakemake.logging import logger

    # The algorithm must be available
    try:
        hasher = hashlib.new(algorithm)
    except ValueError as ex:
        logger.error("%s is not an available algorithm." % algorithm)
        raise ex

    with open(filename, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


def bytesto(bytes, to, bsize=1024):
    """convert bytes to megabytes.
    bytes to mb: bytesto(bytes, 'm')
    bytes to gb: bytesto(bytes, 'g' etc.
    From https://gist.github.com/shawnbutts/3906915
    """
    levels = {"k": 1, "m": 2, "g": 3, "t": 4, "p": 5, "e": 6}
    answer = float(bytes)
    for _ in range(levels[to]):
        answer = answer / bsize
    return answer


def strip_prefix(text, prefix):
    if text.startswith(prefix):
        return text[len(prefix) :]
    return text


def log_location(msg):
    from snakemake.logging import logger

    callerframerecord = inspect.stack()[1]
    frame = callerframerecord[0]
    info = inspect.getframeinfo(frame)
    logger.debug(
        "{}: {info.filename}, {info.function}, {info.lineno}".format(msg, info=info)
    )


def group_into_chunks(n, iterable):
    """Group iterable into chunks of size at most n.

    See https://stackoverflow.com/a/8998040.
    """
    it = iter(iterable)
    while True:
        chunk = tuple(itertools.islice(it, n))
        if not chunk:
            return
        yield chunk


class Rules:
    """A namespace for rules so that they can be accessed via dot notation."""

    def __init__(self):
        self._rules = dict()

    def _register_rule(self, name, rule):
        self._rules[name] = rule

    def __getattr__(self, name):
        from snakemake.exceptions import WorkflowError

        try:
            return self._rules[name]
        except KeyError:
            raise WorkflowError(
                f"Rule {name} is not defined in this workflow. "
                f"Available rules: {', '.join(self._rules)}"
            )


class Scatter:
    """A namespace for scatter to allow items to be accessed via dot notation."""

    pass


class Gather:
    """A namespace for gather to allow items to be accessed via dot notation."""

    pass


FUNC_OVERWRITE_PARAMS_ATTR = "_overwrite_params"


def get_function_params(func: Callable):
    if hasattr(func, FUNC_OVERWRITE_PARAMS_ATTR):
        return getattr(func, FUNC_OVERWRITE_PARAMS_ATTR)
    else:
        return inspect.signature(func).parameters


def overwrite_function_params(func: Callable, params: List[str]):
    """Force function params to be the given list. Useful for functions that
    use *args to get all parameters in dynamically created cases like in
    snakemake.ioutils.subpath.subpath.
    """
    setattr(func, FUNC_OVERWRITE_PARAMS_ATTR, params)


def get_input_function_aux_params(func, candidate_params):
    func_params = get_function_params(func)
    has_var_keyword = any(
        param.kind == param.VAR_KEYWORD for param in func_params.values()
    )
    if has_var_keyword:
        # If the function has a **kwargs parameter, we assume that it can take any
        # parameter, so we return all candidate parameters.
        return candidate_params
    else:
        return {k: v for k, v in candidate_params.items() if k in func_params}


def unique_justseen(iterable, key=None):
    """
    List unique elements, preserving order. Remember only the element just seen.

    From https://docs.python.org/3/library/itertools.html#itertools-recipes
    """
    # unique_justseen('AAAABBBCCDAABBB') --> A B C D A B
    # unique_justseen('ABBcCAD', str.lower) --> A B c A D
    return map(next, map(operator.itemgetter(1), itertools.groupby(iterable, key)))


# Taken from https://stackoverflow.com/a/34333710/7070491.
# Thanks to Laurent Laporte.
@contextlib.contextmanager
def set_env(**environ):
    """
    Temporarily set the process environment variables.

    >>> with set_env(PLUGINS_DIR='test/plugins'):
    ...   "PLUGINS_DIR" in os.environ
    True

    >>> "PLUGINS_DIR" in os.environ
    False

    :type environ: Dict[str, unicode]
    :param environ: Environment variables to set
    """
    old_environ = dict(os.environ)
    os.environ.update(environ)
    try:
        yield
    finally:
        os.environ.clear()
        os.environ.update(old_environ)


def expand_vars_and_user(value):
    if value is not None:
        return os.path.expanduser(os.path.expandvars(value))


# Taken from https://stackoverflow.com/a/2166841/7070491
# Thanks to Alex Martelli.
def is_namedtuple_instance(x):
    t = type(x)
    b = t.__bases__
    if len(b) != 1 or b[0] != tuple:
        return False
    f = getattr(t, "_fields", None)
    if not isinstance(f, tuple):
        return False
    return all(type(n) is str for n in f)


def copy_permission_safe(src: str, dst: str):
    """Copy a file to a given destination.

    If destination exists, it is removed first in order to avoid permission issues when
    the destination permissions are tried to be applied to an already existing
    destination.
    """
    if os.path.exists(dst):
        os.unlink(dst)
    shutil.copy(src, dst)



================================================
FILE: src/snakemake/common/argparse.py
================================================
import argparse
import collections
import dataclasses

import configargparse


class ArgumentParser(configargparse.ArgumentParser):
    def add_argument(
        self,
        *args,
        parse_func=None,
        **kwargs,
    ):
        if parse_func is not None:
            register_parser_action(parse_func, kwargs)
        super().add_argument(*args, **kwargs)

    def add_argument_group(self, *args, **kwargs):
        group = ArgumentGroup(self, *args, **kwargs)
        self._action_groups.append(group)
        return group


class ArgumentGroup(argparse._ArgumentGroup):
    def add_argument(
        self,
        *args,
        parse_func=None,
        **kwargs,
    ):
        if parse_func is not None:
            register_parser_action(parse_func, kwargs)
        super().add_argument(*args, **kwargs)


def register_parser_action(parse_func, kwargs):
    if "action" in kwargs:
        raise ValueError(
            "Cannot specify action if parser argument is provided to add_argument."
        )

    class ParserAction(argparse._StoreAction):
        def __init__(self, *args, **kwargs):
            if "parser" in kwargs:
                del kwargs["parse_func"]
            super().__init__(*args, **kwargs)

        def __call__(
            self,
            parser,
            namespace,
            values,
            option_string=None,
        ):
            parsed = parse_func(values)
            setattr(namespace, self.dest, parsed)

    kwargs["action"] = ParserAction


class ArgumentDefaultsHelpFormatter(argparse.HelpFormatter):
    """Help message formatter which adds default values to argument help.

    Like argparse.ArgumentDefaultsHelpFormatter, but doesn't print
    None/dataclasses._MISSING_TYPE/etc.
    """

    def _get_help_string(self, action):
        if (
            (
                action.option_strings
                or action.nargs in [argparse.OPTIONAL, argparse.ZERO_OR_MORE]
            )
            and action.default not in (None, "", set(), argparse.SUPPRESS)
            and not isinstance(action.default, dataclasses._MISSING_TYPE)
        ):
            if isinstance(action.default, collections.abc.Iterable) and not isinstance(
                action.default, str
            ):

                if isinstance(action.default, (frozenset, set)):
                    default = sorted(map(str, action.default))
                else:
                    default = map(str, action.default)
                return action.help + f" (default: {' '.join(default)})"
            else:
                return action.help + " (default: %(default)s)"
        else:
            return action.help



================================================
FILE: src/snakemake/common/configfile.py
================================================
import collections
import json
from pathlib import Path
from snakemake_interface_common.exceptions import WorkflowError


def _load_configfile(configpath_or_obj, filetype="Config"):
    "Tries to load a configfile first as JSON, then as YAML, into a dict."
    import yaml

    if isinstance(configpath_or_obj, str) or isinstance(configpath_or_obj, Path):
        obj = open(configpath_or_obj, encoding="utf-8")
    else:
        obj = configpath_or_obj

    try:
        with obj as f:
            try:
                return json.load(f, object_pairs_hook=collections.OrderedDict)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yte

                return yte.process_yaml(f, require_use_yte=True)
            except yaml.YAMLError:
                raise WorkflowError(
                    f"{filetype} file is not valid JSON or YAML. "
                    "In case of YAML, make sure to not mix "
                    "whitespace and tab indentation."
                )
    except FileNotFoundError:
        raise WorkflowError(f"{filetype} file {configpath_or_obj} not found.")


def load_configfile(configpath):
    "Loads a JSON or YAML configfile as a dict, then checks that it's a dict."
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(
            "Config file must be given as JSON or YAML with keys at top level."
        )
    return config



================================================
FILE: src/snakemake/common/git.py
================================================
import os
import re

from snakemake.exceptions import WorkflowError


def split_git_path(path):
    file_sub = re.sub(r"^git\+file:/+", "/", path)
    (file_path, version) = file_sub.split("@")
    file_path = os.path.realpath(file_path)
    root_path = get_git_root(file_path)
    if file_path.startswith(root_path):
        file_path = file_path[len(root_path) :].lstrip("/")
    return (root_path, file_path, version)


def get_git_root(path):
    """
    Args:
        path: (str) Path a to a directory/file that is located inside the repo
    Returns:
        path to the root folder for git repo
    """
    import git

    try:
        git_repo = git.Repo(path, search_parent_directories=True)
        return git_repo.git.rev_parse("--show-toplevel")
    except git.exc.NoSuchPathError:
        tail, _ = os.path.split(path)
        return get_git_root_parent_directory(tail, path)


def get_git_root_parent_directory(path, input_path):
    """
    This function will recursively go through parent directories until a git
    repository is found or until no parent directories are left, in which case
    an error will be raised. This is needed when providing a path to a
    file/folder that is located on a branch/tag not currently checked out.

    Args:
        path: (str) Path a to a directory that is located inside the repo
        input_path: (str) origin path, used when raising WorkflowError
    Returns:
        path to the root folder for git repo
    """
    import git

    try:
        git_repo = git.Repo(path, search_parent_directories=True)
        return git_repo.git.rev_parse("--show-toplevel")
    except git.exc.NoSuchPathError:
        tail, _ = os.path.split(path)
        if tail is None:
            raise WorkflowError(
                f"Neither provided git path ({input_path}) "
                + "or parent directories contain a valid git repo."
            )
        else:
            return get_git_root_parent_directory(tail, input_path)


def git_content(git_file):
    """
    This function will extract a file from a git repository, one located on
    the filesystem.
    The expected format is git+file:///path/to/your/repo/path_to_file@version

    Args:
      env_file (str): consist of path to repo, @, version, and file information
                      Ex: git+file:///home/smeds/snakemake-wrappers/bio/fastqc/wrapper.py@0.19.3
    Returns:
        file content or None if the expected format isn't meet
    """
    import git

    if git_file.startswith("git+file:"):
        (root_path, file_path, version) = split_git_path(git_file)
        return git.Repo(root_path).git.show(f"{version}:{file_path}")
    else:
        raise WorkflowError(
            "Provided git path ({}) doesn't meet the "
            "expected format:".format(git_file) + ", expected format is "
            "git+file://PATH_TO_REPO/PATH_TO_FILE_INSIDE_REPO@VERSION"
        )



================================================
FILE: src/snakemake/common/prefix_lookup.py
================================================
import bisect
from collections import defaultdict
from typing import Sequence, TypeVar, Generator

V = TypeVar("V")


class PrefixLookup:
    """A data structure for efficiently looking up entries by their prefix."""

    def __init__(self, entries: Sequence[tuple[str, V]]) -> None:
        grouped = defaultdict(list)
        for key, value in entries:
            grouped[key].append(value)
        self._entries = sorted(grouped.items(), key=lambda x: x[0])

    def match(self, query: str) -> set[V]:
        """Returns a set of all entry values which are attached to prefixes of the given key."""
        return set(m[1] for m in self.match_iter(query))

    def match_iter(self, query: str) -> Generator[V, None, None]:
        """Yields all entries which are prefixes of the given key.

        E.g. if "abc" is the key then "abc", "ab", "a", and "" are considered
        valid prefixes.

        Yields entries as (key, value) tuples as supplied to __init__()
        """
        stop_idx = len(self._entries)
        while stop_idx:
            stop_idx = bisect.bisect_right(
                self._entries, query, hi=stop_idx, key=lambda x: x[0]
            )

            k, entries = self._entries[stop_idx - 1]
            if query.startswith(k):
                for entry in entries:
                    yield (k, entry)

            if not query or not k:
                # Exit loop, if iteration has reached "" query or key
                break

            query = self._common_prefix(k, query)

    def _common_prefix(self, s1: str, s2: str) -> str:
        """Utility function that gives the common prefix of two strings.

        Except, if the strings are identical, returns the string minus one letter.
        Behaviour is undefined if either string is empty.
        """
        for i, (l1, l2) in enumerate(zip(s1, s2)):
            if l1 != l2:
                break

        return s1[:i]



================================================
FILE: src/snakemake/common/tbdstring.py
================================================
# A string that prints as TBD
# whatever interaction happens on this class, <TBD> shall be returned
class TBDString(str):
    # the second arg is necessary to avoid problems when pickling
    def __new__(cls, _=None):
        return str.__new__(cls, "<TBD>")

    def __getitem__(self, __item):
        return self

    def __bool__(self):
        return False

    def __add__(self, __other):
        return self

    def __sub__(self, __other):
        return self

    def __mul__(self, __other):
        return self

    def __matmul__(self, __other):
        return self

    def __truediv__(self, __other):
        return self

    def __floordiv__(self, __other):
        return self

    def __mod__(self, __other):
        return self

    def __divmod__(self, __other):
        return self

    def __pow__(self, __other):
        return self

    def __lshift__(self, __other):
        return self

    def __rshift__(self, __other):
        return self

    def __and__(self, __other):
        return self

    def __xor__(self, __other):
        return self

    def __or__(self, __other):
        return self

    def __neg__(self):
        return self

    def __pos__(self):
        return self

    def __abs__(self):
        return self

    def __invert__(self):
        return self

    def __complex__(self):
        return self

    def __int__(self):
        return self

    def __float__(self):
        return self

    def __index__(self):
        return self

    def __round__(self, ndigits=0):
        return self

    def __trunc__(self):
        return self

    def __floor__(self):
        return self

    def __ceil__(self):
        return self

    def __enter__(self):
        return self

    def __exit__(self, __exc_type, __exc_value, __traceback):
        return self



================================================
FILE: src/snakemake/common/typing.py
================================================
from typing import FrozenSet, Set, TypeVar, Union


T = TypeVar("T")
AnySet = Union[Set[T], FrozenSet[T]]



================================================
FILE: src/snakemake/common/workdir_handler.py
================================================
from dataclasses import dataclass, field
import os
from pathlib import Path
from typing import Optional

from snakemake.logging import logger


@dataclass
class WorkdirHandler:
    workdir: Optional[Path] = None
    olddir: Optional[Path] = field(init=False, default=None)

    def change_to(self):
        if self.workdir is not None:
            self.olddir = Path.cwd()
            if not self.workdir.exists():
                logger.info(f"Creating specified working directory {self.workdir}.")
                self.workdir.mkdir(parents=True)
            os.chdir(self.workdir)

    def change_back(self):
        if self.workdir is not None:
            os.chdir(self.olddir)



================================================
FILE: src/snakemake/common/tests/__init__.py
================================================
from abc import ABC, abstractmethod
from pathlib import Path
import shutil
from typing import List, Mapping, Optional
import uuid

import pytest
from snakemake import api

from snakemake_interface_common.utils import lazy_property
from snakemake_interface_common.plugin_registry.plugin import TaggedSettings
from snakemake_interface_executor_plugins.settings import ExecutorSettingsBase
from snakemake_interface_executor_plugins.registry import ExecutorPluginRegistry
from snakemake_interface_storage_plugins.settings import StorageProviderSettingsBase

from snakemake.settings import types as settings


def handle_testcase(func):
    def wrapper(self, tmp_path):
        if self.expect_exception is None:
            try:
                return func(self, tmp_path)
            finally:
                self.cleanup_test()
        else:
            with pytest.raises(self.expect_exception):
                try:
                    return func(self, tmp_path)
                finally:
                    self.cleanup_test()

    return wrapper


class TestWorkflowsBase(ABC):
    __test__ = False
    expect_exception = None
    omit_tmp = False
    latency_wait = 5
    create_report = False

    @abstractmethod
    def get_executor(self) -> str: ...

    @abstractmethod
    def get_executor_settings(self) -> Optional[ExecutorSettingsBase]: ...

    @abstractmethod
    def get_default_storage_provider(self) -> Optional[str]: ...

    @abstractmethod
    def get_default_storage_prefix(self) -> Optional[str]: ...

    @abstractmethod
    def get_default_storage_provider_settings(
        self,
    ) -> Optional[Mapping[str, TaggedSettings]]: ...

    def get_remote_execution_settings(self) -> settings.RemoteExecutionSettings:
        return settings.RemoteExecutionSettings(
            seconds_between_status_checks=0,
            envvars=self.get_envvars(),
        )

    def get_resource_settings(self) -> settings.ResourceSettings:
        return settings.ResourceSettings()

    def get_deployment_settings(
        self, deployment_method=frozenset()
    ) -> settings.DeploymentSettings:
        return settings.DeploymentSettings(
            deployment_method=deployment_method,
        )

    def get_assume_shared_fs(self) -> bool:
        return True

    def get_envvars(self) -> List[str]:
        return []

    def cleanup_test(self):
        """This method is called after every testcase, also in case of exceptions.

        Override to clean up any test files (e.g. in remote storage).
        """
        pass

    def run_workflow(self, test_name, tmp_path, deployment_method=frozenset()):
        test_path = Path(__file__).parent / "testcases" / test_name
        if self.omit_tmp:
            tmp_path = test_path
        else:
            tmp_path = Path(tmp_path) / test_name
            self._copy_test_files(test_path, tmp_path)

        resource_settings = self.get_resource_settings()

        if self._common_settings().local_exec:
            resource_settings.cores = 3
            resource_settings.nodes = None
        else:
            resource_settings.cores = 1
            resource_settings.nodes = 3

        with api.SnakemakeApi(
            settings.OutputSettings(
                verbose=True,
                show_failed_logs=True,
            ),
        ) as snakemake_api:
            workflow_api = snakemake_api.workflow(
                resource_settings=resource_settings,
                storage_settings=settings.StorageSettings(
                    default_storage_provider=self.get_default_storage_provider(),
                    default_storage_prefix=self.get_default_storage_prefix(),
                    shared_fs_usage=(
                        settings.SharedFSUsage.all()
                        if self.get_assume_shared_fs()
                        else frozenset()
                    ),
                ),
                deployment_settings=self.get_deployment_settings(deployment_method),
                storage_provider_settings=self.get_default_storage_provider_settings(),
                workdir=Path(tmp_path),
                snakefile=tmp_path / "Snakefile",
            )

            dag_api = workflow_api.dag()

            if self.create_report:
                dag_api.create_report(
                    reporter=self.get_reporter(),
                    report_settings=self.get_report_settings(),
                )
            else:
                dag_api.execute_workflow(
                    executor=self.get_executor(),
                    executor_settings=self.get_executor_settings(),
                    execution_settings=settings.ExecutionSettings(
                        latency_wait=self.latency_wait,
                    ),
                    remote_execution_settings=self.get_remote_execution_settings(),
                )

    @handle_testcase
    def test_simple_workflow(self, tmp_path):
        self.run_workflow("simple", tmp_path)

    @handle_testcase
    def test_group_workflow(self, tmp_path):
        self.run_workflow("groups", tmp_path)

    def _copy_test_files(self, test_path, tmp_path):
        shutil.copytree(test_path, tmp_path)

    def _common_settings(self):
        registry = ExecutorPluginRegistry()
        return registry.get_plugin(self.get_executor()).common_settings

    def get_reporter(self):
        raise NotImplementedError()

    def get_report_settings(self):
        raise NotImplementedError()


class TestWorkflowsLocalStorageBase(TestWorkflowsBase):
    def get_default_storage_provider(self) -> Optional[str]:
        return None

    def get_default_storage_prefix(self) -> Optional[str]:
        return None

    def get_default_storage_provider_settings(
        self,
    ) -> Optional[Mapping[str, TaggedSettings]]:
        return None


class TestWorkflowsMinioPlayStorageBase(TestWorkflowsBase):
    def get_default_storage_provider(self) -> Optional[str]:
        return "s3"

    def get_default_storage_prefix(self) -> Optional[str]:
        return f"s3://{self.bucket}"

    def get_default_storage_provider_settings(
        self,
    ) -> Optional[Mapping[str, TaggedSettings]]:
        from snakemake_storage_plugin_s3 import StorageProviderSettings

        self._storage_provider_settings = StorageProviderSettings(
            endpoint_url=self.endpoint_url,
            access_key=self.access_key,
            secret_key=self.secret_key,
        )

        tagged_settings = TaggedSettings()
        tagged_settings.register_settings(self._storage_provider_settings)
        return {"s3": tagged_settings}

    def cleanup_test(self):
        import boto3

        # clean up using boto3
        s3c = boto3.resource(
            "s3",
            endpoint_url=self.endpoint_url,
            aws_access_key_id=self.access_key,
            aws_secret_access_key=self.secret_key,
        )
        try:
            s3c.Bucket(self.bucket).delete()
        except Exception:
            pass

    @lazy_property
    def bucket(self):
        return f"snakemake-{uuid.uuid4().hex}"

    @property
    def endpoint_url(self):
        return "http://127.0.0.1:9000"

    @property
    def access_key(self):
        return "minio"

    @property
    def secret_key(self):
        return "minio123"


class TestReportBase(TestWorkflowsLocalStorageBase):
    create_report = True

    def get_executor(self) -> str:
        return "local"

    def get_executor_settings(self) -> Optional[ExecutorSettingsBase]:
        return None



================================================
FILE: src/snakemake/common/tests/testcases/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/common/tests/testcases/groups/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/common/tests/testcases/groups/caption.rst
================================================
This is a test caption {{ snakemake.output[0] }}.


================================================
FILE: src/snakemake/common/tests/testcases/groups/Snakefile
================================================
rule all:
    input:
        "test3.out"

rule a:
    output:
        "test1.{sample}.out"
    group: "foo"
    shell:
        "touch {output}"


rule b:
    input:
        "test1.{sample}.out"
    output:
        "test2.{sample}.out"
    group: "foo"
    threads: 2
    shell:
        "cp {input} {output}"


rule c:
    input:
        expand("test2.{sample}.out", sample=[1, 2, 3])
    output:
        report(
            "test3.out",
            caption="caption.rst",
            category="Test",
            subcategory="Subtest",
            labels={"label1": "foo", "label2": "bar"},
        )
    resources:
        mem="5MB"
    shell:
        "cat {input} > {output}"


================================================
FILE: src/snakemake/common/tests/testcases/simple/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/common/tests/testcases/simple/caption.rst
================================================
This is a test caption {{ snakemake.output[0] }}.


================================================
FILE: src/snakemake/common/tests/testcases/simple/Snakefile
================================================
rule all:
    input:
        "test3.out",
        collect("res/{somewildcard}.out", somewildcard=["somedir1", "somedir2/subdir"])

rule a:
    output:
        "test1.out"
    log:
        "a.log"
    shell:
        "touch {output} 2> {log}"


rule b:
    input:
        "test1.out"
    output:
        "test2.out"
    log:
        "b.log"
    threads: 2
    shell:
        "cp {input} {output} 2> {log}"


rule c:
    input:
        "test2.out"
    output:
        report(
            "test3.out",
            caption="caption.rst",
            category="Test",
            subcategory="Subtest",
            labels={"label1": "foo", "label2": "bar"},
        )
    log:
        "c.log"
    resources:
        mem="5MB"
    shell:
        "cp {input} {output} 2> {log}"


rule d:
    output:
        "res/{somewildcard}.out"
    shell:
        "touch {output}"


================================================
FILE: src/snakemake/deployment/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/deployment/conda.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import os
from pathlib import Path
import re
from typing import Union
from snakemake.sourcecache import (
    LocalGitFile,
    LocalSourceFile,
    SourceFile,
    infer_source_file,
)
import subprocess
import tempfile
import hashlib
import shutil
import json
from glob import glob
import tarfile
import zipfile
import uuid
from enum import Enum
import threading
import shutil
from abc import ABC, abstractmethod


from snakemake.exceptions import CreateCondaEnvironmentException, WorkflowError
from snakemake.logging import logger
from snakemake.common import (
    copy_permission_safe,
    is_local_file,
    parse_uri,
    ON_WINDOWS,
)
from snakemake.deployment import singularity, containerize
from snakemake.io import (
    IOFile,
    apply_wildcards,
    contains_wildcard,
    _IOFile,
)
from snakemake_interface_common.utils import lazy_property


MIN_CONDA_VER = "24.7.1"


def get_env_setup_done_flag_file(env_path: Path) -> Path:
    return env_path.with_suffix(".env_setup_done")


class CondaCleanupMode(Enum):
    tarballs = "tarballs"
    cache = "cache"

    def __str__(self):
        return self.value


class Env:
    """Conda environment from a given specification file."""

    def __init__(
        self,
        workflow,
        env_file=None,
        env_name=None,
        env_dir=None,
        envs_dir=None,
        container_img=None,
        cleanup=None,
    ):
        self.file = env_file
        if env_file is not None:
            self.file = infer_source_file(env_file)
            assert env_name is None
            assert env_dir is None
        self.name = env_name
        if env_name is not None:
            assert env_file is None
            assert env_dir is None
        self.dir = env_dir
        if env_dir is not None:
            assert env_file is None
            assert env_name is None
        self.workflow = workflow

        self._container_img = container_img
        self._envs_dir = envs_dir or (
            containerize.CONDA_ENV_PATH
            if self.is_containerized
            else workflow.persistence.conda_env_path
        )
        self._hash = None
        self._content_hash = None
        self._content = None
        self._content_deploy = None
        self._content_pin = None
        self._path = None
        self._archive_file = None
        self._cleanup = cleanup
        self._singularity_args = workflow.deployment_settings.apptainer_args

    @property
    def is_externally_managed(self):
        """Return True if the environment is managed by the user."""
        return self.name is not None or self.dir is not None

    @lazy_property
    def conda(self):
        return Conda(container_img=self._container_img, check=True)

    def _path_or_uri_prefix(self):
        prefix = self.file.get_path_or_uri()
        if prefix.endswith(".yaml") or prefix.endswith(".yml"):
            prefix = prefix.rsplit(".", 1)[0]
            return prefix
        else:
            return None

    @lazy_property
    def pin_file(self):
        return self._get_aux_file(
            f".{self.conda.platform}.pin.txt",
            "Omitting search for pin file "
            "(https://snakemake.readthedocs.io/en/stable/snakefiles/"
            "deployment.html#freezing-environments-to-exactly-pinned-packages).",
        )

    @lazy_property
    def post_deploy_file(self):
        return self._get_aux_file(
            ".post-deploy.sh",
            "Omitting search for post-deploy script "
            "(https://snakemake.readthedocs.io/en/stable/snakefiles/"
            "deployment.html#providing-post-deployment-scripts).",
        )

    def _get_aux_file(self, suffix: str, omit_msg: str):
        if self.file:
            prefix = self._path_or_uri_prefix()
            # TODO handle LocalGitFile properly
            if prefix is None:
                logger.warning(
                    f"Conda environment file {self.file.get_path_or_uri()} does not end "
                    f"on .yaml or .yml. {omit_msg}"
                )
                return None
            aux_file = f"{prefix}{suffix}"
            aux_file = infer_source_file(aux_file)
            if self.workflow.sourcecache.exists(aux_file):
                return aux_file
            else:
                return None
        else:
            return None

    def _get_content(self):
        if self.name or self.dir:
            from snakemake.shell import shell

            try:
                content = shell.check_output(
                    f"conda env export {self.address_argument}",
                    stderr=subprocess.STDOUT,
                    text=True,
                )
            except subprocess.CalledProcessError as e:
                raise WorkflowError(
                    f"Error exporting conda environment {self.address_argument}:\n{e.output}"
                )
            return content.encode()
        else:
            return self.workflow.sourcecache.open(self.file, "rb").read()

    def _get_content_deploy(self):
        self.check_is_file_based()
        if self.post_deploy_file:
            return self.workflow.sourcecache.open(self.post_deploy_file, "rb").read()
        return None

    def _get_content_pin(self):
        self.check_is_file_based()
        if self.pin_file:
            return self.workflow.sourcecache.open(self.pin_file, "rb").read()
        return None

    @property
    def _env_archive_dir(self):
        return self.workflow.persistence.conda_env_archive_path

    @property
    def container_img_url(self):
        return self._container_img.url if self._container_img else None

    @property
    def content(self):
        if self._content is None:
            self._content = self._get_content()
        return self._content

    @property
    def content_deploy(self):
        if self._content_deploy is None:
            self._content_deploy = self._get_content_deploy()
        return self._content_deploy

    @property
    def content_pin(self):
        if self._content_pin is None:
            self._content_pin = self._get_content_pin()
        return self._content_pin

    @property
    def hash(self):
        if self._hash is None:
            if self.is_containerized:
                self._hash = self.content_hash
            else:
                self._hash = self._get_hash(
                    include_location=True, include_container_img=True
                )
        return self._hash

    @property
    def content_hash(self):
        if self._content_hash is None:
            self._content_hash = self._get_hash(
                include_location=False, include_container_img=False
            )
        return self._content_hash

    def _get_hash(self, include_location: bool, include_container_img: bool) -> str:
        md5hash = hashlib.md5(usedforsecurity=False)
        if self.name:
            md5hash.update(self.name.encode())
        elif self.dir:
            md5hash.update(self.dir.encode())
        else:
            if include_location:
                # Include the absolute path of the target env dir into the hash.
                # By this, moving the working directory around automatically
                # invalidates all environments. This is necessary, because binaries
                # in conda environments can contain hardcoded absolute RPATHs.
                env_dir = os.path.realpath(self._envs_dir)
                md5hash.update(env_dir.encode())
            if include_container_img and self._container_img:
                md5hash.update(self._container_img.url.encode())
            content_deploy = self.content_deploy
            if content_deploy:
                md5hash.update(content_deploy)
            content_pin = self.content_pin
            if content_pin:
                md5hash.update(content_pin)
            md5hash.update(self.content)
        return md5hash.hexdigest()

    @property
    def is_containerized(self):
        if not self._container_img:
            return False
        return self._container_img.is_containerized

    def check_is_file_based(self):
        assert (
            self.file is not None
        ), "bug: trying to access conda env file based functionality for named environment"

    @property
    def address(self):
        """Path to directory of the conda environment.

        First tries full hash, if it does not exist, (8-prefix) is used
        as default.

        """
        if self.name:
            return self.name
        elif self.dir:
            return self.dir
        else:
            env_dir = self._envs_dir
            get_path = lambda h: os.path.join(env_dir, h)
            hash_candidates = [
                self.hash[:8],
                self.hash,
                self.hash
                + "_",  # activate no-shortcuts behavior (so that no admin rights are needed on win)
            ]  # [0] is the old fallback hash (shortened)
            exists = [os.path.exists(get_path(h)) for h in hash_candidates]
            if self.is_containerized:
                return get_path(hash_candidates[1])
            for candidate, candidate_exists in zip(hash_candidates, exists):
                if candidate_exists or candidate == hash_candidates[-1]:
                    # exists or it is the last (i.e. the desired one)
                    return get_path(candidate)

    @property
    def address_argument(self):
        if self.name:
            return f"--name '{self.address}'"
        else:
            return f"--prefix '{self.address}'"

    @property
    def archive_file(self):
        """Path to archive of the conda environment, which may or may not exist."""
        if self._archive_file is None:
            self._archive_file = os.path.join(self._env_archive_dir, self.content_hash)
        return self._archive_file

    def create_archive(self):
        """Create self-contained archive of environment."""
        from snakemake.shell import shell

        # importing requests locally because it interferes with instantiating conda environments
        import requests

        self.check_is_file_based()

        env_archive = self.archive_file
        if os.path.exists(env_archive):
            return env_archive

        try:
            # Download
            logger.info(
                "Downloading packages for conda environment {}...".format(
                    self.file.get_path_or_uri()
                )
            )
            os.makedirs(env_archive, exist_ok=True)
            try:
                out = shell.check_output(
                    f"conda list --explicit {self.address_argument}",
                    stderr=subprocess.STDOUT,
                    text=True,
                )
                logger.debug(out)
            except subprocess.CalledProcessError as e:
                raise WorkflowError("Error exporting conda packages:\n" + e.output)
            with open(os.path.join(env_archive, "packages.txt"), "w") as pkg_list:
                for l in out.split("\n"):
                    if l and not l.startswith("#") and not l.startswith("@"):
                        pkg_url = l
                        logger.info(pkg_url)
                        parsed = parse_uri(pkg_url)
                        pkg_name = os.path.basename(parsed.uri_path)
                        # write package name to list
                        print(pkg_name, file=pkg_list)
                        # download package
                        pkg_path = os.path.join(env_archive, pkg_name)
                        with open(pkg_path, "wb") as copy:
                            r = requests.get(pkg_url)
                            r.raise_for_status()
                            copy.write(r.content)
                        try:
                            if pkg_path.endswith(".conda"):
                                assert zipfile.ZipFile(pkg_path).testzip() is None
                            else:
                                tarfile.open(pkg_path)
                        except:
                            raise WorkflowError(
                                f"Package is invalid tar/zip archive: {pkg_url}"
                            )
        except (
            requests.exceptions.ChunkedEncodingError,
            requests.exceptions.HTTPError,
        ) as e:
            shutil.rmtree(env_archive)
            raise WorkflowError(f"Error downloading conda package {pkg_url}.")
        except BaseException as e:
            shutil.rmtree(env_archive)
            raise e
        return env_archive

    def execute_deployment_script(self, env_file, deploy_file):
        """Execute post-deployment script if present"""
        from snakemake.shell import shell

        if ON_WINDOWS:
            raise WorkflowError(
                "Post deploy script {} provided for conda env {} but unsupported on windows.".format(
                    deploy_file, env_file
                )
            )
        logger.info(
            "Running post-deploy script {}...".format(
                os.path.relpath(path=deploy_file, start=os.getcwd())
            )
        )

        # Determine interpreter from shebang or use sh as default.
        interpreter = "sh"
        with open(deploy_file, "r") as f:
            first_line = next(iter(f))
            if first_line.startswith("#!"):
                interpreter = first_line[2:].strip()

        shell.check_output(
            self.conda.shellcmd(self.address, f"{interpreter} {deploy_file}"),
            stderr=subprocess.STDOUT,
            text=True,
        )

    def create(self, dryrun=False):
        """Create the conda environment."""
        from snakemake.shell import shell

        self.check_is_file_based()

        # Read env file and create hash.
        env_file = self.file
        deploy_file = None
        pin_file = None
        tmp_env_file = None
        tmp_deploy_file = None
        tmp_pin_file = None

        if not isinstance(env_file, LocalSourceFile) or isinstance(
            env_file, LocalGitFile
        ):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".yaml") as tmp:
                # write to temp file such that conda can open it
                tmp.write(self.content)
                env_file = tmp.name
                tmp_env_file = tmp.name
            if self.post_deploy_file:
                with tempfile.NamedTemporaryFile(
                    delete=False, suffix=".post-deploy.sh"
                ) as tmp:
                    # write to temp file such that conda can open it
                    tmp.write(self.content_deploy)
                    deploy_file = tmp.name
                    tmp_deploy_file = tmp.name
            if self.pin_file and not dryrun:
                with tempfile.NamedTemporaryFile(delete=False, suffix="pin.txt") as tmp:
                    tmp.write(self.content_pin)
                    pin_file = tmp.name
                    tmp_pin_file = tmp.name
        else:
            env_file = env_file.get_path_or_uri()
            deploy_file = self.post_deploy_file
            if not dryrun:
                pin_file = self.pin_file

        env_path = self.address

        if self.is_containerized:
            if not dryrun:
                try:
                    shell.check_output(
                        singularity.shellcmd(
                            self._container_img.path,
                            f"[ -d '{env_path}' ]",
                            args=self._singularity_args,
                            envvars=self.get_singularity_envvars(),
                            quiet=True,
                        ),
                        stderr=subprocess.PIPE,
                        text=True,
                    )
                except subprocess.CalledProcessError as e:
                    raise WorkflowError(
                        "Unable to find environment in container image. "
                        "Maybe a conda environment was modified without containerizing again "
                        "(see snakemake --containerize)?\nDetails:\n{}\n{}".format(
                            e, e.stderr
                        )
                    )
                return env_path
            else:
                # env should be present in the container
                return env_path

        setup_done_flag = get_env_setup_done_flag_file(Path(env_path))

        # Check for broken environment
        if os.path.exists(env_path) and not setup_done_flag.exists():
            if dryrun:
                logger.info(
                    "Incomplete Conda environment {} will be recreated.".format(
                        self.file.simplify_path()
                    )
                )
            else:
                logger.info(
                    "Removing incomplete Conda environment {}...".format(
                        self.file.simplify_path()
                    )
                )
                shutil.rmtree(env_path, ignore_errors=True)

        # Create environment if not already present.
        if not os.path.exists(env_path):
            if dryrun:
                logger.info(
                    "Conda environment {} will be created.".format(
                        self.file.simplify_path()
                    )
                )
                return env_path
            logger.info(f"Creating conda environment {self.file.simplify_path()}...")
            env_archive = self.archive_file
            try:
                # Check if env archive exists. Use that if present.
                if os.path.exists(env_archive):
                    logger.info("Installing archived conda packages.")
                    pkg_list = os.path.join(env_archive, "packages.txt")
                    if os.path.exists(pkg_list):
                        # read packages in correct order
                        # this is for newer env archives where the package list
                        # was stored
                        packages = [
                            os.path.join(env_archive, pkg.rstrip())
                            for pkg in open(pkg_list)
                        ]
                    else:
                        # guess order
                        packages = glob(os.path.join(env_archive, "*.tar.bz2"))

                    # install packages manually from env archive
                    cmd = " ".join(
                        [
                            "conda",
                            "create",
                            "--quiet",
                            "--no-shortcuts" if ON_WINDOWS else "",
                            "--yes",
                            "--no-default-packages",
                            f"--prefix '{env_path}'",
                        ]
                        + packages
                    )
                    if self._container_img:
                        cmd = singularity.shellcmd(
                            self._container_img.path,
                            cmd,
                            args=self._singularity_args,
                            envvars=self.get_singularity_envvars(),
                        )
                    out = shell.check_output(cmd, stderr=subprocess.STDOUT, text=True)
                else:

                    def create_env(env_file, filetype="yaml"):
                        # Copy env file to env_path (because they can be on
                        # different volumes and singularity should only mount one).
                        # In addition, this allows to immediately see what an
                        # environment in .snakemake/conda contains.
                        target_env_file = env_path + f".{filetype}"
                        copy_permission_safe(env_file, target_env_file)

                        logger.info("Downloading and installing remote packages.")

                        strict_priority = (
                            ["conda config --set channel_priority strict &&"]
                            if self._container_img
                            else []
                        )

                        subcommand = ["conda"]
                        yes_flag = ["--yes"]
                        if filetype == "yaml":
                            subcommand.append("env")
                            yes_flag = []

                        cmd = (
                            strict_priority
                            + subcommand
                            + [
                                "create",
                                "--quiet",
                                "--no-default-packages",
                                f'--file "{target_env_file}"',
                                f'--prefix "{env_path}"',
                            ]
                            + yes_flag
                        )
                        cmd = " ".join(cmd)
                        if self._container_img:
                            cmd = singularity.shellcmd(
                                self._container_img.path,
                                cmd,
                                args=self._singularity_args,
                                envvars=self.get_singularity_envvars(),
                            )
                        out = shell.check_output(
                            cmd, stderr=subprocess.STDOUT, text=True
                        )

                        # cleanup if requested
                        if self._cleanup is CondaCleanupMode.tarballs:
                            logger.info("Cleaning up conda package tarballs.")
                            shell.check_output("conda clean -y --tarballs", text=True)
                        elif self._cleanup is CondaCleanupMode.cache:
                            logger.info(
                                "Cleaning up conda package tarballs and package cache."
                            )
                            shell.check_output(
                                "conda clean -y --tarballs --packages", text=True
                            )
                        return out

                    if pin_file is not None:
                        try:
                            logger.info(
                                f"Using pinnings from {self.pin_file.get_path_or_uri()}."
                            )
                            out = create_env(pin_file, filetype="pin.txt")
                        except subprocess.CalledProcessError as e:
                            # remove potential partially installed environment
                            shutil.rmtree(env_path, ignore_errors=True)
                            advice = ""
                            if isinstance(self.file, LocalSourceFile):
                                advice = (
                                    " If that works, make sure to update the pin file with "
                                    f"'snakedeploy pin-conda-env {self.file.get_path_or_uri()}'."
                                )
                            logger.warning(
                                f"Failed to install conda environment from pin file ({self.pin_file.get_path_or_uri()}). "
                                f"Trying regular environment definition file.{advice}\n"
                                f"Error message:\n{e.output}"
                            )
                            out = create_env(env_file, filetype="yaml")
                    else:
                        out = create_env(env_file, filetype="yaml")

                # Execute post-deploy script if present
                if deploy_file:
                    target_deploy_file = env_path + ".post-deploy.sh"
                    copy_permission_safe(deploy_file, target_deploy_file)
                    self.execute_deployment_script(env_file, target_deploy_file)

                # Touch "done" flag file
                with open(setup_done_flag, "w") as _:
                    pass

                logger.debug(out)
                logger.info(
                    f"Environment for {self.file.get_path_or_uri()} created (location: {os.path.relpath(env_path)})"
                )
            except subprocess.CalledProcessError as e:
                # remove potential partially installed environment
                shutil.rmtree(env_path, ignore_errors=True)
                raise CreateCondaEnvironmentException(
                    f"Could not create conda environment from {env_file}:\nCommand:\n{e.cmd}\nOutput:\n{e.output}"
                )

        if tmp_env_file:
            # temporary file was created
            os.remove(tmp_env_file)
        if tmp_deploy_file:
            os.remove(tmp_deploy_file)
        if tmp_pin_file:
            os.remove(tmp_pin_file)

        return env_path

    @classmethod
    def get_singularity_envvars(self):
        return {"CONDA_PKGS_DIRS": f"/tmp/conda/{uuid.uuid4()}"}

    def __hash__(self):
        # this hash is only for object comparison, not for env paths
        if self.name:
            return hash(self.name)
        elif self.dir:
            return hash(self.dir)
        else:
            return hash(self.file)

    def __eq__(self, other):
        if isinstance(other, Env):
            if self.name:
                return self.name == other.name
            elif self.dir:
                return self.dir == other.dir
            else:
                return self.file == other.file
        return False


class Conda:
    instances = dict()
    lock = threading.Lock()

    def __new__(cls, container_img=None, prefix_path=None, check=False):
        with cls.lock:
            if container_img not in cls.instances:
                inst = super().__new__(cls)
                cls.instances[container_img] = inst
                return inst
            else:
                return cls.instances[container_img]

    def __init__(self, container_img=None, prefix_path=None, check=False):
        if not self.is_initialized:  # avoid superfluous init calls
            from snakemake.deployment import singularity
            from snakemake.shell import shell

            if isinstance(container_img, singularity.Image):
                container_img = container_img.path
            self.container_img = container_img

            try:
                self.info = json.loads(
                    shell.check_output(self._get_cmd("conda info --json"), text=True)
                )
            except subprocess.CalledProcessError as e:
                raise WorkflowError(
                    "Error running conda info. "
                    f"Is conda installed and accessible? Error: {e}"
                )

            if prefix_path is None or container_img is not None:
                self.prefix_path = self.info["conda_prefix"]
            else:
                self.prefix_path = prefix_path

            self.platform = self.info["platform"]

            # check conda installation
            if check:
                self._check()

    @property
    def is_initialized(self):
        return hasattr(self, "prefix_path")

    def _get_cmd(self, cmd):
        if self.container_img:
            return singularity.shellcmd(self.container_img, cmd, quiet=True)
        return cmd

    def _check(self):
        from snakemake.shell import shell

        frontend = "conda"
        # Use type here since conda now is a function.
        # type allows to check for both functions and regular commands.
        if not ON_WINDOWS or shell.get_executable():
            locate_cmd = f"type {frontend}"
        else:
            locate_cmd = f"where {frontend}"

        try:
            shell.check_output(
                self._get_cmd(locate_cmd), stderr=subprocess.STDOUT, text=True
            )
        except subprocess.CalledProcessError as e:
            if self.container_img:
                msg = (
                    f"The '{frontend}' command is not "
                    "available inside "
                    "your singularity container "
                    "image. Snakemake mounts "
                    "your conda installation "
                    "into singularity. "
                    "Sometimes, this can fail "
                    "because of shell restrictions. "
                    "It has been tested to work "
                    "with docker://ubuntu, but "
                    "it e.g. fails with "
                    "docker://bash "
                )
            else:
                msg = (
                    f"The '{frontend}' command is not "
                    "available in the "
                    f"shell {shell.get_executable()} that will be "
                    "used by Snakemake. You have "
                    "to ensure that it is in your "
                    "PATH, e.g., first activating "
                    "the conda base environment "
                    "with `conda activate base`."
                )
            raise CreateCondaEnvironmentException(msg)

        try:
            self._check_version()
            self._check_condarc()
        except subprocess.CalledProcessError as e:
            raise CreateCondaEnvironmentException(
                "Unable to check conda installation:\n" + e.stderr.decode()
            )

    def _check_version(self):
        from snakemake.shell import shell
        from packaging.version import Version

        version = shell.check_output(
            self._get_cmd("conda --version"), stderr=subprocess.PIPE, text=True
        )
        version_matches = re.findall(r"\d+\.\d+\.\d+", version)
        if len(version_matches) != 1:
            raise WorkflowError(
                f"Unable to determine conda version. 'conda --version' returned {version}"
            )
        else:
            version = version_matches[0]
        if Version(version) < Version(MIN_CONDA_VER):
            raise CreateCondaEnvironmentException(
                f"Conda must be version {MIN_CONDA_VER} or later, found version {version}. "
                "Please update conda to the latest version. "
                "Note that you can also install conda into the snakemake environment "
                "without modifying your main conda installation."
            )

    def _check_condarc(self):
        if self.container_img:
            # Do not check for strict priorities when running conda in an image
            # Instead, we set priorities to strict ourselves in the image.
            return
        from snakemake.shell import shell

        res = json.loads(
            shell.check_output(
                self._get_cmd("conda config --get channel_priority --json"),
                text=True,
                stderr=subprocess.PIPE,
            )
        )
        if res["get"].get("channel_priority") != "strict":
            logger.warning(
                "Your conda installation is not configured to use strict channel priorities. "
                "This is however important for having robust and correct environments (for details, "
                "see https://conda-forge.org/docs/user/tipsandtricks.html). "
                "Please consider to configure strict priorities by executing "
                "'conda config --set channel_priority strict'."
            )

    def bin_path(self):
        if ON_WINDOWS:
            return os.path.join(self.prefix_path, "Scripts")
        else:
            return os.path.join(self.prefix_path, "bin")

    def shellcmd(self, env_address, cmd):
        # get path to activate script
        activate = os.path.join(self.bin_path(), "activate")

        if ON_WINDOWS:
            activate = activate.replace("\\", "/")
            env_address = env_address.replace("\\", "/")

        return f"source {activate} '{env_address}'; {cmd}"

    def shellcmd_win(self, env_address, cmd):
        """Prepend the windows activate bat script."""
        # get path to activate script
        activate = os.path.join(self.bin_path(), "activate.bat").replace("\\", "/")
        env_address = env_address.replace("\\", "/")

        return f'"{activate}" "{env_address}"&&{cmd}'


class CondaEnvSpec(ABC):
    @abstractmethod
    def apply_wildcards(self, wildcards): ...

    @abstractmethod
    def get_conda_env(
        self, workflow, envs_dir=None, container_img=None, cleanup=None
    ): ...

    @abstractmethod
    def check(self): ...

    @property
    def is_file(self):
        return False

    @property
    @abstractmethod
    def contains_wildcard(self): ...

    @abstractmethod
    def __hash__(self): ...

    @abstractmethod
    def __eq__(self, other): ...


class CondaEnvFileSpec(CondaEnvSpec):
    def __init__(self, filepath, rule=None):
        if isinstance(filepath, SourceFile):
            self.file = IOFile(str(filepath.get_path_or_uri()), rule=rule)
        elif isinstance(filepath, _IOFile):
            self.file = filepath
        else:
            self.file = IOFile(filepath, rule=rule)

    def apply_wildcards(self, wildcards, rule):
        filepath = self.file.apply_wildcards(wildcards)
        if is_local_file(filepath):
            # Normalize 'file:///my/path.yml' to '/my/path.yml'
            filepath = parse_uri(filepath).uri_path
        return CondaEnvFileSpec(filepath, rule)

    def check(self):
        self.file.check()

    def get_conda_env(self, workflow, envs_dir=None, container_img=None, cleanup=None):
        return Env(
            workflow,
            env_file=self.file,
            envs_dir=envs_dir,
            container_img=container_img,
            cleanup=cleanup,
        )

    @property
    def is_file(self):
        return True

    @property
    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def __hash__(self):
        return hash(self.file)

    def __eq__(self, other):
        return self.file == other.file


class CondaEnvDirSpec(CondaEnvSpec):
    def __init__(self, path, rule=None):
        if isinstance(path, SourceFile):
            self.path = IOFile(str(path.get_path_or_uri()), rule=rule)
        elif isinstance(path, _IOFile):
            self.path = path
        else:
            self.path = IOFile(path, rule=rule)

    def apply_wildcards(self, wildcards, rule):
        filepath = self.path.apply_wildcards(wildcards)
        if is_local_file(filepath):
            # Normalize 'file:///my/path.yml' to '/my/path.yml'
            filepath = parse_uri(filepath).uri_path
        return CondaEnvDirSpec(filepath, rule)

    def check(self):
        pass

    def get_conda_env(self, workflow, envs_dir=None, container_img=None, cleanup=None):
        return Env(
            workflow,
            env_dir=self.path,
            envs_dir=envs_dir,
            container_img=container_img,
            cleanup=cleanup,
        )

    @property
    def is_file(self):
        return True

    @property
    def contains_wildcard(self):
        return contains_wildcard(self.path)

    def __hash__(self):
        return hash(self.path)

    def __eq__(self, other):
        return self.path == other.path


class CondaEnvNameSpec(CondaEnvSpec):
    def __init__(self, name: str):
        self.name = name

    def apply_wildcards(self, wildcards, _):
        return CondaEnvNameSpec(apply_wildcards(self.name, wildcards))

    def get_conda_env(self, workflow, envs_dir=None, container_img=None, cleanup=None):
        return Env(
            workflow,
            env_name=self.name,
            envs_dir=envs_dir,
            container_img=container_img,
            cleanup=cleanup,
        )

    def check(self):
        # not a file, nothing to check here
        pass

    @property
    def contains_wildcard(self):
        return contains_wildcard(self.name)

    def __hash__(self):
        return hash(self.name)

    def __eq__(self, other):
        return self.name == other.name


class CondaEnvSpecType(Enum):
    FILE = "file"
    NAME = "name"
    DIR = "dir"

    @classmethod
    def from_spec(cls, spec: Union[str, SourceFile, Path]):
        if isinstance(spec, SourceFile):
            if isinstance(spec, LocalSourceFile):
                spec = spec.get_path_or_uri()
            else:
                spec = spec.get_filename()
        elif isinstance(spec, Path):
            spec = str(spec)

        if spec.endswith(".yaml") or spec.endswith(".yml"):
            return cls.FILE
        elif is_local_file(spec) and os.path.isdir(spec):
            return cls.DIR
        else:
            return cls.NAME



================================================
FILE: src/snakemake/deployment/containerize.py
================================================
from pathlib import Path
import hashlib
import os

from snakemake.exceptions import WorkflowError
from snakemake.logging import logger
from snakemake.sourcecache import LocalSourceFile


CONDA_ENV_PATH = "/conda-envs"


def containerize(workflow, dag):
    if any(
        job.conda_env_spec.contains_wildcard
        for job in dag.jobs
        if job.conda_env_spec is not None
    ):
        raise WorkflowError(
            "Containerization of conda based workflows is not allowed if any conda env definition contains a wildcard."
        )

    def relfile(env):
        if isinstance(env.file, LocalSourceFile):
            return os.path.relpath(env.file.get_path_or_uri(), os.getcwd())
        else:
            return env.file.get_path_or_uri()

    envs = sorted(
        set(
            job.conda_env_spec.get_conda_env(workflow, envs_dir=CONDA_ENV_PATH)
            for job in dag.jobs
            if job.conda_env_spec is not None
        ),
        key=relfile,
    )
    envhash = hashlib.sha256()
    for env in envs:
        logger.info(f"Hashing conda environment {relfile(env)}.")
        # build a hash of the environment contents
        envhash.update(env.content)

    print("FROM condaforge/miniforge3:latest")
    print('LABEL io.github.snakemake.containerized="true"')
    print(f'LABEL io.github.snakemake.conda_env_hash="{envhash.hexdigest()}"')

    generated = set()
    get_env_cmds = []
    generate_env_cmds = []
    for env in envs:
        if env.content_hash in generated:
            # another conda env with the same content was generated before
            continue
        prefix = Path(CONDA_ENV_PATH) / env.content_hash
        env_source_path = relfile(env)
        env_target_path = prefix / "environment.yaml"
        get_env_cmds.append("\n# Conda environment:")
        get_env_cmds.append(f"#   source: {env_source_path}")
        get_env_cmds.append(f"#   prefix: {prefix}")
        get_env_cmds.append(
            "\n".join(map("#   {}".format, env.content.decode().strip().split("\n")))
        )
        get_env_cmds.append(f"RUN mkdir -p {prefix}")
        if isinstance(env.file, LocalSourceFile):
            get_env_cmds.append(f"COPY {env_source_path} {env_target_path}")
        else:
            get_env_cmds.append(f"ADD {env.file.get_path_or_uri()} {env_target_path}")

        generate_env_cmds.append(
            f"conda env create --prefix {prefix} --file {env_target_path} &&"
        )
        generated.add(env.content_hash)

    print("\n# Step 2: Retrieve conda environments")
    for cmd in get_env_cmds:
        print(cmd)

    print("\n# Step 3: Generate conda environments")
    print("\nRUN", " \\\n    ".join(generate_env_cmds), "\\\n    conda clean --all -y")



================================================
FILE: src/snakemake/deployment/env_modules.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"


import hashlib


class EnvModules:
    def __init__(self, *module_names):
        self.names = module_names

    def shellcmd(self, cmd):
        """Return shell command with given modules loaded."""
        return "module purge && MODULES_VERBOSITY=concise module load {to_load}; {cmd}".format(
            to_load=" ".join(self.names), cmd=cmd
        )

    def __str__(self):
        return ", ".join(self.names)

    @property
    def hash(self) -> str:
        md5hash = hashlib.md5(usedforsecurity=False)
        for name in self.names:
            md5hash.update(name.encode())
        return md5hash.hexdigest()



================================================
FILE: src/snakemake/deployment/singularity.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from pathlib import Path
import subprocess
import shutil
import os
import hashlib

from snakemake.common import (
    get_snakemake_searchpaths,
    is_local_file,
    parse_uri,
)
from snakemake.exceptions import WorkflowError
from snakemake.logging import logger
from snakemake_interface_common.utils import lazy_property
from snakemake.common import get_appdirs

SNAKEMAKE_MOUNTPOINT = "/mnt/snakemake"


def get_snakemake_searchpath_mountpoints():
    paths = get_snakemake_searchpaths()
    base = Path("/mnt/snakemake_searchpaths")
    return [str(base / f"item_{i}") for i in range(len(paths))]


class Image:
    def __init__(self, url, dag, is_containerized):
        if " " in url:
            raise WorkflowError("Invalid singularity image URL containing whitespace.")

        self.singularity = Singularity()

        self.url = url
        self._img_dir = dag.workflow.persistence.container_img_path
        self.is_containerized = is_containerized

    @property
    def is_local(self):
        return is_local_file(self.url)

    @lazy_property
    def hash(self):
        md5hash = hashlib.md5(usedforsecurity=False)
        md5hash.update(self.url.encode())
        return md5hash.hexdigest()

    def pull(self, dryrun=False):
        self.singularity.check()
        if self.is_local:
            return
        if dryrun:
            logger.info(f"Singularity image {self.url} will be pulled.")
            return
        logger.debug(f"Singularity image location: {self.path}")
        if not os.path.exists(self.path):
            logger.info(f"Pulling singularity image {self.url}.")
            try:
                p = subprocess.check_output(
                    [
                        "singularity",
                        "pull",
                        "--name",
                        f"{self.hash}.simg",
                        self.url,
                    ],
                    cwd=self._img_dir,
                    stderr=subprocess.STDOUT,
                )
            except subprocess.CalledProcessError as e:
                raise WorkflowError(
                    "Failed to pull singularity image "
                    "from {}:\n{}".format(self.url, e.stdout.decode())
                )

    @property
    def path(self):
        if self.is_local:
            return parse_uri(self.url).uri_path
        return os.path.join(self._img_dir, self.hash) + ".simg"

    def __hash__(self):
        return hash(self.hash)

    def __eq__(self, other):
        return self.url == other.url


def shellcmd(
    img_path,
    cmd,
    args="",
    quiet=False,
    envvars=None,
    shell_executable=None,
    container_workdir=None,
    is_python_script=False,
):
    """Execute shell command inside singularity container given optional args
    and environment variables to be passed."""

    if envvars:
        envvars = " ".join(f"SINGULARITYENV_{k}={v}" for k, v in envvars.items())
    else:
        envvars = ""

    if shell_executable is None:
        shell_executable = "sh"
    else:
        # Ensure to just use the name of the executable, not a path,
        # because we cannot be sure where it is located in the container.
        shell_executable = os.path.split(shell_executable)[-1]

    if is_python_script:
        # mount host snakemake module into container
        args += " ".join(
            f" --bind {repr(searchpath)}:{repr(mountpoint)}"
            for searchpath, mountpoint in zip(
                get_snakemake_searchpaths(), get_snakemake_searchpath_mountpoints()
            )
        )

    if container_workdir:
        args += f" --pwd {repr(container_workdir)}"

    # mount the snakemake cache into the container per default so that
    # params included with workflow.source_path are always mounted in the container
    if len(args) == 0:
        source_cache_path = os.path.join(
            get_appdirs().user_cache_dir, "snakemake/source-cache"
        )
        if os.path.exists(source_cache_path):
            args += "--bind " + source_cache_path
        else:
            logger.debug(
                f"Source cache directory {source_cache_path} does not exist, skipping bind mount"
            )

    cmd = "{} singularity {} exec --home {} {} {} {} -c '{}'".format(
        envvars,
        "--quiet --silent" if quiet else "",
        repr(os.getcwd()),
        args,
        img_path,
        shell_executable,
        cmd.replace("'", r"'\''"),
    )
    logger.debug(cmd)
    return cmd


class Singularity:
    instance = None

    def __new__(cls):
        if cls.instance is not None:
            return cls.instance
        else:
            inst = super().__new__(cls)
            cls.instance = inst
            return inst

    def __init__(self):
        self.checked = False
        self._version = None

    @property
    def version(self):
        assert (
            self._version is not None
        ), "bug: singularity version accessed before check() has been called"
        return self._version

    def parseversion(self, raw_version):
        import packaging

        raw_version = raw_version.rsplit(" ", 1)[-1]
        if raw_version.startswith("v"):
            raw_version = raw_version[1:]

        parsed_version = None
        trimend = len(raw_version)
        while parsed_version is None:
            try:
                parsed_version = packaging.version.Version(raw_version[:trimend])
            except packaging.version.InvalidVersion:
                trimend = trimend - 1
                if trimend == 0:
                    raise WorkflowError(
                        f"Apptainer/Singularity version cannot be parsed: {raw_version}"
                    )
        return parsed_version

    def check(self):
        from packaging.version import parse

        if not self.checked:
            if not shutil.which("singularity"):
                raise WorkflowError(
                    "The apptainer or singularity command has to be "
                    "available in order to use apptainer/singularity "
                    "integration."
                )
            try:
                v = subprocess.check_output(
                    ["singularity", "--version"], stderr=subprocess.PIPE
                ).decode()
            except subprocess.CalledProcessError as e:
                raise WorkflowError(
                    f"Failed to get singularity version:\n{e.stderr.decode()}"
                )
            if v.startswith("apptainer"):
                if self.parseversion(v) < parse("1.0.0"):
                    raise WorkflowError("Minimum apptainer version is 1.0.0.")
            elif self.parseversion(v) < parse("2.4.1"):
                raise WorkflowError("Minimum singularity version is 2.4.1.")
            self._version = v



================================================
FILE: src/snakemake/executors/__init__.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import os
import contextlib

from snakemake.logging import logger


@contextlib.contextmanager
def change_working_directory(directory=None):
    """Change working directory in execution context if provided."""
    if directory:
        try:
            saved_directory = os.getcwd()
            logger.info(f"Changing to shadow directory: {directory}")
            os.chdir(directory)
            yield
        finally:
            os.chdir(saved_directory)
    else:
        yield



================================================
FILE: src/snakemake/executors/dryrun.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2023, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from snakemake_interface_executor_plugins.executors.base import AbstractExecutor
from snakemake_interface_executor_plugins.jobs import (
    JobExecutorInterface,
)
from snakemake_interface_executor_plugins.settings import CommonSettings
from snakemake_interface_executor_plugins.executors.base import SubmittedJobInfo
from snakemake.common import async_run

from snakemake.logging import logger


common_settings = CommonSettings(
    non_local_exec=False,
    dryrun_exec=True,
    implies_no_shared_fs=False,
    job_deploy_sources=False,
    pass_envvar_declarations_to_cmd=False,
    auto_deploy_default_storage_provider=False,
)


class Executor(AbstractExecutor):
    def run_job(
        self,
        job: JobExecutorInterface,
    ):
        job_info = SubmittedJobInfo(job=job)
        self.report_job_submission(job_info)
        self.report_job_success(job_info)

    def get_exec_mode(self):
        raise NotImplementedError()

    def printjob(self, job: JobExecutorInterface):
        super().printjob(job)
        if job.is_group():
            for j in job.jobs:
                self.printcache(j)
        else:
            self.printcache(job)

    def printcache(self, job: JobExecutorInterface):
        cache_mode = self.workflow.get_cache_mode(job.rule)
        if cache_mode:
            if async_run(self.workflow.output_file_cache.exists(job, cache_mode)):
                logger.info(
                    "Output file {} will be obtained from global between-workflow cache.".format(
                        job.output[0]
                    )
                )
            else:
                logger.info(
                    "Output file {} will be written to global between-workflow cache.".format(
                        job.output[0]
                    )
                )

    def cancel(self):
        # nothing to do
        pass

    def shutdown(self):
        # nothing to do
        pass

    def handle_job_success(self, job: JobExecutorInterface):
        # nothing to do
        pass

    def handle_job_error(self, job: JobExecutorInterface):
        # nothing to do
        pass

    @property
    def cores(self):
        return self.workflow.resource_settings.cores



================================================
FILE: src/snakemake/executors/google_lifesciences_helper.py
================================================
#!/usr/bin/env python

# This is a helper script for the Google Life Sciences instance to be able to:
# 1. download a blob from storage, which is required at the onset of the Snakemake
#     gls.py download <bucket> <source> <destination>
# workflow step to obtain the working directory.
# 2. Upload logs back to storage (or some specified directory of files)
#    gls.py save <bucket> <source-dir> <destination-dir>
#    gls.py save <bucket> /google/logs/output source/logs

import argparse

from google.cloud import storage
from glob import glob
import sys
import os


def download_blob(bucket_name, source_blob_name, destination_file_name):
    """Downloads a blob from the bucket."""
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(source_blob_name)

    blob.download_to_filename(destination_file_name)

    print(f"Blob {source_blob_name} downloaded to {destination_file_name}.")


def save_files(bucket_name, source_path, destination_path):
    """given a directory path, save all files recursively to storage"""
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)

    # destination path should be stripped of path indicators too
    bucket_name = bucket_name.strip("/")
    destination_path = destination_path.strip("/")

    # These are fullpaths
    filenames = get_source_files(source_path)
    print("\nThe following files will be uploaded: %s" % "\n".join(filenames))

    if not filenames:
        print("Did not find any filenames under %s" % source_path)

    # Do the upload!
    for filename in filenames:
        # The relative path of the filename from the source path
        relative_path = filename.replace(source_path, "", 1).strip("/")
        # The path in storage includes relative path from destination_path
        storage_path = os.path.join(destination_path, relative_path)
        full_path = os.path.join(bucket_name, storage_path)
        print(f"{filename} -> {full_path}")
        blob = bucket.blob(storage_path)
        print(f"Uploading {filename} to {full_path}")
        blob.upload_from_filename(filename, content_type=".txt")


def get_source_files(source_path):
    """Given a directory, return a listing of files to upload"""
    filenames = []
    if not os.path.exists(source_path):
        print("%s does not exist!" % source_path)
        sys.exit(0)

    for x in os.walk(source_path):
        for name in glob(os.path.join(x[0], "*")):
            if not os.path.isdir(name):
                filenames.append(name)
    return filenames


def add_ending_slash(filename):
    """Since we want to replace based on having an ending slash, ensure it's there"""
    if not filename.endswith("/"):
        filename = "%s/" % filename
    return filename


def blob_commands(args):
    if args.command == "download":
        download_blob(
            args.bucket_name, args.source_blob_name, args.destination_file_name
        )
    elif args.command == "save":
        save_files(args.bucket_name, args.source_path, args.destination_path)


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    subparsers = parser.add_subparsers(dest="command")

    # Download file from storage
    download_parser = subparsers.add_parser("download", help=download_blob.__doc__)
    download_parser.add_argument("bucket_name", help="Your cloud storage bucket.")
    download_parser.add_argument("source_blob_name")
    download_parser.add_argument("destination_file_name")

    # Save logs to storage
    save_parser = subparsers.add_parser("save", help=save_files.__doc__)
    save_parser.add_argument("bucket_name", help="Your cloud storage bucket.")
    save_parser.add_argument("source_path")
    save_parser.add_argument("destination_path")

    args = parser.parse_args()
    blob_commands(args)


if __name__ == "__main__":
    main()



================================================
FILE: src/snakemake/executors/jobscript.sh
================================================
#!/bin/sh
# properties = {properties}
{exec_job}



================================================
FILE: src/snakemake/executors/local.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2023, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"


import os
import sys
import time
import shlex
import concurrent.futures
import subprocess
from functools import partial
from snakemake.common import async_run
from snakemake.executors import change_working_directory
from snakemake.settings.types import DeploymentMethod

from snakemake_interface_executor_plugins.executors.base import SubmittedJobInfo
from snakemake_interface_executor_plugins.executors.real import RealExecutor
from snakemake_interface_executor_plugins.dag import DAGExecutorInterface
from snakemake_interface_executor_plugins.workflow import WorkflowExecutorInterface
from snakemake_interface_executor_plugins.logging import LoggerExecutorInterface
from snakemake_interface_executor_plugins.jobs import (
    JobExecutorInterface,
    SingleJobExecutorInterface,
    GroupJobExecutorInterface,
)
from snakemake_interface_executor_plugins.settings import ExecMode, CommonSettings

from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException, log_verbose_traceback
from snakemake.exceptions import (
    WorkflowError,
    SpawnedJobError,
    CacheMissException,
)


common_settings = CommonSettings(
    non_local_exec=False,
    implies_no_shared_fs=False,
    job_deploy_sources=False,
    pass_envvar_declarations_to_cmd=True,
    auto_deploy_default_storage_provider=False,
)


_ProcessPoolExceptions = (KeyboardInterrupt,)
try:
    from concurrent.futures.process import BrokenProcessPool

    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class Executor(RealExecutor):
    def __post_init__(self):
        self.use_threads = self.workflow.execution_settings.use_threads
        self.keepincomplete = self.workflow.execution_settings.keep_incomplete
        cores = self.workflow.resource_settings.cores

        # Zero thread jobs do not need a thread, but they occupy additional workers.
        # Hence we need to reserve additional workers for them.
        workers = cores + 5 if cores is not None else 5
        self.workers = workers
        self.pool = concurrent.futures.ThreadPoolExecutor(max_workers=self.workers)

    def get_exec_mode(self):
        return ExecMode.SUBPROCESS

    @property
    def job_specific_local_groupid(self):
        return False

    def get_job_exec_prefix(self, job: JobExecutorInterface):
        return f"cd {shlex.quote(self.workflow.workdir_init)}"

    def get_python_executable(self):
        return sys.executable

    def additional_general_args(self):
        return "--quiet progress rules host"

    def get_job_args(self, job: JobExecutorInterface, **kwargs):
        return f"{super().get_job_args(job, **kwargs)}"

    def run_job(
        self,
        job: JobExecutorInterface,
    ):
        if job.is_group():
            # if we still don't have enough workers for this group, create a new pool here
            missing_workers = max(len(job) - self.workers, 0)
            if missing_workers:
                self.workers += missing_workers
                self.pool = concurrent.futures.ThreadPoolExecutor(
                    max_workers=self.workers
                )

            # the future waits for the entire group job
            future = self.pool.submit(self.run_group_job, job)
        else:
            future = self.run_single_job(job)

        job_info = SubmittedJobInfo(job=job)

        future.add_done_callback(partial(self._callback, job_info))
        self.report_job_submission(job_info)

    def job_args_and_prepare(self, job: JobExecutorInterface):
        async_run(job.prepare())

        conda_env = (
            job.conda_env.address
            if DeploymentMethod.CONDA
            in self.workflow.deployment_settings.deployment_method
            and job.conda_env
            else None
        )
        container_img = (
            job.container_img_path
            if DeploymentMethod.APPTAINER
            in self.workflow.deployment_settings.deployment_method
            else None
        )
        env_modules = (
            job.env_modules
            if DeploymentMethod.ENV_MODULES
            in self.workflow.deployment_settings.deployment_method
            else None
        )

        benchmark = None
        benchmark_repeats = job.benchmark_repeats or 1
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        return (
            job.rule,
            job.input._plainstrings(),
            job.output._plainstrings(),
            job.params,
            job.wildcards,
            job.threads,
            job.resources,
            job.log._plainstrings(),
            benchmark,
            benchmark_repeats,
            self.workflow.output_settings.benchmark_extended,
            conda_env,
            container_img,
            self.workflow.deployment_settings.apptainer_args,
            env_modules,
            DeploymentMethod.APPTAINER
            in self.workflow.deployment_settings.deployment_method,
            self.workflow.linemaps,
            self.workflow.execution_settings.debug,
            self.workflow.execution_settings.cleanup_scripts,
            job.shadow_dir,
            job.jobid,
            (
                self.workflow.execution_settings.edit_notebook
                if self.dag.is_edit_notebook_job(job)
                else None
            ),
            self.workflow.conda_base_path,
            job.rule.basedir,
            self.workflow.sourcecache.cache_path,
            self.workflow.sourcecache.runtime_cache_path,
        )

    def run_single_job(self, job: SingleJobExecutorInterface):
        if (
            self.use_threads
            or (not job.is_shadow and not job.is_run)
            or job.is_template_engine
        ):
            future = self.pool.submit(
                self.cached_or_run, job, run_wrapper, *self.job_args_and_prepare(job)
            )
        else:
            # run directive jobs are spawned into subprocesses
            future = self.pool.submit(self.cached_or_run, job, self.spawn_job, job)
        return future

    def run_group_job(self, job: GroupJobExecutorInterface):
        """Run a pipe or service group job.

        This lets all items run simultaneously."""
        # we only have to consider pipe or service groups because in local running mode,
        # these are the only groups that will occur

        service_futures = [self.run_single_job(j) for j in job if j.is_service]
        normal_futures = [self.run_single_job(j) for j in job if not j.is_service]

        while normal_futures:
            for f in list(normal_futures):
                if f.done():
                    logger.debug("Job inside group is finished.")
                    ex = f.exception()
                    if ex is not None:
                        logger.debug(f"Job inside group failed with exception {ex}.")
                        # kill all shell commands of the other group jobs
                        # there can be only shell commands because the
                        # run directive is not allowed for pipe jobs
                        for j in job:
                            shell.kill(j.jobid)
                        raise ex
                    normal_futures.remove(f)
            time.sleep(1)

        if service_futures:
            # terminate all service jobs since all consumers are done
            for j in job:
                if j.is_service:
                    logger.info(
                        f"Terminating service job {j.jobid} since all consuming jobs are finished."
                    )
                    shell.terminate(j.jobid)
                    logger.info(
                        f"Service job {j.jobid} has been successfully terminated."
                    )

    def spawn_job(self, job: SingleJobExecutorInterface):
        cmd = self.format_job_exec(job)
        logger.debug(f"spawned job: {cmd}")

        try:
            subprocess.check_call(cmd, shell=True)
        except subprocess.CalledProcessError:
            raise SpawnedJobError()

    def cached_or_run(self, job: SingleJobExecutorInterface, run_func, *args):
        """
        Either retrieve result from cache, or run job with given function.
        """
        cache_mode = self.workflow.get_cache_mode(job.rule)
        try:
            if cache_mode:
                async_run(self.workflow.output_file_cache.fetch(job, cache_mode))
                return
        except CacheMissException:
            pass
        run_func(*args)
        if cache_mode:
            async_run(self.workflow.output_file_cache.store(job, cache_mode))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job_info: SubmittedJobInfo, future):
        try:
            ex = future.exception()
            if ex is not None:
                print_exception(ex, self.workflow.linemaps)
                self.report_job_error(job_info)
            else:
                self.report_job_success(job_info)
        except _ProcessPoolExceptions:
            self.handle_job_error(job_info.job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except SpawnedJobError:
            # don't print error message, this is done by the spawned subprocess
            self.report_job_error(job_info)
        except Exception as ex:
            if self.workflow.output_settings.verbose or (
                not job_info.job.is_group() and not job_info.job.is_shell
            ):
                print_exception(ex, self.workflow.linemaps)
            self.report_job_error(job_info)

    @property
    def cores(self):
        return self.workflow.resource_settings.cores


def run_wrapper(
    job_rule,
    input,
    output,
    params,
    wildcards,
    threads,
    resources,
    log,
    benchmark,
    benchmark_repeats,
    benchmark_extended,
    conda_env,
    container_img,
    singularity_args,
    env_modules,
    use_singularity,
    linemaps,
    debug,
    cleanup_scripts,
    shadow_dir,
    jobid,
    edit_notebook,
    conda_base_path,
    basedir,
    sourcecache_path,
    runtime_sourcecache_path,
):
    """
    Wrapper around the run method that handles exceptions and benchmarking.

    Arguments
    job_rule   -- the ``job.rule`` member
    input      -- a list of input files
    output     -- a list of output files
    wildcards  -- so far processed wildcards
    threads    -- usable threads
    log        -- a list of log files
    shadow_dir -- optional shadow directory root
    """
    # get shortcuts to job_rule members
    run = job_rule.run_func
    rule = job_rule.name
    is_shell = job_rule.shellcmd is not None

    if os.name == "posix" and debug:
        sys.stdin = open("/dev/stdin")

    if benchmark is not None:
        from snakemake.benchmark import (
            BenchmarkRecord,
            benchmarked,
            write_benchmark_records,
        )

    # Change workdir if shadow defined and not using singularity.
    # Otherwise, we do the change from inside the container.
    passed_shadow_dir = None
    if use_singularity and container_img:
        passed_shadow_dir = shadow_dir
        shadow_dir = None

    try:
        with change_working_directory(shadow_dir):
            if benchmark:
                bench_records = []
                for bench_iteration in range(benchmark_repeats):
                    # Determine whether to benchmark this process or do not
                    # benchmarking at all.  We benchmark this process unless the
                    # execution is done through the ``shell:``, ``script:``, or
                    # ``wrapper:`` stanza.
                    is_sub = (
                        job_rule.shellcmd
                        or job_rule.script
                        or job_rule.wrapper
                        or job_rule.cwl
                    )
                    if is_sub:
                        # The benchmarking through ``benchmarked()`` is started
                        # in the execution of the shell fragment, script, wrapper
                        # etc, as the child PID is available there.
                        bench_record = BenchmarkRecord()
                        run(
                            input,
                            output,
                            params,
                            wildcards,
                            threads,
                            resources,
                            log,
                            rule,
                            conda_env,
                            container_img,
                            singularity_args,
                            use_singularity,
                            env_modules,
                            bench_record,
                            jobid,
                            is_shell,
                            bench_iteration,
                            cleanup_scripts,
                            passed_shadow_dir,
                            edit_notebook,
                            conda_base_path,
                            basedir,
                            sourcecache_path,
                            runtime_sourcecache_path,
                        )
                    else:
                        # The benchmarking is started here as we have a run section
                        # and the generated Python function is executed in this
                        # process' thread.
                        with benchmarked() as bench_record:
                            run(
                                input,
                                output,
                                params,
                                wildcards,
                                threads,
                                resources,
                                log,
                                rule,
                                conda_env,
                                container_img,
                                singularity_args,
                                use_singularity,
                                env_modules,
                                bench_record,
                                jobid,
                                is_shell,
                                bench_iteration,
                                cleanup_scripts,
                                passed_shadow_dir,
                                edit_notebook,
                                conda_base_path,
                                basedir,
                                sourcecache_path,
                                runtime_sourcecache_path,
                            )
                    # Store benchmark record for this iteration
                    bench_records.append(bench_record)
            else:
                run(
                    input,
                    output,
                    params,
                    wildcards,
                    threads,
                    resources,
                    log,
                    rule,
                    conda_env,
                    container_img,
                    singularity_args,
                    use_singularity,
                    env_modules,
                    None,
                    jobid,
                    is_shell,
                    None,
                    cleanup_scripts,
                    passed_shadow_dir,
                    edit_notebook,
                    conda_base_path,
                    basedir,
                    sourcecache_path,
                    runtime_sourcecache_path,
                )
    except (KeyboardInterrupt, SystemExit) as e:
        # Re-raise the keyboard interrupt in order to record an error in the
        # scheduler but ignore it
        raise e
    except BaseException as ex:
        # this ensures that exception can be re-raised in the parent thread
        origin = get_exception_origin(ex, linemaps)
        if origin is not None:
            log_verbose_traceback(ex)
            lineno, file = origin
            raise RuleException(
                format_error(
                    ex, lineno, linemaps=linemaps, snakefile=file, show_traceback=True
                )
            )
        else:
            # some internal bug, just reraise
            raise ex

    if benchmark is not None:
        try:
            # Add job info to (all repeats of) benchmark file
            for bench_record in bench_records:
                bench_record.jobid = jobid
                bench_record.rule_name = job_rule.name
                bench_record.wildcards = wildcards
                bench_record.params = params
                bench_record.resources = resources
                bench_record.input = input
                bench_record.threads = threads
            write_benchmark_records(bench_records, benchmark, benchmark_extended)
        except Exception as ex:
            raise WorkflowError(ex)



================================================
FILE: src/snakemake/executors/touch.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import time

from snakemake_interface_executor_plugins.executors.real import RealExecutor
from snakemake_interface_executor_plugins.dag import DAGExecutorInterface
from snakemake_interface_executor_plugins.workflow import WorkflowExecutorInterface
from snakemake_interface_executor_plugins.logging import LoggerExecutorInterface
from snakemake_interface_executor_plugins.jobs import (
    JobExecutorInterface,
)
from snakemake_interface_executor_plugins.executors.base import SubmittedJobInfo
from snakemake_interface_executor_plugins.settings import CommonSettings
from snakemake.common import async_run
from snakemake.logging import logger
from snakemake.exceptions import print_exception


common_settings = CommonSettings(
    non_local_exec=False,
    implies_no_shared_fs=False,
    job_deploy_sources=False,
    touch_exec=True,
    pass_envvar_declarations_to_cmd=False,
    auto_deploy_default_storage_provider=False,
)


class Executor(RealExecutor):
    SLEEPING_TIME = 0.1

    def run_job(
        self,
        job: JobExecutorInterface,
    ):
        job_info = SubmittedJobInfo(job=job)
        try:

            if job.output:

                async def touch():
                    touch_storage_and_local_files = {
                        f
                        for f in job.output
                        if f.is_storage and await f.exists_in_storage()
                    }
                    touch_files = {
                        f
                        for f in job.output
                        if f not in touch_storage_and_local_files
                        and await f.exists_local()
                    }
                    non_existing_files = (
                        set(job.output) - touch_storage_and_local_files - touch_files
                    )

                    if touch_files or touch_storage_and_local_files:
                        time.sleep(Executor.SLEEPING_TIME)
                        for f in touch_storage_and_local_files:
                            await f.touch_storage_and_local()
                        for f in touch_files:
                            f.touch()
                    if len(non_existing_files) > 0:
                        logger.warning(
                            f"Output files not touched because they don't exist: {', '.join(non_existing_files)}"
                        )

                async_run(touch())

            self.report_job_submission(job_info)
            self.report_job_success(job_info)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            self.report_job_error(job_info)

    def get_exec_mode(self):
        raise NotImplementedError()

    def handle_job_success(self, job: JobExecutorInterface):
        super().handle_job_success(job)

    def cancel(self):
        # nothing to do
        pass

    def shutdown(self):
        # nothing to do
        pass

    def get_python_executable(self):
        raise NotImplementedError()

    @property
    def cores(self):
        return self.workflow.resource_settings.cores



================================================
FILE: src/snakemake/io/fmt.py
================================================
from snakemake.common.tbdstring import TBDString
from snakemake.io import get_flag_value, is_flagged


def fmt_iofile(f, as_input: bool = False, as_output: bool = False):
    as_io = as_input or as_output

    if f.is_storage:
        if as_input:
            if f.storage_object.retrieve:
                storage_phrase = "retrieve from storage"
            else:
                storage_phrase = "only provide storage uri"
        elif as_output:
            storage_phrase = "send to storage"
        else:
            storage_phrase = ""
        f_str = f.storage_object.print_query
    else:
        f_str = f
        storage_phrase = ""

    def annotate(f_str, label=""):
        sep = ", " if label and storage_phrase else ""
        ann = f" ({label}{sep}{storage_phrase})" if label or storage_phrase else ""
        return f"{f_str}{ann}"

    if is_flagged(f, "sourcecache_entry"):
        orig_path_or_uri = get_flag_value(f, "sourcecache_entry")
        return annotate(orig_path_or_uri, "cached")
    if as_io:
        if is_flagged(f, "pipe"):
            return annotate(f_str, "pipe")
        elif is_flagged(f, "service"):
            return annotate(f_str, "service")
        elif is_flagged(f, "nodelocal"):
            return annotate(f_str, "nodelocal")
        elif is_flagged(f, "update"):
            return annotate(f_str, "update")
        elif is_flagged(f, "before_update"):
            return annotate(f_str, "before update")
        elif is_flagged(f, "access_pattern"):
            pattern = f"access: {get_flag_value(f, 'access_pattern')}"
            return annotate(f_str, pattern)
        elif is_flagged(f, "checkpoint_target"):
            return TBDString()
        else:
            return annotate(f_str)
    return f



================================================
FILE: src/snakemake/io/flags/__init__.py
================================================
from dataclasses import dataclass, field
from typing import Callable, Iterable, Mapping, Set, Union

from snakemake.io import get_flag_store_keys, is_flagged


# TODO move all flag definitions to this module

FlaggableItem = Union[str, Callable]

FlaggableItemOrIterable = Union[FlaggableItem, Iterable[FlaggableItem]]


CONTRADICTING_FLAGS = {
    "temp": {"protected"},
    "protected": {"temp"},
}


@dataclass
class DefaultFlags:
    flags_to_store_keys: Mapping[Callable, Set[str]] = field(default_factory=dict)

    def register_flags(self, *flags: Callable):
        self.flags_to_store_keys.clear()
        for flag in flags:
            keys = get_flag_store_keys(flag)
            self.flags_to_store_keys[flag] = keys

    def __iter__(self):
        return iter(self.flags_to_store_keys)

    def clear(self):
        self.flags_to_store_keys.clear()

    def get_store_keys(self, flag: Callable):
        return self.flags_to_store_keys[flag]

    def apply(self, item: FlaggableItem):
        for flag_ in self:
            store_keys = self.get_store_keys(flag_)
            if any(
                is_flagged(item, store_key)
                or any(
                    is_flagged(item, contradicting)
                    for contradicting in self.get_contradicting_flags(store_key)
                )
                for store_key in store_keys
            ):
                continue
            item = flag_(item)
        return item

    @staticmethod
    def get_contradicting_flags(store_key: str):
        return CONTRADICTING_FLAGS.get(store_key, set())



================================================
FILE: src/snakemake/io/flags/access_patterns.py
================================================
from enum import Enum
from snakemake.io import flag
from snakemake.io.flags import FlaggableItemOrIterable


STORE_KEY = "access_pattern"


class AccessPattern(Enum):
    RANDOM = "random"
    SEQUENTIAL = "sequential"
    MULTI = "multi"

    def __str__(self):
        return self.value


class AccessPatternFactory:
    @classmethod
    def random(cls, item: FlaggableItemOrIterable):
        return flag(item, STORE_KEY, AccessPattern.RANDOM)

    @classmethod
    def sequential(cls, item: FlaggableItemOrIterable):
        return flag(item, STORE_KEY, AccessPattern.SEQUENTIAL)

    @classmethod
    def multi(cls, item: FlaggableItemOrIterable):
        return flag(item, STORE_KEY, AccessPattern.MULTI)



================================================
FILE: src/snakemake/ioutils/__init__.py
================================================
from snakemake.ioutils.branch import branch
from snakemake.ioutils.collect import collect
from snakemake.ioutils.evaluate import evaluate
from snakemake.ioutils.exists import exists
from snakemake.ioutils.lookup import lookup
from snakemake.ioutils.rule_items_proxy import rule_item_factory
from snakemake.ioutils.subpath import subpath
from snakemake.ioutils.input import parse_input, extract_checksum, flatten


def register_in_globals(_globals):
    _globals.update(
        {
            "lookup": lookup,
            "evaluate": evaluate,
            "branch": branch,
            "collect": collect,
            "exists": exists,
            "input": rule_item_factory("input"),
            "output": rule_item_factory("output"),
            "params": rule_item_factory("params"),
            "resources": rule_item_factory("resources"),
            "threads": rule_item_factory("threads"),
            "subpath": subpath,
            "parse_input": parse_input,
            "extract_checksum": extract_checksum,
            "flatten": flatten,
        }
    )



================================================
FILE: src/snakemake/ioutils/branch.py
================================================
from collections.abc import Mapping, Callable
from typing import Optional, Union


def branch(
    condition: Union[Callable, bool],
    then: Optional[Union[str, list[str], Callable]] = None,
    otherwise: Optional[Union[str, list[str], Callable]] = None,
    cases: Optional[Mapping] = None,
):
    """Branch based on a condition that is provided as a function pointer (i.e. a Callable)
    or a value.

    If then and optionally otherwise are specified, do the following:
    If the condition is (or evaluates to) True, return the value
    of the then parameter. Otherwise, return the value of the otherwise parameter.

    If cases is specified, do the following:
    Retrieve the value of the cases mapping using the return value of the condition
    (if it is a function), or the condition value itself as a key.

    The given condition function has to take wildcards as its only parameter.
    Similarly, then, otherwise and the values of the cases mapping can be such functions.

    If any such function is given to any of those arguments, this function returns a derived
    input function that will be evaluated once the wildcards are known.
    """

    def convert_none(value):
        return value or []

    def handle_callable(value, wildcards):
        if isinstance(value, Callable):
            return convert_none(value(wildcards))
        else:
            return convert_none(value)

    def do_branch_then_otherwise(wildcards):
        if handle_callable(condition, wildcards):
            return handle_callable(then, wildcards)
        else:
            return handle_callable(otherwise, wildcards)

    def do_branch_cases(wildcards):
        res = handle_callable(condition, wildcards)
        try:
            selected_case = cases[res]
        except KeyError:
            raise KeyError(f"Key {res} not found in given cases of branch function")
        return handle_callable(selected_case, wildcards)

    do_branch = do_branch_then_otherwise
    if cases is not None:
        if otherwise is not None or then is not None:
            raise ValueError("Cannot use cases together with then or otherwise.")
        do_branch = do_branch_cases

    if any(isinstance(value, Callable) for value in (condition, then, otherwise)):

        def inner(wildcards):
            return do_branch(wildcards)

        return inner
    else:
        return do_branch(None)



================================================
FILE: src/snakemake/ioutils/collect.py
================================================
import snakemake.io


# Alias for expand that provides a more intuitive name for the use case of
# collecting files from previous jobs.
collect = snakemake.io.expand



================================================
FILE: src/snakemake/ioutils/evaluate.py
================================================
from snakemake.logging import format_dict
from snakemake_interface_common.exceptions import WorkflowError


def evaluate(expr: str):
    """Evaluate a python expression while replacing any wildcards given as
    {wildcardname} with the wildcard value represented as a string."""

    def inner(wildcards):
        formatted = expr.format(**{w: repr(v) for w, v in wildcards.items()})
        try:
            return eval(formatted, globals())
        except Exception as e:
            raise WorkflowError(
                f"Failed to evaluate expression {expr} with wildcards {format_dict(wildcards)}. "
                f"Formatted expression: {formatted}",
                e,
            )

    return inner



================================================
FILE: src/snakemake/ioutils/exists.py
================================================
import inspect
import os

from snakemake.common import async_run
import snakemake.io
from snakemake_interface_common.exceptions import WorkflowError


def exists(path):
    """Return True if the given file or directory exists.

    This function considers any storage arguments given to Snakemake.
    """
    func_context = inspect.currentframe().f_back.f_locals
    func_context_global = inspect.currentframe().f_back.f_globals

    workflow = func_context.get("workflow") or func_context_global.get("workflow")

    if workflow is None:
        raise WorkflowError(
            "The exists function can only be used within a Snakemake workflow "
            "(the global variable 'workflow' has to be present)."
        )

    path = workflow.modifier.path_modifier.apply_default_storage(path)
    if snakemake.io.is_flagged(path, "storage_object"):
        return async_run(path.flags["storage_object"].managed_exists())
    else:
        return os.path.exists(path)



================================================
FILE: src/snakemake/ioutils/input.py
================================================
from typing import List


def parse_input(infile, parser, **kwargs):
    def _parse_input(wildcards, input):
        _infile = infile(wildcards, input) if callable(infile) else infile
        with open(_infile, "r") as fh:
            if parser is None:
                return fh.read().strip()
            else:
                return parser(fh, **kwargs)

    return _parse_input


def extract_checksum(infile, **kwargs):
    try:
        import pandas as pd

        fix_file_name = lambda x: x.removeprefix("./")
        return (
            pd.read_csv(
                infile,
                sep="  ",
                header=None,
                engine="python",
                converters={1: fix_file_name},
            )
            .set_index(1)
            .loc[fix_file_name(kwargs.get("file"))]
            .item()
        )
    except ImportError:
        raise WorkflowError("Pandas is required to extract checksum from file.")


def flatten(list_of_lists: List) -> List:
    """Flatten an irregular list of lists recursively

    https://stackoverflow.com/a/53778278

    :param list_of_lists: A list of lists
    :return result: A list that has been flattened from a list of lists
    """
    result = list()
    for i in list_of_lists:
        if isinstance(i, list):
            result.extend(flatten(i))
        else:
            result.append(str(i))
    return result



================================================
FILE: src/snakemake/ioutils/lookup.py
================================================
from abc import ABC, abstractmethod
from collections.abc import Mapping
from functools import partial
import re
from typing import List, Optional, Union

import snakemake.io
import snakemake.utils
from snakemake.exceptions import LookupError


class WildcardHandlerBase(ABC):
    fmt_regex = re.compile(r"\{(?P<stmt>[^\{][^\{\}]+)\}[^\}]")

    def __init__(self, func, **namespace):
        self.func = func
        self.namespace = namespace

    def needs_wildcards(self, expression):
        return callable(expression) or any(
            name not in self.namespace
            for name in snakemake.io.get_wildcard_names(expression)
        )

    @abstractmethod
    def apply_func(self, expression, namespace=None): ...

    def handle(self, expression):
        if self.needs_wildcards(expression) or any(
            callable(value) for value in self.namespace.values()
        ):

            def inner(wildcards):
                if self.namespace:
                    # add wildcard values to namespace
                    # do not override namespace
                    # (as it has been chosen explicitly by the dev)
                    namespace = dict(self.namespace)
                    for name, value in list(namespace.items()):
                        # resolve callables given in namespace
                        if callable(value):
                            namespace[name] = value(wildcards)
                    for name, value in wildcards.items():
                        if name not in namespace:
                            namespace[name] = value
                else:
                    namespace = wildcards
                if callable(expression):
                    resolved_expression = expression(wildcards)
                else:
                    resolved_expression = expression
                resolved_expression = snakemake.utils.format(
                    resolved_expression, **namespace
                )
                return self.apply_func(resolved_expression, namespace)

            return inner
        else:
            if self.namespace:
                resolved_expression = snakemake.utils.format(
                    expression, **self.namespace
                )
            else:
                resolved_expression = expression
            return self.apply_func(resolved_expression, self.namespace)


class DpathWildcardHandler(WildcardHandlerBase):
    def apply_func(self, expression, namespace=None):
        return self.func(expression)


class QueryWildcardHandler(WildcardHandlerBase):
    def __init__(self, func, cols=None, is_nrows=None, **namespace):
        super().__init__(func, **namespace)
        self.cols = cols
        self.is_nrows = is_nrows

    def needs_wildcards(self, expression):
        if super().needs_wildcards(expression):
            return True
        if self.cols is None:
            return False
        if isinstance(self.cols, list):
            return any(
                super(QueryWildcardHandler, self).needs_wildcards(col)
                for col in self.cols
            )
        else:
            return super().needs_wildcards(self.cols)

    def apply_func(self, expression, namespace=None):
        cols = self.cols
        if self.cols is not None and namespace is not None:
            if isinstance(self.cols, list):
                cols = [snakemake.utils.format(col, **namespace) for col in self.cols]
            else:
                cols = snakemake.utils.format(self.cols, **namespace)
        return self.func(expression, cols=cols, is_nrows=self.is_nrows)


NODEFAULT = object()


def lookup(
    dpath: Optional[str] = None,
    query: Optional[str] = None,
    cols: Optional[Union[List[str], str]] = None,
    is_nrows: Optional[int] = None,
    within=None,
    default=NODEFAULT,
    **namespace,
):
    """Lookup values in a pandas dataframe, series, or python mapping (e.g. dict).

    Required argument ``within`` should be a pandas dataframe or series (in which
    case use ``query``, and optionally ``cols`` and ``is_nrows``), or a Python
    mapping like a dict (in which case use the ``dpath`` argument is used).

    In case of a pandas dataframe (see https://pandas.pydata.org),
    the query parameter is passed to DataFrame.query().
    If the query results in multiple rows, the result is returned as a list of
    named tuples with the column names as attributes.
    If the query results in a single row, the result is returned as a single
    named tuple with the column names as attributes.
    In both cases, the result can be used by the expand or collect function,
    e.g. `collect("results/{item.sample}.txt", sample=lookup(query="someval > 2", within=samples))`.
    Since the result, in any case, also evaluates to True if it is not empty
    when interpreted as a boolean by Python, it can also be used as a condition
    for the branch function, e.g.
    ``branch(lookup(query="sample == '{sample}' & someval > 2", within=samples), then="foo", otherwise="bar")``.
    In case your dataframe has an index, you can also access the index within the
    query, e.g. for faster, constant time lookups: ``lookup(query="index.loc[{sample}]", within=samples)``.
    Further, it is possible to constrain the output to a list of columns, e.g.
    ``lookup(query="sample == '{sample}'", within=samples, cols=["somecolumn"])`` or to
    a single column, e.g.
    ``lookup(query="sample == '{sample}'", within=samples, cols="somecolumn")``.
    In the latter case, just a list of items in that column is returned.
    Finally, if the integer argument ``is_nrows`` is used, this returns true
    if there are that many rows in the query results, false otherwise.

    In case of a pandas series, the series is converted into a dataframe via
    Series.to_frame() and the same logic as for a dataframe is applied.

    In case of a python mapping, the ``dpath`` parameter is passed to
    ``dpath.values()`` (see https://github.com/dpath-maintainers/dpath-python),
    and the ``query``, ``cols``, and ``is_nrows`` arguments are ignored. If the
    dpath is not found, a ``LookupError`` is raised, unless a default fallback
    value is provided via the ``default`` argument.

    Query, dpath and cols may contain wildcards (e.g. {sample}).
    In that case, this function returns a Snakemake input function which takes
    wildcards as its only argument and will be evaluated by Snakemake
    once the wildcard values are known.

    In addition to wildcard values, dpath, query and cols may refer via the same syntax
    to auxiliary namespace arguments given to the lookup function, e.g.
    ``lookup(query="cell_type == '{sample.cell_type}'", within=samples, sample=lookup("sample == '{sample}'", within=samples))``
    This way, one can e.g. pass additional variables or chain lookups into more complex queries.
    """
    error = partial(LookupError, query=query, dpath=dpath)

    if within is None:
        raise error(
            msg="Must provide a dataframe, series, or mapping to search within."
        )
    if cols is not None and not isinstance(cols, (str, list)):
        raise error(msg="The cols argument has to be either a str or a list of str.")
    if is_nrows is not None and not isinstance(is_nrows, int):
        raise error(msg="The is_nrows argument has to be an int.")

    if query is not None:
        if isinstance(within, Mapping):
            raise error(
                msg="Query parameter can only be used with pandas DataFrame or Series objects."
            )

        import pandas as pd

        if isinstance(within, pd.Series):
            within = within.to_frame()

        def do_query(query, cols=None, is_nrows=None):
            try:
                res = within.query(query)
            except Exception as e:
                raise LookupError(query=query, exc=e)

            if is_nrows is not None:
                return is_nrows == len(res)
            if cols is not None:
                res = res[cols]
                if not isinstance(cols, list):
                    # single column select, just return a list of values
                    return res.to_list()
            res = list(res.itertuples(index=cols is None))
            if len(res) == 1:
                # just return the item if it is only one
                return res[0]
            return res

        return QueryWildcardHandler(
            do_query, cols=cols, is_nrows=is_nrows, **namespace
        ).handle(query)

    elif dpath is not None:
        if not isinstance(within, Mapping):
            raise error(
                msg="Dpath parameter can only be used with python mapping (e.g. dict)."
            )
        import dpath as dp

        def do_dpath(dpath):
            try:
                return dp.get(within, dpath)
            except ValueError:
                return dp.values(within, dpath)
            except KeyError:
                if default is not NODEFAULT:
                    return default
                raise LookupError(dpath=dpath, msg="Dpath not found.")

        return DpathWildcardHandler(do_dpath, **namespace).handle(dpath)
    else:
        raise error("Must provide either a query or dpath parameter.")



================================================
FILE: src/snakemake/ioutils/rule_items_proxy.py
================================================
from typing import Union
from enum import Enum


def rule_item_factory(name: str):
    """Allows to access input, output etc. from statements inside
    a rule but outside of run/shell etc. blocks. Returns an object that
    defers evaluation to the DAG phase.
    """
    if name == "threads":

        def inner(_wildcards, threads):
            return threads

        return inner
    return RuleItemProxy(name)


class KeyKind(Enum):
    ATTRIBUTE = 0
    ITEM = 1


class RuleItemProxy:
    """Proxy class for deferring access to attributes and keys of the given item to DAG
    phase.
    """

    def __init__(self, name):
        self.name = name

    def __getattr__(self, name: str):
        return self._deferred_get(name, KeyKind.ATTRIBUTE)

    def __getitem__(self, key):
        return self._deferred_get(key, KeyKind.ITEM)

    def _deferred_get(self, key: Union[str, int], kind: KeyKind):
        if kind == KeyKind.ATTRIBUTE:

            def _get(item):
                return getattr(item, key)

        elif kind == KeyKind.ITEM:

            def _get(item):
                return item[key]

        else:
            raise ValueError("kind must be 'attribute' or 'item'")

        if self.name == "output":

            def inner(wildcards, output):
                return _get(output)

        elif self.name == "input":

            def inner(wildcards, input):
                return _get(input)

        elif self.name == "params":

            def inner(wildcards, params):
                return _get(params)

        elif self.name == "resources":

            def inner(wildcards, resources):
                return _get(resources)

        else:
            raise ValueError(f"Unknown item type: {self.name}")

        return inner



================================================
FILE: src/snakemake/ioutils/subpath.py
================================================
import os
from pathlib import Path
from typing import Callable, Optional, Union

from snakemake.common import get_function_params, overwrite_function_params
from snakemake.io import is_callable


def subpath(
    path_or_func: Union[Callable, str, Path],
    strip_suffix: Optional[str] = None,
    basename: bool = False,
    parent: bool = False,
    ancestor: Optional[int] = None,
):
    """Return the subpath of a given path.

    Args:
        path_or_func: A string, pathlib.Path, or a function returning a string or pathlib.Path.
        strip_suffix: If given, strip the suffix from the path.
        basename: If True, return the basename of the path (cannot be used together with parent or ancestor).
        parent: If True, return the parent directory of the path (cannot be used together with ancestor).
        ancestor: If given, return the ancestor directory of the path (cannot be used together with parent).
    """

    # TODO typecheck arguments, maybe find a decorator for that purpose
    def do(path):
        if isinstance(path, Path):
            path = str(path)
        if not isinstance(path, str):
            raise ValueError(
                "Value passed to subpath "
                "must be a single string or pathlib.Path (or a function returning those). "
                f"Obtained value: {repr(path)}"
            )
        if strip_suffix is not None:
            if not path.endswith(strip_suffix):
                raise ValueError(
                    f"Path {path} does not end with the specified suffix {strip_suffix}"
                )
            path = path[: -len(strip_suffix)]
        if basename:
            if parent or ancestor is not None:
                raise ValueError(
                    "basename cannot be used together with parent or ancestor"
                )
            path = os.path.basename(path)
        elif parent:
            if ancestor is not None:
                raise ValueError("parent cannot be used together with ancestor")
            path = os.path.dirname(path)
            if path == "":
                path = "."
        elif ancestor is not None:
            if ancestor < 1:
                raise ValueError("ancestor must be greater than 0")
            for _ in range(ancestor):
                path = os.path.dirname(path)
                if path == "":
                    path = "."
                    break
        return path

    if is_callable(path_or_func):
        params = get_function_params(path_or_func)

        def inner(wildcards, **args):
            value = path_or_func(wildcards, **args)
            return do(value)

        overwrite_function_params(inner, params)
        return inner
    else:
        return do(path_or_func)



================================================
FILE: src/snakemake/linting/__init__.py
================================================
import textwrap
import shutil
import inspect
from abc import ABC, abstractmethod

from snakemake.logging import logger

# (?!\\+) is a negative lookahead, which removes trailing
# '+'s from the match. There is a minor risk, that a user
# intentionally uses file name (parts) with a trailing '+'
# intentionally. The regex extension _should_ allow simple
# regexes as '\s+' in place of a tab separator to pass.
NAME_PATTERN = "[a-zA-Z_][a-zA-Z_0-9]*(?!\\+)"


class Linter(ABC):
    def __init__(self, workflow, items):
        self.items = items
        self.workflow = workflow

    def read_item(self, item):
        return item

    def lint(self, json=False):
        json_lints = [] if json else None
        linted = False
        for item in self.items:
            item_lints = [
                lint
                for lint_item in self.lints()
                for lint in lint_item(self.read_item(item))
            ]
            if not item_lints:
                continue
            linted = True
            if json:
                json_lints.append(
                    {
                        "for": self.item_desc_json(item),
                        "lints": [lint.__dict__ for lint in item_lints],
                    }
                )
            else:
                logger.warning(
                    "Lints for {}:\n{}\n".format(
                        self.item_desc_plain(item),
                        "\n".join(map("    * {}".format, item_lints)),
                    )
                )
        return json_lints, linted

    @abstractmethod
    def item_desc_json(self, item):
        pass

    @abstractmethod
    def item_desc_plain(self, item):
        pass

    def lints(self):
        return (
            method
            for name, method in inspect.getmembers(self)
            if name.startswith("lint_")
        )


class Lint:
    def __init__(self, title, body, links=None):
        self.title = title
        self.body = body
        self.links = links or []

    def __str__(self) -> str:
        width, _ = shutil.get_terminal_size()
        output = "{}:\n{}".format(
            self.title,
            "\n".join(
                map("      {}".format, textwrap.wrap(self.body, max(width - 6, 20)))
            ),
        )
        if self.links:
            output += "\n      Also see:\n{}".format(
                "\n".join(map("      {}".format, self.links))
            )
        return output



================================================
FILE: src/snakemake/linting/links.py
================================================
package_management = "https://snakemake.readthedocs.io/en/latest/snakefiles/deployment.html#integrated-package-management"
containers = "https://snakemake.readthedocs.io/en/latest/snakefiles/deployment.html#running-jobs-in-containers"
rules = "https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#rules"
external_scripts = (
    "https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#external-scripts"
)
notebooks = "https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#jupyter-notebook-integration"
checkpoints = "https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#data-dependent-conditional-execution"
params = "https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#non-file-parameters-for-rules"
log = "https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#log-files"
input_functions = "https://snakemake.readthedocs.io/en/stable/tutorial/advanced.html#tutorial-input-functions"
envvars = "https://snakemake.readthedocs.io/en/latest/snakefiles/configuration.html#environment-variables"
includes = (
    "https://snakemake.readthedocs.io/en/latest/snakefiles/modularization.html#includes"
)
config = "https://snakemake.readthedocs.io/en/latest/snakefiles/configuration.html#configuration"



================================================
FILE: src/snakemake/linting/rules.py
================================================
from itertools import chain
import re
import sys

from snakemake.linting import Linter, Lint, links, NAME_PATTERN
from snakemake.rules import Rule


class RuleLinter(Linter):
    def item_desc_plain(self, rule):
        lineno = self.get_lineno(rule)
        return f"rule {rule.name} (line {lineno}, {rule.snakefile})"

    def item_desc_json(self, rule):
        lineno = self.get_lineno(rule)
        return {"rule": rule.name, "line": lineno, "snakefile": rule.snakefile}

    def get_lineno(self, rule: Rule) -> int | None:
        linemaps = self.workflow.linemaps
        if linemaps and rule.snakefile in linemaps:
            return linemaps[rule.snakefile][rule.lineno]
        return rule.lineno

    def lint_params_prefix(self, rule):
        for param, value in rule.params.items():
            if (
                isinstance(value, str)
                and value
                and any(
                    f.startswith(value)
                    for f in chain(rule.input, rule.output)
                    if isinstance(f, str)
                )
            ):
                yield Lint(
                    title="Param {} is a prefix of input or output file but hardcoded".format(
                        param
                    ),
                    body="If this is meant to represent a file path prefix, it will fail when running "
                    "workflow in environments without a shared filesystem. "
                    "Instead, provide a function that infers the appropriate prefix from the input or "
                    "output file, e.g.: lambda w, input: os.path.splitext(input[0])[0]",
                    links=[links.params, links.input_functions],
                )

    def lint_log_directive(self, rule):
        if not rule.log and not rule.norun and not rule.is_handover:
            yield Lint(
                title="No log directive defined",
                body="Without a log directive, all output will be printed "
                "to the terminal. In distributed environments, this means "
                "that errors are harder to discover. In local environments, "
                "output of concurrent jobs will be mixed and become unreadable.",
                links=[links.log],
            )

    def lint_not_used_params(
        self,
        rule,
        valid_names={
            "input",
            "output",
            "log",
            "params",
            "wildcards",
            "threads",
            "resources",
        },
        regex=re.compile(f"{{(?P<name>{NAME_PATTERN}).*?}}"),
    ):
        if rule.shellcmd:
            for match in regex.finditer(rule.shellcmd):
                name = match.group("name")

                before = match.start() - 1
                after = match.end()

                if name not in valid_names and (
                    not (before >= 0 and after < len(rule.shellcmd))
                    or (rule.shellcmd[before] != "{" and rule.shellcmd[after] != "}")
                ):
                    yield Lint(
                        title="Shell command directly uses variable {} from outside of the rule".format(
                            name
                        ),
                        body="It is recommended to pass all files as input and output, and non-file parameters "
                        "via the params directive. Otherwise, provenance tracking is less accurate.",
                        links=[links.params],
                    )

    def lint_long_run(self, rule):
        func_code = rule.run_func.__code__.co_code
        max_len = 70 if sys.version_info < (3, 11) else 210
        if rule.is_run and len(func_code) > max_len:
            yield Lint(
                title="Migrate long run directives into scripts or notebooks",
                body="Long run directives hamper workflow readability. Use the script or notebook directive instead. "
                "Note that the script or notebook directive does not involve boilerplate. Similar to run, you "
                "will have direct access to params, input, output, and wildcards."
                "Only use the run directive for a handful of lines.",
                links=[links.external_scripts, links.notebooks],
            )

    def lint_iofile_by_index(self, rule, regex=re.compile(r"(input|output)\[[0-9]+\]")):
        if rule.shellcmd and regex.search(rule.shellcmd):
            yield Lint(
                title="Do not access input and output files individually by index in shell commands",
                body="When individual access to input or output files is needed (i.e., just writing '{input}' "
                "is impossible), use names ('{input.somename}') instead of index based access.",
                links=[links.rules],
            )

    def lint_missing_software_definition(self, rule):
        if (
            not rule.norun
            and not rule.is_handover
            and not rule.is_run
            and not (rule.conda_env or rule.container_img)
        ):
            if rule.env_modules:
                yield Lint(
                    title="Additionally specify a conda environment or container for each rule, environment modules are not enough",
                    body="While environment modules allow to document and deploy the required software on a certain "
                    "platform, they lock your workflow in there, disabling easy reproducibility on other machines "
                    "that don't have exactly the same environment modules. Hence env modules (which might be beneficial "
                    "in certain cluster environments), should always be complemented with equivalent conda "
                    "environments.",
                    links=[links.package_management, links.containers],
                )
            else:
                yield Lint(
                    title="Specify a conda environment or container for each rule.",
                    body="This way, the used software for each specific step is documented, and "
                    "the workflow can be executed on any machine without prerequisites.",
                    links=[links.package_management, links.containers],
                )



================================================
FILE: src/snakemake/linting/snakefiles.py
================================================
import re
from itertools import chain

from snakemake.linting import Linter, Lint, links, NAME_PATTERN

ABS_PATH_PATTERN = "(?P<quote>['\"])(?P<path>(?:/[^/\\n]+?)+?)(?P=quote)"
PATH_PATTERN = "(?P<quote>['\"])(?P<path>/?(?:[^/]+?/)+?(?:[^/]+?)?)(?P=quote)"


class SnakefileLinter(Linter):
    def item_desc_plain(self, snakefile):
        return f"snakefile {snakefile}"

    def item_desc_json(self, snakefile):
        return {"snakefile": snakefile}

    def read_item(self, snakefile):
        return self.workflow.sourcecache.open(snakefile).read()

    def lint_absolute_paths(self, snakefile, regex=re.compile(ABS_PATH_PATTERN)):
        for match in regex.finditer(snakefile):
            line = get_line(match, snakefile)
            yield Lint(
                title='Absolute path "{}" in line {}'.format(match.group("path"), line),
                body="Do not define absolute paths inside of the workflow, since this "
                "renders your workflow irreproducible on other machines. "
                "Use path relative to the working directory instead, or make the path "
                "configurable via a config file.",
                links=[links.config],
            )

    def lint_mixed_func_and_rules(
        self,
        snakefile,
        rule_regex=re.compile("rule .+?:"),
        func_regex=re.compile("def .+?:"),
    ):
        if rule_regex.search(snakefile) and func_regex.search(snakefile):
            yield Lint(
                title="Mixed rules and functions in same snakefile.",
                body="Small one-liner functions used only once should be "
                "defined as lambda expressions. Other functions should be collected "
                "in a common module, e.g. 'rules/common.smk'. This makes the workflow "
                "steps more readable.",
                links=[links.includes],
            )

    def lint_path_add(
        self,
        snakefile,
        regex1=re.compile(f"{NAME_PATTERN} *\\+ *{PATH_PATTERN}"),
        regex2=re.compile(f"{PATH_PATTERN} *\\+ *{NAME_PATTERN}"),
    ):
        for match in chain(regex1.finditer(snakefile), regex2.finditer(snakefile)):
            line = get_line(match, snakefile)
            yield Lint(
                title=f"Path composition with '+' in line {line}",
                body="This becomes quickly unreadable. Usually, it is better to endure some "
                "redundancy against having a more readable workflow. Hence, just repeat common "
                'prefixes. If path composition is unavoidable, use pathlib or (python >= 3.6) string formatting with f"...". ',
            )

    def lint_envvars(
        self,
        snakefile,
        regex=re.compile(r"os.environ\[(?P<quote>['\"])(?P<name>.+)?(?P=quote)\]"),
    ):
        for match in regex.finditer(snakefile):
            line = get_line(match, snakefile)
            name = match.group("name")
            if name not in self.workflow.envvars:
                yield Lint(
                    title="Environment variable {} used but not asserted with envvars directive in line {}.".format(
                        name, line
                    ),
                    body="Asserting existence of environment variables with the envvars directive ensures proper error "
                    "messages if the user fails to invoke a workflow with all required environment variables defined. "
                    "Further, it allows snakemake to pass them on in case of distributed execution.",
                    links=[links.envvars],
                )

    def lint_singularity(self, snakefile, regex=re.compile("singularity:")):
        for match in regex.finditer(snakefile):
            line = get_line(match, snakefile)
            yield Lint(
                title="Deprecated singularity directive used for container definition in line {}.".format(
                    line
                ),
                body="Use the container directive instead (it is agnostic of the underlying container runtime).",
                links=[links.containers],
            )

    def lint_tab_usage(self, snakefile, regex=re.compile(r"^ *\t")):
        for match in regex.finditer(snakefile):
            line = get_line(match, snakefile)
            yield Lint(
                title=f"Tab usage in line {line}.",
                body="Both Python and Snakemake can get confused when mixing tabs and spaces for indentation. "
                "It is recommended to only use spaces for indentation.",
            )


def get_line(match, snakefile):
    return snakefile[: match.start()].count("\n") + 1



================================================
FILE: src/snakemake/remote/__init__.py
================================================
# remote providers have been replaced by storage plugins, see
# https://github.com/snakemake/snakemake-interface-storage-plugins


class RemoteProviderBase:
    def __init__(self, *args, **kwargs):
        # TODO add URL to plugin documentation/catalog
        raise NotImplementedError(
            "Remote providers have been replaced by Snakemake storage plugins. Please "
            "use the corresponding storage plugin instead (snakemake-storage-plugin-*)."
        )



================================================
FILE: src/snakemake/remote/AzBlob.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/dropbox.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/EGA.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/FTP.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/gfal.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/gridftp.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/GS.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/HTTP.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/iRODS.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/NCBI.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/S3.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/S3Mocked.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/SFTP.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/webdav.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/XRootD.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/remote/zenodo.py
================================================
from snakemake.remote import RemoteProviderBase


class RemoteProvider(RemoteProviderBase):
    pass



================================================
FILE: src/snakemake/report/__init__.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

from asyncio import TaskGroup
from dataclasses import InitVar, dataclass, field
import os
import sys
import mimetypes
import base64
import textwrap
import datetime
import io
from typing import Any, Dict, List, Optional, Type, Union
import uuid
import itertools
from collections import defaultdict
import hashlib
from pathlib import Path
import numbers
from yte import process_yaml


from docutils.parsers.rst.directives.images import Image, Figure
from docutils.parsers.rst import directives
from docutils.core import publish_file, publish_parts
from humanfriendly import format_size

import snakemake
from snakemake import script, wrapper, notebook
from snakemake.io.fmt import fmt_iofile
from snakemake.jobs import Job
from snakemake.report.common import data_uri_from_file, mime_from_file
from snakemake.rules import Rule
from snakemake.utils import format
from snakemake.logging import logger
from snakemake.io import (
    is_callable,
    is_flagged,
    get_flag_value,
    glob_wildcards,
    Wildcards,
    apply_wildcards,
    contains_wildcard,
)
from snakemake.exceptions import InputFunctionException, WorkflowError
from snakemake.script import Snakemake
from snakemake.common import (
    get_input_function_aux_params,
)
from snakemake import logging
from snakemake_interface_report_plugins.registry.plugin import Plugin as ReportPlugin
from snakemake_interface_report_plugins.settings import ReportSettingsBase
from snakemake.settings.types import GlobalReportSettings
from snakemake_interface_report_plugins.interfaces import (
    CategoryInterface,
    RuleRecordInterface,
    ConfigFileRecordInterface,
    JobRecordInterface,
    FileRecordInterface,
)
from snakemake.common import get_report_id
from snakemake.exceptions import WorkflowError


class EmbeddedMixin(object):
    """
    Replaces the URI of a directive with a base64-encoded version.

    Useful for embedding images/figures in reports.
    """

    def run(self):
        """
        Image.run() handles most of the
        """
        result = Image.run(self)
        reference = directives.uri(self.arguments[0])
        self.options["uri"] = data_uri_from_file(reference)
        return result


# Create (and register) new image:: and figure:: directives that use a base64
# data URI instead of pointing to a filename.


class EmbeddedImage(Image, EmbeddedMixin):
    pass


directives.register_directive("embeddedimage", EmbeddedImage)


class EmbeddedFigure(Figure, EmbeddedMixin):
    pass


directives.register_directive("embeddedfigure", EmbeddedFigure)


# legacy report code
def report(
    text,
    path,
    stylesheet=None,
    defaultenc="utf8",
    template=None,
    metadata=None,
    **files,
):
    if stylesheet is None:
        os.path.join(os.path.dirname(__file__), "report.css")
    outmime, _ = mimetypes.guess_type(path)
    if outmime != "text/html":
        raise ValueError("Path to report output has to be an HTML file.")
    definitions = textwrap.dedent(
        """[]
    .. role:: raw-html(raw)
       :format: html

    """
    )

    metadata = textwrap.dedent(
        """

    .. container::
       :name: metadata

       {metadata}{date}

    """
    ).format(
        metadata=metadata + " | " if metadata else "",
        date=datetime.date.today().isoformat(),
    )

    text = format(textwrap.dedent(text), stepout=3)

    attachments = []
    if files:
        attachments = [
            textwrap.dedent(
                """
            .. container::
               :name: attachments

            """
            )
        ]
        for name, _files in sorted(files.items()):
            if not isinstance(_files, list):
                _files = [_files]
            links = []
            for file in sorted(_files):
                data = data_uri_from_file(file)
                links.append(
                    ':raw-html:`<a href="{data}" download="{filename}" draggable="true">{filename}</a>`'.format(
                        data=data, filename=os.path.basename(file)
                    )
                )
            links = "\n\n              ".join(links)
            attachments.append(
                """
       .. container::
          :name: {name}

          {name}:
              {links}
                """.format(
                    name=name, links=links
                )
            )

    text = definitions + text + "\n\n" + "\n\n".join(attachments) + metadata

    overrides = dict()
    if template is not None:
        overrides["template"] = template
    if stylesheet is not None:
        overrides["stylesheet_path"] = stylesheet
    html = open(path, "w")
    publish_file(
        source=io.StringIO(text),
        destination=html,
        writer_name="html",
        settings_overrides=overrides,
    )


async def expand_report_argument(item, wildcards, job):
    if is_callable(item):
        aux_params = get_input_function_aux_params(
            item, {"params": job.params, "input": job.input, "output": job.output}
        )
        io_items = ["input", "output"]
        if any(io_item in aux_params for io_item in io_items):
            # retrieve all input or output files from storage before evaluating function
            async with TaskGroup() as tg:
                for io_item in io_items:
                    if io_item in aux_params:
                        for f in aux_params[io_item]:
                            if f.is_storage:
                                tg.create_task(f.retrieve_from_storage())

        try:
            item = item(wildcards, **aux_params)
        except Exception as e:
            raise InputFunctionException(e, rule=job.rule, wildcards=wildcards)
    if isinstance(item, str):
        try:
            return apply_wildcards(item, wildcards)
        except AttributeError as e:
            raise WorkflowError("Failed to resolve wildcards.", e, rule=job.rule)
    elif isinstance(item, numbers.Number) and not isinstance(item, (int, float)):
        return str(item)
    else:
        return item


@dataclass(slots=True)
class Category(CategoryInterface):
    name: Optional[str]
    is_other: bool = field(init=False)
    id: str = field(init=False)

    def __post_init__(self):
        if self.name is None:
            self.name = "Other"

        self.is_other = self.name == "Other"
        h = hashlib.sha256()
        h.update(self.name.encode())
        self.id = h.hexdigest()

    def __eq__(self, other):
        return self.name.__eq__(other.name)

    def __hash__(self):
        return self.name.__hash__()

    def __lt__(self, other):
        if self.name == "other":
            return False
        return self.name.__lt__(other.name)


def render_iofile(iofile):
    if is_callable(iofile):
        return "<function>"
    else:
        return str(iofile)


@dataclass(slots=True)
class RuleRecord(RuleRecordInterface):
    job: InitVar
    job_rec: InitVar
    name: str = field(init=False)
    container_img_url: Optional[str] = field(init=False)
    conda_env: Optional[str] = field(init=False)
    n_jobs: int = field(init=False)
    id: str = field(init=False)
    language: str = field(init=False)
    source: str = field(init=False)

    def __post_init__(self, job, job_rec):
        import yaml

        self.name = job_rec.rule
        self._rule = job.rule
        self.container_img_url = job_rec.container_img_url
        self.conda_env = None
        self._conda_env_raw = None
        if job_rec.conda_env:
            self._conda_env_raw = base64.b64decode(job_rec.conda_env).decode()
            self.conda_env = yaml.load(self._conda_env_raw, Loader=yaml.Loader)
        self.n_jobs = 1
        self.id = uuid.uuid4()

    def init_source(self):
        sources, language = None, None
        if self._rule.shellcmd is not None:
            sources = [self._rule.shellcmd]
            language = "bash"
        elif self._rule.script is not None and not contains_wildcard(self._rule.script):
            logger.info(f"Loading script code for rule {self.name}")
            _, source, language, _, _ = script.get_source(
                self._rule.script, self._rule.workflow.sourcecache, self._rule.basedir
            )
            sources = [source]
        elif self._rule.wrapper is not None and not contains_wildcard(
            self._rule.wrapper
        ):
            logger.info(f"Loading wrapper code for rule {self.name}")
            _, source, language, _, _ = script.get_source(
                wrapper.get_script(
                    self._rule.wrapper,
                    self._rule.workflow.sourcecache,
                    prefix=self._rule.workflow.workflow_settings.wrapper_prefix,
                ),
                self._rule.workflow.sourcecache,
            )
            sources = [source]
        elif self._rule.notebook is not None and not contains_wildcard(
            self._rule.notebook
        ):
            _, source, language, _, _ = script.get_source(
                self._rule.notebook, self._rule.workflow.sourcecache, self._rule.basedir
            )
            language = language.split("_")[1]
            sources = notebook.get_cell_sources(source)
        else:
            # A run directive. There is no easy way yet to obtain
            # the actual uncompiled source code.
            sources = []
            language = "python"

        self.source = "\n\n".join(sources)
        self.language = language

    def add(self, job_rec):
        self.n_jobs += 1

    @property
    def output(self):
        return [render_iofile(f) for f in self._rule.output]

    @property
    def input(self):
        return [render_iofile(f) for f in self._rule.input]

    def __eq__(self, other):
        return (
            self.name == other.name
            and self.conda_env == other.conda_env
            and self.container_img_url == other.container_img_url
        )


@dataclass(slots=True)
class ConfigfileRecord(ConfigFileRecordInterface):
    configfile: InitVar
    path: Path = field(init=False)
    source: str = field(init=False)

    def __post_init__(self, configfile):
        self.path = Path(configfile)
        self.source = self.path.read_text()


@dataclass(slots=True)
class JobRecord(JobRecordInterface):
    job: Job = field(init=False)
    rule: Rule = field(init=False)
    starttime: int = sys.maxsize
    endtime: int = 0
    output: list = field(default_factory=list)
    conda_env_file: Optional[Path] = field(init=False)
    container_img_url: Optional[Path] = field(init=False)


@dataclass(slots=True)
class FileRecord(FileRecordInterface):
    path: Path
    job: Job
    parent_path: Optional[Path] = None
    category: Optional[str] = None
    wildcards_overwrite: Optional[Wildcards] = None
    labels: Optional[dict] = None
    raw_caption: Optional[Path] = None
    aux_files: List[Path] = field(default_factory=list)
    name_overwrite: Optional[str] = None
    size: int = field(init=False)
    params: str = field(init=False)
    wildcards: str = field(init=False)
    mime: str = field(init=False)
    caption: str = field(init=False)
    id: str = field(init=False)
    target: str = field(init=False)

    def __post_init__(self):
        self.target = str(self.path.name)
        self.size = os.path.getsize(self.path)
        logger.info(f"Adding {self.name} ({format_size(self.size)}).")
        self.mime, _ = mime_from_file(self.path)

        self.id = get_report_id(self.parent_path or self.path)
        self.wildcards = logging.format_wildcards(self.raw_wildcards)
        self.params = (
            logging.format_dict(self.job.params)
            .replace("\n", r"\n")
            .replace('"', r"\"")
        )
        self.aux_files = self.aux_files or []

    @property
    def raw_wildcards(self):
        return (
            self.job.wildcards
            if self.wildcards_overwrite is None
            else self.wildcards_overwrite
        )

    def render(self, env, rst_links, categories, files):
        if self.raw_caption is not None:
            job = self.job
            snakemake = Snakemake(
                job.input,
                job.output,
                job.params,
                self.raw_wildcards,
                job.threads,
                job.resources,
                job.log,
                job.rule.module_globals["config"],
                job.rule.name,
                None,
            )

            try:
                caption = (
                    self.workflow.sourcecache.open(self.raw_caption).read() + rst_links
                )
                caption = env.from_string(caption).render(
                    snakemake=snakemake, categories=categories, files=files
                )
                self.caption = publish_parts(caption, writer_name="html")["body"]
            except Exception as e:
                raise WorkflowError(
                    "Error loading caption file {} of output marked for report.".format(
                        self.raw_caption.get_path_or_uri()
                    ),
                    e,
                )
        else:
            self.caption = ""

    @property
    def name(self):
        if self.name_overwrite:
            return self.name_overwrite
        return os.path.basename(self.path)

    @property
    def filename(self):
        if self.parent_path is None:
            return os.path.basename(self.path)
        else:
            return str(self.path.relative_to(self.parent_path))

    @property
    def workflow(self):
        return self.job.rule.workflow


async def expand_labels(labels, wildcards, job):
    if labels is None:
        return None
    labels = await expand_report_argument(labels, wildcards, job)

    if labels is None:
        return None

    if not isinstance(labels, dict) or not all(
        isinstance(col, str) or isinstance(col, numbers.Number)
        for col in labels.values()
    ):
        if isinstance(labels, dict):
            label_types = {name: type(value) for name, value in labels.items()}
        else:
            label_types = type(labels)
        raise WorkflowError(
            "Expected dict of strings or numbers as labels argument given to report flag. "
            f"Received: {label_types}",
            rule=job.rule,
        )
    return {
        name: await expand_report_argument(col, wildcards, job)
        for name, col in labels.items()
    }


async def auto_report(
    dag,
    report_plugin: ReportPlugin,
    report_settings: ReportSettingsBase,
    global_report_settings: GlobalReportSettings,
):
    try:
        from jinja2 import Environment, PackageLoader, UndefinedError
    except ImportError as e:
        raise WorkflowError(
            "Python package jinja2 must be installed to create reports."
        )

    logger.info("Creating report...")

    persistence = dag.workflow.persistence
    results = defaultdict(lambda: defaultdict(list))
    records = defaultdict(JobRecord)
    recorded_files = set()

    env = Environment(
        trim_blocks=True,
        lstrip_blocks=True,
    )

    for job in dag.jobs:
        for f in job.output:
            meta = persistence.metadata(f)
            if not meta:
                logger.warning(
                    "Missing metadata for file {}. Maybe metadata "
                    "was deleted or it was created using an older "
                    "version of Snakemake. This is a non critical "
                    "warning.".format(f)
                )
                continue

            def get_time(rectime, metatime, sel_func):
                if metatime is None:
                    return rectime
                return sel_func(metatime, rectime)

            try:
                job_hash = meta["job_hash"]
                rule = meta["rule"]
                job_rec = records[(job_hash, rule)]
                job_rec.rule = rule
                job_rec.job = job
                job_rec.starttime = get_time(job_rec.starttime, meta["starttime"], min)
                job_rec.endtime = get_time(job_rec.endtime, meta["endtime"], max)
                job_rec.conda_env_file = None
                job_rec.conda_env = meta["conda_env"]
                job_rec.container_img_url = meta["container_img_url"]
                job_rec.output.append(f)
            except KeyError as e:
                logger.warning(
                    "Metadata for file {} was created with a too "
                    "old Snakemake version.".format(f)
                )

        for f in itertools.chain(job.output, job.input):
            if is_flagged(f, "report") and f not in recorded_files:
                if not await f.exists():
                    raise WorkflowError(
                        "File {} marked for report but does not exist.".format(f)
                    )
                report_obj = get_flag_value(f, "report")

                async def register_file(
                    f,
                    parent_path=None,
                    wildcards_overwrite=None,
                    aux_files=None,
                    name_overwrite=None,
                ):
                    wildcards = wildcards_overwrite or job.wildcards

                    async def expand_cat_name(cat_name, wildcards, job):
                        if cat_name is not None:
                            return await expand_report_argument(
                                cat_name, wildcards, job
                            )
                        else:
                            return cat_name

                    category = Category(
                        name=await expand_cat_name(report_obj.category, wildcards, job),
                    )
                    subcategory = Category(
                        name=await expand_cat_name(
                            report_obj.subcategory, wildcards, job
                        ),
                    )
                    labels = await expand_labels(report_obj.labels, wildcards, job)

                    results[category][subcategory].append(
                        FileRecord(
                            path=Path(f),
                            parent_path=(
                                Path(parent_path) if parent_path is not None else None
                            ),
                            job=job,
                            category=category,
                            raw_caption=report_obj.caption,
                            wildcards_overwrite=wildcards_overwrite,
                            aux_files=aux_files,
                            name_overwrite=name_overwrite,
                            labels=labels,
                        )
                    )
                    recorded_files.add(f)

                if f.is_storage:
                    await f.retrieve_from_storage()
                if os.path.isfile(f):
                    await register_file(f)
                elif os.path.isdir(f):
                    if report_obj.htmlindex:
                        aux_files = []
                        index_found = False
                        for root, dirs, files in os.walk(f):
                            for name in files:
                                if name != ".snakemake_timestamp":
                                    filepath = os.path.join(root, name)
                                    if (
                                        os.path.relpath(filepath, f)
                                        != report_obj.htmlindex
                                    ):
                                        aux_files.append(filepath)
                                    else:
                                        index_found = True
                        if not index_found:
                            raise WorkflowError(
                                "Given htmlindex {} not found in directory "
                                "marked for report".format(report_obj.htmlindex)
                            )
                        await register_file(
                            os.path.join(f, report_obj.htmlindex),
                            aux_files=aux_files,
                            name_overwrite=f"{os.path.basename(f)}.html",
                        )
                    elif report_obj.patterns:
                        if not isinstance(report_obj.patterns, list):
                            raise WorkflowError(
                                "Invalid patterns given for report. Must be list.",
                                rule=job.rule,
                            )

                        found_something = False
                        for pattern in report_obj.patterns:
                            pattern = os.path.join(f, pattern)
                            wildcards = glob_wildcards(pattern)._asdict()
                            found_something |= len(wildcards) > 0
                            names = wildcards.keys()
                            for w in zip(*wildcards.values()):
                                w = dict(zip(names, w))
                                w.update(job.wildcards_dict)
                                w = Wildcards(fromdict=w)
                                subfile = apply_wildcards(pattern, w)
                                await register_file(
                                    subfile, parent_path=f, wildcards_overwrite=w
                                )
                        if not found_something:
                            logger.warning(
                                "No files found for patterns given to report marker "
                                f"in rule {job.rule.name} for output {fmt_iofile(f)}. Make sure "
                                "that the patterns are correctly specified."
                            )
                    else:
                        raise WorkflowError(
                            "Directory marked for report but neither file patterns "
                            "given via patterns=[...], nor htmlindex given. "
                            "See report documentation.",
                            rule=job.rule,
                        )

    for subcats in results.values():
        for catresults in subcats.values():
            catresults.sort(key=lambda res: res.name)

    # prepare per-rule information
    rules = defaultdict(list)
    for job_rec in records.values():
        rule = RuleRecord(job_rec.job, job_rec)
        if job_rec.rule not in rules:
            rules[job_rec.rule].append(rule)
            rule.init_source()
        else:
            merged = False
            for other in rules[job_rec.rule]:
                if rule == other:
                    other.add(job_rec)
                    merged = True
                    break
            if not merged:
                rules[job_rec.rule].append(rule)
                rule.init_source()
    # In theory there could be more than one rule with the same name kept from above.
    # For now, we just keep the first.
    rules = {rulename: items[0] for rulename, items in rules.items()}

    # configfiles
    configfiles = [ConfigfileRecord(f) for f in dag.workflow.configfiles]

    seen = set()
    files = [
        seen.add(res.target) or res
        for cat in results.values()
        for subcat in cat.values()
        for res in subcat
        if res.target not in seen
    ]

    rst_links = textwrap.dedent(
        """

    .. _Workflow: javascript:show_panel('workflow')
    .. _Statistics: javascript:show_panel('statistics')
    {% for cat, catresults in categories|dictsort %}
    .. _{{ cat.name }}: javascript:app.showCategory('{{ cat.name|urlencode }}')
    {% endfor %}
    {% for res in files %}
    .. _{{ res.target }}: javascript:app.showResultInfo('{{ res.path|urlencode }}')
    {% endfor %}
    """
    )
    for cat, subcats in results.items():
        for subcat, catresults in subcats.items():
            for res in catresults:
                res.render(env, rst_links, results, files)

    # global description
    workflow_description = ""
    if dag.workflow.report_text:
        with dag.workflow.sourcecache.open(dag.workflow.report_text) as f:

            class Snakemake:
                config = dag.workflow.config

            workflow_description = f.read() + rst_links

            try:
                workflow_description = publish_parts(
                    env.from_string(workflow_description).render(
                        snakemake=Snakemake, categories=results, files=files
                    ),
                    writer_name="html",
                )[
                    "html_body"
                ]  # html_body is required to extract also the title, if given
            except UndefinedError as e:
                raise WorkflowError(
                    "Error rendering global report caption {}:".format(
                        dag.workflow.report_text.get_path_or_uri()
                    ),
                    e,
                )

    metadata = {}
    if global_report_settings.metadata_template:
        # parse metadata from yte template
        with open(global_report_settings.metadata_template, "r") as template:
            metadata = process_yaml(template)

            # ensure that metadata is a key value dictionary
            # allowed values: str, int, float, list[str|int|float]
            if not _validate_flat_dict(metadata):
                raise WorkflowError(
                    (
                        "Metadata must be single level "
                        "dict[str, str | int | float | "
                        "list[str] | list[int] | list[float]]]"
                    )
                )
            render_metadata(metadata)

    reporter = report_plugin.reporter(
        rules,
        results,
        configfiles,
        sorted(records.values(), key=lambda rec: rec.rule),  # this contains the jobs
        report_settings,
        workflow_description,
        dag=dag,
        metadata=metadata,
    )

    reporter.render()
    logger.info("Report created.")


def _is_valid_flat_value(value) -> bool:
    if isinstance(value, (str, int, float)):
        return True
    elif isinstance(value, list):
        return all(isinstance(item, (str, int, float)) for item in value)
    else:
        return False


def _validate_flat_dict(metadata: dict) -> bool:
    if not isinstance(metadata, dict):
        return False
    for k, v in metadata.items():
        if not isinstance(k, str):
            return False
        if not _is_valid_flat_value(v):
            return False
    return True


def render_metadata(
    metadata: Dict[str, Union[str, int, float, List[str], List[int], List[float]]],
) -> None:
    """Render string values in metadata with restructured text"""

    # we modify the dict while iterating over it, so we need to copy the keys
    for key in list(metadata):
        value = metadata[key]
        if isinstance(value, str):
            metadata[key] = publish_parts(value)
        elif isinstance(value, list):
            for i in range(len(value)):
                if isinstance(value[i], str):
                    value[i] = publish_parts(value[i])



================================================
FILE: src/snakemake/report/common.py
================================================
import base64
import mimetypes
import os
from pathlib import Path
from snakemake.common import is_local_file
from snakemake.logging import logger

from snakemake_interface_common.exceptions import WorkflowError


def data_uri(data, filename, encoding="utf8", mime="text/plain"):
    """Craft a base64 data URI from file with proper encoding and mimetype."""
    data = base64.b64encode(data)
    return f'data:{mime};charset={encoding};filename={filename};base64,{data.decode("utf-8")}'


def mime_from_file(file):
    mime, encoding = mimetypes.guess_type(file)
    if mime is None:
        mime = "text/plain"
        logger.info(
            "Could not detect mimetype for {}, assuming text/plain.".format(file)
        )
    return mime, encoding


def data_uri_from_file(file, defaultenc="utf8"):
    """Craft a base64 data URI from file with proper encoding and mimetype."""
    if isinstance(file, Path):
        file = str(file)
    mime, encoding = mime_from_file(file)
    if encoding is None:
        encoding = defaultenc
    with open(file, "rb") as f:
        return data_uri(f.read(), os.path.basename(file), encoding, mime)



================================================
FILE: src/snakemake/report/rulegraph_spec.py
================================================
from functools import partial


def rulegraph_spec(dag):
    # get toposorting, and keep only one job of each rule per level
    representatives = dict()

    toposorted = [
        get_representatives(level, representatives) for level in dag.toposorted()
    ]

    jobs = [job for level in toposorted for job in level]

    nodes = [
        {"rule": job.rule.name, "fx": 10, "fy": i * 50} for i, job in enumerate(jobs)
    ]
    idx = {job: i for i, job in enumerate(jobs)}

    xmax = 100
    ymax = max(node["fy"] for node in nodes)

    _get_links = partial(get_links, jobs, dag, idx, nodes, representatives)

    return (
        {
            "nodes": nodes,
            "links": list(_get_links(direct=False)),
            "links_direct": list(_get_links(direct=True)),
        },
        xmax,
        ymax,
    )


def get_representatives(level: list, representatives: dict):
    unique = dict()
    for job in level:
        if job.rule.name in unique:
            representatives[job] = unique[job.rule.name]
        else:
            representatives[job] = job
            unique[job.rule.name] = job
    return sorted(unique.values(), key=lambda job: job.rule.name)


def get_links(jobs, dag, idx, nodes, representatives, direct: bool):
    for u in jobs:
        for v in dag.dependencies[u]:
            target = idx[u]
            source = idx[representatives[v]]
            if target - source == 1:
                if not direct:
                    continue
            else:
                if direct:
                    continue

            yield {
                "target": target,
                "source": source,
                "sourcerule": nodes[source]["rule"],
                "targetrule": nodes[target]["rule"],
                "value": 1,
            }



================================================
FILE: src/snakemake/report/html_reporter/__init__.py
================================================
from dataclasses import dataclass, field
import datetime
import json
import os
from pathlib import Path
import textwrap
import time
from typing import Optional
from zipfile import ZIP_DEFLATED, ZipFile

from jinja2 import Environment, PackageLoader
from snakemake.report.common import data_uri_from_file
from snakemake.report.html_reporter import data

from snakemake.report.rulegraph_spec import rulegraph_spec
from snakemake.report.html_reporter.common import get_resource_as_string, get_result_uri

from snakemake_interface_common.exceptions import WorkflowError
from snakemake_interface_report_plugins.reporter import ReporterBase
from snakemake_interface_report_plugins.settings import ReportSettingsBase


@dataclass
class ReportSettings(ReportSettingsBase):
    path: Optional[Path] = field(
        default=None,
        metadata={
            "help": "Path to the report file (either .html or .zip). Use zip if your "
            "report contains large results or directories with htmlindex as results.",
            "env_var": False,
            "required": True,
        },
    )
    stylesheet_path: Optional[Path] = field(
        default=None,
        metadata={
            "help": "Path to a custom stylesheet for the report.",
            "env_var": False,
            "required": False,
        },
    )


class Reporter(ReporterBase):
    def __post_init__(self):
        self.mode_embedded = True
        if self.settings.path.suffix == ".zip":
            self.mode_embedded = False
        elif self.settings.path.suffix != ".html":
            raise WorkflowError("Report file does not end with .html or .zip")

        self.custom_stylesheet = None
        if self.settings.stylesheet_path is not None:
            try:
                with open(self.settings.stylesheet_path) as s:
                    self.custom_stylesheet = s.read()
            except Exception as e:
                raise WorkflowError("Unable to read custom report stylesheet.", e)

        self.env = Environment(
            loader=PackageLoader("snakemake", "report/html_reporter/template"),
            trim_blocks=True,
            lstrip_blocks=True,
        )
        self.env.filters["get_resource_as_string"] = get_resource_as_string

    def render(self):
        # prepare runtimes
        runtimes = [
            {"rule": rec.rule, "runtime": rec.endtime - rec.starttime}
            for rec in self.jobs
        ]

        def get_datetime(rectime):
            try:
                return datetime.datetime.fromtimestamp(rectime).isoformat()
            except OSError:
                return None

        # prepare end times
        timeline = [
            {
                "rule": rec.rule,
                "starttime": get_datetime(rec.starttime),
                "endtime": get_datetime(rec.endtime),
            }
            for rec in self.jobs
        ]

        # rulegraph
        rulegraph, _, _ = rulegraph_spec(self.dag)

        # record time
        now = f"{datetime.datetime.now().ctime()} {time.tzname[0]}"

        try:
            from pygments.formatters import HtmlFormatter
        except ImportError:
            raise WorkflowError(
                "Python package pygments must be installed to create reports."
            )

        categories = data.render_categories(self.results)
        rendered_results = data.render_results(self.results, self.mode_embedded)
        rulegraph = data.render_rulegraph(
            rulegraph["nodes"], rulegraph["links"], rulegraph["links_direct"]
        )
        rules = data.render_rules(self.rules)
        runtimes = data.render_runtimes(runtimes)
        timeline = data.render_timeline(timeline)
        packages = data.get_packages()
        metadata = data.render_metadata(self.metadata)

        template = self.env.get_template("index.html.jinja2")

        rendered = template.render(
            results=rendered_results,
            categories=categories,
            rulegraph=rulegraph,
            rules=rules,
            workflow_desc=json.dumps(self.workflow_description),
            runtimes=runtimes,
            timeline=timeline,
            packages=packages,
            metadata=metadata,
            pygments_css=HtmlFormatter(style="stata-dark").get_style_defs(".source"),
            custom_stylesheet=self.custom_stylesheet,
            logo=data_uri_from_file(Path(__file__).parent / "template" / "logo.svg"),
            now=now,
        )

        # TODO look into supporting .WARC format, also see (https://webrecorder.io)

        if not self.mode_embedded:
            with ZipFile(
                self.settings.path, compression=ZIP_DEFLATED, mode="w"
            ) as zipout:
                folder = Path(Path(self.settings.path).stem)
                # store results in data folder
                for subcats in self.results.values():
                    for catresults in subcats.values():
                        for result in catresults:
                            if self.mode_embedded and result.aux_files:
                                raise WorkflowError(
                                    "Directory marked for inclusion in report. "
                                    "This is unsupported when requesting a pure HTML report. "
                                    "Please use store as zip instead (--report report.zip)."
                                )
                            # write raw data
                            zipout.write(
                                result.path,
                                str(
                                    folder.joinpath(
                                        get_result_uri(result, self.mode_embedded)
                                    )
                                ),
                            )
                            # write aux files
                            parent = folder.joinpath(
                                get_result_uri(result, self.mode_embedded)
                            ).parent
                            for aux_path in result.aux_files:
                                zipout.write(
                                    aux_path,
                                    str(
                                        parent.joinpath(
                                            os.path.relpath(
                                                aux_path, os.path.dirname(result.path)
                                            )
                                        )
                                    ),
                                )

                # write report html
                zipout.writestr(str(folder.joinpath("report.html")), rendered)
        else:
            with open(self.settings.path, "w", encoding="utf-8") as htmlout:
                htmlout.write(rendered)



================================================
FILE: src/snakemake/report/html_reporter/common.py
================================================
import os
from pathlib import Path
from snakemake.common import is_local_file
from snakemake.report.common import data_uri_from_file

from snakemake_interface_common.exceptions import WorkflowError


def get_resource_as_string(path_or_uri):
    import requests

    if is_local_file(path_or_uri):
        return open(Path(__file__).parent / "template" / path_or_uri).read()
    else:
        r = requests.get(path_or_uri)
        if r.status_code == requests.codes.ok:
            return r.text
        raise WorkflowError(
            "Failed to download resource needed for report: {}".format(path_or_uri)
        )


def get_result_uri(result, mode_embedded):
    if mode_embedded:
        return data_uri_from_file(result.path)
    else:
        return os.path.join("data/raw", result.id, result.filename)



================================================
FILE: src/snakemake/report/html_reporter/data/__init__.py
================================================
from .results import render_results
from .categories import render_categories
from .rulegraph import render_rulegraph
from .rules import render_rules
from .runtimes import render_runtimes
from .timeline import render_timeline
from .packages import get_packages
from .metadata import render_metadata



================================================
FILE: src/snakemake/report/html_reporter/data/categories.py
================================================
import json


def render_categories(results):
    return json.dumps(
        {
            cat.name: {
                subcat.name: [str(res.path) for res in catresults]
                for subcat, catresults in subcats.items()
                if catresults
            }
            for cat, subcats in results.items()
            if subcats and sum(len(catresults) for catresults in subcats.values())
        }
    )



================================================
FILE: src/snakemake/report/html_reporter/data/common.py
================================================




================================================
FILE: src/snakemake/report/html_reporter/data/configfiles.py
================================================
import json
from snakemake_interface_common.exceptions import WorkflowError


def render_configfiles(configfiles):
    return json.dumps(
        [
            {
                "path": configfile.path,
                "code": render_code(configfile),
            }
            for configfile in configfiles.items()
        ]
    )


def render_code(configfile):
    try:
        from pygments.lexers import get_lexer_by_name
        from pygments.formatters import HtmlFormatter
        from pygments import highlight
    except ImportError:
        raise WorkflowError(
            "Python package pygments must be installed to create reports."
        )

    file_ext = configfile.path.suffix
    if file_ext in (".yml", ".yaml"):
        language = "yaml"
    elif file_ext == ".json":
        language = "json"
    else:
        raise ValueError(
            "Config file extension {} is not supported - must be YAML or JSON".format(
                file_ext
            )
        )

    source = configfile.source
    lexer = get_lexer_by_name(language)
    return highlight(
        source,
        lexer,
        HtmlFormatter(linenos=True, cssclass="source", wrapcode=True),
    )



================================================
FILE: src/snakemake/report/html_reporter/data/metadata.py
================================================
import json


def render_metadata(metadata):
    """Render the metadata to be displayed on the report landing page.

    Arguments
    ---------
    metadata: dict -- Metadata dictionary containing (user specified) metadata
    """
    return json.dumps(metadata)



================================================
FILE: src/snakemake/report/html_reporter/data/packages.py
================================================
import json
import snakemake

from snakemake.assets import AssetDownloadError, Assets
from snakemake_interface_common.exceptions import WorkflowError


def get_packages():
    try:
        import pygments
    except ImportError:
        raise WorkflowError(
            "Python package pygments must be installed to create reports."
        )

    # packages declared here must be downloaded to the prefix share/snakemake/assets
    # via setuptools_download in setup.cfg
    return Packages(
        {
            "snakemake": Package(
                version=snakemake.__version__.split("+")[0],
                license_path="snakemake/LICENSE.md",
            ),
            "pygments": Package(
                version=pygments.__version__,
                license_path="pygments/LICENSE",
            ),
            "tailwindcss": Package(
                license_path="tailwindcss/LICENSE",
                source_path="tailwindcss/tailwind.css",
            ),
            "react": Package(
                license_path="react/LICENSE",
                main="react/react.production.min.js",
                dom="react/react-dom.production.min.js",
            ),
            "vega": Package(
                source_path="vega/vega.js",
                license_path="vega/LICENSE",
            ),
            # Begin dependencies for vega, included in vega/vega.js
            "d3-array": Package(
                license_path="d3-array/LICENSE",
            ),
            "d3-format": Package(
                license_path="d3-format/LICENSE",
            ),
            "d3-time-format": Package(
                license_path="d3-time-format/LICENSE",
            ),
            "d3-time": Package(
                license_path="d3-time/LICENSE",
            ),
            "d3-interpolate": Package(
                license_path="d3-interpolate/LICENSE",
            ),
            "d3-color": Package(
                license_path="d3-color/LICENSE",
            ),
            "d3-scale": Package(
                license_path="d3-scale/LICENSE",
            ),
            "@types-estree": Package(
                license_path="@types-estree/LICENSE",
            ),
            "d3-force": Package(
                license_path="d3-force/LICENSE",
            ),
            "d3-dispatch": Package(
                license_path="d3-dispatch/LICENSE",
            ),
            "d3-quadtree": Package(
                license_path="d3-quadtree/LICENSE",
            ),
            "d3-timer": Package(
                license_path="d3-timer/LICENSE",
            ),
            "d3-geo": Package(
                license_path="d3-geo/LICENSE",
            ),
            "d3-hierarchy": Package(
                license_path="d3-hierarchy/LICENSE",
            ),
            "d3-dsv": Package(
                license_path="d3-dsv/LICENSE",
            ),
            "topojson-client": Package(
                license_path="topojson-client/LICENSE",
            ),
            "d3-geo-projection": Package(
                license_path="d3-geo-projection/LICENSE",
            ),
            "d3-path": Package(
                license_path="d3-path/LICENSE",
            ),
            "d3-shape": Package(
                license_path="d3-shape/LICENSE",
            ),
            "d3-delaunay": Package(
                license_path="d3-delaunay/LICENSE",
            ),
            "delaunator": Package(
                license_path="delaunator/LICENSE",
            ),
            # End dependencies for vega, included in vega/vega.js
            # Begin copied/derived/adapted code in vega, included in vega/vega.js
            "hashlru": Package(
                license_path="hashlru/LICENSE",
            ),
            "d3-regression": Package(
                license_path="d3-regression/LICENSE",
            ),
            "regression": Package(
                license_path="regression/LICENSE",
            ),
            "science": Package(
                license_path="science/LICENSE",
            ),
            "quickselect": Package(
                license_path="quickselect/LICENSE",
            ),
            "commons-math": Package(
                license_path="commons-math/LICENSE.txt",
                # This must be included with LICENSE.txt.
                notice="commons-math/NOTICE.txt",
            ),
            "esprima": Package(
                license_path="esprima/LICENSE.BSD",
            ),
            "fabric": Package(
                license_path="fabric/LICENSE",
            ),
            "d3-contour": Package(
                license_path="d3-contour/LICENSE",
            ),
            # End copied/derived/adapted code in vega, included in vega/vega.js
            "vega-lite": Package(
                source_path="vega-lite/vega-lite.js",
                license_path="vega-lite/LICENSE",
            ),
            # Begin dependencies for vega-lite, included in vega-lite/vega-lite.js
            # (excluding those shared with vega and therefore already documented)
            "@types-clone": Package(
                license_path="@types-clone/LICENSE",
            ),
            "array-flat-polyfill": Package(
                license_path="array-flat-polyfill/LICENSE",
            ),
            "clone": Package(
                license_path="clone/LICENSE",
            ),
            "fast-deep-equal": Package(
                license_path="fast-deep-equal/LICENSE",
            ),
            "fast-json-stable-stringify": Package(
                license_path="fast-json-stable-stringify/LICENSE",
            ),
            "json-stringify-pretty-compact": Package(
                license_path="json-stringify-pretty-compact/LICENSE",
            ),
            # End dependencies for vega-lite, included in vega-lite/vega-lite.js
            # Any copied/derived/adapted code in vega-lite is shared with vega.
            "vega-embed": Package(
                source_path="vega-embed/vega-embed.js",
                license_path="vega-embed/LICENSE",
            ),
            # Begin dependencies for vega-embed, included in vega-embed/vega-embed.js
            # (excluding those shared with vega/vega-lite and therefore already documented)
            "fast-json-patch": Package(
                license_path="fast-json-patch/LICENSE",
            ),
            "semver": Package(
                license_path="semver/LICENSE",
            ),
            "vega-schema-url-parser": Package(
                license_path="vega-schema-url-parser/LICENSE",
            ),
            "vega-themes": Package(
                license_path="vega-themes/LICENSE",
            ),
            "vega-tooltip": Package(
                license_path="vega-tooltip/LICENSE",
            ),
            # End dependencies for vega-embed, included in vega-embed/vega-embed.js
            # Any copied/derived/adapted code in vega-embed is shared with vega and/or vega-lite.
            "heroicons": Package(
                license_path="heroicons/LICENSE",
            ),
            "prop-types": Package(
                source_path="prop-types/prop-types.min.js",
                license_path="prop-types/LICENSE",
            ),
        }
    )


class Packages:
    def __init__(self, packages):
        self.packages = packages

    def __getitem__(self, package):
        return self.packages[package]

    def get_json(self):
        return json.dumps(
            {name: package.get_record() for name, package in self.packages.items()}
        )


class Package:
    def __init__(
        self, version=None, license_path=None, source_path=None, **source_paths
    ):
        self.version = (
            version
            or Assets.get_version(source_path)
            or Assets.get_version(license_path)
        )

        try:
            self.license = Assets.get_content(license_path)
            if source_path is not None:
                self.source = Assets.get_content(source_path)
            else:
                self.sources = {
                    name: Assets.get_content(path)
                    for name, path in source_paths.items()
                }
        except AssetDownloadError as e:
            raise WorkflowError(e)

    def get_record(self):
        return {"version": self.version, "license": self.license}



================================================
FILE: src/snakemake/report/html_reporter/data/results.py
================================================
import json
import os

from humanfriendly import format_size

from snakemake.report.html_reporter.common import get_result_uri


def render_results(results, mode_embedded):
    return json.dumps(
        {
            str(res.path): {
                "name": res.name,
                "filename": res.filename,
                "labels": res.labels,
                "size": format_size(res.size),
                "caption": res.caption,
                "mime_type": res.mime,
                "job_properties": {
                    "rule": res.job.rule.name,
                    "wildcards": res.wildcards,
                    "params": res.params,
                },
                "data_uri": get_result_uri(res, mode_embedded),
            }
            for cat, subcats in results.items()
            for subcat, catresults in subcats.items()
            for res in catresults
        }
    )



================================================
FILE: src/snakemake/report/html_reporter/data/rulegraph.py
================================================
import json


def render_rulegraph(nodes, links, links_direct):
    return json.dumps(
        {
            "$schema": "https://vega.github.io/schema/vega/v5.json",
            "padding": 0,
            "signals": [
                {"name": "cx", "update": "width / 2"},
                {"name": "cy", "update": "height / 2"},
            ],
            "data": [
                {"name": "node-data", "values": nodes},
                {"name": "link-data", "values": links},
                {"name": "link-data-direct", "values": links_direct},
            ],
            "scales": [
                {
                    "name": "color",
                    "type": "ordinal",
                    "range": {"scheme": "category20c"},
                },
                {"name": "x", "type": "linear"},
                {"name": "y", "type": "linear"},
            ],
            "marks": [
                {
                    "name": "nodes",
                    "type": "symbol",
                    "zindex": 1,
                    "from": {"data": "node-data"},
                    "encode": {
                        "enter": {
                            "fill": {"scale": "color", "field": "rule"},
                            "x": {"field": "fx", "scale": "x"},
                            "y": {"field": "fy", "scale": "y"},
                            "tooltip": {"value": "Click to show rule details."},
                        },
                        "update": {"size": {"value": 70}},
                        "hover": {"size": {"value": 140}},
                    },
                    "transform": [
                        {
                            "type": "force",
                            "iterations": 1,
                            "static": True,
                            "forces": [{"force": "link", "links": "link-data"}],
                        },
                        {
                            "type": "force",
                            "iterations": 1,
                            "static": True,
                            "forces": [{"force": "link", "links": "link-data-direct"}],
                        },
                    ],
                },
                {
                    "name": "labels",
                    "type": "text",
                    "zindex": 2,
                    "from": {"data": "node-data"},
                    "encode": {
                        "enter": {
                            "fill": {"value": "black"},
                            "fontWeight": {"value": "normal"},
                            "text": {"field": "rule"},
                            "x": {"field": "fx", "scale": "x"},
                            "y": {"field": "fy", "scale": "y"},
                            "dx": {"value": -5},
                            "dy": {"value": -5},
                            "align": {"value": "right"},
                        }
                    },
                },
                {
                    "type": "path",
                    "from": {"data": "link-data-direct"},
                    "interactive": True,
                    "encode": {
                        "update": {
                            "stroke": {"value": "#ccc"},
                            "strokeWidth": {"value": 1.0},
                        },
                        "hover": {"strokeWidth": {"value": 4.0}},
                    },
                    "transform": [
                        {
                            "type": "linkpath",
                            "shape": "diagonal",
                            "sourceX": "datum.source.x",
                            "sourceY": "datum.source.y",
                            "targetX": "datum.target.x",
                            "targetY": "datum.target.y",
                        }
                    ],
                },
                {
                    "type": "path",
                    "from": {"data": "link-data"},
                    "interactive": True,
                    "encode": {
                        "enter": {
                            "tooltip": {
                                "signal": "{\"from rule\": datum['sourcerule'], \"to rule\": datum['targetrule']}"
                            }
                        },
                        "update": {
                            "stroke": {"value": "#ccc"},
                            "strokeWidth": {"value": 1.0},
                        },
                        "hover": {"strokeWidth": {"value": 4.0}},
                    },
                    "transform": [
                        {
                            "type": "linkpath",
                            "shape": "curve",
                            "orient": "horizontal",
                            "sourceX": "datum.source.x",
                            "sourceY": "datum.source.y",
                            "targetX": "datum.target.x",
                            "targetY": "datum.target.y",
                        }
                    ],
                },
            ],
        }
    )



================================================
FILE: src/snakemake/report/html_reporter/data/rules.py
================================================
import json
from snakemake_interface_common.exceptions import WorkflowError


def render_rules(rules):
    return json.dumps(
        {
            rulename: {
                "input": rule.input,
                "output": rule.output,
                "conda_env": rule.conda_env,
                "container_img_url": rule.container_img_url,
                "code": render_code(rule),
                "n_jobs": rule.n_jobs,
            }
            for rulename, rule in rules.items()
        }
    )


def render_code(rule):
    try:
        from pygments.lexers import get_lexer_by_name
        from pygments.formatters import HtmlFormatter
        from pygments import highlight
        import pygments.util
    except ImportError:
        raise WorkflowError(
            "Python package pygments must be installed to create reports."
        )

    try:
        lexer = get_lexer_by_name(rule.language)

        highlighted = highlight(
            rule.source,
            lexer,
            HtmlFormatter(linenos=True, cssclass="source", wrapcode=True),
        )

        return highlighted
    except pygments.util.ClassNotFound:
        return [f'<pre class="source"><code>{rule.source}</code></pre>']



================================================
FILE: src/snakemake/report/html_reporter/data/runtimes.py
================================================
import json


def render_runtimes(runtimes):
    return json.dumps(
        {
            "$schema": "https://vega.github.io/schema/vega-lite/v3.json",
            "description": "Runtimes of jobs.",
            "data": {"values": runtimes},
            "mark": "point",
            "encoding": {
                "x": {
                    "field": "runtime",
                    "type": "quantitative",
                    "axis": {"title": "runtime [s]", "labelAngle": -90},
                    "scale": {"type": "log"},
                },
                "y": {"field": "rule", "type": "nominal"},
                "color": {"value": "#007bff"},
            },
        }
    )



================================================
FILE: src/snakemake/report/html_reporter/data/timeline.py
================================================
import json


def render_timeline(timeline):
    return json.dumps(
        {
            "$schema": "https://vega.github.io/schema/vega-lite/v3.json",
            "description": "Timeline of jobs.",
            "data": {"values": timeline},
            "mark": "point",
            "encoding": {
                "x": {
                    "field": "endtime",
                    "type": "temporal",
                    "timeUnit": "yearmonthdatehoursminutes",
                    "axis": {"labelAngle": -90, "title": "end date"},
                },
                "y": {"field": "rule", "type": "nominal"},
                "color": {"value": "#007bff"},
            },
        }
    )



================================================
FILE: src/snakemake/report/html_reporter/template/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/report/html_reporter/template/index.html.jinja2
================================================
<!doctype html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>Snakemake Report</title>

    <script>{{ packages["tailwindcss"].source }}</script>
    <style>{{ pygments_css|safe }}</style>
    <style>{{ "style.css"|get_resource_as_string }}</style>

    {% if custom_stylesheet is not none %}
    <style>{{ custom_stylesheet }}</style>
    {% endif %}
</head>

<body>

    <div id="loading-screen">
      <p id="loading-animation">Loading Snakemake Report...</p>
      <p id="jswarning">Please enable Javascript in your browser to see this report.</p>
    </div>
    <script>
    document.querySelector('#loading-screen #jswarning').style.display = "none";
    </script>

    <div id="app">
    </div>

    <script>{{packages["react"].sources["main"]}}</script>
    <script>{{packages["react"].sources["dom"]}}</script>
    <script>{{packages["prop-types"].source}}</script>
    <script>{{packages["vega"].source}}</script>
    <script>{{packages["vega-lite"].source}}</script>
    <script>{{packages["vega-embed"].source}}</script>

    <script id="workflow-desc">
    var workflow_desc = {{workflow_desc}};
    </script>

    <script id="results">
    var results = {{results}};
    </script>

    <script id="categories">
    var categories = {{categories}};
    </script>

    <script id="rulegraph">
    var rulegraph_spec = {{rulegraph}};
    </script>

    <script id="runtimes">
    var runtimes_spec = {{runtimes}};
    </script>

    <script id="timeline">
    var timeline_spec = {{timeline}};
    </script>

    <script id="rules">
    var rules = {{rules}};
    </script>

    <script id="packages">
    var packages = {{packages.get_json()}};
    </script>

    <script id="metadata">
    var metadata = {{metadata}};
    </script>

    <script id="logo">
    var logo_uri = "{{logo}}";
    </script>

    <script>{{"components/metadata.js"|get_resource_as_string}}</script>
    <script>{{"components/common.js"|get_resource_as_string}}</script>
    <script>{{"components/list_item.js"|get_resource_as_string}}</script>
    <script>{{"components/list_heading.js"|get_resource_as_string}}</script>
    <script>{{"components/icon.js"|get_resource_as_string}}</script>
    <script>{{"components/button.js"|get_resource_as_string}}</script>
    <script>{{"components/toggle.js"|get_resource_as_string}}</script>
    <script>{{"components/abstract_view_manager.js"|get_resource_as_string}}</script>
    <script>{{"components/result_view_button.js"|get_resource_as_string}}</script>
    <script>{{"components/abstract_menu.js"|get_resource_as_string}}</script>
    <script>{{"components/abstract_results.js"|get_resource_as_string}}</script>
    <script>{{"components/breadcrumbs.js"|get_resource_as_string}}</script>
    <script>{{"components/menu.js"|get_resource_as_string}}</script>
    <script>{{"components/report_info.js"|get_resource_as_string}}</script>
    <script>{{"components/category.js"|get_resource_as_string}}</script>
    <script>{{"components/subcategory.js"|get_resource_as_string}}</script>
    <script>{{"components/search_results.js"|get_resource_as_string}}</script>
    <script>{{"components/result_info.js"|get_resource_as_string}}</script>
    <script>{{"components/rule_graph.js"|get_resource_as_string}}</script>
    <script>{{"components/rule_info.js"|get_resource_as_string}}</script>
    <script>{{"components/stats.js"|get_resource_as_string}}</script>
    <script>{{"components/content.js"|get_resource_as_string}}</script>
    <script>{{"components/navbar.js"|get_resource_as_string}}</script>
    <script>{{"components/app.js"|get_resource_as_string}}</script>

    <script>
    document.querySelector('#loading-screen').style.display = "none";
    </script>

</body>

</html>


================================================
FILE: src/snakemake/report/html_reporter/template/style.css
================================================
.vega-embed summary {
    z-index: 49 !important;
    /* ensure that menu is always below navbar*/
}

.source {
    background-color: transparent!important;
}

#loading-screen {
    background-color: white;
    width: 100%;
    height: 100%;
    position: fixed;
    top: 0;
    left: 0;
    z-index: 2000;
    padding-top: 50vh;
}

@keyframes fadeinout {
    0% {
        opacity: 1;
    }
    50% {
        opacity: 0;
    }
    100% {
        opacity: 1;
    }
}

#loading-animation {
    animation: fadeinout 2.5s infinite;
    text-align: center;
    font-size: 200%;
}

#jswarning {
    text-align: center;
    color: red;
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/report/html_reporter/template/components/abstract_menu.js
================================================
'use strict';

class AbstractMenu extends React.Component {
    buttonProps = { className: "transition-all block hover:text-emerald-500 rounded hover:bg-slate-800 p-1 flex items-center gap-2" };
    iconProps = { className: "text-emerald-500" };

    getMenuItem(label, iconName, onClick) {
        return e(
            "li",
            { key: label },
            e(
                "a",
                { href: "#", onClick: onClick, ...this.buttonProps },
                e(Icon, { iconName: iconName, ...this.iconProps }),
                e(
                    "span",
                    {},
                    label
                )
            )
        );
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/abstract_results.js
================================================
function arrayKey(array) {
    return array.join(",");
}

class ToggleViewManager extends AbstractViewManager {
    constructor(app) {
        super();
        this.app = app;
    }

    handleImg(entry, resultPath) {
        this.app.setView({
            content: "img",
            contentPath: entry.data_uri,
            resultPath: resultPath
        });
    }

    handleHtml(entry, resultPath) {
        this.app.setView({
            content: "html",
            contentPath: entry.data_uri,
            resultPath: resultPath
        });
    }

    handlePdf(entry, resultPath) {
        this.app.setView({
            content: "pdf",
            contentPath: entry.data_uri,
            resultPath: resultPath
        });
    }

    handleDefault(entry, resultPath) {
        // do nothing in this case
    }
}

class AbstractResults extends React.Component {
    constructor(props) {
        super(props);
        let data = this.getData();
        let toggles = this.getInitToggleState(data.toggleLabels);
        this.state = { toggles, data };
        this.toggleCallback = this.toggleCallback.bind(this);
        this.toggleViewManager = new ToggleViewManager(this.props.app);
    }

    render() {
        if (this.state.data.toggleLabels.size > 0) {
            return e(
                "div",
                {},
                e(
                    "div",
                    { className: "p-2 flex flex-wrap gap-2 rounded bg-slate-800 text-xs" },
                    this.getToggleControls(this.state.data.toggleLabels),
                ),
                this.getResultsTable(this.state.data)
            )
        } else {
            return this.getResultsTable(this.state.data);
        }
    }

    getInitToggleState(toggleLabels) {
        let toggles = new Map();
        toggleLabels.forEach(function (value, key) {
            // Prefer "yes" as initial value if present, otherwise use the first value.
            let initialValue = value[1] === "yes" ? value[1] : value[0];
            toggles.set(key, initialValue);
        })
        return toggles;
    }

    getToggleControls(toggleLabels) {
        let toggleCallback = this.toggleCallback;
        let toggleState = this.state.toggles;
        return toggleLabels.entries().map(function (entry) {
            let [name, values] = entry;
            return e(
                Toggle,
                {
                    label: name,
                    values: values,
                    defaultValue: toggleState.get(name),
                    callback: function (selected) {
                        toggleCallback(name, selected);
                    }
                }
            )
        })
    }

    toggleCallback(name, selected) {
        let data = this.state.data;
        let _this = this;
        this.setState(function (prevState) {
            let toggles = new Map(prevState.toggles);
            toggles.set(name, selected);
            return { data: data, toggles };
        }, function () {
            if (_this.state.data.resultPathsToEntryLabels.has(_this.props.app.state.resultPath)) {
                let toggleLabels = Array.from(data.toggleLabels.keys()).map((label) => _this.state.toggles.get(label));
                let entryLabels = _this.state.data.resultPathsToEntryLabels.get(_this.props.app.state.resultPath);
                let targetPath = _this.state.data.entries.get(arrayKey(entryLabels)).get(arrayKey(toggleLabels));
                _this.toggleViewManager.handleSelectedResult(targetPath);
            }
        });
    }

    getResultsTable(data) {
        return e(
            "table",
            { className: "table-auto text-white text-sm w-full" },
            e(
                "thead",
                {},
                this.renderHeader(data),
            ),
            e(
                "tbody",
                {},
                this.renderEntries(data),
            )
        )
    }

    getResults() {
        throw new Error("Unimplemented!");
    }

    getCategory() {
        throw new Error("Unimplemented!");
    }

    getSubcategory() {
        throw new Error("Unimplemented!");
    }

    getSearchTerm() {
        throw new Error("Unimplemented!");
    }

    getLabels() {
        let first_index = {};
        this.getResults().map(function ([path, result]) {
            let i = 0;
            for (let key in result.labels) {
                if (!(key in first_index)) {
                    first_index[key] = i;
                }
                i += 1;
            }
        })
        let labels = Object.keys(first_index);

        return labels.sort(function (a, b) {
            return first_index[a] - first_index[b];
        });
    }

    getData() {
        let labels;
        if (this.isLabelled()) {
            labels = this.getLabels();
        }

        let results = this.getResults();

        let toggleLabels = new Map();
        // If there are at least two labels, consider all but the first label for
        // being shown as toggles. The latter is possible if a label
        // has exactly two values, each of which occur in half of the results.
        // Example: a plot which is created twice for each sample, once with and 
        // once without legend (for inclusion in larger panel figures where a 
        // repeating legend would be superfluous).
        if (labels !== undefined && labels.length > 1) {
            labels.slice(1).forEach(function (label) {
                let values = results.map(function ([path, entry]) {
                    return entry.labels[label];
                }).filter(function (value) {
                    return value !== undefined;
                });

                let uniqueValues = [...new Set(values)];
                if (
                    uniqueValues.length == 2 &&
                    uniqueValues.every(value => values.filter(v => v === value).length === results.length / 2)
                ) {
                    uniqueValues.sort();
                    toggleLabels.set(label, uniqueValues);
                }
            });
        }
        // Only allow one toggle label for now, in order to avoid confusion in the UI
        if (toggleLabels.size > 1) {
            toggleLabels = new Map();
        }

        let entries = new Map();
        let entryLabelValues = [];
        let resultPathsToEntryLabels = new Map();

        results.forEach(function ([path, entry]) {
            let entryLabels = [];
            let entryToggleLabels = [];
            if (labels !== undefined) {
                labels.forEach(function (label) {
                    if (!toggleLabels.has(label)) {
                        entryLabels.push(entry.labels[label] || "");
                    } else {
                        entryToggleLabels.push(entry.labels[label]);
                    }
                });
            } else {
                entryLabels = [path];
            }

            let key = arrayKey(entryLabels);

            if (!entries.has(key)) {
                entries.set(key, new Map());
                entryLabelValues.push(entryLabels);
            }

            entries.get(key).set(arrayKey(entryToggleLabels), path);
            resultPathsToEntryLabels.set(path, entryLabels);
        });

        entryLabelValues = entryLabelValues.sort(function (aLabels, bLabels) {
            // sort labels lexicographically, first element is the most important
            for (let i = 0; i < aLabels.length; i++) {
                let comparison = aLabels[i].localeCompare(bLabels[i]);
                if (comparison !== 0) {
                    return comparison;
                }
            }
            return 0;
        });

        if (labels === undefined) {
            labels = ["File"];
        }

        return {
            entryLabels: labels.filter((label) => !toggleLabels.has(label)),
            entryLabelValues,
            toggleLabels,
            entries,
            resultPathsToEntryLabels
        }
    }

    isLabelled() {
        return this.getResults().every(function ([path, result]) {
            return result.labels;
        });
    }

    renderHeader(data) {
        return e(
            "tr",
            {},
            data.entryLabels.map(function (label) {
                return e(
                    "th",
                    { className: "text-left p-1 uppercase" },
                    label
                )
            }),
            e(
                "th",
                { className: "text-right p-1 w-fit" },
            )
        )
    }

    renderEntries(data) {
        AbstractResults.propTypes = {
            app: PropTypes.object.isRequired,
        };

        let app = this.props.app;
        let state = this.state;

        return data.entryLabelValues.map(function (entryLabels) {
            let toggleLabels = Array.from(data.toggleLabels.keys()).map((label) => state.toggles.get(label));
            let entryPath = data.entries.get(arrayKey(entryLabels)).get(arrayKey(toggleLabels));

            let actions = e(
                "td",
                { className: "p-1 text-right" },
                e(
                    "div",
                    { className: "inline-flex gap-1", role: "group" },
                    e(
                        ResultViewButton,
                        { resultPath: entryPath, app: app }
                    ),
                    e(
                        Button,
                        {
                            href: "#",
                            onClick: () => app.showResultInfo(entryPath),
                            iconName: "information-circle"
                        }
                    )
                )
            );

            return [
                e(
                    "tr",
                    { key: entryLabels.join(",") },
                    entryLabels.map(function (labelValue) {
                        return e(
                            "td",
                            { className: "p-1" },
                            labelValue
                        );
                    }),
                    actions
                )
            ];
        });
    }
}



================================================
FILE: src/snakemake/report/html_reporter/template/components/abstract_view_manager.js
================================================
class AbstractViewManager {
    handleSelectedResult(resultPath) {
        let entry = results[resultPath];
        const mimeType = getResultMimeType(resultPath);

        switch (mimeType) {
            case "image/svg+xml":
            case "image/png":
            case "image/jpeg":
                return this.handleImg(entry, resultPath);
            case "text/html":
                return this.handleHtml(entry, resultPath);
            case "application/pdf":
                return this.handlePdf(entry, resultPath);
            default:
                return this.handleDefault(entry, resultPath);
        }
    }

    handleImg(entry, resultPath) {
        throw new Error("Unimplemented!");
    }

    handleHtml(entry, resultPath) {
        throw new Error("Unimplemented!");
    }

    handlePdf(entry, resultPath) {
        throw new Error("Unimplemented!");
    }

    handleDefault(entry, resultPath) {
        throw new Error("Unimplemented!");
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/app.js
================================================
'use strict';


let app;


class App extends React.Component {
    constructor(props) {
        super(props)
        this.content = "rulegraph";
        if (Object.keys(metadata).length > 0) {
            this.content = "metadata";
        }
        this.state = { hideNavbar: false, navbarMode: "menu", content: this.content, ruleinfo: undefined, category: undefined, subcategory: undefined, searchTerm: undefined, resultPath: undefined, contentPath: undefined, renderTrigger: undefined };
        this.setView = this.setView.bind(this);
        this.showCategory = this.showCategory.bind(this);
        this.showResultInfo = this.showResultInfo.bind(this);
        this.showReportInfo = this.showReportInfo.bind(this);
        // store in global variable
        app = this;
    }

    render() {
        return [
            e(
                "div",
                { class: "flex flex-row w-screen h-screen" },
                e(Navbar, { key: "navbar", app: this }),
                e(ContentDisplay, { key: "content", app: this })
            )
        ];
    }

    setView(view) {
        this.setState({
            hideNavbar: view.hideNavbar || this.hideNavbar,
            navbarMode: view.navbarMode || this.state.navbarMode,
            content: view.content || this.state.content,
            ruleinfo: view.ruleinfo || this.state.ruleinfo,
            category: view.category || this.state.category,
            subcategory: view.subcategory || this.state.subcategory,
            searchTerm: view.searchTerm || this.state.searchTerm,
            resultPath: view.resultPath || this.state.resultPath,
            contentPath: view.contentPath || this.state.contentPath,
            contentText: view.contentText || this.state.contentText,
            renderTrigger: view.content !== undefined ? Math.random() : this.state.renderTrigger,
        });
    }

    showCategory(category) {
        let subcategory;
        let mode = "category";
        if (isSingleSubcategory(category)) {
            subcategory = Object.keys(categories[category])[0];
            mode = "subcategory";
        }
        this.setView({ navbarMode: mode, category: category, subcategory: subcategory })
    }

    showReportInfo() {
        this.setView({ navbarMode: "reportinfo" });
    }

    showResultInfo(resultPath) {
        this.setView({ navbarMode: "resultinfo", resultPath: resultPath });
    }

    showLicense(package_name) {
        this.setView({
            content: "text",
            contentText: packages[package_name].license
        });
    }
}

const root = ReactDOM.createRoot(document.querySelector('#app'));
root.render(e(App));


================================================
FILE: src/snakemake/report/html_reporter/template/components/breadcrumbs.js
================================================
'use strict';

class Breadcrumbs extends React.Component {
    constructor(props) {
        super(props);
        this.state = { searchTerm: "" };
    }

    render() {
        return e(
            "nav",
            { className: "text-white whitespace-nowrap align-middle text-xs m-2 p-2 rounded bg-slate-800 min-w-fit" },
            e(
                "ol",
                { className: "list-reset flex items-center gap-1" },
                this.renderEntries(),
                this.renderSearch()
            )
        )
    }

    renderEntries() {
        let entries = this.props.entries.filter(function (entry) {
            return entry !== undefined
        });

        return entries.map(function (entry, index) {
            const isLast = index == entries.length - 1;

            let content = entry.name;
            if (entry.icon !== undefined) {
                content = e(
                    Icon,
                    { iconName: entry.icon }
                )
            }


            let link = content;
            if (entry.func !== undefined) {
                link = e(
                    "a",
                    { className: "hover:text-emerald-600", href: "#", onClick: entry.func },
                    content
                );
            }

            let props = {};
            if (isLast) {
                props = { className: "grow" }
            }

            let item = e(
                "li",
                { key: entry.name, ...props },
                link
            );
            if (!isLast) {
                item = [
                    item,
                    e(
                        "li",
                        { key: `sep-${index}` },
                        e(
                            Icon,
                            { iconName: "chevron-right", className: "text-emerald-500" }
                        )
                    )
                ];
            }

            return item;
        });
    }

    renderSearch() {
        let _this = this;
        return e(
            "li",
            { className: "flex-0" },
            e(
                "input",
                {
                    type: "text",
                    placeholder: "Search...",
                    title: "Search all results. Prefix with 're:' to perform regexp search.",
                    size: 10,
                    className: "border-0 bg-transparent text-white h-3 w-fit text-xs text-right form-control rounded",
                    onKeyPress: function (event) {
                        if (event.charCode == 13) {
                            event.preventDefault();
                            _this.showSearchResults();
                        }
                    },
                    onChange: function (event) {
                        _this.setState({ searchTerm: event.target.value });
                    }
                },
            )
        )
    }

    showSearchResults() {
        this.props.setView({ navbarMode: "searchresults", searchTerm: this.state.searchTerm })
    }
}

Breadcrumbs.propTypes = {
    entries: PropTypes.arrayOf(PropTypes.object).isRequired,
    setView: PropTypes.func.isRequired,
};


================================================
FILE: src/snakemake/report/html_reporter/template/components/button.js
================================================
'use strict';

class Button extends React.Component {
    
    render() {
        return this.renderButton(this.props.iconName, this.props);
    }

    renderButton(iconName, props) {
        return e(
            "a",
            { type: "button", className: `transition-all inline-block p-1 text-emerald-500 rounded hover:bg-slate-800`, ...props },
            e(Icon, { iconName: iconName })
        );
    }
}

Button.propTypes = {
    iconName: PropTypes.string.isRequired,
};



================================================
FILE: src/snakemake/report/html_reporter/template/components/category.js
================================================
'use strict';

class Category extends AbstractMenu {
    render() {
        return e(
            "ul",
            {},
            this.getSubcategoryMenuitems()
        )
    }

    showSubcategory(subcategory) {
        this.props.setView({ navbarMode: "subcategory", category: this.props.category, subcategory: subcategory })
    }

    getSubcategoryMenuitems() {
        let _this = this
        let items = Object.keys(categories[this.props.category]).sort(
            (a, b) => a.localeCompare(b)
        ).map(function (subcategory) {
            return _this.getMenuItem(subcategory, "folder", () => _this.showSubcategory(subcategory));
        });
        return items;
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/common.js
================================================
const e = React.createElement;

function isNoResults() {
    return Object.keys(categories).length == 0;
}

function isSingleCategory() {
    return Object.keys(categories).length == 1;
}

function isSingleDefaultCategory() {
    return isSingleCategory() && Object.keys(categories)[0] == "Other";
}

function isSingleSubcategory(category) {
    return Object.keys(categories[category]).length == 1;
}

function getResultMimeType(resultPath) {
    return results[resultPath].mime_type
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/content.js
================================================
'use strict';



class ContentDisplay extends React.Component {
    render() {
        return e(
            "div",
            { className: "grow flex items-center justify-center h-screen overflow-auto" },
            this.renderContent()
        )
    }

    renderContent() {
        let setView = this.props.app.setView;
        ContentDisplay.propTypes = {
            app: PropTypes.shape({
                state: PropTypes.shape({
                    content: PropTypes.string.isRequired,
                    contentPath: PropTypes.string,
                    contentText: PropTypes.string
                }).isRequired,
                setView: PropTypes.func.isRequired
            }).isRequired
        };

        switch (this.props.app.state.content) {
            case "metadata":
                return e(
                    "div",
                    { className: "grow flex gap-3 p-3 items-start" },
                    e(
                        "div",
                        { className: "py-2" },
                        e(
                            "div",
                            { id: "brand" }
                        )
                    ),
                    e(MetaData, { setView: setView })
                );
            case "rulegraph":
                return e(
                    "div",
                    { className: "grow flex gap-3 p-3 items-start" },
                    e(
                        "div",
                        { className: "py-2" },
                        e(
                            "div",
                            {
                                className: "prose prose-sm max-w-lg",
                                dangerouslySetInnerHTML: { __html: workflow_desc }
                            }
                        ),
                        e(
                            "div",
                            { id: "brand" }
                        )
                    ),
                    e(RuleGraph, { setView: setView })
                );
            case "stats":
                return e(
                    "div",
                    { className: "p-3" },
                    e(Stats)
                );
            case "img":
                return e(
                    "div",
                    { className: "p-3" },
                    e(
                        "img",
                        { src: this.props.app.state.contentPath }
                    )
                );
            case "html":
            case "pdf":
                return e(
                    "iframe",
                    { src: this.props.app.state.contentPath, className: "w-full h-screen", key: `${this.props.app.state.renderTrigger}` }
                );
            case "text":
                return e(
                    "div",
                    { className: "p-3 w-full" },
                    e(
                        "pre",
                        { className: "whitespace-pre-line text-sm" },
                        this.props.app.state.contentText
                    )
                );
        }
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/icon.js
================================================
'use strict';

class Icon extends React.Component {
    // paths are imported from https://heroicons.com
    paths = {
        "dots-vertical": [
            { path: "M10 6a2 2 0 110-4 2 2 0 010 4zM10 12a2 2 0 110-4 2 2 0 010 4zM10 18a2 2 0 110-4 2 2 0 010 4z" }
        ],
        "cube": [
            { path: "M11 17a1 1 0 001.447.894l4-2A1 1 0 0017 15V9.236a1 1 0 00-1.447-.894l-4 2a1 1 0 00-.553.894V17zM15.211 6.276a1 1 0 000-1.788l-4.764-2.382a1 1 0 00-.894 0L4.789 4.488a1 1 0 000 1.788l4.764 2.382a1 1 0 00.894 0l4.764-2.382zM4.447 8.342A1 1 0 003 9.236V15a1 1 0 00.553.894l4 2A1 1 0 009 17v-5.764a1 1 0 00-.553-.894l-4-2z" }
        ],
        "chevron-right": [
            { rule: "evenodd", path: "M7.293 14.707a1 1 0 010-1.414L10.586 10 7.293 6.707a1 1 0 011.414-1.414l4 4a1 1 0 010 1.414l-4 4a1 1 0 01-1.414 0z" }
        ],
        "menu": [
            { rule: "evenodd", path: "M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" }
        ],
        "x": [
            { rule: "evenodd", path: "M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" }
        ],
        "arrow-down": [
            { rule: "evenodd", path: "M16.707 10.293a1 1 0 010 1.414l-6 6a1 1 0 01-1.414 0l-6-6a1 1 0 111.414-1.414L9 14.586V3a1 1 0 012 0v11.586l4.293-4.293a1 1 0 011.414 0z" }
        ],
        "arrow-circle-left": [
            { rule: "evenodd", path: "M10 18a8 8 0 100-16 8 8 0 000 16zm.707-10.293a1 1 0 00-1.414-1.414l-3 3a1 1 0 000 1.414l3 3a1 1 0 001.414-1.414L9.414 11H13a1 1 0 100-2H9.414l1.293-1.293z" }
        ],
        "arrow-circle-right": [
            { rule: "evenodd", path: "M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-8.707l-3-3a1 1 0 00-1.414 1.414L10.586 9H7a1 1 0 100 2h3.586l-1.293 1.293a1 1 0 101.414 1.414l3-3a1 1 0 000-1.414z" }
        ],
        "information-circle": [
            { rule: "evenodd", path: "M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a1 1 0 000 2v3a1 1 0 001 1h1a1 1 0 100-2v-3a1 1 0 00-1-1H9z" }
        ],
        "home": [
            { rule: "evenodd", path: "M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z" }
        ],
        "share": [
            { rule: "evenodd", path: "M15 8a3 3 0 10-2.977-2.63l-4.94 2.47a3 3 0 100 4.319l4.94 2.47a3 3 0 10.895-1.789l-4.94-2.47a3.027 3.027 0 000-.74l4.94-2.47C13.456 7.68 14.19 8 15 8z" }
        ],
        "chart": [
            { rule: "evenodd", path: "M2 11a1 1 0 011-1h2a1 1 0 011 1v5a1 1 0 01-1 1H3a1 1 0 01-1-1v-5zM8 7a1 1 0 011-1h2a1 1 0 011 1v9a1 1 0 01-1 1H9a1 1 0 01-1-1V7zM14 4a1 1 0 011-1h2a1 1 0 011 1v12a1 1 0 01-1 1h-2a1 1 0 01-1-1V4z" }
        ],
        "folder": [
            { rule: "evenodd", path: "M2 6a2 2 0 012-2h5l2 2h5a2 2 0 012 2v6a2 2 0 01-2 2H4a2 2 0 01-2-2V6z" }
        ],
        "eye": [
            { rule: undefined, path: "M10 12a2 2 0 100-4 2 2 0 000 4z" },
            { rule: "evenodd", path: "M.458 10C1.732 5.943 5.522 3 10 3s8.268 2.943 9.542 7c-1.274 4.057-5.064 7-9.542 7S1.732 14.057.458 10zM14 10a4 4 0 11-8 0 4 4 0 018 0z" },
        ]
    }

    render() {
        return e(
            "svg",
            { xmlns: "http://www.w3.org/2000/svg", className: `h-4 w-4 ${this.props.className}`, viewBox: "0 0 20 20", fill: "currentColor", },
            this.renderPath().map(function (item, index) {

                return e(
                    "path",
                    { fillRule: item.rule, clipRule: item.rule, d: item.path, key: index }
                );
            })
        )
    }

    

    renderPath() {
        return this.paths[this.props.iconName];
    }
}

Icon.propTypes = {
        iconName: PropTypes.string.isRequired,
        className: PropTypes.string,
};


================================================
FILE: src/snakemake/report/html_reporter/template/components/list_heading.js
================================================
'use strict';

class ListHeading extends React.Component {
    static propTypes = {
        text: PropTypes.string.isRequired
    };

    render() {
        return e(
            "li",
            { className: "uppercase font-bold p-1" },
            this.props.text
        );
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/list_item.js
================================================
'use strict';

class ListItem extends React.Component {
    static propTypes = {
        children: PropTypes.node.isRequired,
    };

    render() {
        return e(
            "li",
            { className: "p-1", ...this.props },
            this.props.children
        );
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/menu.js
================================================
'use strict';

class Menu extends AbstractMenu {
    constructor(props) {
        super(props);
        this.showWorkflow = this.showWorkflow.bind(this);
        this.showStatistics = this.showStatistics.bind(this);
    }

    render() {
        return e(
            "ul",
            {},
            this.getHeading(),
            this.getMenuItem("Workflow", "share", this.showWorkflow),
            this.getMenuItem("Statistics", "chart", this.showStatistics),
            this.getMenuItem("About", "information-circle", this.props.app.showReportInfo),
            this.getCategoryMenumitems()
        )
    }

    showWorkflow() {
        this.props.app.setView({ content: "rulegraph" });
    }

    showStatistics() {
        this.props.app.setView({ content: "stats" });
    }

    getHeading() {
        if (isSingleCategory()) {
            return [];
        } else {
            return e(
                ListHeading,
                { text: "General" }
            )
        }
    }

    getCategoryMenumitems() {
        let _this = this;
        let app = this.props.app;
        if (isSingleCategory()) {
            let category = Object.keys(categories)[0];
            return this.getMenuItem("Results", "folder", () => app.showCategory(category));
        } else if (isNoResults()) {
            return [];
        } else {
            let items = [e(
                ListHeading,
                { key: "Results", text: "Results" }
            )];

            items.push(...Object.keys(categories).sort(
                (a, b) => a.localeCompare(b)
            ).map(function (category) {
                return _this.getMenuItem(category, "folder", () => app.showCategory(category));
            }));

            return items;
        }
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/metadata.js
================================================
'use strict';

class MetaData extends React.Component {

    render() {
        let metadatalist = []
        // If workflow description is given, show it on the landing page with the metadata
        if (workflow_desc) {
            metadatalist.push(...
            [
                e(
                    "div",
                    {
                        className: "prose prose-sm max-w-lg",
                        dangerouslySetInnerHTML: { __html: workflow_desc }
                    }
                ),
            ]
            );
        }

        // Iterate over the metadata dictionary and add the entries to the landing page
        for (const [key, value] of Object.entries(metadata)) {
            metadatalist.push(... this.innerRender(key, value));
        }
        console.log(workflow_desc)
        return e(
            //workflow_desc,
            "ul",
            { },
            metadatalist
        )
    }

    innerRender(key, value) {
        // check if the value is an dictionary, if this is the case, recursively
        // add the elements to the landing page
        if (value.constructor == Object) {
            let result = [];
            for (const [inner_key, inner_value] of Object.entries(value)) {
                result.push(...this.innerRender(inner_key, inner_value));
            }
            return result;
        }
        return [
            e(
                ListHeading,
                { key: `${key}-heading`, text: key }
            ),
            e(
                ListItem,
                {
                    key: `${key}`,
                    className: "p-1",
                    // this is safe as the value is already rendered by docutils
                    dangerouslySetInnerHTML: { __html: value }
                }
            )
        ]
    }

}



================================================
FILE: src/snakemake/report/html_reporter/template/components/navbar.js
================================================
'use strict';

class Navbar extends React.Component {
    render() {
        let showHideNavbar = `${this.getWidth()}`;
        let showHideShowButton = "-translate-x-full";

        if (this.props.app.state.hideNavbar) {
            showHideNavbar = "w-0";
            showHideShowButton = "";
        }
        return [
            e(
                "div",
                { className: `fixed z-50 p-2 transition-translate bg-white/70 backdrop-blur-sm rounded-br-lg ${showHideShowButton}` },
                this.getShowButton()
            ),
            e(
                "nav",
                { className: `transition-all ${showHideNavbar} text-white text-sm bg-slate-900/70 backdrop-blur-sm h-screen overflow-auto` },
                e(
                    "h1",
                    { className: "sticky relative top-0 left-0 bg-white/80 backdrop-blur-sm text-slate-700 text-l tracking-wide px-3 py-1 mb-1 flex items-center" },
                    e(
                        "img",
                        { src: logo_uri, className: "h-4" }
                    ),
                    e(
                        "span",
                        { className: "font-bold mx-1" },
                        "Snakemake"
                    ),
                    e(
                        "span",
                        { className: "grow mr-5" },
                        "Report"
                    ),
                    this.getHideButton()
                ),
                e(
                    "div",
                    {},
                    this.renderBreadcrumbs(),
                    e(
                        "div",
                        { className: "p-3" },
                        this.renderContent()
                    )
                )
            )
        ];
    }

    getHideButton() {
        let setView = this.props.app.setView;
        return e(
            "a",
            {
                type: "button",
                className: "bg-transparent hover:text-emerald-500",
                href: "#",
                onClick: () => setView({ hideNavbar: true })
            },
            e(
                Icon,
                { iconName: "x" }
            )
        )
    }

    getShowButton() {
        let setView = this.props.app.setView;
        return e(
            "a",
            {
                type: "button",
                href: "#",
                className: "bg-transparent hover:text-emerald-500",
                onClick: () => setView({ hideNavbar: false })
            },
            e(
                Icon,
                { iconName: "menu" }
            )
        )
    }

    renderContent() {
        let setView = this.props.app.setView;
        switch (this.props.app.state.navbarMode) {
            case "menu":
                return e(Menu, { app: this.props.app });
            case "category":
                return e(Category, { setView: setView, category: this.props.app.state.category });
            case "subcategory":
                return e(Subcategory, { app: this.props.app, setView: setView, category: this.props.app.state.category, subcategory: this.props.app.state.subcategory });
            case "searchresults":
                return e(SearchResults, { app: this.props.app, setView: setView, searchTerm: this.props.app.state.searchTerm });
            case "resultinfo":
                return e(ResultInfo, { resultPath: this.props.app.state.resultPath, app: this.props.app });
            case "ruleinfo":
                return e(RuleInfo, { rule: this.props.app.state.ruleinfo });
            case "reportinfo":
                return e(ReportInfo, { app: this.props.app });
        }
    }

    renderBreadcrumbs() {
        let setView = this.props.app.setView;
        switch (this.props.app.state.navbarMode) {
            case "menu":
                return e(
                    Breadcrumbs,
                    { entries: [this.getMenuBreadcrumb()], setView: setView }
                );
            case "category":
                return e(
                    Breadcrumbs,
                    { entries: [this.getMenuBreadcrumb(), this.getResultBreadcrumb(), this.getCategoryBreadcrumb()], setView: setView }
                );
            case "subcategory":
                return e(
                    Breadcrumbs,
                    { entries: [this.getMenuBreadcrumb(), this.getResultBreadcrumb(), this.getCategoryBreadcrumb(), this.getSubcategoryBreadcrumb()], setView: setView }
                );
            case "resultinfo":
                return e(
                    Breadcrumbs,
                    { entries: [this.getMenuBreadcrumb(), this.getResultBreadcrumb(), this.getCategoryBreadcrumb(), this.getSubcategoryBreadcrumb(), this.getSearchResultsBreadcrumb(), this.getResultinfoBreadcrumb()], setView: setView }
                );
            case "searchresults":
                return e(
                    Breadcrumbs,
                    { entries: [this.getMenuBreadcrumb(), this.getSearchResultsBreadcrumb()], setView: setView }
                )
            case "ruleinfo":
                if (this.props.app.state.resultPath) {
                    return e(
                        Breadcrumbs,
                        { entries: [this.getMenuBreadcrumb(), this.getResultBreadcrumb(), this.getCategoryBreadcrumb(), this.getSubcategoryBreadcrumb(), this.getSearchResultsBreadcrumb(), this.getResultinfoBreadcrumb(), this.getRuleBreadcrumb(), this.getRuleinfoBreadcrumb()], setView: setView }
                    )
                } else {
                    return e(
                        Breadcrumbs,
                        { entries: [this.getMenuBreadcrumb(), this.getRuleBreadcrumb(), this.getRuleinfoBreadcrumb()], setView: setView }
                    )
                }
            case "reportinfo":
                return e(
                    Breadcrumbs,
                    { entries: [this.getMenuBreadcrumb(), this.getReportInfoBreadcrumb()], setView: setView }
                )
        }
    }

    getMenuBreadcrumb() {
        let setView = this.props.app.setView;
        return {
            name: "menu",
            icon: "home",
            func: function () {
                setView({
                    navbarMode: "menu",
                    category: undefined,
                    subcategory: undefined,
                    content: "metadata",
                })
            }
        };
    }

    getReportInfoBreadcrumb() {
        let setView = this.props.app.setView;
        return { name: "reportinfo", icon: "information-circle", func: function () { setView({ navbarMode: "reportinfo", category: undefined, subcategory: undefined }) } };
    }

    getRuleBreadcrumb() {
        return { name: "Rule", func: undefined }
    }

    getRuleinfoBreadcrumb() {
        return { name: this.props.app.state.ruleinfo, func: undefined }
    }

    getCategoryBreadcrumb() {
        let category = this.props.app.state.category;
        if (category === undefined) {
            return undefined;
        }
        let subcategory;
        let mode = "category";
        if (isSingleSubcategory(category)) {
            subcategory = this.props.app.state.subcategory;
            mode = "subcategory";
        }

        let name = this.props.app.state.category;
        if (isSingleDefaultCategory()) {
            name = "Results";
        }
        let setView = this.props.app.setView;
        return { name: name, func: function () { setView({ navbarMode: mode, category: category, subcategory: subcategory }) } };
    }

    getSubcategoryBreadcrumb() {
        let subcategory = this.props.app.state.subcategory;
        let category = this.props.app.state.category;
        if (subcategory === undefined || isSingleSubcategory(category)) {
            return undefined;
        }
        let setView = this.props.app.setView;
        return { name: this.props.app.state.subcategory, func: function () { setView({ navbarMode: "subcategory", category: category, subcategory: subcategory }) } };
    }

    getResultBreadcrumb() {
        if (isSingleDefaultCategory()) {
            return undefined;
        }
        let setView = this.props.app.setView;
        return { name: "Results", func: function () { setView({ navbarMode: "menu", category: undefined, subcategory: undefined }) } };
    }

    getSearchResultsBreadcrumb() {
        let searchTerm = this.props.app.state.searchTerm;
        if (searchTerm === undefined) {
            return undefined;
        }
        let setView = this.props.app.setView;
        return {
            name: "Search results", func: function () {
                setView({ navbarMode: "searchresults", searchTerm: searchTerm })
            }
        };
    }

    getResultinfoBreadcrumb() {
        let setView = this.props.app.setView;
        return {
            name: this.props.app.state.resultPath, func: function () { setView({ navbarMode: "resultinfo" }) }
        };
    }

    getWidth() {
        switch (this.props.app.state.navbarMode) {
            case "menu":
                return "w-1/5 min-w-fit"
            case "category":
            case "subcategory":
            case "searchresults":
                return "w-1/4 min-w-fit"
            case "resultinfo":
                return "w-1/2"
            case "ruleinfo":
                return "w-1/2"
            default:
                return "w-1/5 min-w-fit"
        }
    }
}

Navbar.propTypes = {
    app: PropTypes.shape({
        state: PropTypes.shape({
            ruleinfo: PropTypes.string,
            subcategory: PropTypes.string,
            category: PropTypes.string,
            searchTerm: PropTypes.string,
            resultPath: PropTypes.string,
            hideNavbar: PropTypes.bool.isRequired,
            navbarMode: PropTypes.string.isRequired
        }).isRequired,
        setView: PropTypes.func.isRequired
    }).isRequired
};


================================================
FILE: src/snakemake/report/html_reporter/template/components/report_info.js
================================================
'use strict';

class ReportInfo extends AbstractMenu {
    render() {
        return e(
            "ul",
            {},
            e(
                ListHeading,
                { text: "Embedded packages" },
            ),
            this.getPackages()
        );
    }

    getPackages() {
        let _this = this;
        return Object.entries(packages).map(function ([name, record]) {
            return _this.getMenuItem(`${name} ${record.version}`, "cube", () => _this.props.app.showLicense(name))
        }
        );
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/result_info.js
================================================
'use strict';

class ResultInfo extends React.Component {
    render() {
        return e(
            "ul",
            {},
            this.getDescriptor(),
            this.getCaption(),
            this.getSize(),
            this.getRule(),
            this.getParamsAndWildcards(),
        )
    }

    getResult() {
        return results[this.props.resultPath];
    }

    getDescriptor() {
        let result = this.getResult();
        let resultPath = this.props.resultPath;
        let app = this.props.app;

        if (result.labels) {
            const labels = Object.keys(result.labels).sort((a, b) => a.localeCompare(b));
            return [
                e(
                    ListItem,
                    {},
                    e(
                        "table",
                        { className: "table-auto text-white text-sm items-center" },
                        e(
                            "thead",
                            {},
                            e(
                                "tr",
                                {},
                                labels.map(function (label) {
                                    return e(
                                        "th",
                                        { className: "text-left uppercase pr-2 whitespace-nowrap" },
                                        label
                                    );
                                }),
                                e("th", {})
                            )
                        ),
                        e(
                            "tbody",
                            {},
                            e(
                                "tr",
                                {},
                                labels.map(function (label, index) {
                                    const value = result.labels[label];
                                    let item = value;
                                    if (index == labels.length - 1) {
                                        item = e(
                                            "span",
                                            { className: "flex items-center gap-2" },
                                            e("span", {}, value),
                                            e(
                                                ResultViewButton,
                                                { resultPath: resultPath, app: app }
                                            )
                                        );
                                    }

                                    return e(
                                        "td",
                                        { className: "pr-2" },
                                        item
                                    );
                                }),
                            )
                        )
                    )
                )
            ];
        } else {
            return [
                e(
                    ListHeading,
                    { text: "Path", key: "result" }
                ),
                e(
                    ListItem,
                    { key: "path" },
                    e(
                        "span",
                        { className: "flex items-center gap-2" },
                        e(
                            "span",
                            {},
                            this.props.resultPath
                        ),
                        e(
                            ResultViewButton,
                            { resultPath: this.props.resultPath, app: app }
                        )
                    )
                ),
            ];
        }
    }

    getCaption() {
        const caption = this.getResult().caption;
        if (caption) {
            return [
                e(
                    ListHeading,
                    { key: "caption-heading", text: "Description" }
                ),
                e(
                    ListItem,
                    {
                        key: "caption",
                        className: "p-1 prose prose-invert prose-sm",
                        dangerouslySetInnerHTML: { __html: caption }
                    }
                )
            ];
        } else {
            return [];
        }
    }

    getRule() {
        const setView = this.props.app.setView;
        const rule = this.getResult().job_properties.rule;
        return [
            e(
                ListHeading,
                { key: "rule-heading", text: "Snakemake rule" }
            ),
            e(
                ListItem,
                { key: "rulename" },
                e(
                    "span",
                    { className: "flex items-center gap-1" },
                    [
                        e(
                            "span",
                            {},
                            rule
                        ),
                        e(
                            "a",
                            {
                                type: "button",
                                href: "#",
                                className: `transition-all inline-block p-1 text-emerald-500 rounded hover:bg-slate-800`,
                                onClick: () => setView({ navbarMode: "ruleinfo", ruleinfo: rule })
                            },
                            e(Icon, { iconName: "information-circle" })
                        )
                    ]
                )
            )
        ]
    }

    getParamsAndWildcards() {
        const jobProperties = this.getResult().job_properties;
        if (jobProperties.params || jobProperties.wildcards) {
            let items = [
                e(
                    ListHeading,
                    { key: "params-heading", text: "Job parameters" }
                ),
            ];
            if (jobProperties.wildcards) {
                items.push(e(
                    ListItem,
                    { key: "wildcards" },
                    jobProperties.wildcards
                ));
            }
            if (jobProperties.params) {
                items.push(e(
                    ListItem,
                    { key: "params" },
                    jobProperties.params
                ));
            }
            return items;
        } else {
            return [];
        }
    }

    getSize() {
        return [
            e(
                ListHeading,
                { key: "size-heading", text: "Size" }
            ),
            e(
                ListItem,
                { key: "size" },
                this.getResult().size
            )
        ];
    }
}

ResultInfo.propTypes = {
    resultPath: PropTypes.string.isRequired,
    app: PropTypes.shape({
        state: PropTypes.shape({
            ruleinfo: PropTypes.string,
            subcategory: PropTypes.string,
            category: PropTypes.string,
            searchTerm: PropTypes.string,
            resultPath: PropTypes.string,
            hideNavbar: PropTypes.bool.isRequired,
            navbarMode: PropTypes.string.isRequired
        }).isRequired,
        setView: PropTypes.func.isRequired
    }).isRequired
};


================================================
FILE: src/snakemake/report/html_reporter/template/components/result_view_button.js
================================================
'use strict';


class ButtonViewManager extends AbstractViewManager {
    constructor(app) {
        super();
        this.app = app;
    }

    handleImg(entry, resultPath) {
        let app = this.app;
        let props = {
            href: "#",
            onClick: function() {
                app.setView({
                    content: "img",
                    contentPath: entry.data_uri,
                    resultPath: resultPath
                });
            }
        };
        return this.renderButton(props);
    }

    handleHtml(entry, resultPath) {
        let app = this.app;
        let props = {
            href: "#",
            onClick: function() {
                app.setView({
                    content: "html",
                    contentPath: entry.data_uri,
                    resultPath: resultPath
                });
            }
        };
        return this.renderButton(props);
    }

    handlePdf(entry, resultPath) {
        let app = this.app;
        let props = {
            href: "#",
            onClick: function() {
                app.setView({
                    content: "pdf",
                    contentPath: entry.data_uri,
                    resultPath: resultPath
                });
            }
        };
        return this.renderButton(props);
    }

    handleDefault(entry, resultPath) {
        let props = {
            href: entry.data_uri,
            download: entry.name,
            target: "_blank"
        };
        return this.renderButton(props);
    }

    renderButton(props) {
        return e(
            Button,
            { iconName: "eye", ...props }
        );
    }
}


class ResultViewButton extends React.Component {
    constructor(props) {
        super(props);
        this.viewManager = new ButtonViewManager(this.props.app);
    }

    render() {
        return this.viewManager.handleSelectedResult(this.props.resultPath);
    }
}

ResultViewButton.propTypes = {
    resultPath: PropTypes.string.isRequired,
    app: PropTypes.object.isRequired
};


================================================
FILE: src/snakemake/report/html_reporter/template/components/rule_graph.js
================================================
'use strict';

class RuleGraph extends React.Component {
    constructor(props) {
        super(props);
        this.showRuleProperties = this.showRuleProperties.bind(this);
    }

    render() {
        return e(
            "div",
            { id: "rulegraph", className: "max-h-screen py-2" }
        )
    }

    componentDidMount() {
        let showRuleProperties = this.showRuleProperties;
        vegaEmbed("#rulegraph", rulegraph_spec).then(function (ret) {
            ret.view.addEventListener("click", function (event, item) {
                if (item && "rule" in item.datum) {
                    let rule = item.datum.rule;
                    showRuleProperties(rule);
                }
            });
        });
    }

    showRuleProperties(rule) {
        this.props.setView({
            navbarMode: "ruleinfo",
            ruleinfo: rule
        });
    }
}

RuleGraph.propTypes = {
    setView: PropTypes.func.isRequired
};


================================================
FILE: src/snakemake/report/html_reporter/template/components/rule_info.js
================================================
'use strict';

function flattenList(inputList) {
    const flattened = [];

    for (const item of inputList) {
        if (typeof item === 'object' && !Array.isArray(item)) {
            for (const [key, value] of Object.entries(item)) {
                if (Array.isArray(value)) {
                    flattened.push(...value.map(subItem => `${key}: ${subItem}`));
                } else {
                    flattened.push(`${key}: ${value}`);
                }
            }
        } else {
            flattened.push(item);
        }
    }

    return flattened;
}

class RuleInfo extends React.Component {
    static propTypes = {
        rule: PropTypes.object.isRequired,
    };

    render() {
        let rule = rules[this.props.rule];
        if (rule === undefined) {
            return e(
                "span",
                { className: "p-1" },
                `No metadata available for rule ${this.props.rule}`
            );
        }

        return e(
            "ol",
            {},
            this.renderItems("Input", rule.input, {}, false),
            this.renderItems("Output", rule.output),
            this.renderSoftware(),
            this.renderItems("Container", [rule.container_img_url]),
            this.renderCode(),
        )
    }

    renderSoftware() {
        let rule = rules[this.props.rule];
        if (rule.conda_env) {
            return this.renderItems("Software", flattenList(rule.conda_env.dependencies));
        } else {
            return [];
        }
    }

    renderCode() {
        let rule = rules[this.props.rule];
        if (rule.code.length) {
            return [
                e(
                    ListHeading,
                    { key: "code-heading", text: "Code" }
                ),
                e(
                    ListItem,
                    {
                        key: "code",
                        className: "p-1",
                        dangerouslySetInnerHTML: { __html: rule.code }
                    }
                )
            ];
        } else {
            return [];
        }
    }

    renderItems(heading, items, props = {}, margin = true) {
        if (items.length && items.every((item) => item !== undefined)) {
            let headingProps = {};
            if (margin) {
                headingProps = { className: "" }
            }
            return [
                e(
                    ListHeading,
                    { text: heading, ...headingProps }
                ),
                items.map(function (item) {
                    return e(
                        ListItem,
                        props,
                        item
                    );
                })
            ];
        } else {
            return [];
        }
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/search_results.js
================================================
'use strict';

class SearchResults extends AbstractResults {
    getSearchFunc() {
        let term = this.props.searchTerm;
        if (term.startsWith("re:")) {
            let regexp = new RegExp(term.slice(3));
            return function (text) {
                return regexp.test(text);
            }
        } else {
            return function (text) {
                return text.includes(term);
            }
        }
    }

    getResults() {
        let searchFunc = this.getSearchFunc();
        return Object.entries(results).map(function ([path, result]) {
            if (searchFunc(path)) {
                return [path, result];
            }
            const columns = result.columns || [];
            for (const columnValue in columns) {
                if (searchFunc(columnValue)) {
                    return [path, result];
                }
            }
            return undefined;
        }).filter(function (item) {
            return item !== undefined;
        })
    }

    getCategory() {
        return undefined;
    }

    getSubcategory() {
        return undefined;
    }

    getSearchTerm() {
        return this.props.searchTerm;
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/stats.js
================================================
'use strict';

class Stats extends React.Component {
    render() {
        return e(
            "div",
            { className: "flex" },
            e(
                "div",
                { id: "runtimes" }
            ),
            e(
                "div",
                { id: "timeline" }
            )
        )
    }

    componentDidMount() {
        vegaEmbed("#runtimes", runtimes_spec);
        vegaEmbed("#timeline", timeline_spec);
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/subcategory.js
================================================
'use strict';

class Subcategory extends AbstractResults {
    getResults() {
        return categories[this.props.category][this.props.subcategory].map(function (path) {
            return [path, results[path]];
        });
    }

    getCategory() {
        return this.props.category;
    }

    getSubcategory() {
        return this.props.subcategory;
    }

    getSearchTerm() {
        return undefined;
    }
}


================================================
FILE: src/snakemake/report/html_reporter/template/components/toggle.js
================================================
'use strict';

class Toggle extends React.Component {
    constructor(props) {
        super(props);
        this.state = { value: this.props.defaultValue };
        this.selectValue = this.selectValue.bind(this);

    }

    selectValue(selected) {
        // swap the value
        this.setState({ value: selected });
        this.props.callback(selected);
    }

    render() {
        let props = this.props;
        function valueId(idx) {
            return `toggle-${props.label}-${props.values[idx]}`;
        }

        const radioClasses = "appearance-none hidden";
        const labelClasses = "flex gap-2 items-center transition-all inline-block p-1 has-[:checked]:bg-emerald-500 has-[:checked]:text-slate-800 hover:bg-slate-500 bg-slate-600"

        let selectValue = this.selectValue;

        return e(
            "fieldset",
            {className: "flex gap-2 items-center shrink-0"},
            e(
                "span",
                {className: "uppercase font-bold"},
                `${props.label}`
            ),
            e(
                "span",
                {className: "flex gap-0 shrink-0"},
                e(
                    "label",
                    {for: valueId(0), className: `${labelClasses} rounded-l`},
                    e(
                        "input",
                        {
                            className: radioClasses,
                            type: "radio",
                            checked: this.state.value == props.values[0],
                            id: valueId(0),
                            name: props.label, 
                            value: props.values[0],
                            onClick: () => selectValue(props.values[0])
                        },
                    ),
                    props.values[0]
                ),
                e(
                    "label",
                    {for: valueId(1), className: `${labelClasses} rounded-r`},
                    e(
                        "input",
                        {
                            className: radioClasses,
                            type: "radio",
                            checked: this.state.value == props.values[1], 
                            id: valueId(1),
                            name: props.label, 
                            value: props.values[1], 
                            onClick: () => selectValue(props.values[1])
                        },
                    ),
                    props.values[1]
                )
            )
        )
    }

    // render2() {
    //     return e(
    //         "label",
    //         { for: this.props.label, className: "p-1 relative inline-flex cursor-pointer items-center"},
    //         e(
    //             "span",
    //             {},
    //             this.props.label
    //         ),
    //         e(
    //             "div",
    //             {className: "relative inline-flex cursor-pointer items-center"},
    //             e(
    //                 "input",
    //                 { id: this.props.label, type: "checkbox", className: "peer sr-only", checked: this.state.checked, onChange: this.handleToggle }
    //             ),
    //             e(
    //                 "div",
    //                 { className: "peer flex h-8 items-center gap-4 rounded-full bg-emerald-500 px-3 after:absolute after:left-1 after: after:h-6 after:w-8 after:rounded-full after:bg-white/40 after:transition-all after:content-[''] peer-checked:bg-stone-600 peer-checked:after:translate-x-full peer-focus:outline-none text-sm text-white" },
    //                 e(
    //                     "span",
    //                     {},
    //                     this.props.values[0]
    //                 ),
    //                 e(
    //                     "span",
    //                     {},
    //                     this.props.values[1]
    //                 )
    //             )
    //         )
    //     )
    // }
}


================================================
FILE: src/snakemake/scheduling/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/scheduling/greedy.py
================================================
from dataclasses import dataclass, field
from typing import Dict, Mapping, Optional, Sequence, Union
from snakemake_interface_scheduler_plugins.base import SchedulerBase
from snakemake_interface_scheduler_plugins.settings import SchedulerSettingsBase
from snakemake_interface_scheduler_plugins.interfaces.jobs import JobSchedulerInterface
from snakemake_interface_common.io import AnnotatedStringInterface


@dataclass
class SchedulerSettings(SchedulerSettingsBase):
    greediness: Optional[float] = field(
        default=1.0,
        metadata={
            "help": "Set the greediness of scheduling. This value between 0 and 1 "
            "determines how careful jobs are selected for execution. The default "
            "value (1.0) provides the best speed and still acceptable scheduling "
            "quality.",
        },
    )
    omit_prioritize_by_temp_and_input: bool = field(
        default=False,
        metadata={
            "help": "If set, jobs with larger temporary or input files are "
            "not prioritized. The rationale of the prioritization is that temp files "
            "should be removed as soon as possible, and larger input files may take "
            "longer to process, so it is better to start them earlier.",
        },
    )

    def __post_init__(self):
        if self.greediness is None:
            self.greediness = 1.0
        if not (0 <= self.greediness <= 1.0):
            raise ValueError("greediness must be >=0 and <=1")


class Scheduler(SchedulerBase):

    def __post_init__(self):
        self._input_sizes = {}

    def select_jobs(
        self,
        selectable_jobs: Sequence[JobSchedulerInterface],
        remaining_jobs: Sequence[JobSchedulerInterface],
        available_resources: Mapping[str, Union[int, str]],
        input_sizes: Dict[AnnotatedStringInterface, int],
    ) -> Sequence[JobSchedulerInterface]:
        """
        Using the greedy heuristic from
        "A Greedy Algorithm for the General Multidimensional Knapsack
        Problem", Akcay, Li, Xu, Annals of Operations Research, 2012

        Args:
            jobs (list):    list of jobs
        """
        # each job is an item with one copy (0-1 MDKP)
        n = len(selectable_jobs)
        x = [0] * n  # selected jobs
        E = set(range(n))  # jobs still free to select
        u = [1] * n
        a = [
            self.job_weight(job, available_resources) for job in selectable_jobs
        ]  # resource usage of jobs

        c = [self.job_reward(job, input_sizes) for job in selectable_jobs]

        def calc_reward():
            return [c_j * y_j for c_j, y_j in zip(c, y)]

        b = [
            available_resources[name] for name in available_resources
        ]  # resource capacities

        while True:
            # Step 2: compute effective capacities
            y = [
                (
                    min(
                        (min(u[j], b_i // a_j_i) if a_j_i > 0 else u[j])
                        for b_i, a_j_i in zip(b, a[j])
                        if a_j_i
                    )
                    if j in E
                    else 0
                )
                for j in range(n)
            ]
            if not any(y):
                break
            y = [
                (max(1, int(self.settings.greediness * y_j)) if y_j > 0 else 0)
                for y_j in y
            ]

            # Step 3: compute rewards on cumulative sums
            reward = calc_reward()
            j_sel = max(E, key=reward.__getitem__)  # argmax

            # Step 4: batch increment
            y_sel = y[j_sel]

            # Step 5: update information
            x[j_sel] += y_sel
            b = [b_i - (a_j_i * y_sel) for b_i, a_j_i in zip(b, a[j_sel])]
            u[j_sel] -= y_sel
            if not u[j_sel] or self.settings.greediness == 1:
                E.remove(j_sel)
            if not E:
                break

        solution = [job for job, sel in zip(selectable_jobs, x) if sel]
        return solution

    def job_weight(self, job, available_resources: Mapping[str, Union[int, str]]):
        res = job.scheduler_resources
        return [res.get(name, 0) for name in available_resources]

    def job_reward(self, job, input_sizes: Dict[AnnotatedStringInterface, int]):
        if self.settings.omit_prioritize_by_temp_and_input:
            return job.priority
        else:
            # Usually, this should guide the scheduler to first schedule all jobs
            # that remove the largest temp file, then the second largest and so on.
            # Since the weight is summed up, it can in theory be that it sometimes
            # prefers a set of many jobs that all depend on smaller temp files though.
            # A real solution to the problem is therefore to use dummy jobs that
            # ensure selection of groups of jobs that together delete the same temp
            # file.
            return (
                job.priority,
                sum(input_sizes[f] or 0 for f in job.input if f.is_flagged("temp")),
                sum(input_sizes[f] or 0 for f in job.input),
            )



================================================
FILE: src/snakemake/scheduling/job_scheduler.py
================================================
__author__ = "Johannes Köster"
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import asyncio
from bisect import bisect
from collections import deque
import signal
import sys
import threading

from itertools import chain, accumulate, filterfalse, repeat
from contextlib import ContextDecorator
import time
from typing import Iterable, Sequence

from snakemake_interface_executor_plugins.scheduler import JobSchedulerExecutorInterface
from snakemake_interface_executor_plugins.registry import ExecutorPluginRegistry
from snakemake_interface_executor_plugins.registry import Plugin as ExecutorPlugin
from snakemake_interface_executor_plugins.settings import ExecMode
from snakemake_interface_logger_plugins.common import LogEvent
from snakemake.io import _IOFile
from snakemake.jobs import AbstractJob
from snakemake_interface_scheduler_plugins.base import SchedulerBase
from snakemake_interface_scheduler_plugins.registry import SchedulerPluginRegistry
from snakemake.common import async_run

from snakemake.exceptions import RuleException, WorkflowError, print_exception
from snakemake.logging import logger
from snakemake.scheduling.greedy import SchedulerSettings as GreedySchedulerSettings

from snakemake.settings.enums import Quietness
from snakemake.settings.types import MaxJobsPerTimespan

registry = ExecutorPluginRegistry()


def cumsum(iterable, zero=[0]):
    return list(chain(zero, accumulate(iterable)))


_ERROR_MSG_FINAL = (
    "Exiting because a job execution failed. Look below for error messages"
)

_ERROR_MSG_ISSUE_823 = (
    "BUG: Out of jobs ready to be started, but not all files built yet."
    " Please check https://github.com/snakemake/snakemake/issues/823 for more information."
)


class DummyRateLimiter(ContextDecorator):
    async def __aenter__(self):
        return self

    async def __aexit__(self, *args):
        return False


class JobScheduler(JobSchedulerExecutorInterface):
    def __init__(
        self,
        workflow,
        executor_plugin: ExecutorPlugin,
        scheduler: SchedulerBase,
        greedy_scheduler_settings: GreedySchedulerSettings,
    ):
        """Create a new instance of KnapsackJobScheduler."""
        self.workflow = workflow

        self.dryrun = self.workflow.dryrun
        self.touch = self.workflow.touch
        self.quiet = self.workflow.output_settings.quiet
        self.keepgoing = self.workflow.execution_settings.keep_going
        self.running = set()
        self.failed = set()
        self.finished_jobs = 0
        self.greediness = self.workflow.scheduling_settings.greediness
        self.subsample = self.workflow.scheduling_settings.subsample
        self._tofinish = []
        self._toerror = []
        self._validated_jobs = set()
        self.handle_job_success = True
        self.update_resources = True
        self.print_progress = (
            not self.quiet or Quietness.PROGRESS not in self.quiet
        ) and not self.dryrun
        self.update_checkpoint_dependencies = not self.dryrun
        self.job_rate_limiter = (
            JobRateLimiter(self.workflow.scheduling_settings.max_jobs_per_timespan)
            if not (self.dryrun or self.touch)
            and self.workflow.scheduling_settings.max_jobs_per_timespan
            else None
        )

        nodes_unset = workflow.global_resources["_nodes"] is None

        self.global_resources = {
            name: (sys.maxsize if res is None else res)
            for name, res in workflow.global_resources.items()
        }

        if not nodes_unset:
            # Do not restrict cores locally if nodes are used (i.e. in case of cluster/cloud submission).
            self.global_resources["_cores"] = sys.maxsize
        # register job count resource (always initially unrestricted)
        self.global_resources["_job_count"] = sys.maxsize

        self.resources = dict(self.global_resources)

        self._open_jobs = threading.Semaphore(0)
        self._lock = threading.Lock()

        self._errors = False
        self._executor_error = None
        self._finished = False
        self._job_queue = None
        self._last_job_selection_empty = False
        self._last_update_queue_input_jobs = 0
        self.submit_callback = self._noop
        self.finish_callback = self._proceed
        self._run_performed = None
        self._input_sizes = {}

        if workflow.remote_execution_settings.immediate_submit:
            self.submit_callback = self._proceed
            self.finish_callback = self._noop

        self._local_executor = None

        if self.workflow.local_exec:
            self._executor = executor_plugin.executor(
                self.workflow,
                logger,
            )
        else:
            self._executor = executor_plugin.executor(
                self.workflow,
                logger,
            )
            self._local_executor = (
                ExecutorPluginRegistry()
                .get_plugin("local")
                .executor(
                    self.workflow,
                    logger,
                )
            )

        self._greedy_scheduler = (
            SchedulerPluginRegistry()
            .get_plugin("greedy")
            .scheduler(self.workflow.dag, greedy_scheduler_settings, logger)
        )

        # Choose job selector (greedy or ILP)
        self.job_selector_greedy = self._greedy_scheduler.select_jobs
        self._job_selector = scheduler.select_jobs

        self._user_kill = None
        try:
            signal.signal(signal.SIGTERM, self.exit_gracefully)
        except ValueError:
            # If this fails, it is due to scheduler not being invoked in the main thread.
            # This can only happen with --gui, in which case it is fine for now.
            pass
        self._open_jobs.release()

    def executor_error_callback(self, exception):
        with self._lock:
            self._executor_error = exception
            # next scheduling round to catch and raise error
            self._open_jobs.release()

    @property
    def stats(self):
        return self._stats

    @property
    def open_jobs(self):
        """Return open jobs."""
        jobs = list(self.workflow.dag.ready_jobs)
        return jobs

    @property
    def remaining_jobs(self):
        """Return jobs to be scheduled including not yet ready ones."""
        return [
            job
            for job in self.workflow.dag.needrun_jobs()
            if job not in self.running
            and not self.workflow.dag.finished(job)
            and job not in self.failed
        ]

    async def update_input_sizes(self, jobs: Iterable[AbstractJob]):
        async def get_size(path: _IOFile):
            return path, await path.size() if await path.exists() else None

        paths = {path for job in jobs for path in job.input}
        if paths:
            self._input_sizes.update(
                await asyncio.gather(*[get_size(path) for path in paths])
            )

        if len(self._input_sizes) > 10000:
            for path in list(filterfalse(paths.__contains__, self._input_sizes)):
                del self._input_sizes[path]

    def schedule(self):
        """Schedule jobs that are ready, maximizing cpu usage."""
        try:
            while True:
                if self.workflow.dag.queue_input_jobs:
                    self.update_queue_input_jobs()
                # work around so that the wait does not prevent keyboard interrupts
                # while not self._open_jobs.acquire(False):
                #    time.sleep(1)
                self._open_jobs.acquire()

                # obtain needrun and running jobs in a thread-safe way
                with self._lock:
                    self._finish_jobs()
                    self._error_jobs()
                    needrun = set(self.open_jobs)
                    running = list(self.running)
                    errors = self._errors
                    executor_error = self._executor_error
                    user_kill = self._user_kill

                # handle errors
                if user_kill or (not self.keepgoing and errors) or executor_error:
                    if user_kill == "graceful":
                        logger.info(
                            "Will exit after finishing currently running jobs (scheduler)."
                        )

                    if executor_error:
                        print_exception(executor_error, self.workflow.linemaps)

                    if executor_error or not running:
                        logger.info("Shutting down, this might take some time.")
                        self._executor.shutdown()
                        if not user_kill:
                            logger.error(_ERROR_MSG_FINAL)
                            for job in self.failed:
                                job.log_error()
                        return False
                    continue

                # all runnable jobs have finished, normal shutdown
                if (
                    not needrun
                    and (
                        not running
                        or self.workflow.remote_execution_settings.immediate_submit
                    )
                    and not self.workflow.dag.has_unfinished_queue_input_jobs()
                ):
                    self._executor.shutdown()
                    if errors:
                        logger.error(_ERROR_MSG_FINAL)
                        for job in self.failed:
                            job.log_error()
                    # we still have unfinished jobs. this is not good. direct
                    # user to github issue
                    if self.remaining_jobs and not self.keepgoing:
                        logger.error(_ERROR_MSG_ISSUE_823)
                        logger.error(
                            "Remaining jobs:\n"
                            + "\n".join(
                                " - " + str(job) + ": " + ", ".join(job.output)
                                for job in self.remaining_jobs
                            )
                        )
                        return False
                    return not errors

                # continue if no new job needs to be executed
                if not needrun:
                    if self.workflow.dag.has_unfinished_queue_input_jobs():
                        logger.info("Waiting for queue input...")
                        # schedule a reevaluation in 10 seconds
                        self._schedule_reevalutation(
                            self.workflow.execution_settings.queue_input_wait_time
                        )
                    continue

                # select jobs by solving knapsack problem (omit with dryrun)
                if self.dryrun:
                    run = needrun
                else:
                    # Reset params and resources because they might still contain TBDs
                    # or old values from before files have been regenerated.
                    # Now, they can be recalculated as all input is present and up to date.
                    for job in needrun:
                        job.reset_params_and_resources()

                    logger.debug(f"Resources before job selection: {self.resources}")

                    # Subsample jobs to be run (to speedup solver)
                    n_total_needrun = len(needrun)
                    if self.subsample and n_total_needrun > self.subsample:
                        import random

                        needrun = set(random.sample(tuple(needrun), k=self.subsample))
                        logger.debug(
                            f"Ready subsampled jobs: {len(needrun)} (out of {n_total_needrun})"
                        )
                    else:
                        logger.debug(f"Ready jobs: {n_total_needrun}")

                    if not self._last_job_selection_empty:
                        logger.info("Select jobs to execute...")
                    run = self.job_selector(needrun)
                    self._last_job_selection_empty = not run

                    logger.debug(f"Selected jobs: {len(run)}")
                    logger.debug(f"Resources after job selection: {self.resources}")

                # update running jobs
                with self._lock:
                    self.running.update(run)
                    # remove from ready_jobs
                    self.workflow.dag.register_running(set(run))

                if run:
                    if not self.dryrun:
                        logger.info(
                            f"Execute {len(run)} jobs...",
                            extra=dict(
                                event=LogEvent.JOB_STARTED, jobs=[j.jobid for j in run]
                            ),
                        )

                    # actually run jobs
                    local_runjobs = [job for job in run if job.is_local]
                    runjobs = [job for job in run if not job.is_local]
                    if local_runjobs:
                        if (
                            not self.workflow.remote_exec
                            and not self.workflow.local_exec
                        ):
                            # Workflow uses a remote plugin and this scheduling run
                            # is on the main process. Hence, we have to download
                            # non-shared remote files for the local jobs.
                            async_run(
                                self.workflow.dag.retrieve_storage_inputs(
                                    jobs=local_runjobs, also_missing_internal=True
                                )
                            )
                        self.run(
                            local_runjobs,
                            executor=self._local_executor or self._executor,
                        )
                    if runjobs:
                        self.run(runjobs)
                if not self.dryrun:

                    if self._run_performed is None or self._run_performed:
                        if self.running:
                            logger.debug("Waiting for running jobs to complete.")
                        else:
                            logger.debug("Waiting for more resources.")
                    if self.job_rate_limiter is not None:
                        # need to reevaluate because after the timespan we can
                        # schedule more jobs again
                        self._schedule_reevalutation(self.job_rate_limiter.timespan)
        except (KeyboardInterrupt, SystemExit):
            logger.info(
                "Terminating processes on user request, this might take some time."
            )
            self._executor.cancel()
            return False
        except Exception as e:
            # Other exceptions should cause the executor to cancel the jobs
            # as well, so that no unmanaged jobs remain.
            self._executor.cancel()
            raise e

    def _schedule_reevalutation(self, delay: int) -> None:
        threading.Timer(
            delay,
            lambda: self._open_jobs.release(),
        ).start()

    def _finish_jobs(self):
        # must be called from within lock
        # clear the global tofinish such that parallel calls do not interfere
        async def postprocess():
            for job in self._tofinish:
                # IMPORTANT: inside of this loop, there may be no calls that have
                # a complexity of at least the number of jobs.
                # Otherwise the function would be quadratic in the number of jobs.
                if not self.workflow.dryrun:
                    try:
                        if self.workflow.exec_mode == ExecMode.DEFAULT:
                            await job.postprocess(
                                store_in_storage=not self.touch,
                                handle_log=True,
                                handle_touch=not self.touch,
                                ignore_missing_output=self.touch,
                            )
                        elif self.workflow.exec_mode == ExecMode.SUBPROCESS:
                            await job.postprocess(
                                store_in_storage=False,
                                handle_log=True,
                                handle_touch=True,
                            )
                        else:
                            # remote job execution
                            await job.postprocess(
                                # storage upload will be done after all jobs of
                                # this remote job (e.g. in case of group) are finished
                                # DAG.store_storage_outputs()
                                store_in_storage=False,
                                handle_log=True,
                                handle_touch=True,
                            )
                    except (RuleException, WorkflowError) as e:
                        # if an error occurs while processing job output,
                        # we do the same as in case of errors during execution
                        print_exception(e, self.workflow.linemaps)
                        await job.postprocess(error=True)
                        self._handle_error(job, postprocess_job=False)
                        continue

                if self.handle_job_success:
                    self.get_executor(job).handle_job_success(job)

                if self.update_resources:
                    # normal jobs have len=1, group jobs have len>1
                    self.finished_jobs += len(job)
                    self.running.remove(job)
                    self._free_resources(job)

                if self.print_progress:
                    if job.is_group():
                        for j in job:
                            logger.info(
                                f"Finished jobid: {j.jobid} (Rule: {j.rule.name})",
                                extra=dict(event=LogEvent.JOB_FINISHED, job_id=j.jobid),
                            )
                    else:
                        logger.info(
                            f"Finished jobid: {job.jobid} (Rule: {job.rule.name})",
                            extra=dict(event=LogEvent.JOB_FINISHED, job_id=job.jobid),
                        )
                    self.progress()

                await self.workflow.dag.finish(
                    job,
                    update_checkpoint_dependencies=self.update_checkpoint_dependencies,
                )

        async_run(postprocess())
        self._tofinish.clear()

    def update_queue_input_jobs(self):
        currtime = time.time()
        if currtime - self._last_update_queue_input_jobs >= 10:
            self._last_update_queue_input_jobs = currtime
            async_run(self.workflow.dag.update_queue_input_jobs())

    def _error_jobs(self):
        # must be called from within lock
        for job in self._toerror:
            self._handle_error(job)
        self._toerror.clear()

    def run(self, jobs, executor=None):
        self._run_performed = True
        if executor is None:
            executor = self._executor
        executor.run_jobs(jobs)

    def get_executor(self, job):
        if job.is_local and self._local_executor is not None:
            return self._local_executor
        return self._executor

    def _noop(self, job):
        pass

    def _free_resources(self, job):
        for name, value in job.scheduler_resources.items():
            if name in self.resources and name != "_job_count":
                self.resources[name] += value

    def _proceed(self, job):
        """Do stuff after job is finished."""
        with self._lock:
            self._tofinish.append(job)

            if self.dryrun:
                if len(self.running) - len(self._tofinish) - len(self._toerror) <= 0:
                    # During dryrun, only release when all running jobs are done.
                    # This saves a lot of time, as self.open_jobs has to be
                    # evaluated less frequently.
                    self._open_jobs.release()
            else:
                # go on scheduling if there is any free core
                self._open_jobs.release()

    def error_callback(self, job):
        with self._lock:
            self._toerror.append(job)
            self._open_jobs.release()

    def _handle_error(self, job, postprocess_job: bool = True):
        """Clear jobs and stop the workflow.

        If Snakemake is configured to restart jobs then the job might have
        "restart_times" left and we just decrement and let the scheduler
        try to run the job again.
        """
        # must be called from within lock
        if postprocess_job and not self.workflow.dryrun:

            async_run(
                job.postprocess(
                    error=True,
                )
            )
        self.get_executor(job).handle_job_error(job)
        self.running.remove(job)
        self._free_resources(job)
        # attempt starts counting from 1, but the first attempt is not
        # a restart, hence we subtract 1.
        if job.restart_times > job.attempt - 1:
            logger.info(f"Trying to restart job {self.workflow.dag.jobid(job)}.")
            job.attempt += 1
            # add job to those being ready again
            self.workflow.dag._ready_jobs.add(job)
        else:
            self._errors = True
            self.failed.add(job)

    def exit_gracefully(self, *args):
        with self._lock:
            self._user_kill = "graceful"
        self._open_jobs.release()

    def job_selector(self, jobs):
        for job in jobs:
            self.validate_job(job)

        async_run(self.update_input_sizes(jobs))

        def run_selector(job_selector) -> Sequence[AbstractJob]:
            with self._lock:
                if self.resources["_cores"] == 0:
                    return []
                if len(jobs) == 1:
                    return self.job_selector_greedy(
                        jobs, self.remaining_jobs, self.resources, self._input_sizes
                    )
                selected = job_selector(
                    jobs, self.remaining_jobs, self.resources, self._input_sizes
                )
                if selected is None:
                    selected = self.job_selector_greedy(
                        jobs, self.remaining_jobs, self.resources, self._input_sizes
                    )
                self.update_available_resources(selected)
            return selected

        # get number of free jobs to submit
        if self.job_rate_limiter is None:
            # ensure that the job count is not restricted
            assert (
                self.resources["_job_count"] == sys.maxsize
            ), f"Job count is {self.resources['_job_count']}, but should be {sys.maxsize}"
            return run_selector(self._job_selector)
        n_free_jobs = self.job_rate_limiter.get_free_jobs()
        if n_free_jobs == 0:
            logger.info("Job rate limit reached, waiting for free slots.")
            return set()
        else:
            self.resources["_job_count"] = n_free_jobs
            selected = run_selector(self._job_selector)
            # update job rate limiter
            self.job_rate_limiter.register_jobs(len(selected))
            return selected

    def update_available_resources(self, selected_jobs):
        for name in self.global_resources:
            # _job_count is updated per JobRateLimiter before scheduling
            if name != "_job_count":
                self.resources[name] -= sum(
                    [job.scheduler_resources.get(name, 0) for job in selected_jobs]
                )

    def validate_job(self, job):
        if job in self._validated_jobs:
            return
        self._validated_jobs.add(job)

        for name, available in self.global_resources.items():
            assert isinstance(available, int), (
                f"Global resource {name}={available} is not an integer. "
                "This is likely a bug in Snakemake."
            )
            if isinstance(available, str):
                continue
            value = job.scheduler_resources.get(name, 0)
            if isinstance(value, str):
                raise WorkflowError(
                    f"Resource {name}={value} of rule {job.rule.name} is not an "
                    "integer but global resource is defined as integer."
                )
            if value > available:
                if name == "_cores":
                    name = "threads"
                raise WorkflowError(
                    f"Job needs {name}={value} but only {name}={available} "
                    "are available. This is likely because two "
                    "jobs are connected via a pipe or a service output and have to run "
                    "simultaneously. Consider providing more "
                    "resources (e.g. via --cores)."
                )

    def progress(self):
        """Display the progress."""
        logger.info(
            None,
            extra=dict(
                event=LogEvent.PROGRESS,
                done=self.finished_jobs,
                total=len(self.workflow.dag),
            ),
        )


class JobRateLimiter:
    def __init__(self, limit: MaxJobsPerTimespan):
        self._limit: MaxJobsPerTimespan = limit
        self._jobs = deque()
        logger.debug(
            f"Submitting maximum {self._limit.max_jobs} job(s) over {self._limit.timespan} second(s)."
        )

    @property
    def max_jobs(self) -> int:
        return self._limit.max_jobs

    @property
    def timespan(self) -> int:
        return self._limit.timespan

    def register_jobs(self, n_jobs: int):
        currtime = time.time()
        self._jobs.extend(repeat(currtime, n_jobs))

    def get_free_jobs(self):
        # get the index of the last element that is older than the timespan
        index = bisect(self._jobs, time.time() - self.timespan)
        # remove the first index elements from the deque
        for _ in range(index):
            self._jobs.popleft()
        n_free = max(self.max_jobs - len(self._jobs), 0)
        return n_free



================================================
FILE: src/snakemake/scheduling/milp.py
================================================
from dataclasses import dataclass, field
import math
import os
from pathlib import Path
from typing import Dict, Mapping, Optional, Sequence, Union
from snakemake_interface_scheduler_plugins.base import SchedulerBase
from snakemake_interface_scheduler_plugins.settings import SchedulerSettingsBase
from snakemake_interface_scheduler_plugins.interfaces.jobs import JobSchedulerInterface
from snakemake_interface_common.io import AnnotatedStringInterface


try:
    import pulp

    lp_solvers = pulp.listSolvers(onlyAvailable=True)
except Exception:
    # Default list for the case that pulp is not available
    lp_solvers = ["COIN_CMD"]


@dataclass
class SchedulerSettings(SchedulerSettingsBase):
    solver: Optional[str] = field(
        default="COIN_CMD",
        metadata={
            "help": "Set MILP solver to use",
            "choices": lp_solvers,
        },
    )
    solver_path: Optional[Path] = field(
        default=None,
        metadata={"help": "Set the PATH to search for scheduler solver binaries."},
    )


class Scheduler(SchedulerBase):
    def select_jobs(
        self,
        selectable_jobs: Sequence[JobSchedulerInterface],
        remaining_jobs: Sequence[JobSchedulerInterface],
        available_resources: Mapping[str, Union[int, str]],
        input_sizes: Dict[AnnotatedStringInterface, int],
    ) -> Sequence[JobSchedulerInterface]:
        import pulp
        from pulp import lpSum

        scheduled_jobs = {
            job: pulp.LpVariable(
                f"job_{idx}", lowBound=0, upBound=1, cat=pulp.LpInteger
            )
            for idx, job in enumerate(selectable_jobs)
        }

        job_temp_files = {}
        for job in remaining_jobs:
            job_temp_files[job] = {
                infile for infile in job.input if infile.is_flagged("temp")
            }

        temp_files = {
            f for job in selectable_jobs for f in job.input if f.is_flagged("temp")
        }

        temp_sizes_gb = {f: input_sizes[f] / 1e9 for f in temp_files}

        temp_job_improvement = {
            temp_file: pulp.LpVariable(
                f"temp_file_{idx}", lowBound=0, upBound=1, cat="Continuous"
            )
            for idx, temp_file in enumerate(temp_files)
        }

        temp_file_deletable = {
            temp_file: pulp.LpVariable(
                f"deletable_{idx}",
                lowBound=0,
                upBound=1,
                cat=pulp.LpInteger,
            )
            for idx, temp_file in enumerate(temp_files)
        }
        prob = pulp.LpProblem("JobScheduler", pulp.LpMaximize)

        total_temp_size = max(
            sum([temp_sizes_gb[temp_file] for temp_file in temp_files]), 1
        )
        total_core_requirement = sum(
            [
                max(job.scheduler_resources.get("_cores", 1), 1)
                for job in selectable_jobs
            ]
        )
        # Objective function
        # Job priority > Core load
        # Core load > temp file removal
        # Instant removal > temp size
        prob += (
            2
            * total_core_requirement
            * 2
            * total_temp_size
            * lpSum([job.priority * scheduled_jobs[job] for job in selectable_jobs])
            + 2
            * total_temp_size
            * lpSum(
                [
                    max(job.scheduler_resources.get("_cores", 1), 1)
                    * scheduled_jobs[job]
                    for job in selectable_jobs
                ]
            )
            + total_temp_size
            * lpSum(
                [
                    temp_file_deletable[temp_file] * temp_sizes_gb[temp_file]
                    for temp_file in temp_files
                ]
            )
            + lpSum(
                [
                    temp_job_improvement[temp_file] * temp_sizes_gb[temp_file]
                    for temp_file in temp_files
                ]
            )
        )

        # Constraints:
        for name in available_resources:
            prob += (
                lpSum(
                    [
                        scheduled_jobs[job] * job.scheduler_resources.get(name, 0)
                        for job in selectable_jobs
                    ]
                )
                <= available_resources[name]
            )

        # Choose jobs that lead to "fastest" (minimum steps) removal of existing temp file
        for temp_file in temp_files:
            prob += temp_job_improvement[temp_file] <= lpSum(
                [
                    scheduled_jobs[job]
                    for job in selectable_jobs
                    if temp_file in job_temp_files[job]
                ]
            ) / lpSum([1 for job in remaining_jobs if temp_file in job_temp_files[job]])

            prob += temp_file_deletable[temp_file] <= temp_job_improvement[temp_file]

        status = self._solve_ilp(prob, time_limit=10)
        if pulp.LpStatus[status] != "Optimal":
            if pulp.LpStatus[status] == "Not Solved":
                self.logger.warning(
                    "Failed to solve scheduling problem with ILP solver in time (10s)."
                )
            elif pulp.LpStatus[status] == "Infeasible":
                self.logger.warning(
                    "Failed to solve scheduling problem with ILP solver."
                )
            return None

        selected_jobs = [
            job
            for job, variable in scheduled_jobs.items()
            if math.isclose(variable.value(), 1.0)
        ]

        if not selected_jobs:
            # No selected jobs. This could be due to insufficient resources or a failure in the ILP solver
            # Hence, we silently fall back to the greedy solver to make sure that we don't miss anything.
            return None

        return selected_jobs

    def _solve_ilp(self, prob, threads=2, time_limit=10):
        import pulp

        old_path = os.environ["PATH"]
        if self.settings.solver_path is not None:
            # Temporarily prepend the given snakemake env to the path, such that the solver can be found in any case.
            # This is needed for cluster envs, where the cluster job might have a different environment but
            # still needs access to the solver binary.
            os.environ["PATH"] = "{}:{}".format(
                self.settings.solver_path,
                os.environ["PATH"],
            )
        try:
            solver = (
                pulp.getSolver(self.settings.solver)
                if self.settings.solver
                else pulp.apis.LpSolverDefault
            )
        finally:
            os.environ["PATH"] = old_path
        solver.optionsDict["threads"] = threads
        solver.timeLimit = time_limit
        solver.msg = False  # Suppress solver output
        return prob.solve(solver)



================================================
FILE: src/snakemake/settings/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/settings/enums.py
================================================
from typing import List
from snakemake_interface_common.settings import SettingsEnumBase


class RerunTrigger(SettingsEnumBase):
    MTIME = 0
    PARAMS = 1
    INPUT = 2
    SOFTWARE_ENV = 3
    CODE = 4


class ChangeType(SettingsEnumBase):
    CODE = 0
    INPUT = 1
    PARAMS = 2


class CondaCleanupPkgs(SettingsEnumBase):
    TARBALLS = 0
    CACHE = 1


class Quietness(SettingsEnumBase):
    RULES = 0
    PROGRESS = 1
    ALL = 2
    HOST = 3
    REASON = 4


class StrictDagEvaluation(SettingsEnumBase):
    FUNCTIONS = 0
    CYCLIC_GRAPH = 1
    PERIODIC_WILDCARDS = 2


class PrintDag(SettingsEnumBase):
    DOT = 0
    MERMAID_JS = 1



================================================
FILE: src/snakemake/settings/types.py
================================================
from abc import ABC
from dataclasses import dataclass, field
import os
from pathlib import Path
import re
from typing import Any, Optional, Union
from typing import Mapping, Sequence, Set

import immutables

from snakemake.common.typing import AnySet
from snakemake_interface_common.exceptions import ApiError
from snakemake_interface_executor_plugins.settings import (
    RemoteExecutionSettingsExecutorInterface,
    DeploymentSettingsExecutorInterface,
    ExecutionSettingsExecutorInterface,
    StorageSettingsExecutorInterface,
    DeploymentMethod,
    ExecMode,
    SharedFSUsage,
)
from snakemake_interface_logger_plugins.settings import (
    LogHandlerSettingsBase,
    OutputSettingsLoggerInterface,
)

from snakemake.common import (
    dict_to_key_value_args,
    expand_vars_and_user,
    get_container_image,
)
from snakemake.common.configfile import load_configfile
from snakemake.resources import DefaultResources
from snakemake.utils import update_config
from snakemake.exceptions import WorkflowError
from snakemake.settings.enums import (
    RerunTrigger,
    ChangeType,
    CondaCleanupPkgs,
    Quietness,
    StrictDagEvaluation,
    PrintDag,
)


class SettingsBase(ABC):
    def __post_init__(self):
        self._check()

    def _check(self):
        # by default, nothing to check
        # override this method in subclasses if needed
        pass


class NotebookEditMode:
    def __init__(self, server_addr: Optional[str] = None, draft_only: bool = False):
        if server_addr is not None:
            self.ip, self.port = server_addr.split(":")
        self.draft_only = draft_only


class MaxJobsPerTimespan:
    arg_re = re.compile(r"(?P<count>\d+)/(?P<timespan>\d+(h|m|s|ms|w|d))")

    def __init__(self, max_jobs: int, timespan: Optional[str] = None):
        import humanfriendly

        self.max_jobs = max_jobs
        if timespan is not None:
            self.timespan = humanfriendly.parse_timespan(timespan)
        else:
            self.timespan = 1

    @classmethod
    def parse_choice(cls, arg: str):
        m = cls.arg_re.match(arg)
        if m is None:
            raise WorkflowError(
                "Invalid max jobs per timespan definition. "
                "Must be of the form <max_jobs>/<timespan> with <max_jobs> being an "
                "integer, and <timespan> being an integer with "
                f"unit h, m, s ms, w, d. Given instead: {arg}"
            )
        max_jobs, timespan = m.group("count"), m.group("timespan")
        max_jobs = int(max_jobs)
        return cls(max_jobs, timespan=timespan)


@dataclass
class ExecutionSettings(SettingsBase, ExecutionSettingsExecutorInterface):
    """
    Parameters
    ----------

    batch:
        whether to compute only a partial DAG, defined by the given Batch object
    cache:
        list of rules to cache
    cores:
        the number of provided cores (ignored when using cluster/cloud support)
    nodes:
        the number of provided cluster nodes (ignored without cluster/cloud support)
    local_cores:
        the number of provided local cores if in cluster mode (ignored without cluster/cloud support)
    """

    latency_wait: int = 3
    keep_going: bool = False
    debug: bool = False
    standalone: bool = False
    ignore_ambiguity: bool = False
    lock: bool = True
    ignore_incomplete: bool = False
    wait_for_files: Sequence[str] = tuple()
    no_hooks: bool = False
    retries: int = 0
    attempt: int = 1
    use_threads: bool = False
    shadow_prefix: Optional[Path] = None
    keep_incomplete: bool = False
    keep_metadata: bool = True
    edit_notebook: Optional[NotebookEditMode] = None
    cleanup_scripts: bool = True
    queue_input_wait_time: int = 10


@dataclass
class WorkflowSettings(SettingsBase):
    wrapper_prefix: Optional[str] = None
    exec_mode: ExecMode = ExecMode.DEFAULT
    cache: Optional[Sequence[str]] = None
    consider_ancient: Mapping[str, AnySet[Union[str, int]]] = field(
        default_factory=dict
    )


class Batch:
    """Definition of a batch for calculating only a partial DAG."""

    def __init__(self, rulename: str, idx: int, batches: int):
        assert idx <= batches
        assert idx > 0
        self.rulename = rulename
        self.idx = idx
        self.batches = batches

    def get_batch(self, items: list):
        """Return the defined batch of the given items.
        Items are usually input files."""
        # make sure that we always consider items in the same order
        if len(items) < self.batches:
            raise WorkflowError(
                "Batching rule {} has less input files than batches. "
                "Please choose a smaller number of batches.".format(self.rulename)
            )
        items = sorted(items)

        # we can equally split items using divmod:
        # len(items) = (self.batches * quotient) + remainder
        # Because remainder always < divisor (self.batches),
        # each batch will be equal to quotient + (1 or 0 item)
        # from the remainder
        k, m = divmod(len(items), self.batches)

        # self.batch is one-based, hence we have to subtract 1
        idx = self.idx - 1

        # First n batches will have k (quotient) items +
        # one item from the remainder (m). Once we consume all items
        # from the remainder, last batches only contain k items.
        i = idx * k + min(idx, m)
        batch_len = (idx + 1) * k + min(idx + 1, m)

        if self.is_final:
            # extend the last batch to cover rest of list
            return items[i:]
        else:
            return items[i:batch_len]

    @property
    def is_final(self):
        return self.idx == self.batches

    def __str__(self):
        return f"{self.idx}/{self.batches} (rule {self.rulename})"

    def __eq__(self, other):
        return (
            self.rulename == other.rulename
            and self.idx == other.idx
            and self.batches == other.batches
        )


@dataclass
class DAGSettings(SettingsBase):
    targets: AnySet[str] = frozenset()
    target_jobs: AnySet[str] = frozenset()
    target_files_omit_workdir_adjustment: bool = False
    batch: Optional[Batch] = None
    forcetargets: bool = False
    forceall: bool = False
    forcerun: AnySet[str] = frozenset()
    until: AnySet[str] = frozenset()
    omit_from: AnySet[str] = frozenset()
    force_incomplete: bool = False
    allowed_rules: AnySet[str] = frozenset()
    rerun_triggers: AnySet[RerunTrigger] = RerunTrigger.all()
    max_inventory_wait_time: int = 20
    trust_io_cache: bool = False
    max_checksum_file_size: int = 1000000
    strict_evaluation: AnySet[StrictDagEvaluation] = frozenset()
    print_dag_as: PrintDag = PrintDag.DOT
    # strict_functions_evaluation: bool = False
    # strict_cycle_evaluation: bool = False
    # strict_wildcards_recursion_evaluation: bool = False

    def _check(self):
        if self.batch is not None and self.forceall:
            raise WorkflowError(
                "--batch may not be combined with --forceall, because recomputed upstream "
                "jobs in subsequent batches may render already obtained results outdated."
            )


@dataclass
class StorageSettings(SettingsBase, StorageSettingsExecutorInterface):
    default_storage_provider: Optional[str] = None
    default_storage_prefix: Optional[str] = None
    shared_fs_usage: AnySet[SharedFSUsage] = SharedFSUsage.all()
    keep_storage_local: bool = False
    retrieve_storage: bool = True
    local_storage_prefix: Path = Path(".snakemake/storage")
    remote_job_local_storage_prefix: Optional[Path] = None
    omit_flags: AnySet[str] = frozenset()
    notemp: bool = False
    all_temp: bool = False
    unneeded_temp_files: AnySet[str] = frozenset()
    wait_for_free_local_storage: Optional[int] = None

    def __post_init__(self):
        if self.remote_job_local_storage_prefix is None:
            self.remote_job_local_storage_prefix = self.local_storage_prefix


@dataclass
class DeploymentSettings(SettingsBase, DeploymentSettingsExecutorInterface):
    """
    Parameters
    ----------

    deployment_method
        deployment method to use (CONDA, APPTAINER, ENV_MODULES)
    conda_prefix:
        the directory in which conda environments will be created (default None)
    conda_cleanup_pkgs:
        whether to clean up conda tarballs after env creation (default None), valid values: "tarballs", "cache"
    conda_create_envs_only:
        if specified, only builds the conda environments specified for each job, then exits.
    list_conda_envs:
        list conda environments and their location on disk.
    conda_base_path:
        Path to conda base environment (this can be used to overwrite the search path for conda, mamba, and activate).
    """

    deployment_method: AnySet[DeploymentMethod] = frozenset()
    conda_prefix: Optional[Path] = None
    conda_cleanup_pkgs: Optional[CondaCleanupPkgs] = None
    conda_base_path: Optional[Path] = None
    conda_frontend: str = "conda"
    conda_not_block_search_path_envvars: bool = False
    apptainer_args: str = ""
    apptainer_prefix: Optional[Path] = None

    def imply_deployment_method(self, method: DeploymentMethod):
        self.deployment_method = set(self.deployment_method)
        self.deployment_method.add(method)

    def __post_init__(self):
        from snakemake.logging import logger

        if self.apptainer_prefix is None:
            self.apptainer_prefix = os.environ.get("APPTAINER_CACHEDIR", None)
        self.apptainer_prefix = expand_vars_and_user(self.apptainer_prefix)
        self.conda_prefix = expand_vars_and_user(self.conda_prefix)
        if self.conda_frontend != "conda":
            logger.warning(
                "Support for alternative conda frontends has been deprecated in "
                "favor of simpler support and code base. "
                "This should not cause issues since current conda releases rely on "
                "fast solving via libmamba. "
                f"Ignoring the alternative conda frontend setting ({self.conda_frontend})."
            )
            self.conda_frontend = "conda"


@dataclass
class SchedulingSettings(SettingsBase):
    """
    Parameters
    ----------

    prioritytargets:
        list of targets that shall be run with maximum priority (default [])
    scheduler:
        Select scheduling algorithm (default ilp, allowed: ilp, greedy or any scheduler plugin name).
    ilp_solver:
        Set solver for ilp scheduler. deprecated, use scheduler_settings instead
    solver_path:
        Set the PATH to search for scheduler solver binaries. deprecated, use scheduler_settings instead
    greediness:
        Set the greediness of scheduling. This value, between 0 and 1, determines how careful jobs are selected for execution. The default value (0.5 if prioritytargets are used, 1.0 else) provides the best speed and still acceptable scheduling quality.
        Deprecated, use snakemake.scheduling.greedy.Settings instead and pass it as greedy_scheduler_settings to DAGApi.execute_workflow().
    subsample:
        Set the number of jobs to be considered for scheduling. If number of ready jobs is greater than this value, this number of jobs is randomly chosen for scheduling; if number of ready jobs is lower, this option has no effect. This can be useful on very large DAGs, where the scheduler can take some time selecting which jobs to run."
    """

    prioritytargets: AnySet[str] = frozenset()
    scheduler: str = "ilp"
    ilp_solver: Optional[str] = None  # deprecated, use scheduler_settings instead
    solver_path: Optional[Path] = None  # deprecated, use scheduler_settings instead
    greediness: Optional[float] = (
        None  # deprecated, use greedy_scheduler_settings instead
    )
    subsample: Optional[int] = None
    max_jobs_per_second: Optional[int] = None
    max_jobs_per_timespan: Optional[MaxJobsPerTimespan] = None

    def __post_init__(self):
        self.greediness = self._get_greediness()
        if self.max_jobs_per_second is not None and self.max_jobs_per_timespan is None:
            self.max_jobs_per_timespan = MaxJobsPerTimespan(self.max_jobs_per_second)

    def _get_greediness(self):
        if self.greediness is None:
            return 0.5 if self.prioritytargets else 1.0
        else:
            return self.greediness

    def _check(self):
        if self.subsample:
            if not isinstance(self.subsample, int) or self.subsample < 1:
                raise ApiError("subsample must be a positive integer")


@dataclass
class ResourceSettings(SettingsBase):
    cores: Optional[int] = None
    nodes: Optional[int] = None
    local_cores: Optional[int] = None
    max_threads: Optional[int] = None
    resources: Mapping[str, int] = immutables.Map()
    overwrite_threads: Mapping[str, int] = immutables.Map()
    overwrite_scatter: Mapping[str, int] = immutables.Map()
    overwrite_resource_scopes: Mapping[str, str] = immutables.Map()
    overwrite_resources: Mapping[str, Mapping[str, Any]] = immutables.Map()
    default_resources: Optional[DefaultResources] = None

    def __post_init__(self):
        if self.default_resources is None:
            self.default_resources = DefaultResources(mode="bare")


@dataclass
class ConfigSettings(SettingsBase):
    config: Mapping[str, str] = immutables.Map()
    configfiles: Sequence[Path] = tuple()
    config_args: Optional[str] = None
    replace_workflow_config: bool = False

    def __post_init__(self):
        self.overwrite_config = self._get_overwrite_config()
        self.configfiles = self._get_configfiles()
        self.config_args = self._get_config_args()

    def _get_overwrite_config(self):
        overwrite_config = dict()
        if self.configfiles:
            for f in self.configfiles:
                update_config(overwrite_config, load_configfile(f))
        if self.config:
            update_config(overwrite_config, self.config)
        return overwrite_config

    def _get_configfiles(self):
        return list(map(Path.absolute, self.configfiles))

    def _get_config_args(self):
        if self.config_args is None:
            return dict_to_key_value_args(self.config, repr_obj=True)
        else:
            return self.config_args


@dataclass
class OutputSettings(SettingsBase, OutputSettingsLoggerInterface):
    dryrun: bool = False
    printshellcmds: bool = False
    nocolor: bool = False
    quiet: Optional[AnySet[Quietness]] = None
    debug_dag: bool = False
    verbose: bool = False
    show_failed_logs: bool = False
    log_handler_settings: Mapping[str, LogHandlerSettingsBase] = immutables.Map()
    keep_logger: bool = False
    stdout: bool = False
    benchmark_extended: bool = False


@dataclass
class PreemptibleRules:
    rules: AnySet[str] = frozenset()
    all: bool = False

    def is_preemptible(self, rulename: str):
        return self.all or rulename in self.rules


@dataclass
class RemoteExecutionSettings(SettingsBase, RemoteExecutionSettingsExecutorInterface):
    jobname: str = "snakejob.{rulename}.{jobid}.sh"
    jobscript: Optional[Path] = None
    max_status_checks_per_second: float = 100.0
    seconds_between_status_checks: int = 10
    container_image: str = get_container_image()
    preemptible_retries: Optional[int] = None
    preemptible_rules: PreemptibleRules = field(default_factory=PreemptibleRules)
    envvars: Sequence[str] = tuple()
    immediate_submit: bool = False
    precommand: Optional[str] = None
    job_deploy_sources: bool = True


@dataclass
class GroupSettings(SettingsBase):
    overwrite_groups: Mapping[str, str] = immutables.Map()
    group_components: Mapping[str, int] = immutables.Map()
    local_groupid: str = "local"


@dataclass
class GlobalReportSettings(SettingsBase):
    """Global settings that apply to all report plugins."""

    metadata_template: Optional[Path] = None



================================================
FILE: src/snakemake/template_rendering/__init__.py
================================================
from abc import ABC, abstractmethod

from snakemake.exceptions import WorkflowError


class TemplateRenderer(ABC):
    def __init__(self, input, output, params, wildcards, config):
        if len(output) != 1:
            raise ValueError(
                "More than one output file specified for template_engine rule."
            )
        if len(input) != 1:
            if "template" not in input.keys():
                raise ValueError(
                    "More than one input file specified for template engine rule, but no "
                    "input file named as 'template'."
                )
            else:
                self.input_file = input.template
        else:
            self.input_file = input[0]

        self.input = input
        self.output_file = output[0]
        self.params = params
        self.wildcards = wildcards
        self.config = config

    @property
    def variables(self):
        return {
            "params": self.params,
            "wildcards": self.wildcards,
            "config": self.config,
            "input": self.input,
        }

    @abstractmethod
    def render(self): ...


def render_template(engine, input, output, params, wildcards, config, rule):
    try:
        if engine == "yte":
            from snakemake.template_rendering.yte import YteRenderer

            return YteRenderer(input, output, params, wildcards, config).render()
        elif engine == "jinja2":
            from snakemake.template_rendering.jinja2 import Jinja2Renderer

            return Jinja2Renderer(input, output, params, wildcards, config).render()
        else:
            raise WorkflowError(
                f"Unsupported template engine {engine} in rule {rule}. "
                "So far, only yte and jinja2 are supported."
            )
    except Exception as e:
        raise WorkflowError(f"Error rendering template in rule {rule}.", e)


def check_template_output(job):
    with open(job.output[0]) as out:
        for l in out:
            for f in job.input:
                if f.is_storage and f in l:
                    raise WorkflowError(
                        "Output of template_engine rule contains local path to input file "
                        f"from storage: {f} for {f.storage_object.print_query}. "
                        "However, this path is variable as it can change between runs (e.g. when "
                        "the storage local prefix is modified). To circumvent this issue, place the "
                        "rule in one group with the consumer(s) and mark the output as temp()."
                    )



================================================
FILE: src/snakemake/template_rendering/jinja2.py
================================================
from snakemake.exceptions import WorkflowError
from snakemake.template_rendering import TemplateRenderer


class Jinja2Renderer(TemplateRenderer):
    def render(self):
        import jinja2

        try:
            with open(self.input_file, "r") as infile:
                template = jinja2.Template(infile.read())
            with open(self.output_file, "w") as outfile:
                outfile.write(template.render(**self.variables))
        except Exception as e:
            raise WorkflowError("Failed to render jinja2 template.", e)



================================================
FILE: src/snakemake/template_rendering/yte.py
================================================
from snakemake.exceptions import WorkflowError
from snakemake.template_rendering import TemplateRenderer


class YteRenderer(TemplateRenderer):
    def render(self):
        import yte

        try:
            with (
                open(self.output_file, "w") as outfile,
                open(self.input_file, "r") as infile,
            ):
                yte.process_yaml(infile, outfile=outfile, variables=self.variables)
        except Exception as e:
            raise WorkflowError("Failed to render yte template.", e)



================================================
FILE: src/snakemake/unit_tests/__init__.py
================================================
from itertools import groupby
from pathlib import Path
import shutil
from snakemake.common import async_run

from snakemake.logging import logger
from snakemake import __version__
from snakemake.exceptions import WorkflowError


class RuleTest:
    def __init__(self, job, basedir):
        self.name = job.rule.name
        self.output = job.output
        self.path = basedir / self.name

    @property
    def target(self):
        return self.output or self.name

    @property
    def config_path(self):
        return self.path / "config"

    @property
    def data_path(self):
        return self.path / "data"

    @property
    def expected_path(self):
        return self.path / "expected"


def generate(
    dag, path: Path, deploy=None, snakefile=None, configfiles=None, rundir=None
):
    """Generate unit tests from given dag at a given path."""
    logger.info("Generating unit tests for each rule...")

    def copy_files(files, path):
        for f in set(files):
            f = Path(f)
            parent = f.parent
            if parent.is_absolute():
                root = str(f.parents[len(f.parents) - 1])
                parent = str(parent)[len(root) :]
            target = path / parent
            if f.is_dir():
                shutil.copytree(f, target / f.name)
            else:
                target.mkdir(parents=True, exist_ok=True)
                shutil.copy(f, target)
                (target / f.name).chmod(0o444)
        if not files:
            path.mkdir(parents=True, exist_ok=True)
            # touch gitempty file if there are no input files
            open(path / ".gitempty", "w").close()

    try:
        from jinja2 import Environment, PackageLoader
    except ImportError:
        raise WorkflowError(
            "Python package jinja2 must be installed to create reports."
        )

    env = Environment(
        loader=PackageLoader("snakemake", "unit_tests/templates"),
        trim_blocks=True,
        lstrip_blocks=True,
    )

    path.mkdir(parents=True, exist_ok=True)

    with open(path / "common.py", "w") as common:
        print(
            env.get_template("common.py.jinja2").render(version=__version__),
            file=common,
        )

    with open(path / "conftest.py", "w") as conftest:
        print(
            env.get_template("conftest.py.jinja2").render(version=__version__),
            file=conftest,
        )

    for rulename, jobs in groupby(dag.jobs, key=lambda job: job.rule.name):
        jobs = list(jobs)
        if jobs[0].rule.norun:
            logger.info(
                f"Skipping rule {rulename} because it does not execute anything."
            )
            continue

        testpath = path / f"test_{rulename}.py"

        if testpath.exists():
            logger.info(
                f"Skipping rule {rulename} as a unit test already exists for it: {testpath}."
            )
            continue

        written = False
        for job in jobs:
            if all(async_run(f.exists()) for f in job.input):
                logger.info(f"Generating unit test for rule {rulename}: {testpath}.")
                (path / rulename).mkdir(parents=True, exist_ok=True)

                copy_files(list(Path().glob("config*")), path / rulename / "config")
                copy_files(job.input, path / rulename / "data")
                copy_files(job.output, path / rulename / "expected")

                with open(testpath, "w") as test:
                    print(
                        env.get_template("ruletest.py.jinja2").render(
                            version=__version__,
                            ruletest=RuleTest(job, path.absolute().relative_to(rundir)),
                            deploy=deploy,
                            snakefile=Path(snakefile).absolute().relative_to(rundir),
                            configfiles=[
                                Path(config).absolute().relative_to(rundir)
                                for config in configfiles
                            ],
                        ),
                        file=test,
                    )

                written = True
                break

        if not written:
            logger.warning(
                "No job with all input files present for rule {}. "
                "Consider re-executing the workflow with --notemp in order "
                "have all input files present before generating unit tests."
            )



================================================
FILE: src/snakemake/unit_tests/templates/__init__.py
================================================
[Empty file]


================================================
FILE: src/snakemake/unit_tests/templates/common.py.jinja2
================================================
"""
Common code for unit testing of rules generated with Snakemake {{ version }}.
"""

import os
from pathlib import Path
from subprocess import check_output


class OutputChecker:
    def __init__(self, data_path, expected_path, workdir):
        self.data_path = data_path
        self.expected_path = expected_path
        self.workdir = workdir

    def check(self):
        # Input files
        input_files = set(
            (Path(path) / f).relative_to(self.data_path)
            for path, subdirs, files in os.walk(self.data_path)
            for f in files
        )
        print(f"input: {input_files}")  # DEBUG
        # Workdir files
        workdir_files = set(
            (Path(path) / f).relative_to(self.workdir)
            for path, subdirs, files in os.walk(self.workdir)
            for f in files
        )
        print(f"workdir: {workdir_files}")  # DEBUG
        # Expected files
        expected_files = set(
            (Path(path) / f).relative_to(self.expected_path)
            for path, subdirs, files in os.walk(self.expected_path)
            for f in files
        )
        print(f"expected: {expected_files}")  # DEBUG

        assert expected_files.issubset(
            workdir_files
        ), f"Output files missing: {expected_files - workdir_files}"

        # Compare output and expected files
        for f in expected_files:
            self.compare_files(self.expected_path / f, self.workdir / f)

    def compare_files(self, expected_file, generated_file):
        if expected_file.suffix == ".gz":
            cmp = "zcmp"
        elif expected_file.suffix == ".bz2":
            cmp = "bzcmp"
        elif expected_file.suffix == ".xz":
            cmp = "xzcmp"
        else:
            cmp = "cmp"

        check_output([cmp, expected_file, generated_file])



================================================
FILE: src/snakemake/unit_tests/templates/conftest.py.jinja2
================================================
"""
conftest.py for unit testing of rules generated with Snakemake {{ version }}.
"""

from pytest import fixture


def pytest_addoption(parser):
    parser.addoption("--conda-prefix", action="store", default=None)


@fixture()
def conda_prefix(request):
    conda_prefix = request.config.option.conda_prefix
    if conda_prefix:
        return ["--conda-prefix", conda_prefix]
    else:
        return []



================================================
FILE: src/snakemake/unit_tests/templates/ruletest.py.jinja2
================================================
"""
Rule test code for unit testing of rules generated with Snakemake {{ version }}.
"""


import os
import sys
import shutil
import tempfile
from pathlib import Path
from subprocess import check_output

sys.path.insert(0, os.path.dirname(__file__))


def test_{{ ruletest.name }}(conda_prefix):

    with tempfile.TemporaryDirectory() as tmpdir:
        workdir = Path(tmpdir) / "workdir"
        config_path = Path("{{ ruletest.config_path.as_posix() }}")
        data_path = Path("{{ ruletest.data_path.as_posix() }}")
        expected_path = Path("{{ ruletest.expected_path.as_posix() }}")

        # Copy config to the temporary workdir.
        shutil.copytree(config_path, workdir)

        # Copy data to the temporary workdir.
        shutil.copytree(data_path, workdir, dirs_exist_ok=True)

        # Run the test job.
        check_output(
            [
                "python",
                "-m",
                "snakemake",
                {% for target in ruletest.target %}
                "{{ target }}",
                {% endfor %}
                {% if snakefile %}
                "--snakefile",
                "{{ snakefile }}",
                {% endif %}
                "-f",
                "--notemp",
                "-j1",
                "--target-files-omit-workdir-adjustment",
                {% if configfiles %}
                "--configfile",
                {% for configfile in configfiles %}
                "{{ configfile }}",
                {% endfor %}
                {% endif %}
                {% if deploy %}
                "--software-deployment-method",
                {% for sdm in deploy %}
                "{{ sdm }}",
                {% endfor %}
                {% endif %}
                "--directory",
                workdir,
            ]
            + conda_prefix
        )

        # Check the output byte by byte using cmp/zmp/bzcmp/xzcmp.
        # To modify this behavior, you can inherit from common.OutputChecker in here
        # and overwrite the method `compare_files(generated_file, expected_file), 
        # also see common.py.
        import common
        common.OutputChecker(data_path, expected_path, workdir).check()



================================================
FILE: tests/README.md
================================================
# Unit and regression tests for Snakemake

All tests are written to by run by [pytest](https://docs.pytest.org/en/stable/). Note that not
all tests in this directory are expected to pass, but the specific tests set up for continuous
integration in `.github/workflows/main.yml` should all work.

For info on setting up a working conda environment to run these tests, and other general guidance,
see [the Snakemake contributing guide](
https://snakemake.readthedocs.io/en/stable/project_info/contributing.html#testing-guidelines).




================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/common.py
================================================
__authors__ = ["Tobias Marschall", "Marcel Martin", "Johannes Köster"]
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import os
from pathlib import Path
import signal
import sys
import shlex
import shutil
import time
from os.path import join
import tempfile
import hashlib
import urllib
import pytest
import glob
import subprocess
import tarfile

from snakemake_interface_executor_plugins.settings import SharedFSUsage
from snakemake_interface_executor_plugins.registry import ExecutorPluginRegistry

from snakemake import api
from snakemake.common import ON_WINDOWS
from snakemake.report.html_reporter import ReportSettings
from snakemake.resources import ResourceScopes
from snakemake.scheduling.milp import SchedulerSettings
from snakemake.settings import types as settings


def dpath(path):
    """get the path to a data file (relative to the directory this
    test lives in)"""
    return (Path(__file__).parent / path).resolve()


def md5sum(filename, ignore_newlines=False):
    if ignore_newlines:
        with open(filename, "r", encoding="utf-8", errors="surrogateescape") as f:
            data = f.read().strip().encode("utf8", errors="surrogateescape")
    else:
        data = open(filename, "rb").read().strip()
    return hashlib.md5(data, usedforsecurity=False).hexdigest()


# test skipping
def is_connected():
    try:
        urllib.request.urlopen("http://www.google.com", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def is_ci():
    return "CI" in os.environ


def has_gcloud_service_key():
    return "GCP_AVAILABLE" in os.environ


def has_azbatch_account_url():
    return os.environ.get("AZ_BATCH_ACCOUNT_URL")


def has_zenodo_token():
    return os.environ.get("ZENODO_SANDBOX_PAT")


def has_apptainer():
    return (shutil.which("apptainer") is not None) or (
        shutil.which("singularity") is not None
    )


def has_conda():
    return shutil.which("conda") is not None


gcloud = pytest.mark.skipif(
    not is_connected() or not has_gcloud_service_key(),
    reason="Skipping GCLOUD tests because not on "
    "CI, no inet connection or not logged "
    "in to gcloud.",
)

azbatch = pytest.mark.skipif(
    not is_connected() or not has_azbatch_account_url(),
    reason="Skipping AZBATCH tests because "
    "no inet connection or no AZ_BATCH_ACCOUNT_URL.",
)

connected = pytest.mark.skipif(not is_connected(), reason="no internet connection")

ci = pytest.mark.skipif(not is_ci(), reason="not in CI")
not_ci = pytest.mark.skipif(is_ci(), reason="skipped in CI")

apptainer = pytest.mark.skipif(
    not has_apptainer(),
    reason="Skipping Apptainer tests because no "
    "apptainer/singularity executable available.",
)

conda = pytest.mark.skipif(
    not has_conda(),
    reason="Skipping Conda tests because no conda executable available.",
)

zenodo = pytest.mark.skipif(
    not has_zenodo_token(), reason="no ZENODO_SANDBOX_PAT provided"
)


def copy(src, dst):
    if os.path.isdir(src):
        shutil.copytree(src, os.path.join(dst, os.path.basename(src)))
    else:
        shutil.copy(src, dst)


def get_expected_files(results_dir):
    """Recursively walk through the expected-results directory to enumerate
    all expected files."""
    return [
        os.path.relpath(f, results_dir)
        for f in glob.iglob(os.path.join(results_dir, "**/**"), recursive=True)
        if not os.path.isdir(f)
    ]


def untar_folder(tar_file, output_path):
    if not os.path.isdir(output_path):
        with tarfile.open(tar_file) as tar:
            tar.extractall(path=output_path)


def print_tree(path, exclude=None):
    for root, _dirs, files in os.walk(path):
        if exclude and root.startswith(os.path.join(path, exclude)):
            continue
        level = root.replace(path, "").count(os.sep)
        indent = " " * 4 * level
        print(f"{indent}{os.path.basename(root)}/")
        subindent = " " * 4 * (level + 1)
        for f in files:
            print(f"{subindent}{f}")


def run(
    path,
    shouldfail=False,
    snakefile="Snakefile",
    subpath=None,
    no_tmpdir=False,
    check_md5=True,
    check_results=None,
    cores=3,
    nodes=None,
    set_pythonpath=True,
    cleanup=True,
    conda_frontend="conda",
    config=dict(),
    targets=set(),
    container_image=os.environ.get("CONTAINER_IMAGE", "snakemake/snakemake:latest"),
    shellcmd=None,
    sigint_after=None,
    overwrite_resource_scopes=None,
    executor="local",
    executor_settings=None,
    cleanup_scripts=True,
    scheduler_ilp_solver=None,
    report=None,
    report_after_run=False,
    report_stylesheet=None,
    report_metadata=None,
    deployment_method=frozenset(),
    shadow_prefix=None,
    until=frozenset(),
    omit_from=frozenset(),
    forcerun=frozenset(),
    trust_io_cache=False,
    conda_list_envs=False,
    conda_create_envs=False,
    conda_prefix=None,
    wrapper_prefix=None,
    printshellcmds=False,
    default_storage_provider=None,
    default_storage_prefix=None,
    local_storage_prefix=Path(".snakemake/storage"),
    remote_job_local_storage_prefix=None,
    archive=None,
    cluster=None,
    cluster_status=None,
    retries=0,
    resources=dict(),
    default_resources=None,
    group_components=dict(),
    max_threads=None,
    overwrite_groups=dict(),
    configfiles=list(),
    overwrite_resources=dict(),
    batch=None,
    envvars=list(),
    cache=None,
    edit_notebook=None,
    overwrite_scatter=dict(),
    generate_unit_tests=None,
    force_incomplete=False,
    containerize=False,
    forceall=False,
    all_temp=False,
    cleanup_metadata=None,
    rerun_triggers=settings.RerunTrigger.all(),
    storage_provider_settings=None,
    shared_fs_usage=None,
    benchmark_extended=False,
    apptainer_args="",
    tmpdir=None,
):
    """
    Test the Snakefile in the path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results. If cleanup is False, we return the temporary
    directory to the calling test for inspection, and the test should
    clean it up.
    """
    if check_results is None:
        if not shouldfail:
            check_results = True
        else:
            check_results = False

    if set_pythonpath:
        # Enforce current workdir (the snakemake source dir) to also be in PYTHONPATH
        # when subprocesses are invoked in the tempdir defined below.
        os.environ["PYTHONPATH"] = os.getcwd()
    elif "PYTHONPATH" in os.environ:
        del os.environ["PYTHONPATH"]

    results_dir = path / "expected-results"
    original_snakefile = path / snakefile
    original_dirname = original_snakefile.parent.name
    assert original_snakefile.exists()
    if check_results:
        assert (
            results_dir.exists() and results_dir.is_dir()
        ), f"{results_dir} does not exist"

    if tmpdir is None:
        # If we need to further check results, we won't cleanup tmpdir
        tmpdir = next(tempfile._get_candidate_names())
        tmpdir = os.path.join(
            tempfile.gettempdir(), f"snakemake-{original_dirname}-{tmpdir}"
        )
        os.mkdir(tmpdir)

        # copy files
        for f in os.listdir(path):
            copy(os.path.join(path, f), tmpdir)

    # Snakefile is now in temporary directory
    snakefile = join(tmpdir, snakefile)

    snakemake_api = None
    exception = None

    config = dict(config)

    # run snakemake
    if shellcmd:
        if not shellcmd.startswith("snakemake"):
            raise ValueError("shellcmd does not start with snakemake")
        shellcmd = "{} -m {}".format(sys.executable, shellcmd)
        try:
            if sigint_after is None:
                res = subprocess.run(
                    shellcmd,
                    cwd=path if no_tmpdir else tmpdir,
                    check=True,
                    shell=True,
                    stderr=subprocess.STDOUT,
                    stdout=subprocess.PIPE,
                )
                print(res.stdout.decode())
                success = True
            else:
                with subprocess.Popen(
                    shlex.split(shellcmd),
                    cwd=path if no_tmpdir else tmpdir,
                    stderr=subprocess.STDOUT,
                    stdout=subprocess.PIPE,
                ) as process:
                    time.sleep(sigint_after)
                    process.send_signal(signal.SIGINT)
                    time.sleep(2)
                    success = process.returncode == 0
                    if success:
                        print(process.stdout.read().decode())
        except subprocess.CalledProcessError as e:
            success = False
            print(e.stdout.decode(), file=sys.stderr)
    else:
        assert sigint_after is None, "Cannot sent SIGINT when calling directly"

        if cluster is not None:
            executor = "cluster-generic"
            plugin = ExecutorPluginRegistry().get_plugin(executor)
            executor_settings = plugin.settings_cls(
                submit_cmd=cluster, status_cmd=cluster_status
            )
            nodes = 3

        if shared_fs_usage is None:
            shared_fs_usage = SharedFSUsage.all()

        success = True

        with api.SnakemakeApi(
            settings.OutputSettings(
                verbose=True,
                printshellcmds=printshellcmds,
                show_failed_logs=True,
            ),
        ) as snakemake_api:
            try:
                workflow_api = snakemake_api.workflow(
                    resource_settings=settings.ResourceSettings(
                        cores=cores,
                        nodes=nodes,
                        overwrite_resource_scopes=(
                            ResourceScopes(overwrite_resource_scopes)
                            if overwrite_resource_scopes is not None
                            else dict()
                        ),
                        overwrite_resources=overwrite_resources,
                        resources=resources,
                        default_resources=default_resources,
                        max_threads=max_threads,
                        overwrite_scatter=overwrite_scatter,
                    ),
                    config_settings=settings.ConfigSettings(
                        config=config,
                        configfiles=configfiles,
                    ),
                    storage_settings=settings.StorageSettings(
                        default_storage_provider=default_storage_provider,
                        default_storage_prefix=default_storage_prefix,
                        all_temp=all_temp,
                        shared_fs_usage=shared_fs_usage,
                        local_storage_prefix=local_storage_prefix,
                        remote_job_local_storage_prefix=remote_job_local_storage_prefix,
                    ),
                    storage_provider_settings=storage_provider_settings,
                    workflow_settings=settings.WorkflowSettings(
                        wrapper_prefix=wrapper_prefix,
                        cache=cache,
                    ),
                    deployment_settings=settings.DeploymentSettings(
                        conda_frontend=conda_frontend,
                        conda_prefix=conda_prefix,
                        deployment_method=deployment_method,
                        apptainer_args=apptainer_args,
                    ),
                    snakefile=Path(original_snakefile if no_tmpdir else snakefile),
                    workdir=Path(path if no_tmpdir else tmpdir),
                )

                dag_api = workflow_api.dag(
                    dag_settings=settings.DAGSettings(
                        targets=targets,
                        until=until,
                        omit_from=omit_from,
                        forcerun=forcerun,
                        batch=batch,
                        force_incomplete=force_incomplete,
                        forceall=forceall,
                        rerun_triggers=rerun_triggers,
                        trust_io_cache=trust_io_cache,
                    ),
                )

                if report is not None and not report_after_run:
                    if report_stylesheet is not None:
                        report_stylesheet = Path(report_stylesheet)
                    if report_metadata is not None:
                        report_metadata = Path(report_metadata)
                    report_settings = ReportSettings(
                        path=Path(report), stylesheet_path=report_stylesheet
                    )
                    global_report_settings = settings.GlobalReportSettings(
                        metadata_template=report_metadata
                    )
                    dag_api.create_report(
                        reporter="html",
                        report_settings=report_settings,
                        global_report_settings=global_report_settings,
                    )
                elif conda_create_envs:
                    dag_api.conda_create_envs()
                elif conda_list_envs:
                    dag_api.conda_list_envs()
                elif archive is not None:
                    dag_api.archive(Path(archive))
                elif generate_unit_tests is not None:
                    dag_api.generate_unit_tests(Path(generate_unit_tests))
                elif containerize:
                    dag_api.containerize()
                elif cleanup_metadata:
                    dag_api.cleanup_metadata(cleanup_metadata)
                else:
                    dag_api.execute_workflow(
                        executor=executor,
                        execution_settings=settings.ExecutionSettings(
                            cleanup_scripts=cleanup_scripts,
                            shadow_prefix=shadow_prefix,
                            retries=retries,
                            edit_notebook=edit_notebook,
                        ),
                        remote_execution_settings=settings.RemoteExecutionSettings(
                            container_image=container_image,
                            seconds_between_status_checks=0,
                            envvars=envvars,
                        ),
                        group_settings=settings.GroupSettings(
                            group_components=group_components,
                            overwrite_groups=overwrite_groups,
                        ),
                        executor_settings=executor_settings,
                        scheduler_settings=SchedulerSettings(
                            solver=scheduler_ilp_solver,
                        ),
                    )

                if report_after_run and report:
                    if report_stylesheet is not None:
                        report_stylesheet = Path(report_stylesheet)
                    report_settings = ReportSettings(
                        path=Path(report), stylesheet_path=report_stylesheet
                    )
                    dag_api.create_report(
                        reporter="html",
                        report_settings=report_settings,
                    )
            except Exception as e:
                success = False
                exception = e

    if shouldfail:
        assert not success, "expected error on execution"
    else:
        if not success:
            if snakemake_api is not None and exception is not None:
                snakemake_api.print_exception(exception)
            print("Workdir:")
            print_tree(tmpdir, exclude=".snakemake/conda")
            if exception is not None:
                raise exception
        assert success, "expected successful execution"

    if check_results:
        for resultfile in get_expected_files(results_dir):
            if resultfile in [".gitignore", ".gitkeep"] or not os.path.isfile(
                os.path.join(results_dir, resultfile)
            ):
                # this means tests cannot use directories as output files
                continue
            targetfile = join(tmpdir, resultfile)
            expectedfile = join(results_dir, resultfile)

            if ON_WINDOWS:
                if os.path.exists(join(results_dir, resultfile + "_WIN")):
                    continue  # Skip test if a Windows specific file exists
                if resultfile.endswith("_WIN"):
                    targetfile = join(tmpdir, resultfile[:-4])
            elif resultfile.endswith("_WIN"):
                # Skip win specific result files on Posix platforms
                continue

            assert os.path.exists(targetfile), 'expected file "{}" not produced'.format(
                resultfile
            )
            if check_md5:
                md5expected = md5sum(expectedfile, ignore_newlines=ON_WINDOWS)
                md5target = md5sum(targetfile, ignore_newlines=ON_WINDOWS)
                if md5target != md5expected:
                    with open(expectedfile) as expected:
                        expected_content = expected.read().strip()
                    with open(targetfile) as target:
                        content = target.read().strip()
                    assert (
                        False
                    ), "wrong result produced for file '{resultfile}':\n------found------\n{content}\n-----expected-----\n{expected_content}\n-----------------".format(
                        resultfile=resultfile,
                        content=content,
                        expected_content=expected_content,
                    )

    if not cleanup:
        return Path(tmpdir)
    shutil.rmtree(tmpdir, ignore_errors=ON_WINDOWS)



================================================
FILE: tests/conftest.py
================================================
import os
import sys
import pytest
from contextlib import suppress

from snakemake.common import ON_WINDOWS
from snakemake.utils import find_bash_on_windows
from snakemake.shell import shell

ON_MACOS = sys.platform == "darwin"
skip_on_windows = pytest.mark.skipif(ON_WINDOWS, reason="Unix stuff")
only_on_windows = pytest.mark.skipif(not ON_WINDOWS, reason="Windows stuff")
needs_strace = pytest.mark.xfail(
    os.system("strace -o /dev/null true") != 0, reason="Missing strace"
)


@pytest.fixture(autouse=True)
def reset_paths_between_tests():
    """Ensure that changes to sys.path are reset between tests"""
    org_path = sys.path.copy()
    yield
    sys.path = org_path


bash_cmd = find_bash_on_windows()

if ON_WINDOWS and bash_cmd:

    @pytest.fixture(autouse=True)
    def prepend_usable_bash_to_path(monkeypatch):
        monkeypatch.setenv("PATH", os.path.dirname(bash_cmd), prepend=os.pathsep)

    @pytest.fixture(autouse=True)
    def reset_shell_exec_on_windows(prepend_usable_bash_to_path):
        shell.executable(None)


@pytest.fixture
def s3_storage():
    from snakemake_storage_plugin_s3 import StorageProviderSettings
    from snakemake_interface_common.plugin_registry.plugin import TaggedSettings
    import uuid
    import boto3

    endpoint_url = "http://127.0.0.1:9000"
    access_key = "minio"
    secret_key = "minio123"
    bucket = f"snakemake-{uuid.uuid4().hex}"

    tagged_settings = TaggedSettings()
    tagged_settings.register_settings(
        StorageProviderSettings(
            endpoint_url=endpoint_url,
            access_key=access_key,
            secret_key=secret_key,
        )
    )

    yield f"s3://{bucket}", {"s3": tagged_settings}

    # clean up using boto3
    s3c = boto3.resource(
        "s3",
        endpoint_url=endpoint_url,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
    )
    with suppress(Exception):
        s3c.Bucket(bucket).delete()



================================================
FILE: tests/test_api.py
================================================
import sys, os, subprocess

from snakemake.executors import local

sys.path.insert(0, os.path.dirname(__file__))

from .common import *

from snakemake import api
from snakemake.settings import types as settings
import copy


def test_deploy_sources(s3_storage):
    s3_prefix, s3_settings = s3_storage

    with api.SnakemakeApi(
        settings.OutputSettings(
            verbose=True,
            show_failed_logs=True,
        ),
    ) as snakemake_api:
        workflow_api = snakemake_api.workflow(
            storage_settings=settings.StorageSettings(
                default_storage_prefix=s3_prefix,
                default_storage_provider="s3",
                shared_fs_usage=frozenset(),
            ),
            resource_settings=settings.ResourceSettings(
                cores=1,
            ),
            storage_provider_settings=s3_settings,
            snakefile=Path(dpath("test_deploy_sources/Snakefile")),
        )
        dag_api = workflow_api.dag()

        workflow = dag_api.workflow_api._workflow
        # add dummy remote execution settings as we do not actually execute here
        # (in reality they are present)
        workflow.remote_execution_settings = settings.RemoteExecutionSettings()
        workflow._prepare_dag(
            forceall=False,
            ignore_incomplete=False,
            lock_warn_only=False,
        )
        workflow._build_dag()
        workflow.upload_sources()

        cmd = workflow.spawned_job_args_factory.precommand(local.common_settings)
        assert cmd

        origdir = os.getcwd()
        env = copy.copy(os.environ)
        env.update(workflow.spawned_job_args_factory.envvars())
        with tempfile.TemporaryDirectory() as tmpdir:
            os.chdir(tmpdir)
            try:
                subprocess.run(cmd, shell=True, check=True, env=env)
            finally:
                os.chdir(origdir)



================================================
FILE: tests/test_args.py
================================================
__authors__ = ["K.D. Murray"]
__copyright__ = "Copyright 2024, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"


from snakemake.settings.types import Batch


def test_parse_batch():
    from snakemake.cli import parse_batch

    assert parse_batch("aggregate=1/2") == Batch("aggregate", 1, 2)



================================================
FILE: tests/test_executor_test_suite.py
================================================
from pathlib import Path
from typing import Mapping, Optional

from snakemake_interface_common.plugin_registry.plugin import TaggedSettings
import snakemake.common.tests

from snakemake_interface_executor_plugins.settings import ExecutorSettingsBase
from snakemake_executor_plugin_cluster_generic import ExecutorSettings


class TestWorkflows(snakemake.common.tests.TestWorkflowsBase):
    __test__ = True

    def get_executor(self) -> str:
        return "cluster-generic"

    def _get_cmd(self) -> str:
        return str((Path(__file__).parent / "test_group_jobs" / "qsub").absolute())

    def get_executor_settings(self) -> Optional[ExecutorSettingsBase]:
        return ExecutorSettings(submit_cmd=self._get_cmd())

    def get_default_storage_provider(self) -> Optional[str]:
        return None

    def get_default_storage_prefix(self) -> Optional[str]:
        return None

    def get_default_storage_provider_settings(
        self,
    ) -> Optional[Mapping[str, TaggedSettings]]:
        return None



================================================
FILE: tests/test_expand.py
================================================
from snakemake.io import expand
from snakemake.exceptions import WildcardError
import pytest


def test_simple_expand():
    # single filepattern
    assert expand("{a}.out", a="test") == ["test.out"]
    # multiple filepatterns
    assert expand(["{a}.out", "{b}.out"], a="a", b="b") == ["a.out", "b.out"]
    # multiple wildcards
    assert expand("{a}.out", a=["1", "2", "3"]) == ["1.out", "2.out", "3.out"]
    # multiple wildcards and patterns
    assert expand(["{a}_{b}.ab", "{b}.b"], a="1 2".split(), b="3 4".split()) == [
        "1_3.ab",
        "1_4.ab",
        "2_3.ab",
        "2_4.ab",
        "3.b",
        "4.b",
    ]
    # replace product
    assert expand(["{a}_{b}.ab", "{b}.b"], zip, a="1 2".split(), b="3 4".split()) == [
        "1_3.ab",
        "2_4.ab",
        "3.b",
        "4.b",
    ]


def test_allow_missing():
    # single filepattern
    assert expand("{a}_{b}.out", allow_missing=True) == ["{a}_{b}.out"]
    assert expand("{a}_{b}.out", a="test", allow_missing=True) == ["test_{b}.out"]
    # none missing
    assert expand("{a}.out", a="test", allow_missing=True) == ["test.out"]
    # wildcard is allow_missing
    assert expand("{allow_missing}.out", allow_missing=True) == ["True.out"]
    # allow_missing not True
    assert expand("{a}.out", a="test", allow_missing="test2") == ["test.out"]
    with pytest.raises(WildcardError) as e:
        expand("{a}.out", allow_missing="test2")
    assert str(e.value) == "No values given for wildcard 'a'."

    # multiple filepatterns
    assert expand(["{a}.out", "{b}.out"], allow_missing=True) == ["{a}.out", "{b}.out"]
    # multiple wildcards
    assert expand("{a}_{b}.out", a=["1", "2", "3"], allow_missing=True) == [
        "1_{b}.out",
        "2_{b}.out",
        "3_{b}.out",
    ]
    # multiple wildcards and patterns
    assert expand(
        ["{a}_{b}_{C}.ab", "{b}_{c}.b"],
        a="1 2".split(),
        b="3 4".split(),
        allow_missing=True,
    ) == ["1_3_{C}.ab", "1_4_{C}.ab", "2_3_{C}.ab", "2_4_{C}.ab", "3_{c}.b", "4_{c}.b"]
    # replace product
    assert expand(
        ["{a}_{b}_{C}.ab", "{b}_{c}.b"],
        zip,
        a="1 2".split(),
        b="3 4".split(),
        allow_missing=True,
    ) == ["1_3_{C}.ab", "2_4_{C}.ab", "3_{c}.b", "4_{c}.b"]



================================================
FILE: tests/test_internals.py
================================================
from snakemake.ioutils import subpath
from snakemake.io import Wildcards, InputFiles


def test_subpath():
    assert subpath("test.txt", strip_suffix=".txt") == "test"
    assert subpath("test.txt", basename=True) == "test.txt"
    assert subpath("test.txt", parent=True) == "."
    assert subpath("test.txt", ancestor=1) == "."
    assert subpath("test.txt", ancestor=2) == "."

    assert (
        subpath(lambda wildcards, input: "test.txt", strip_suffix=".txt")(
            Wildcards(), input=InputFiles()
        )
        == "test"
    )
    assert subpath("results/foo/test.txt", parent=True) == "results/foo"
    assert subpath("results/foo/test.txt", ancestor=1) == "results/foo"
    assert subpath("results/foo/test.txt", ancestor=2) == "results"
    assert subpath("results/foo/test.txt", ancestor=3) == "."



================================================
FILE: tests/test_io.py
================================================
from pathlib import PosixPath

from snakemake.io import WILDCARD_REGEX, expand
from snakemake.exceptions import WildcardError


def test_wildcard_regex():
    def matches(text):
        return [
            (match.group("name"), match.group("constraint"))
            for match in WILDCARD_REGEX.finditer(text)
        ]

    # without constraints
    assert matches("") == []
    assert matches("{") == []
    assert matches("}") == []
    assert matches("{}") == []
    assert matches("{0}") == [("0", None)]
    assert matches("{abc}") == [("abc", None)]
    assert matches("abc{def}{ghi}") == [("def", None), ("ghi", None)]

    # with constraints
    assert matches("{w,constraint}") == [("w", "constraint")]
    assert matches("{w , constraint}") == [("w", "constraint")]
    # fails because constraint is detected as 'constraint '
    # assert matches('{w,constraint }') == [('w', 'constraint')]
    assert matches("abc { w , constraint} def") == [("w", "constraint")]

    # multiple wildcards
    assert matches("{a,1} {b,2} {c,3}") == [("a", "1"), ("b", "2"), ("c", "3")]

    # more complicated constraints
    assert matches(r"{w,([a-z]+|pat\|t*ern)}") == [("w", r"([a-z]+|pat\|t*ern)")]
    assert matches(r"{w,([a-z]+|pat\|te{1,3}rn){5,7}}") == [
        ("w", r"([a-z]+|pat\|te{1,3}rn){5,7}")
    ]

    # This used to be very slow with an older version of the regex
    assert matches("{w, long constraint without closing brace") == []


def test_expand():
    wildcards = {"a": [1, 2], "b": [3, 4], "c": [5]}

    # each provided wildcard is used in the filepattern
    assert sorted(expand("{a}{b}{c}", **wildcards)) == sorted(
        ["135", "145", "235", "245"]
    )

    # redundant wildcards are provided
    assert sorted(expand("{a}{c}", **wildcards)) == sorted(["15", "25"])

    # missing wildcards (should fail)
    try:
        expand("{a}{d}", **wildcards)
        assert False
    except WildcardError:
        pass

    # do not expand on strings and non iterables
    assert expand("{x}{y}", **{"x": "Hello, ", "y": "world!"}) == ["Hello, world!"]
    assert expand("{x}{y}", **{"x": 4, "y": 2}) == ["42"]

    # format-minilang: field names
    assert sorted(
        expand("first letter of sample: {samples[0]}", samples=["A123", "B456", "C789"])
    ) == sorted(
        [
            "first letter of sample: A",
            "first letter of sample: B",
            "first letter of sample: C",
        ]
    )
    assert expand("{str.__class__}", str="") == ["<class 'str'>"]

    # format-minilang: conversions
    class ConvTest:
        def __str__(self):
            return "string"

        def __repr__(self):
            return "representation"

    assert expand("{test!r}", test=ConvTest()) == ["representation"]
    assert expand("{test!s}", test=ConvTest()) == ["string"]

    # format-minilang: format specifications
    assert sorted(
        expand(
            "The answer to life, the universe, and everything: {answer:f}",
            answer=range(41, 43),
        )
    ) == sorted(
        [
            "The answer to life, the universe, and everything: 41.000000",
            "The answer to life, the universe, and everything: 42.000000",
        ]
    )

    # multiple filepatterns with different wildcards
    assert sorted(
        expand(["a: {a} + b: {b}", "c: {c}"], a="aa", b=["b", "bb"], c=["c", "cc"])
    ) == sorted(["a: aa + b: b", "a: aa + b: bb", "c: c", "c: cc"])

    # expand on pathlib.Path objects
    assert expand(PosixPath() / "{x}" / "{y}", x="Hello", y="world") == ["Hello/world"]



================================================
FILE: tests/test_linting.py
================================================
import os, sys
from pathlib import Path
import subprocess as sp
from itertools import product

import pytest

LINT_DIR = Path(__file__).parent.joinpath("linting")


@pytest.mark.parametrize(
    "lint, case", product(os.listdir(LINT_DIR), ["positive", "negative"])
)
def test_lint(lint, case):
    lint = LINT_DIR.joinpath(lint)

    try:
        out = (
            sp.check_output(
                [
                    "python",
                    "-m",
                    "snakemake",
                    "--lint",
                    "--directory",
                    str(lint),
                    "--snakefile",
                    str(lint.joinpath(case).with_suffix(".smk")),
                ],
                stderr=sp.STDOUT,
            )
            .decode()
            .strip()
        )
        if case == "positive":
            assert out == "Congratulations, your workflow is in a good condition!"
        else:
            print(out, file=sys.stderr)
            assert (
                False
            ), "Negative lint example but linting command exited with status 0."

    except sp.CalledProcessError as e:
        if case == "negative":
            assert e.output.decode().strip()
            if "not_used_params" in lint.name:
                # Check that the correct line number is reported
                assert "line 3" in e.output.decode().strip()
        else:
            print(e.output.decode().strip(), file=sys.stderr)
            raise e

    else:
        print(out, file=sys.stderr)
        assert out



================================================
FILE: tests/test_output_index.py
================================================
from unittest.mock import Mock

import pytest

from snakemake.output_index import OutputIndex


@pytest.fixture()
def rule_0(mocker):
    mock_products_rule_0 = [
        mocker.Mock(
            constant_prefix=lambda: "test", constant_suffix=lambda: "-something.txt"
        ),
        mocker.Mock(
            constant_prefix=lambda: "test", constant_suffix=lambda: "-something.csv"
        ),
    ]
    return mocker.Mock(products=lambda: mock_products_rule_0, name="rule_0")


@pytest.fixture()
def rule_1(mocker):
    mock_products_rule_1 = [
        mocker.Mock(
            constant_prefix=lambda: "other/file-",
            constant_suffix=lambda: "-something.csv",
        )
    ]
    return mocker.Mock(products=lambda: mock_products_rule_1, name="rule_1")


@pytest.fixture()
def rule_2(mocker):
    mock_products_rule_2 = [
        mocker.Mock(
            constant_prefix=lambda: "other/file-dupe-",
            constant_suffix=lambda: "-something.csv",
        )
    ]
    return mocker.Mock(products=lambda: mock_products_rule_2, name="rule_2")


@pytest.fixture()
def rule_empty_suffix(mocker):
    return mocker.Mock(
        products=lambda: [
            Mock(constant_prefix=lambda: "test", constant_suffix=lambda: "")
        ]
    )


@pytest.fixture()
def rule_empty_prefix(mocker):
    return mocker.Mock(
        products=lambda: [
            Mock(constant_prefix=lambda: "", constant_suffix=lambda: "something.txt")
        ]
    )


@pytest.fixture()
def output_index(rule_0, rule_1, rule_2):
    return OutputIndex(rules=[rule_0, rule_1, rule_2])


@pytest.mark.parametrize(
    "target,expected_rules",
    [
        ("test-{wildcard}-something.txt", {"rule_0"}),
        ("test-{wildcard}-something.csv", {"rule_0"}),
        ("test-something.txt", {"rule_0"}),
        ("other/file--something.csv", {"rule_1"}),
        ("other/file-something.csv", {"rule_1"}),
        ("other/not-file-something.csv", set()),
        ("other/file-dupe-something.csv", {"rule_1", "rule_2"}),
    ],
)
def test_match(request, output_index, target, expected_rules):
    expected = {request.getfixturevalue(rule) for rule in expected_rules}
    assert output_index.match(target) == expected


@pytest.mark.parametrize(
    "target,expected_rules",
    [
        ("test-something", {"rule_empty_suffix"}),
        ("something", set()),
        ("something.txt", {"rule_empty_prefix"}),
        ("test-something.txt", {"rule_empty_suffix", "rule_empty_prefix"}),
    ],
)
def test_match_with_empty_components(
    request, rule_empty_suffix, rule_empty_prefix, target, expected_rules
):
    output_index = OutputIndex([rule_empty_suffix, rule_empty_prefix])
    expected = {request.getfixturevalue(rule) for rule in expected_rules}
    assert output_index.match(target) == expected


def test_empty_pattern_matches_everything(mocker):
    """Test that empty patterns match any filename"""
    rule = mocker.Mock(
        products=lambda: [Mock(constant_prefix=lambda: "", constant_suffix=lambda: "")]
    )
    output_index = OutputIndex([rule])
    assert rule in output_index.match("")
    assert rule in output_index.match("anything")
    assert rule in output_index.match("file.txt")


def test_empty_prefix_and_suffix(mocker):
    """Test with empty prefix and suffix"""
    rule = mocker.Mock(
        products=lambda: [Mock(constant_prefix=lambda: "", constant_suffix=lambda: "")]
    )
    output_index = OutputIndex([rule])
    matches = output_index.match("anything.txt")
    assert rule in matches


def test_case_sensitivity(mocker):
    """Test case sensitivity of matching"""
    rule = mocker.Mock(
        products=lambda: [
            Mock(constant_prefix=lambda: "Test", constant_suffix=lambda: "TXT")
        ]
    )
    output_index = OutputIndex([rule])

    matches_exact = output_index.match("Test.TXT")
    matches_lower = output_index.match("test.txt")
    assert rule in matches_exact
    assert rule not in matches_lower


@pytest.mark.parametrize(
    "target,expected_match",
    [
        ("test.txt", True),
        ("testing.txt", True),
        ("test.doc", False),
        ("other.txt", False),
        ("", False),
        ("test", False),
        ("bad.txt", False),
    ],
)
def test_parametrized_matches(mocker, target, expected_match):
    """Parametrized test for various matching scenarios"""
    rule = mocker.Mock(
        products=lambda: [
            Mock(constant_prefix=lambda: "test", constant_suffix=lambda: "txt")
        ]
    )
    output_index = OutputIndex([rule])
    matches = output_index.match(target)
    assert (rule in matches) == expected_match



================================================
FILE: tests/test_path_modifier.py
================================================
[Empty file]


================================================
FILE: tests/test_persistence.py
================================================
import tempfile
from pathlib import Path
from unittest.mock import patch

from snakemake.persistence import Persistence


class TestCleanupContainers:
    @patch("snakemake.deployment.singularity.Image", autospec=True, create=True)
    @patch("snakemake.dag.DAG", autospec=True, create=True)
    def test_one_unrequired_container_gets_removed(self, mock_dag, mock_img):
        snakecode = """rule all:
    input:
        "foo.txt"

rule foo:
    output:
        "foo.txt"
    container:
        "docker://quay.io/mbhall88/rasusa:0.7.0"
    shell:
        "rasusa --help &> {output}"
"""
        with tempfile.TemporaryDirectory() as tmpdirname:
            tmpdirpath = Path(tmpdirname)
            singularity_dir = tmpdirpath / ".snakemake/singularity"
            singularity_dir.mkdir(parents=True)
            snakefile = tmpdirpath / "Snakefile"
            snakefile.write_text(snakecode)

            unrequired_img_path = (
                singularity_dir / "32077ccc4d05977ef8b94ee6f74073fd.simg"
            )
            unrequired_img_path.touch()
            assert unrequired_img_path.exists()

            required_img_path = (
                singularity_dir / "0581af3d3099c1cc8cf0088c8efe1439.simg"
            )
            required_img_path.touch()
            assert required_img_path.exists()
            mock_img.path = str(required_img_path)
            container_imgs = {"docker://quay.io/mbhall88/rasusa:0.7.0": mock_img}
            mock_dag.container_imgs = container_imgs
            persistence = Persistence(dag=mock_dag)
            persistence.container_img_path = str(singularity_dir)

            persistence.cleanup_containers()

            assert required_img_path.exists()
            assert not unrequired_img_path.exists()



================================================
FILE: tests/test_prefix_lookup.py
================================================
from snakemake.common.prefix_lookup import PrefixLookup


def test_basic_match():
    lookup = PrefixLookup([("a", 1), ("b", 2)])
    assert lookup.match("a") == {1}
    assert lookup.match("b") == {2}


def test_multiple_matches():
    lookup = PrefixLookup([("", 0), ("a", 1), ("ab", 2), ("abc", 3)])
    assert lookup.match("abcd") == {0, 1, 2, 3}
    assert lookup.match("abc") == {0, 1, 2, 3}
    assert lookup.match("ab") == {0, 1, 2}
    assert lookup.match("a") == {0, 1}
    assert lookup.match("") == {0}


def test_no_matches():
    lookup = PrefixLookup([("a", 1), ("b", 2)])
    assert lookup.match("c") == set()
    assert lookup.match("cc") == set()


def test_empty_lookup():
    lookup = PrefixLookup([])
    assert lookup.match("abc") == set()


def test_overlapping_prefixes():
    lookup = PrefixLookup(
        [
            ("a", 1),
            ("aa", 2),
            ("a", 3),
        ]
    )
    assert lookup.match("aaa") == {1, 2, 3}
    assert lookup.match("a") == {1, 3}


def test_case_sensitivity():
    lookup = PrefixLookup([("a", 1), ("A", 2), ("ab", 3)])
    assert lookup.match("a") == {1}
    assert lookup.match("A") == {2}
    assert lookup.match("ab") == {3, 1}


def test_special_characters():
    lookup = PrefixLookup([(" ", 1), ("\t", 2), ("\n", 3), ("$#@!", 4), ("test$", 5)])
    assert lookup.match(" hello") == {1}
    assert lookup.match("\tworld") == {2}
    assert lookup.match("\ntest") == {3}
    assert lookup.match("$#@!test") == {4}
    assert lookup.match("test$abc") == {5}


def test_unicode_characters():
    lookup = PrefixLookup([("é", 1), ("ñ", 2), ("🌟", 3), ("é日本", 4)])
    assert lookup.match("étest") == {1}
    assert lookup.match("ñtest") == {2}
    assert lookup.match("🌟test") == {3}
    assert lookup.match("é日本語") == {1, 4}


def test_sorting_behavior():
    # Test that internal sorting doesn't affect matching
    lookup1 = PrefixLookup([("b", 1), ("a", 2), ("c", 3)])
    lookup2 = PrefixLookup([("a", 2), ("c", 3), ("b", 1)])
    assert lookup1.match("b") == lookup2.match("b")
    assert lookup1.match("abc") == lookup2.match("abc")


def test_different_value_types():
    lookup = PrefixLookup([("a", 42), ("b", "hello"), ("c", (1, 2, 3)), ("d", None)])
    assert lookup.match("abc") == {42}
    assert lookup.match("bcd") == {"hello"}
    assert lookup.match("cde") == {(1, 2, 3)}
    assert lookup.match("def") == {None}


def test_edge_cases():
    lookup = PrefixLookup(
        [
            ("", 1),
            (" ", 2),
            ("  ", 3),
            ("   ", 4),
        ]
    )
    assert lookup.match("abc") == {1}
    assert lookup.match(" abc") == {1, 2}
    assert lookup.match("  abc") == {1, 2, 3}
    assert lookup.match("   abc") == {1, 2, 3, 4}
    assert lookup.match("    abc") == {1, 2, 3, 4}


def test_prefix_matching_non_consecutive():
    """Test that demonstrates why we need to keep checking even after seeing a longer prefix,
    when prefixes aren't consecutive (like 'ab' vs 'abbc')."""
    lookup = PrefixLookup([("a", 1), ("ab", 2), ("abbcc", 3), ("abbc", 4), ("abbd", 5)])

    # Query "abbz" process:
    # 1. Try "abbd" (len 4) - no match, last_len = 4
    # 2. Try "abbc" (len 4) - no match, last_len = 4
    # 3. Try "ab" (len 2) - MATCHES, last_len = 2
    # 4. Try "a" (len 1) - MATCHES, last_len = 1
    result = lookup.match("abbz")
    assert result == {1, 2}  # Matches "a" and "ab"

    assert lookup.match("abbc") == {1, 2, 4}  # Matches "a", "ab", and "abbc"
    assert lookup.match("abbd") == {1, 2, 5}  # Matches "a", "ab", and "abbd"
    assert lookup.match("abbcc") == {
        1,
        2,
        3,
        4,
    }  # Matches "a", "ab", "abbcc", and "abbc"


def test_first_match_is_empty_string():
    lookup = PrefixLookup([("", 1)])
    matches = lookup.match("anything.txt")
    assert matches == {1}



================================================
FILE: tests/test_schema.py
================================================
import json
import pytest
from snakemake.utils import validate
import pandas as pd

CONFIG_SCHEMA = """$schema: "https://json-schema.org/draft/2020-12/schema#"
description: Configuration schema
type: object
properties:
  param:
    type: object
    description: parameter settings
    # Necessary in case config is empty!
    default: {}
    properties:
      foo:
        type: string
        default: bar
"""

BAR_SCHEMA = """$schema: "https://json-schema.org/draft/2020-12/schema#"
definitions:
  bar:
    type: string
    description: bar entry
    default: foo
"""

BAR_JSON_SCHEMA = {
    "$schema": "https://json-schema.org/draft/2020-12/schema#",
    "definitions": {
        "jsonbar": {"type": "string", "description": "bar entry", "default": "foo"}
    },
}

DF_SCHEMA = """$schema: "https://json-schema.org/draft/2020-12/schema#"
description: an entry in the sample sheet
properties:
  sample:
    type: string
    description: sample name/identifier
  condition:
    type: string
    description: sample condition that will be compared during differential expression analysis (e.g. a treatment, a tissue time, a disease)
  case:
    type: boolean
    default: true
    description: boolean that indicates if sample is case or control
  date:
    type: string
    format: date-time
    description: collection date
    default: "2018-10-10"

required:
  - sample
  - condition
"""


@pytest.fixture
def schemadir(tmpdir):
    p = tmpdir.mkdir("schema")
    return p


@pytest.fixture
def bar_schema(schemadir):
    p = schemadir.join("bar.schema.yaml")
    p.write(BAR_SCHEMA)
    return p


@pytest.fixture
def json_bar_schema(schemadir):
    p = schemadir.join("bar.schema.json")
    p.write(json.dumps(BAR_JSON_SCHEMA))
    return p


@pytest.fixture
def df_schema(schemadir):
    p = schemadir.join("df.schema.yaml")
    p.write(DF_SCHEMA)
    return p


@pytest.fixture
def config_schema(schemadir):
    p = schemadir.join("config.schema.yaml")
    p.write(CONFIG_SCHEMA)
    return p


@pytest.fixture
def config_schema_ref(schemadir, bar_schema, json_bar_schema):
    p = schemadir.join("config.ref.schema.yaml")
    p.write(
        CONFIG_SCHEMA
        + "\n".join(
            [
                "      bar:",
                '        default: "yaml"',
                '        $ref: "{bar}"'.format(
                    bar=str(bar_schema) + "#/definitions/bar"
                ),
                "      jsonbar:",
                '        default: "json"',
                '        $ref: "{bar}"'.format(
                    bar=str(json_bar_schema) + "#/definitions/jsonbar"
                ),
                "",
            ]
        )
    )
    return p


def test_config(config_schema):
    config = {}
    validate(config, str(config_schema), False)
    assert config == {}
    validate(config, str(config_schema))
    assert dict(config) == {"param": {"foo": "bar"}}


def test_config_ref(config_schema_ref):
    config = {}
    validate(config, str(config_schema_ref))
    assert config["param"]["foo"] == "bar"
    assert config["param"]["bar"] == "yaml"
    assert config["param"]["jsonbar"] == "json"
    # Make sure regular validator works
    config["param"]["bar"] = 1
    config["param"]["jsonbar"] = 2
    from snakemake.exceptions import WorkflowError

    with pytest.raises(WorkflowError):
        validate(config, str(config_schema_ref), False)


def test_dataframe(df_schema):
    df = pd.DataFrame([{"sample": "foo", "condition": "bar"}])
    validate(df, str(df_schema), False)
    assert sorted(df.columns) == sorted(["sample", "condition"])
    validate(df, str(df_schema))
    assert sorted(df.columns) == sorted(["sample", "condition", "case", "date"])
    assert df.case.loc[0]



================================================
FILE: tests/test_script.py
================================================
from textwrap import dedent

from snakemake.io import InputFiles
from snakemake.script import RustScript, BashEncoder


class TestRustScriptExtractManifest:
    def test_single_line_manifest_with_shebang_and_second_manifest(self):
        source = dedent(
            """#!/usr/bin/env rust-script
// cargo-deps: time="0.1.25", serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = '// cargo-deps: time="0.1.25", serde="*"\n'

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        assert remaining_src == expected_remaining_src

    def test_single_line_manifest_not_at_start_with_shebang(self):
        source = dedent(
            """#!/usr/bin/env rust-script
// this is where cargo-deps should be
// cargo-deps: time="0.1.25", serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = ""

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """// this is where cargo-deps should be
// cargo-deps: time="0.1.25", serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        assert remaining_src == expected_remaining_src

    def test_single_line_manifest_not_at_start_without_shebang(self):
        source = dedent(
            """// this is where cargo-deps should be
// cargo-deps: time="0.1.25", serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = ""

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """// this is where cargo-deps should be
// cargo-deps: time="0.1.25", serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        assert remaining_src == expected_remaining_src

    def test_single_line_manifest_with_empty_line_without_shebang(self):
        source = dedent(
            """
// cargo-deps: time="0.1.25", serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = '\n// cargo-deps: time="0.1.25", serde="*"\n'

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        assert remaining_src == expected_remaining_src

    def test_single_line_manifest_is_case_insensitive(self):
        source = dedent(
            """
// Cargo-deps: time="0.1.25", serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = '\n// Cargo-deps: time="0.1.25", serde="*"\n'

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        assert remaining_src == expected_remaining_src

    def test_single_line_manifest_spacing_has_no_impact(self):
        source = dedent(
            """
// cargo-deps : time="0.1.25", serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = '\n// cargo-deps : time="0.1.25", serde="*"\n'

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        assert remaining_src == expected_remaining_src

    def test_single_line_manifest_formatting_not_touched_even_if_wrong(self):
        """The dependency delimiter is wrong, but we let rust-script deal with it"""
        source = dedent(
            """
// cargo-deps: time="0.1.25"; serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = '\n// cargo-deps: time="0.1.25"; serde="*"\n'

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        assert remaining_src == expected_remaining_src

    def test_single_line_manifest_spelt_wrong(self):
        source = dedent(
            """
// cargo-dependencies: time="0.1.25"; serde="*"
// You can also leave off the version number, in which case, it's assumed
// to be "*".  Also, the `cargo-deps` comment *must* be a single-line
// comment, and it *must* be the first thing in the file, after the
// shebang.
// This second dependency line should be ignored
// cargo-deps: time="0.1.25", libc="0.2.5"
fn main() {
    println!("{}", time::now().rfc822z());
}
"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = ""

        assert manifest == expected_manifest

        expected_remaining_src = source

        assert remaining_src == expected_remaining_src

    def test_code_block_manifest_with_shebang(self):
        source = dedent(
            """#!/usr/bin/env rust-script
//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```cargo
//! [dependencies]
//! time = "0.1.25"
//! ```
fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = dedent(
            """//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```cargo
//! [dependencies]
//! time = "0.1.25"
//! ```
"""
        )

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        assert remaining_src == expected_remaining_src

    def test_code_block_manifest_without_shebang(self):
        source = dedent(
            """
//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```cargo
//! [dependencies]
//! time = "0.1.25"
//! ```
fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = dedent(
            """\n//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```cargo
//! [dependencies]
//! time = "0.1.25"
//! ```
"""
        )

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        assert remaining_src == expected_remaining_src

    def test_code_block_manifest_spacing_around_language(self):
        source = dedent(
            """
//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```  cargo
//! [dependencies]
//! time = "0.1.25"
//! ```
fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = dedent(
            """\n//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```  cargo
//! [dependencies]
//! time = "0.1.25"
//! ```
"""
        )

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        assert remaining_src == expected_remaining_src

    def test_code_block_manifest_has_non_cargo_block(self):
        source = dedent(
            """
//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```rust
//! [dependencies]
//! time = "0.1.25"
//! ```
fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = ""

        assert manifest == expected_manifest

        expected_remaining_src = source

        assert remaining_src == expected_remaining_src

    def test_code_block_manifest_missing_closing_fence(self):
        source = dedent(
            """
//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```cargo
//! [dependencies]
//! time = "0.1.25"
//! 
fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = ""

        assert manifest == expected_manifest

        expected_remaining_src = source

        assert remaining_src == expected_remaining_src

    def test_code_block_manifest_not_in_first_comment_block(self):
        source = dedent(
            """//! crate comment
static FOO: &str = "foo";
//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```cargo
//! [dependencies]
//! time = "0.1.25"
//! ```
//! 
fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = ""

        assert manifest == expected_manifest

        expected_remaining_src = source

        assert remaining_src == expected_remaining_src

    def test_code_block_manifest_with_outer_line_doc_comment(self):
        source = dedent(
            """#!/usr/bin/env rust-script
/// This is a regular crate doc comment, but it also contains a partial
/// Cargo manifest.  Note the use of a *fenced* code block, and the
/// `cargo` "language".
///
/// ```cargo
/// [dependencies]
/// time = "0.1.25"
/// ```
fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        manifest, remaining_src = RustScript.extract_manifest(source)

        expected_manifest = dedent(
            """/// This is a regular crate doc comment, but it also contains a partial
/// Cargo manifest.  Note the use of a *fenced* code block, and the
/// `cargo` "language".
///
/// ```cargo
/// [dependencies]
/// time = "0.1.25"
/// ```
"""
        )

        assert manifest == expected_manifest

        expected_remaining_src = dedent(
            """fn main() {
    println!("{}", time::now().rfc822z());
}

"""
        )

        assert remaining_src == expected_remaining_src


class TestBashEncoder:
    def test_named_list_one_named_one_str(self):
        """InputFiles is a subclass of snakemake.io.NamedInput
        ierate over input and store each with the integer index - i.e 0, 1, 2
        then use input.items() to iterate over the named files and store them as named also
        check how this works with named things being lists
        """
        named_list = InputFiles(["test.in", "named.in"])
        named_list._set_name("named", 1)

        actual = BashEncoder.encode_namedlist(named_list)
        expected = r"""( [0]="test.in" [1]="named.in" [named]="named.in" )"""

        assert actual == expected

    def test_named_list_named_is_list(self):
        """Named lists that are lists of files become a space-separated string as you
        can't nest arrays in bash"""
        named_list = InputFiles(["test1.in", ["test2.in", "named.in"]])
        named_list._set_name("named", 1)

        actual = BashEncoder.encode_namedlist(named_list)
        expected = r"""( [0]="test1.in" [1]="test2.in named.in" [named]="test2.in named.in" )"""

        assert actual == expected



================================================
FILE: tests/test_sourcecache.py
================================================
from snakemake.sourcecache import GitlabFile


def test_GitlabFile_host_propagation():
    file = GitlabFile(repo="owner/repo", path="parent/path", tag="tag", host="host.com")

    assert file.get_basedir().host == file.host
    assert file.join("another/path").host == file.host



================================================
FILE: tests/tests_using_conda.py
================================================
__authors__ = ["Tobias Marschall", "Marcel Martin", "Johannes Köster"]
__copyright__ = "Copyright 2022, Johannes Köster"
__email__ = "johannes.koester@uni-due.de"
__license__ = "MIT"

import os
import shutil
import sys
import subprocess as sp
from pathlib import Path
import tempfile

import pytest
from snakemake.deployment.conda import get_env_setup_done_flag_file

sys.path.insert(0, os.path.dirname(__file__))

from .common import run, dpath, apptainer, conda, connected
from .conftest import skip_on_windows, only_on_windows, ON_WINDOWS

from snakemake_interface_executor_plugins.settings import (
    DeploymentMethod,
)


xfail_permissionerror_on_win = (
    pytest.mark.xfail(raises=PermissionError) if ON_WINDOWS else lambda x: x
)


@skip_on_windows
@conda
def test_script():
    run(
        dpath("test_script"),
        deployment_method={DeploymentMethod.CONDA},
        check_md5=False,
    )


@skip_on_windows
@conda
def test_script_rs():
    run(
        dpath("test_script_rs"),
        deployment_method={DeploymentMethod.CONDA},
        check_md5=False,
    )


@conda
def test_conda():
    run(dpath("test_conda"), deployment_method={DeploymentMethod.CONDA})


@conda
def test_conda_list_envs():
    run(dpath("test_conda"), conda_list_envs=True, check_results=False)


# TODO failing with FAILED tests/tests.py::test_conda_create_envs_only -
# PermissionError: [WinError 32] The process cannot access the file because
# it is being used by another process:
# 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\snakemake-2q4osog0\\test-env.yaml'
@skip_on_windows
@conda
def test_conda_create_envs_only():
    tmpdir = run(
        dpath("test_conda"),
        conda_create_envs=True,
        check_results=False,
        cleanup=False,
        cleanup_scripts=False,
    )
    env_dir = next(
        (p for p in Path(tmpdir, ".snakemake", "conda").iterdir() if p.is_dir()), None
    )
    assert env_dir is not None
    assert get_env_setup_done_flag_file(Path(env_dir)).exists()
    shutil.rmtree(tmpdir)


@conda
def test_upstream_conda():
    run(
        dpath("test_conda"),
        deployment_method={DeploymentMethod.CONDA},
        conda_frontend="conda",
    )


@skip_on_windows
@conda
def test_deploy_script():
    run(dpath("test_deploy_script"), deployment_method={DeploymentMethod.CONDA})


@skip_on_windows
@conda
def test_deploy_hashing():
    tmpdir = run(
        dpath("test_deploy_hashing"),
        deployment_method={DeploymentMethod.CONDA},
        cleanup=False,
    )
    assert len(next(os.walk(os.path.join(tmpdir, ".snakemake/conda")))[1]) == 2


@conda
def test_conda_custom_prefix():
    run(
        dpath("test_conda_custom_prefix"),
        deployment_method={DeploymentMethod.CONDA},
        conda_prefix="custom",
        set_pythonpath=False,
    )


@only_on_windows
@conda
def test_conda_cmd_exe():
    # Tests the conda environment activation when cmd.exe
    # is used as the shell
    run(dpath("test_conda_cmd_exe"), deployment_method={DeploymentMethod.CONDA})


@skip_on_windows  # wrappers are for linux and macos only
@conda
def test_wrapper():
    run(
        dpath("test_wrapper"),
        deployment_method={DeploymentMethod.CONDA},
        check_md5=False,
    )


@skip_on_windows  # wrappers are for linux and macos only
@conda
def test_wrapper_local_git_prefix():
    import git

    with tempfile.TemporaryDirectory() as tmpdir:
        print("Cloning wrapper repo...")
        repo = git.Repo.clone_from(
            "https://github.com/snakemake/snakemake-wrappers", tmpdir
        )
        print(f"Cloning of {repo} complete.")

        run(
            dpath("test_wrapper"),
            deployment_method={DeploymentMethod.CONDA},
            wrapper_prefix=f"git+file://{tmpdir}",
            check_md5=False,
        )


@skip_on_windows
@apptainer
@connected
@conda
def test_singularity_conda():
    run(
        dpath("test_singularity_conda"),
        deployment_method={DeploymentMethod.CONDA, DeploymentMethod.APPTAINER},
        conda_frontend="conda",
    )


@conda
@pytest.mark.needs_envmodules
def test_archive():
    run(dpath("test_archive"), archive="workflow-archive.tar.gz")


@skip_on_windows
@conda
def test_issue635():
    run(
        dpath("test_issue635"),
        deployment_method={DeploymentMethod.CONDA},
        check_md5=False,
    )


@conda
def test_issue1046():
    run(dpath("test_issue1046"))


@skip_on_windows
@conda
def test_issue1093():
    run(dpath("test_issue1093"), deployment_method={DeploymentMethod.CONDA})


@conda
def test_jupyter_notebook():
    run(dpath("test_jupyter_notebook"), deployment_method={DeploymentMethod.CONDA})


@conda
def test_jupyter_notebook_draft():
    from snakemake.settings.types import NotebookEditMode

    run(
        dpath("test_jupyter_notebook_draft"),
        deployment_method={DeploymentMethod.CONDA},
        edit_notebook=NotebookEditMode(draft_only=True),
        targets=["results/result_intermediate.txt"],
        check_md5=False,
    )


@skip_on_windows
@apptainer
@conda
def test_containerized():
    run(
        dpath("test_containerized"),
        deployment_method={DeploymentMethod.CONDA, DeploymentMethod.APPTAINER},
    )


@skip_on_windows
@conda
def test_containerize():
    run(dpath("test_conda"), containerize=True, check_results=False)


@conda
def test_converting_path_for_r_script():
    run(
        dpath("test_converting_path_for_r_script"),
        cores=1,
        deployment_method={DeploymentMethod.CONDA},
    )


@skip_on_windows
@conda
def test_conda_named():
    run(dpath("test_conda_named"), deployment_method={DeploymentMethod.CONDA})


@skip_on_windows
@conda
def test_conda_function():
    run(
        dpath("test_conda_function"),
        deployment_method={DeploymentMethod.CONDA},
        cores=1,
    )


@skip_on_windows  # the testcase only has a linux-64 pin file
@conda
def test_conda_pin_file():
    run(dpath("test_conda_pin_file"), deployment_method={DeploymentMethod.CONDA})


@conda
def test_conda_python_script():
    run(dpath("test_conda_python_script"), deployment_method={DeploymentMethod.CONDA})


@conda
def test_conda_python_3_7_script():
    run(
        dpath("test_conda_python_3_7_script"),
        deployment_method={DeploymentMethod.CONDA},
    )


@conda
def test_prebuilt_conda_script():
    sp.run(
        f"conda env create -f {dpath('test_prebuilt_conda_script/env.yaml')}",
        shell=True,
    )
    run(dpath("test_prebuilt_conda_script"), deployment_method={DeploymentMethod.CONDA})


@skip_on_windows
@conda
def test_conda_global():
    run(
        dpath("test_conda_global"),
        deployment_method={DeploymentMethod.CONDA},
        executor="dryrun",
    )


@conda
def test_script_pre_py39():
    run(dpath("test_script_pre_py39"), deployment_method={DeploymentMethod.CONDA})


@conda
def test_resource_string_in_cli_or_profile():
    test_path = dpath("test_resource_string_in_cli_or_profile")
    profile = os.path.join(test_path, "profiles")
    # workflow profile is loaded by default
    run(
        test_path,
        snakefile="Snakefile",
        shellcmd=f"snakemake --workflow-profile {profile} -c1 --default-resources slurm_account=foo other_resource='--test'",
    )


@skip_on_windows
@conda
def test_script_xsh():
    run(
        dpath("test_script_xsh"),
        deployment_method={DeploymentMethod.CONDA},
    )


@conda
def test_conda_run():
    run(dpath("test_conda_run"), deployment_method={DeploymentMethod.CONDA})


@conda
def test_issue_3192():
    sp.run(
        "conda create -n test_issue3192 python",
        shell=True,
    )
    run(dpath("test_issue3192"), deployment_method={DeploymentMethod.CONDA})


# Test that container and conda can be run independently using sdm
@skip_on_windows
@apptainer
@connected
@conda
def test_issue_3202():
    run(dpath("test_issue_3202"), deployment_method={DeploymentMethod.APPTAINER})
    run(dpath("test_issue_3202"), deployment_method={DeploymentMethod.CONDA})


# These tests have no explicit dependency on Conda and do not build new conda envs,
# but will fail if 'conda info --json' does not work as expected, because the wrapper
# code uses this to examine the installed software environment.


def test_get_log_none():
    run(dpath("test_get_log_none"))


def test_get_log_both():
    run(dpath("test_get_log_both"))


def test_get_log_stderr():
    run(dpath("test_get_log_stderr"))


def test_get_log_stdout():
    run(dpath("test_get_log_stdout"))


def test_get_log_complex():
    run(dpath("test_get_log_complex"))



================================================
FILE: tests/knapsack/1.txt
================================================
[Empty file]


================================================
FILE: tests/knapsack/2.txt
================================================
[Empty file]


================================================
FILE: tests/knapsack/3.txt
================================================
[Empty file]


================================================
FILE: tests/knapsack/Snakefile
================================================
# kate: syntax python;

DATASETS = '1 2 3'.split()

rule mapmature_all:
	input: '{ds}.bam'.format(ds=ds) for ds in DATASETS

rule gzip:
	output: '{file}.gz'
	input: '{file}'
	shell: 'gzip < {input} > {output}'

rule cutadapt:
	output: fastq=temp('{ds}.fastq')
	input: '{ds}.txt'
	shell:
		'echo hello > {output.fastq}'

rule bwa_mature:
	output: bam='{ds}.bam'
	input: reads='{ds}.fastq.gz'
	threads: 4
	shell:
		'echo starting with {threads} threads; sleep 10; touch {output.bam}'




================================================
FILE: tests/linting/absolute_paths/negative.smk
================================================
rule a:
    input:
        "/home/johannes/someproject/test.in"
    output:
        "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/absolute_paths/positive.smk
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/envvars/negative.smk
================================================
os.environ["THRESHOLD"] = "1.0"

rule a:
    input:
        "test.in"
    output:
        "test.out"
    params:
        threshold=os.environ["THRESHOLD"]
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "echo {params.threshold} > {output}"


================================================
FILE: tests/linting/envvars/positive.smk
================================================
os.environ["THRESHOLD"] = "1.0"

envvars:
    "THRESHOLD"

rule a:
    output:
        "test.out"
    params:
        threshold=os.environ["THRESHOLD"]
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "echo {params.threshold} > {output}"


================================================
FILE: tests/linting/iofile_by_index/negative.smk
================================================
rule a:
    input:
        "test.in",
        "test.aux"
    output:
        "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input[0]} {output}"


================================================
FILE: tests/linting/iofile_by_index/positive.smk
================================================
rule a:
    input:
        some="test.in",
        other="test.aux"
    output:
        "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input.some} {output}"


================================================
FILE: tests/linting/log_directive/negative.smk
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/log_directive/positive.smk
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/long_run/negative.smk
================================================
rule a:
    input:
        "test.csv"
    output:
        "test.out.csv"
    log:
        "logs/a.log"
    run:
        # this becomes too much and should be migrated into a script directive
        import pandas as pd

        t = pd.read_csv(snakemake.input[0])
        t.sort(inplace=True)

        t.loc["foo"] += 16
        t.loc["bar"] /= 237879.4

        t.to_csv(snakemake.output[0])


================================================
FILE: tests/linting/long_run/positive.smk
================================================
rule a:
    input:
        "test.csv"
    output:
        "test.out.csv"
    log:
        "logs/a.log"
    run:
        # a few lines of code are still ok
        import pandas as pd

        t = pd.read_csv(snakemake.input[0])
        t.sort(inplace=True)

        t.to_csv(snakemake.output[0])


================================================
FILE: tests/linting/missing_software_definition/negative.smk
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    log:
        "logs/a.log"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/missing_software_definition/positive.smk
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


rule b:
    input:
        "test.in"
    output:
        "test2.out"
    log:
        "logs/a.log"
    container:
        "docker://alpine"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/mixed_func_and_rules/common.smk
================================================
def get_dataset(wildcards):
    return config["datasets"]


================================================
FILE: tests/linting/mixed_func_and_rules/negative.smk
================================================
def get_dataset(wildcards):
    return config["datasets"]

rule a:
    input:
        get_dataset
    output:
        "test.{dataset}.out"
    log:
        "logs/a.{dataset}.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/mixed_func_and_rules/positive.smk
================================================
include: "common.smk"

rule a:
    input:
        get_dataset
    output:
        "test.{dataset}.out"
    log:
        "logs/a.{dataset}.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/not_used_params/config.yaml
================================================
threshold: 1.0


================================================
FILE: tests/linting/not_used_params/negative.smk
================================================
configfile: "config.yaml"

rule a:
    output:
        "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "echo {config[threshold]} > {output}"


================================================
FILE: tests/linting/not_used_params/positive.smk
================================================
configfile: "config.yaml"

rule a:
    output:
        "{sample}.out"
    params:
        threshold=config["threshold"]
    log:
        "logs/{sample}.log"
    conda:
        "envs/software.yaml"
    shell:
        "echo {input} {wildcards.sample} {params.threshold} > {output} 2> {log}"


rule b:
    input:
        "test.in"
    output:
        "test.out"
    log:
        "logs/test.log"
    conda:
        "envs/software.yaml"
    shell:
        "awk {{print $1}} <{input} > {output} 2> {log}"



================================================
FILE: tests/linting/params_prefix/negative.smk
================================================
rule a:
    input:
        "path/test.txt"
    output:
        "test.txt"
    params:
        prefix="path"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/params_prefix/positive.smk
================================================
rule a:
    input:
        "path/test.txt"
    output:
        "test.txt"
    params:
        prefix=lambda w, input: os.path.dirname(input[0])
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/path_add/negative.smk
================================================
TABLES = "results/tables/"

rule a:
    input:
        "test.in"
    output:
        TABLES + "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/path_add/positive.smk
================================================
rule a:
    input:
        "test.in"
    output:
        "results/tables/test.out"
    log:
        "logs/a.log"
    conda:
        "envs/software.yaml"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/singularity/negative.smk
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    log:
        "logs/a.log"
    singularity:
        "docker://bash"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/singularity/positive.smk
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    log:
        "logs/a.log"
    container:
        "docker://bash"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/linting/tab_usage/negative.smk
================================================
rule a:
    input:
        "test.txt"
    log:
        "logs/a.log"
    conda:
        "envs/a.yaml"
	output:
    	"test.out"
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/linting/tab_usage/positive.smk
================================================
rule a:
    input:
        "test.txt"
    output:
        "test.out"
    log:
        "logs/a.log"
    conda:
        "envs/a.yaml"
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/profile/Snakefile
================================================


rule all:
    input: expand("{name}.d", name=range(1000))

rule:
    output: "{name}.a"
    shell: "touch {output}"

rule:
    input: "{name}.a"
    output: "{name}.b"
    shell: "touch {output}"

rule:
    input: "{name}.b"
    output: "{name}.c"
    shell: "touch {output}"

rule:
    input: "{name}.c"
    output: "{name}.d"
    shell: "touch {output}"



================================================
FILE: tests/resources/slurmcluster/Makefile
================================================
# taken and modified from https://github.com/stevekm/slurm-cluster-vagrant

SHELL:=/bin/bash

# Create Vagrant VMs
# copy munge authentication key from slurmmaster to node
# !! need cp -p or else munge keys do not work
setup:
	vagrant up #&& \
	#vagrant up node-1 && \
	#vagrant up node-2 
	#vagrant ssh slurmmaster -- -t 'ssh-keygen -b 2048 -t rsa -q -N "" -f /home/vagrant/.ssh/id_rsa' && \
	#vagrant ssh slurmmaster -- -t 'cp /home/vagrant/.ssh/id_rsa.pub /vagrant/id_rsa.slurmmaster.pub' && \
	#vagrant ssh node-1 -- -t 'ssh-keygen -b 2048 -t rsa -q -N "" -f /home/vagrant/.ssh/id_rsa' && \
	#vagrant ssh node-1 -- -t 'cp /home/vagrant/.ssh/id_rsa.pub /vagrant/id_rsa.server.pub' && \
	#vagrant ssh node-2 -- -t 'ssh-keygen -b 2048 -t rsa -q -N "" -f /home/vagrant/.ssh/id_rsa' && \
	#vagrant ssh node-2 -- -t 'cp /home/vagrant/.ssh/id_rsa.pub /vagrant/id_rsa.server.pub' && \
	#vagrant ssh slurmmaster -- -t 'cat /vagrant/id_rsa.node-1.pub >> .ssh/authorized_keys' && \
	#vagrant ssh slurmmaster -- -t 'cat /vagrant/id_rsa.node-2.pub >> .ssh/authorized_keys' && \
	#rm -f  id_rsa.slurmmaster.pub id_rsa.server.pub

# make sure 'slurm' dir is writable for VMs
# start munge in both VMs
# start slurmctld, wait many seconds for it to fully start
# start slurmd
#start:
#	find slurm -type d -exec chmod a+rwx {} \; && \
#	vagrant ssh slurmmaster -- -t 'sudo /etc/init.d/munge start; sleep 5' && \
#	vagrant ssh node-1 -- -t 'sudo /etc/init.d/munge start; sleep 5' && \
#	vagrant ssh node-2 -- -t 'sudo /etc/init.d/munge start; sleep 5' && \
#	vagrant ssh slurmmaster -- -t 'sudo slurmctld; sleep 30' && \
#	vagrant ssh node-1 -- -t 'sudo slurmd; sleep 30' && \
#	vagrant ssh node-2 -- -t 'sudo slurmd; sleep 30' && \
#	vagrant ssh slurmmaster -- -t 'sudo scontrol update nodename=server state=resume; sinfo; sleep 5'
#
sinfo:
	vagrant ssh slurmmaster -- -t 'sinfo'

# might need this to fix node down state?
# fix:
# 	vagrant ssh slurmmaster -- -t 'sudo scontrol update nodename=server state=resume'

# https://slurm.schedmd.com/troubleshoot.html
# munge log: /var/log/munge/munged.log
test:
	@printf ">>> Checking munge keys on both machines\n"
	@vagrant ssh slurmmaster -- -t 'sudo md5sum /etc/munge/munge.key; ls -l /etc/munge/munge.key'
	@vagrant ssh server -- -t 'sudo md5sum /etc/munge/munge.key; ls -l /etc/munge/munge.key'
	@printf "\n\n>>> Checking if slurmmaster can contact node (network)\n"
	@vagrant ssh slurmmaster -- -t 'ping 10.10.10.4 -c1'
	@printf "\n\n>>> Checking if SLURM slurmmaster is running\n"
	@vagrant ssh slurmmaster -- -t 'scontrol ping'
	@printf "\n\n>>> Checking if slurmctld is running on slurmmaster\n"
	@vagrant ssh slurmmaster -- -t 'ps -el | grep slurmctld'
	@printf "\n\n>>> Checking cluster status\n"
	@vagrant ssh slurmmaster -- -t 'sinfo'
	@printf "\n\n>>> Checking if node can contact slurmmaster (network)\n"
	@vagrant ssh node-1 -- -t 'ping 10.10.10.3 -c1'
	@printf "\n\n>>> Checking if node can contact SLURM slurmmaster\n"
	@vagrant ssh node-1 -- -t 'scontrol ping'
	@printf "\n\n>>> Checking if slurmd is running on node\n"
	@vagrant ssh node-1 -- -t 'ps -el | grep slurmd'
	@printf "\n\n>>> Running a test job\n"
	@vagrant ssh slurmmaster -- -t 'sbatch --wrap="hostname"'
	@printf "\n\n>>> Running another test job\n"
	@vagrant ssh slurmmaster -- -t 'sbatch /vagrant/job.sh'
	@printf "\n\n>>> Checking node status\n"
	@vagrant ssh slurmmaster -- -t 'scontrol show nodes=server'

# pull the plug on the VMs
stop:
	vagrant halt --force slurmmaster
	vagrant halt --force node-1
	vagrant halt --force node-2

# delete the VMs
remove:
	vagrant destroy slurmmaster
	vagrant destroy node-1
	vagrant destroy node-2

# location of the SLURM default config generators for making new conf files
get-config-html:
	vagrant ssh slurmmaster -- -t 'cp /usr/share/doc/slurmctld/*.html /vagrant/'

# get rid of the SLURM log files
clean:
	find slurm -type f ! -name ".gitkeep" -exec rm -f {} \;



================================================
FILE: tests/resources/slurmcluster/Vagrantfile
================================================
# -*- mode: ruby -*-
# vi: set ft=ruby :

# save the current directory for later
vagrant_dir = File.expand_path(File.dirname(__FILE__))

#Provisioning inline script
$script = <<SCRIPT
#yum install -y -q vim slurm-wlm openmpi-bin libopenmpi-dev
#yum install -y -q qemu qemu-kvm libvirt-clients libvirt-daemon-system virtinst bridge-utils
echo vagrant ALL=NOPASSWD:ALL > /etc/sudoers.d/vagrant
SCRIPT

# All Vagrant configuration is done below. The "2" in Vagrant.configure
# configures the configuration version (we support older styles for
# backwards compatibility). Please don't change it unless you know what
# you're doing.

Vagrant.configure("2") do |config|
    config.vm.box = "almalinux/8" # the image is 8.5, the URL does not specify the minor version
    config.vm.define "slurmmaster", primary: true do |slurmmaster|
    slurmmaster.vm.hostname = "slurmmaster"
    slurmmaster.vm.post_up_message ="SLURM master is starting"
    slurmmaster.vm.network "private_network", ip: "192.168.55.100"
    slurmmaster.vm.network "forwarded_port", guest: 6817, host: 6817
    slurmmaster.vm.synced_folder "workspace" ENV["GITHUB_WORKSPACE"]
    slurmmaster.vm.synced_folder "resources/slurmconf/", "/cluster/slurm/slurmconf",
              owner: "slurm", group: "slurm"
    slurmmaster.vm.synced_folder "slurm", "/slurmsrc"
    slurmmaster.vm.provision "shell", path: "provision/provision-master.sh"
    config.ssh.username = 'root'
    config.ssh.password = 'vagrant'
    config.ssh.insert_key = 'true'
    #VM provisioning
#    config.vm.provision :shell,
#                :inline => $script

    slurmmaster.vm.provider "virtualbox" do |vb|
    #   # Display the VirtualBox GUI when booting the machine
    #   vb.gui = true
       vb.cpus = 2
       vb.memory = "1024"
       vb.name = "slurmmaster"
       vb.customize ["modifyvm", :id,"--cpuexecutioncap","50"]
    end
  end
  (1..2).each do |i|
    config.vm.define "node-#{i}", autostart: false do |node|
      node.vm.provision :shell, :inline => "setenforce 0", run: "always"
      node.vm.provision "shell", inline: "echo hello from node #{i}"
      node.vm.hostname = "node-#{i}"
      node.vm.network  "private_network", ip: "192.168.55.1#{i}"
      node.vm.network "forwarded_port", guest: 6818, host: 6819
      node.vm.synced_folder "resources/slurmconf/", "/cluster/slurm/slurmconf",
              owner: "slurm", group: "slurm"
      node.vm.synced_folder "slurm", "/slurmsrc"
      node.vm.provision "shell", path: 'provision/provision-node.sh'
      node.vm.provider "virtualbox" do |vbnode|
        vbnode.linked_clone = true
        vbnode.memory = "512"
        vbnode.name = "node-#{i}"
        vbnode.customize ["modifyvm", :id, "--cpuexecutioncap","30"]
      end
    end
  end

    # Some setup on the host for the jgu-slurm-repo to be cloned if not existent
  config.trigger.before :up do |trigger|
  	if File.directory?(File.join(vagrant_dir,'slurm')) then
 		   trigger.warn = "slurm src repo already exists"
  	else
             trigger.info = "cloning SLURM"
             trigger.run = {path: "clone.sh"}
	  end
  end

end



================================================
FILE: tests/test (with parenthese's)/Snakefile
================================================
shell.executable("bash")

rule:
	input: "test.in"
	output: "test.out"
	run:
		test = shell("echo 42;", read=True)
		assert int(test) == 42
		with open(output[0], "w") as f:
			for l in shell("cat {input}", iterable=True):
				print(l, file=f)



================================================
FILE: tests/test (with parenthese's)/test.in
================================================
foo
bar



================================================
FILE: tests/test01/Snakefile
================================================
from snakemake.utils import min_version

from snakemake import __version__
print(__version__, file=sys.stderr)

# The next line does not seem to be needed for this test to pass.
# If it is re-enabled then the default setting must be restored afterwards
# because this is a global setting and breaks some other tests.
#shell.prefix("source ~/.bashrc; ")

TEST = "abc"


onstart:
    print("Workflow starting")
    print("Log:")
    print(log)

onsuccess:
    print("Workflow finished")
    print("Log:")
    print(log)


onerror:
    print("Workflow failed")
    print("Log:")
    print(log)




ruleorder: rule2 > rule4

def testin(wildcards):
	return "test.in"

def version():
	return "3.3"

rule rule1:
	# This rule creates an intermediate file.
	input: 
		'test.inter'
	output: 'dir/test.out'
	log:    a='log/logfile.log'
	threads: 3
	shell: 
		'if [ {threads} -ne 3 ]; then echo "This test has to be run with -j3 in order to succeed!"; exit 1; fi; ' \
		'echo {TEST}; cp {input[0]} {output[0]}; ' # append a comment
		'echo test > {log.a}'

rule rule2:
	input: testin
	output: 'test.inter'
	message: 'Copying {input[0]} to {output[0]}'
	shell: 
		'''
		cp {input[0]} {output[0]}
		'''

rule rule4:
	input: "test.in"
	output: "test.inter"
	shell: "cp {input} {output}"


# this should be ignored since test.in is present
rule rule3:
	input: "dir/{a}.out"
	output: "{a}.in"
	shell: "cp {input} {output}"




================================================
FILE: tests/test01/test.in
================================================
testz0r



================================================
FILE: tests/test01/expected-results/test.inter
================================================
testz0r



================================================
FILE: tests/test02/Snakefile
================================================
shell.executable("bash")

rule all:
	input: 'test.out'

rule rule1:
	input: '{name}.in'
	output: '{name}.inter'
	#message: 'Copying {input[0]} to {output[0]}'
	log: "logs/{name}.log"
	shell:
		'echo test > {log}; cp {input[0]} {output[0]}'

rule rule2:
	input: '{name}.inter'
	output: '{name}.out'
	#message: 'Copying {input[0]} to {output[0]}'
	shell: 
		'cp {input[0]} {output[0]}'




================================================
FILE: tests/test02/test.in
================================================
testz0r



================================================
FILE: tests/test03/params
================================================
test.out



================================================
FILE: tests/test03/Snakefile
================================================
rule rule1:
	input: '{name}.in'
	output: '{name}.out'
	message: 'Copying {input[0]} to {output[0]}'
	shell:
		'cp {input[0]} {output[0]}'



================================================
FILE: tests/test03/test.in
================================================
testz0r



================================================
FILE: tests/test04/params
================================================
test.out



================================================
FILE: tests/test04/Snakefile
================================================
rule rule1:
	input: '{name}.bogus.in'
	output: '{name}.out'
	message: 'Writing junk to {output[0]}'
	shell: 'echo "junk" > {output[0]}; cat {input}'

rule rule2:
	input: '{name}.in'
	output: '{name}.out'
	message: 'Copying {input[0]} to {output[0]}'
	shell: 'cp {input[0]} {output[0]}'

rule rule3:
	input: '{name}.more.bogus.in'
	output: '{name}.out'
	message: 'Writing junk to {output[0]}'
	shell: 'echo "junk" > {output[0]}; cat{input}'



================================================
FILE: tests/test04/test.in
================================================
testz0r



================================================
FILE: tests/test05/Snakefile
================================================
# Only effects for tests on Win
shell.executable("bash")


chromosomes = [1, 2, 3]

# shell('rm test.*.inter 2> /dev/null | true')


rule all:
    input:
        "test.predictions",


rule compute1:
    input:
        "{name}.in",
    output:
        inter=expand("{{name}}.{chr}.inter", chr=chromosomes),
    resources:
        gpu=1,
    run:
        assert len(output.inter) > 0
        print(output.inter)
        for out in output:
            shell('(cat {input[0]} && echo "Part {out}") > {out}')


rule compute2:
    input:
        "{name}.{chromosome}.inter",
        "other.txt",
    output:
        "{name}.{chromosome}.inter2",
    threads: 2
    resources:
        io=1,
    shell:
        "cp {input[0]} {output[0]}"


rule gather:
    input:
        ["{name}.%s.inter2" % c for c in chromosomes],
    output:
        "{name}.predictions",
    run:
        shell("cat {} > {}".format(" ".join(input), output[0]))


rule other:
    output:
        "other.txt",
    priority: 50
    resources:
        gpu=1,
    shell:
        "touch other.txt"



================================================
FILE: tests/test05/test.in
================================================
testz0r



================================================
FILE: tests/test05/expected-results/test.1.inter
================================================
testz0r
Part test.1.inter



================================================
FILE: tests/test05/expected-results/test.1.inter2
================================================
testz0r
Part test.1.inter



================================================
FILE: tests/test05/expected-results/test.2.inter
================================================
testz0r
Part test.2.inter



================================================
FILE: tests/test05/expected-results/test.2.inter2
================================================
testz0r
Part test.2.inter



================================================
FILE: tests/test05/expected-results/test.3.inter
================================================
testz0r
Part test.3.inter



================================================
FILE: tests/test05/expected-results/test.3.inter2
================================================
testz0r
Part test.3.inter



================================================
FILE: tests/test05/expected-results/test.predictions
================================================
testz0r
Part test.1.inter
testz0r
Part test.2.inter
testz0r
Part test.3.inter



================================================
FILE: tests/test06/Snakefile
================================================
shell.executable("bash")

rule all:
	input: 'test.bla.out'

rule wildcards:
	input: 'test.in'
	output: 'test.{xyz}.out'
	message: 'Creating file {output[0]}, xyz={wildcards.xyz}'
	shell: 'echo {wildcards.xyz} > {output[0]}'



================================================
FILE: tests/test06/test.in
================================================
testz0r



================================================
FILE: tests/test07/Snakefile
================================================

rule rule1:
	input: 'test.in'
	output: 'test.out'
	shell: 
		'cp {input} {output}'

rule rule2:
	input: 'test.in'
	output: 'test2.out'
	shell:
		'cp {input} {output}'



================================================
FILE: tests/test07/test.in
================================================
testz0r



================================================
FILE: tests/test08/Snakefile
================================================
rule rule1:
	input: '{file}.in'
	output: '{file}.inter'
	shell: 
		'cp {input} {output}'

rule rule2:
	input: '{file}.inter'
	output: '{file}.out'
	shell: 
		'cp {input} {output}'



================================================
FILE: tests/test08/test.in
================================================
testz0r



================================================
FILE: tests/test08/test2.in
================================================
Hoi



================================================
FILE: tests/test09/Snakefile
================================================
def fail(input, output):
    shell("false && cp {input} {output}")


def x(input, output):
    fail(input, output)


rule rule2:
    input:
        "test.inter",
    output:
        "test.out",
    shell:
        "cp {input} {output}"


rule rule1:
    input:
        "test.in",
    output:
        "test.inter",
    log:
        "logs/test.log",
    shell:
        "touch {log} && false && cp {input} {output}"



================================================
FILE: tests/test09/test.in
================================================
testz0r



================================================
FILE: tests/test10/Snakefile
================================================
shell.executable("bash")

rule rule1:
	input: 'test.in'
	output: 'test.out'
	run:
		shell('cp {input} {output}')



================================================
FILE: tests/test10/test.in
================================================
testz0r



================================================
FILE: tests/test11/import.snakefile
================================================
rule:
	input: 'test.in'
	output: 'test.inter'
	shell: 
		'cp {input} {output}'



================================================
FILE: tests/test11/Snakefile
================================================
include: "import.snakefile"

rule:
	input: 'test.inter'
	output: 'test.out'
	shell: 
		'cp {input} {output}'



================================================
FILE: tests/test11/test.in
================================================
testz0r



================================================
FILE: tests/test11/expected-results/test.inter
================================================
testz0r



================================================
FILE: tests/test12/Snakefile
================================================

rule rule1:
	input: 'test.inter'
	output: 'test.out'
	#message: 'Copying {input[0]} to {output[0]}'
	shell: 
		'cp {input[0]} {output[0]}'

rule rule2:
	input: 'test.in'
	output: temp('test.inter')
	#message: 'Copying {input[0]} to {output[0]}'
	shell:
		'cp {input[0]} {output[0]}'



================================================
FILE: tests/test12/test.in
================================================
testz0r



================================================
FILE: tests/test13/Snakefile
================================================

# Has only effect on Win
shell.executable("bash")


rule all:
	input: 'test.algo1-p7-improved.out'

rule run_algo1:
	input: '{dataset}.in'
	output: '{dataset}.algo1-p{param,[0-9]+}.out'
	shell: 'echo "algo1 / {wildcards.param}" > {output}'

rule postprocess:
	input: '{dataset}.{algorithm}.out'
	output: '{dataset}.{algorithm}-improved.out'
	shell: 'cp {input} {output} && (echo "IMPROVED" >> {output})'



================================================
FILE: tests/test13/test.in
================================================
testz0r



================================================
FILE: tests/test15/Snakefile
================================================

rule a:
	input: test = lambda wildcards: "test2.in" if os.path.exists("test2.in") else "test.in"
	output: "test.out"
	shell: "cp {input.test} {output}"





================================================
FILE: tests/test15/test.in
================================================
testz0r



================================================
FILE: tests/test_access_patterns/sbatch
================================================
#!/bin/bash
set -x
echo --sbatch-- >> sbatch.log
echo `date` >> sbatch.log
tail -n1 $1 >> sbatch.log
cat $1 >> sbatch.log
# daemonize job script
#nohup sh $1 0<&- &>/dev/null &
# print PID for job number
#echo $!
sh $1



================================================
FILE: tests/test_access_patterns/Snakefile
================================================
inputflags:
    access.sequential


rule all:
    input:
        collect("test6.{dataset}.out", dataset=range(1, 3))


rule a:
    output:
        "test1.out"
    shell:
        "echo test > {output}"


rule b:
    input:
        "test1.out" # expected as local copy (because accessed multiple times in group, see test parameters)
    output:
        "test2.{dataset}.out"
    log:
        "logs/b/{dataset}.log"
    shell:
        "(set +eo pipefail; readlink {input} > {output} || true) 2> {log}"


rule c:
    input:
        "test2.{dataset}.out" # expected as local copy (because accessed locally in group, no download from storage)
    output:
        "test3.{dataset}.out"
    log:
        "logs/c/{dataset}.log"
    shell:
        "(set +eo pipefail; readlink {input} > {output} || true) 2> {log}"


rule d:
    input:
        access.random("test3.{dataset}.out") # expected as local copy (because of random access)
    output:
        "test4.{dataset}.out"
    log:
        "logs/d/{dataset}.log"
    shell:
        "(set +eo pipefail; readlink {input} > {output} || true) 2> {log}"


def get_e_input(wildcards):
    return f"test4.{wildcards.dataset}.out"


rule e:
    input:
        get_e_input # expected as symlink (because of sequential access once per cluster job)
    output:
        "test5.{dataset}.out"
    log:
        "logs/e/{dataset}.log"
    shell:
        "(set +eo pipefail; readlink {input} > {output} || true) 2> {log}"


def get_f_input(wildcards):
    return f"test5.{wildcards.dataset}.out"


rule f:
    input:
        access.multi(get_f_input) # expected as local copy (because of multi-sequential access)
    output:
        "test6.{dataset}.out"
    log:
        "logs/f/{dataset}.log"
    shell:
        "(set +eo pipefail; readlink {input} > {output} || true) 2> {log}"


================================================
FILE: tests/test_access_patterns/fs-storage/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_all_temp/Snakefile
================================================
rule all:
    input:
        "test2.txt"
    run:
        import os
        if os.path.exists("test1.txt"):
            raise ValueError("test1.txt still present!")


rule a:
    output:
        "test1.txt"
    shell:
        "touch {output}"


rule b:
    input:
        "test1.txt"
    output:
        "test2.txt"
    shell:
        "touch {output}"


================================================
FILE: tests/test_all_temp/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_ambiguousruleexception/Snakefile
================================================
rule all:
    input:
        [
            "A/one.txt",
            "B/one.txt",
            "A/two.txt",
            "B/two.txt",
            "A/three.txt",
            "B/three.txt",
        ]

rule A:
    output:
        a="A/{sample}.txt",
        c=temp("C/{sample}.txt"),
    shell:
        """
        touch {output.a}
        touch {output.c}
        """

rule B:
    output:
        b="B/{sample}.txt",
        c=temp("C/{sample}.txt"),
    shell:
        """
        touch {output.b}
        touch {output.c}
        """



================================================
FILE: tests/test_ambiguousruleexception/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_ancient/Snakefile
================================================
shell.executable("bash")

shell("touch C && sleep 1 && touch B && sleep 1 && touch A && touch D && \
    mkdir -p .snakemake/storage/http/github.com/snakemake/snakemake/raw/series-7/tests/test_remote_http/expected-results && \
    touch .snakemake/storage/http/github.com/snakemake/snakemake/raw/series-7/tests/test_remote_http/expected-results/landsat-data.txt && \
    touch -t 200101010101 old_file")

#Will not be executed even though A is newer
rule a:
    input:
        ancient("A")
    output:
        "B"
    shell:
        "echo \"B recreated\" > {output}"

#Will be executed because B is newer
rule b:
    input:
        "B"
    output:
        "C"
    shell:
        "echo \"C recreated\" > {output}"

rule c:
    input:
        ancient("C")
    output:
        "D"
    shell:
        "echo \"D recreated\" > {output}"

# This should not run even though output is older than input
rule remote_ancient:
    input:
        storage(
            "https://github.com/snakemake/snakemake/raw/series-7/tests/test_remote_http/expected-results/landsat-data.txt"
        )
    output:
        "old_file"
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_ancient/expected-results/A
================================================
[Empty file]


================================================
FILE: tests/test_ancient/expected-results/B
================================================
[Empty file]


================================================
FILE: tests/test_ancient/expected-results/C
================================================
C recreated



================================================
FILE: tests/test_ancient/expected-results/D
================================================
[Empty file]


================================================
FILE: tests/test_ancient/expected-results/old_file
================================================
[Empty file]


================================================
FILE: tests/test_ancient_cli/Snakefile
================================================
shell.executable("bash")

shell("echo '2' > 2 && echo '4' > 4 && sleep 1 && echo '1' > 1 && echo '3' > 3")

rule all:
    input:
        "2",
        "4"

rule A:
    input:
        "1"
    output:
        "2",
    shell:
        "cat {input} > {output}"

rule B:
    input:
        x="3"
    output:
        "4",
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_ancient_cli/expected-results/1
================================================
1



================================================
FILE: tests/test_ancient_cli/expected-results/2
================================================
2



================================================
FILE: tests/test_ancient_cli/expected-results/3
================================================
3



================================================
FILE: tests/test_ancient_cli/expected-results/4
================================================
4



================================================
FILE: tests/test_ancient_dag/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "d.out",

rule A:
    output:
        "a.out",
    shell:
        "echo 'text' > {output}"

rule B:
    input:
        ancient("a.out"),
    output:
        "b_{i}.out",
    shell:
        "cat {input} > {output}"

# rule C is required for #946
#  use `range(20)` so test will pass in < 5% of cases where issue is present
rule C:
    input:
        expand("b_{i}.out", i=list(range(20))),
    output:
        "c.out",
    shell:
        "cat {input} > {output}"

# For #946, 'a.out' is required as input, but does not need to be `ancient()`
rule D:
    input:
        "a.out",
        "c.out",
    output:
        "d.out",
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_ancient_dag/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_archive/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.out"
    conda:
        "test-env.yaml"
    shell:
        "touch {output}"



================================================
FILE: tests/test_archive/test-env.yaml
================================================
channels:
  - bioconda
  - conda-forge
dependencies:
  - zlib =1.2.11



================================================
FILE: tests/test_azure_batch/Snakefile
================================================
# Test Azure Batch Executor
rule all:
    input:
        "test.txt",


rule write_test:
    output:
        "test.txt",
    shell:
        "echo 'test' > {output}"



================================================
FILE: tests/test_bash/Snakefile
================================================

shell.executable("bash")

rule a:
    output:
        "test.out"
    shell:
        "basename $BASH > {output}"



================================================
FILE: tests/test_batch/Snakefile
================================================
SAMPLES = ["a", "b", "c"]

rule all:
    input:
        "foo.txt",
        "bar.txt"


rule aggregate:
    input:
        expand("{sample}.inter.txt", sample=SAMPLES)
    output:
        "foo.txt"
    shell:
        "touch {output}"


rule feed:
    output:
        temp("{sample}.inter.txt")
    shell:
        "touch {output}"


rule independent:
    output:
        "bar.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_batch/expected-results/a.inter.txt
================================================
[Empty file]


================================================
FILE: tests/test_batch/expected-results/bar.txt
================================================
[Empty file]


================================================
FILE: tests/test_batch_final/Snakefile
================================================
SAMPLES = ["a", "b", "c"]

rule all:
    input:
        "foo.txt",
        "bar.txt"


rule aggregate:
    input:
        expand("{sample}.inter.txt", sample=SAMPLES)
    output:
        "foo.txt"
    shell:
        "touch {output}"


rule feed:
    output:
        temp("{sample}.inter.txt")
    shell:
        "touch {output}"


rule independent:
    output:
        "bar.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_batch_final/expected-results/bar.txt
================================================
[Empty file]


================================================
FILE: tests/test_batch_final/expected-results/foo.txt
================================================
[Empty file]


================================================
FILE: tests/test_benchmark/script.py
================================================
#!/usr/bin/env python3

import sys
import time

print("Hello World!", file=sys.stderr)
time.sleep(1)



================================================
FILE: tests/test_benchmark/Snakefile
================================================
import time


rule all:
    input:
        "test.benchmark_shell.txt",
        "test.benchmark_script.txt",
        "test.benchmark_run.txt",
        "test.benchmark_run_shell.txt",


rule bench_shell:
    benchmark:
        repeat("test.benchmark_shell.txt", 2)
    threads: 2
    params:
        timeout = 4
    shell:
        "stress-ng --cpu {threads} --timeout {params.timeout}s"


rule bench_script:
    benchmark:
        "test.benchmark_script.txt"
    script: 'script.py'


rule bench_run:
    benchmark:
        "test.benchmark_run.txt"
    run:
        time.sleep(1)

rule bench_run_shell:
    benchmark:
        "test.benchmark_run_shell.txt"
    run:
        shell("sleep 1")



================================================
FILE: tests/test_benchmark/expected-results/test.benchmark_run.txt
================================================
s	h:m:s	max_rss	max_vms	max_uss	max_pss	io_in	io_out	mean_load	cpu_time	jobid	rule_name	wildcards	threads	cpu_usage	resources	input_size_mb
1.0338	0:00:01	76.62	251.66	53.71	60.87	0.00	0.01	0.00	0.74	0	bench_run	{}	1	0.00	{'_cores': 1, '_nodes': 1, 'mem_mb': 1000, 'mem_mib': 954, 'disk_mb': 1000, 'disk_mib': 954, 'tmpdir': '/tmp'}	{}



================================================
FILE: tests/test_benchmark/expected-results/test.benchmark_run_shell.txt
================================================
s	h:m:s	max_rss	max_vms	max_uss	max_pss	io_in	io_out	mean_load	cpu_time	jobid	rule_name	wildcards	threads	cpu_usage	resources	input_size_mb
1.0359	0:00:01	82.98	272.39	55.28	63.38	0.00	0.01	0.00	0.76	0	bench_run_shell	{}	1	0.00	{'_cores': 1, '_nodes': 1, 'mem_mb': 1000, 'mem_mib': 954, 'disk_mb': 1000, 'disk_mib': 954, 'tmpdir': '/tmp'}	{}



================================================
FILE: tests/test_benchmark/expected-results/test.benchmark_script.txt
================================================
s	h:m:s	max_rss	max_vms	max_uss	max_pss	io_in	io_out	mean_load	cpu_time	jobid	rule_name	wildcards	threads	cpu_usage	resources	input_size_mb
1.2341	0:00:01	29.10	57.64	14.92	17.68	0.00	0.00	0.00	0.10	2	bench_script	{}	1	0.00	{'_cores': 1, '_nodes': 1, 'tmpdir': '/tmp'}	{}



================================================
FILE: tests/test_benchmark/expected-results/test.benchmark_shell.txt
================================================
s	h:m:s	max_rss	max_vms	max_uss	max_pss	io_in	io_out	mean_load	cpu_time	jobid	rule_name	wildcards	threads	cpu_usage	resources	input_size_mb
10.0037	0:00:10	4.25	35.97	0.39	0.85	0.00	0.00	186.26	19.73	1	bench_shell	{}	2	1863.32	{'_cores': 2, '_nodes': 1, 'tmpdir': '/tmp'}	{}
10.0050	0:00:10	4.38	35.97	0.40	0.88	0.00	0.00	185.92	19.65	1	bench_shell	{}	2	1860.17	{'_cores': 2, '_nodes': 1, 'tmpdir': '/tmp'}	{}



================================================
FILE: tests/test_benchmark_jsonl/input1.zero
================================================
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                


================================================
FILE: tests/test_benchmark_jsonl/input2.zero
================================================
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                


================================================
FILE: tests/test_benchmark_jsonl/script.py
================================================
#!/usr/bin/env python3

import sys
import time

print("Hello World!", file=sys.stderr)
time.sleep(1)



================================================
FILE: tests/test_benchmark_jsonl/Snakefile
================================================
import time


rule all:
    input:
        "test.benchmark_shell.jsonl",
        "test.benchmark_script.jsonl",
        "test.benchmark_run.jsonl",
        "test.benchmark_run_shell.jsonl",


rule bench_shell:
    input:
        "input1.zero",
        "input2.zero",
    benchmark:
        repeat("{name}.benchmark_shell.{ext}", 2)
    params:
        timeout = 4,
    threads: 2
    shell:
        "stress-ng --cpu {threads} --timeout {params.timeout}s"


rule bench_script:
    input:
        "input1.zero",
        "input2.zero",
    benchmark:
        "{name}.benchmark_script.{ext}"
    script:
        "script.py"


rule bench_run:
    input:
        "input1.zero",
        "input2.zero",
    benchmark:
        "{name}.benchmark_run.{ext}"
    params:
        sleep=1,
    run:
        time.sleep(params.sleep)


rule bench_run_shell:
    input:
        "input1.zero",
        "input2.zero",
    benchmark:
        "{name}.benchmark_run_shell.{ext}"
    params:
        sleep=1,
    run:
        shell("sleep {params.sleep}")



================================================
FILE: tests/test_benchmark_jsonl/expected-results/test.benchmark_run.jsonl
================================================
{"cpu_time": 0.51, "cpu_usage": 0.0, "h:m:s": "0:00:01", "input_size_mb": {"input1.zero": 0.00048828125, "input2.zero": 0.00048828125}, "io_in": 0.00390625, "io_out": 0.0078125, "jobid": 0, "max_pss": 44.8125, "max_rss": 52.43359375, "max_uss": 38.67578125, "max_vms": 216.90234375, "mean_load": 0.0, "params": {"sleep": 1}, "resources": {"_cores": 1, "_nodes": 1, "disk_mb": 1000, "disk_mib": 954, "mem_mb": 1000, "mem_mib": 954, "tmpdir": "/tmp"}, "rule_name": "bench_run", "s": "1.0313", "threads": 1, "wildcards": {"ext": "jsonl", "name": "test"}}



================================================
FILE: tests/test_benchmark_jsonl/expected-results/test.benchmark_run_shell.jsonl
================================================
{"cpu_time": 0.52, "cpu_usage": 0.0, "h:m:s": "0:00:01", "input_size_mb": {"input1.zero": 0.00048828125, "input2.zero": 0.00048828125}, "io_in": 0.00390625, "io_out": 0.0078125, "jobid": 0, "max_pss": 45.6328125, "max_rss": 56.0703125, "max_uss": 39.0234375, "max_vms": 236.640625, "mean_load": 0.0, "params": {"sleep": 1}, "resources": {"_cores": 1, "_nodes": 1, "disk_mb": 1000, "disk_mib": 954, "mem_mb": 1000, "mem_mib": 954, "tmpdir": "/tmp"}, "rule_name": "bench_run_shell", "s": "1.0323", "threads": 1, "wildcards": {"ext": "jsonl", "name": "test"}}



================================================
FILE: tests/test_benchmark_jsonl/expected-results/test.benchmark_script.jsonl
================================================
{"cpu_time": 0.1, "cpu_usage": 0.0, "h:m:s": "0:00:01", "input_size_mb": {"input1.zero": 0.00048828125, "input2.zero": 0.00048828125}, "io_in": 0.0, "io_out": 0.0, "jobid": 2, "max_pss": 20.1259765625, "max_rss": 28.90234375, "max_uss": 14.97265625, "max_vms": 57.59765625, "mean_load": 0.0, "params": {}, "resources": {"_cores": 1, "_nodes": 1, "tmpdir": "/tmp"}, "rule_name": "bench_script", "s": "1.2320", "threads": 1, "wildcards": {"ext": "jsonl", "name": "test"}}



================================================
FILE: tests/test_benchmark_jsonl/expected-results/test.benchmark_shell.jsonl
================================================
{"cpu_time": 9.84, "cpu_usage": 930.1355365276336, "h:m:s": "0:00:10", "input_size_mb": {"input1.zero": 0.00048828125, "input2.zero": 0.00048828125}, "io_in": 0.0, "io_out": 0.0, "jobid": 1, "max_pss": 0.9453125, "max_rss": 4.18359375, "max_uss": 0.40625, "max_vms": 28.1796875, "mean_load": 92.97852629914158, "params": {"timeout": 10}, "resources": {"_cores": 1, "_nodes": 1, "tmpdir": "/tmp"}, "rule_name": "bench_shell", "s": "10.0038", "threads": 1, "wildcards": {"ext": "jsonl", "name": "test"}}
{"cpu_time": 9.82, "cpu_usage": 930.0991464614868, "h:m:s": "0:00:10", "input_size_mb": {"input1.zero": 0.00048828125, "input2.zero": 0.00048828125}, "io_in": 0.0, "io_out": 0.0, "jobid": 1, "max_pss": 0.947265625, "max_rss": 4.171875, "max_uss": 0.39453125, "max_vms": 28.1796875, "mean_load": 92.9763578008559, "params": {"timeout": 10}, "resources": {"_cores": 1, "_nodes": 1, "tmpdir": "/tmp"}, "rule_name": "bench_shell", "s": "10.0036", "threads": 1, "wildcards": {"ext": "jsonl", "name": "test"}}



================================================
FILE: tests/test_cache_multioutput/Snakefile
================================================
rule a:
    output:
        "1.txt",
        "2.txt",
    cache: True
    shell:
        "touch {output}"


================================================
FILE: tests/test_cache_multioutput/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_censored_path/Snakefile
================================================
storage:
    provider="xrootd",
    username="my_username",
    password="my_password",
    url_decorator="url + '?param_name=param_value'"

rule:
    output: storage.xrootd("root://somehost//path/to/file.root")
    shell: "echo"



================================================
FILE: tests/test_checkpoint_allowed_rules/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
cat $1 >> qsub.log
sh $1



================================================
FILE: tests/test_checkpoint_allowed_rules/Snakefile
================================================
checkpoint a:
    output: touch("a.txt")

rule b:
    output: touch("b.txt")

def get_input(wildcards):
    checkpoints.a.get()
    return "b.txt"

rule c:
    input: get_input
    shell: "echo {input}"


================================================
FILE: tests/test_checkpoint_allowed_rules/expected-results/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_checkpoint_missout/problem.txt
================================================
[Empty file]


================================================
FILE: tests/test_checkpoint_missout/Snakefile
================================================
rule final:
    input:
        "problem.txt",
    output:
        "final.txt",
    shell:
        "cat {input} > {output}"


checkpoint my_checkpoint:
    output:
        directory("checkpoint_dir"),
    shell:
        "mkdir -p {output}; cd {output}; touch 1 2"


def my_input_func(wc):
    print("!!!!!!!!!! input function called", flush=True)
    checkpoint_file = checkpoints.my_checkpoint.get().output[0]
    assert os.path.exists(checkpoint_file), f"{checkpoint_file} does not exist!"
    print("!!!!!!!!!! checkpoint file exists", flush=True)

    from pathlib import Path

    return list(Path(checkpoint_file).glob("*"))


rule problem:
    input:
        my_input_func,
        #"checkpoint_dir",
    output:
        touch("problem.txt"),



================================================
FILE: tests/test_checkpoint_missout/expected-results/final.txt
================================================
[Empty file]


================================================
FILE: tests/test_checkpoint_open/Snakefile
================================================
def get_input(wildcards):
    with checkpoints.a.get().output[0].open() as f:
        f.read()
        return "test.txt"

rule b:
    input:
        get_input
    output:
        "test2.txt"
    shell:
        "cp {input} {output}"


checkpoint a:
    output:
        "test.txt"
    shell:
        "echo hello > {output}"



================================================
FILE: tests/test_checkpoint_open/expected-results/storage/test2.txt
================================================
hello



================================================
FILE: tests/test_checkpoint_open/storage/test.txt
================================================
hello



================================================
FILE: tests/test_checkpoints/Snakefile
================================================

rule all:
    input:
        "aggregated/a.txt",
        "aggregated/b.txt"


checkpoint somestep:
    input:
        "samples/{sample}.txt"
    output:
        "somestep/{sample}.txt"
    shell:
        # simulate some output vale
        "echo {wildcards.sample} > {output}"


rule intermediate:
    input:
        "somestep/{sample}.txt"
    output:
        "post/{sample}.txt"
    shell:
        "touch {output}"


rule alt_intermediate:
    input:
        "somestep/{sample}.txt"
    output:
        "alt/{sample}.txt"
    shell:
        "touch {output}"


def aggregate_input(wildcards):
    # decision based on content of output file
    with checkpoints.somestep.get(**wildcards).output[0].open() as f:
        if f.read().strip() == "a":
            return "post/{sample}.txt"
        else:
            return "alt/{sample}.txt"


rule aggregate:
    input:
        aggregate_input
    output:
        "aggregated/{sample}.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_checkpoints/expected-results/aggregated/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_checkpoints/samples/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_checkpoints/samples/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_checkpoints_dir/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "aggregated/a.txt",
        "aggregated/b.txt"


# create three txt files 1.txt to 3.txt containing
# the numbers 1, 2, and 3 respectively.
checkpoint clustering:
    input:
        "samples/{sample}.txt"
    output:
        clusters=directory("clustering/{sample}")
    shell:
        "mkdir clustering/{wildcards.sample}; "
        "for i in 1 2 3; do echo $i > clustering/{wildcards.sample}/$i.txt; done"


rule intermediate:
    input:
        "clustering/{sample}/{i}.txt"
    output:
        "post/{sample}/{i}.txt"
    shell:
        "cp {input} {output}"


# input function collecting all txt files (all i's)
# for a given sample. E.g. for sample b:
# ['post/b/1.txt', 'post/b/3.txt', 'post/b/2.txt']
# To avoid different order based on different architectures,
# this input file list is sorted. For details see
# https://github.com/snakemake/snakemake/pull/826#issue-550400376
def aggregate_input(wildcards):
    return sorted(
        expand(
            "post/{sample}/{i}.txt",
            sample=wildcards.sample,
            i=glob_wildcards(
                os.path.join(
                    checkpoints.clustering.get(**wildcards).output[0], "{i}.txt"
                )
            ).i,
        )
    )


rule aggregate:
    input:
        aggregate_input
    output:
        "aggregated/{sample}.txt"
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_checkpoints_dir/expected-results/aggregated/a.txt
================================================
1
2
3



================================================
FILE: tests/test_checkpoints_dir/expected-results/aggregated/b.txt
================================================
1
2
3



================================================
FILE: tests/test_checkpoints_dir/samples/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_checkpoints_dir/samples/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_checkpoints_many/Snakefile
================================================
import glob
import random
from pathlib import Path

ALL_SAMPLES = ["s1"]


rule all:
    input:
        expand("collect/{sample}/all_done.txt", sample=ALL_SAMPLES),


rule before:
    output:
        ALL_SAMPLES,
    run:
        for sample in ALL_SAMPLES:
            Path(sample).touch()


checkpoint first:
    input:
        expand("{sample}", sample=ALL_SAMPLES),
    output:
        directory("first/{sample}"),
    run:
        for i in range(1, 5):
            Path(f"{output[0]}/{i}").mkdir(parents=True, exist_ok=True)
            Path(f"{output[0]}/{i}/test.txt").touch()


checkpoint second:
    input:
        "first/{sample}/{i}/test.txt",
    output:
        directory("second/{sample}/{i}"),
    run:
        for j in range(6, 10):
            Path(f"{output[0]}/{j}").mkdir(parents=True, exist_ok=True)
            Path(f"{output[0]}/{j}/test2.txt").touch()


rule copy:
    input:
        "second/{sample}/{i}/{j}/test2.txt",
    output:
        touch("copy/{sample}/{i}/{j}/test2.txt"),


def aggregate(wildcards):

    outputs_i = glob.glob(f"{checkpoints.first.get(**wildcards).output}/*/")

    outputs_i = [output.split("/")[-2] for output in outputs_i]

    split_files = []
    for i in outputs_i:
        s2out = checkpoints.second.get(**wildcards, i=i).output[0]
        assert Path(s2out).exists()
        output_j = glob.glob(f"{s2out}/*/")
        outputs_j = [output.split("/")[-2] for output in output_j]
        for j in outputs_j:
            split_files.extend(
                expand(f"copy/{{sample}}/{i}/{j}/test2.txt", sample=wildcards.sample)
            )
    return split_files


rule collect:
    input:
        aggregate,
    output:
        touch("collect/{sample}/all_done.txt"),



================================================
FILE: tests/test_checkpoints_many/expected-results/collect/s1/all_done.txt
================================================
[Empty file]


================================================
FILE: tests/test_cloud_checkpoints_issue574/config.json
================================================
{"message": "hahaha"}



================================================
FILE: tests/test_cloud_checkpoints_issue574/env.yml
================================================
channels:
  - conda-forge
dependencies:
  - bzip2



================================================
FILE: tests/test_cloud_checkpoints_issue574/Snakefile
================================================
import os

# This test file is adapted from this one:
# https://github.com/snakemake/snakemake/blob/758fabdb64255f8ca79e9c1483ceab67eb39ff07/tests/test_google_lifesciences/Snakefile
from snakemake.remote.GS import RemoteProvider as GSRemoteProvider
GS = GSRemoteProvider()

rule all:
    input:
        "landsat-data.txt.bz2"

checkpoint copy:
    input:
        GS.remote("gcp-public-data-landsat/LC08/01/001/003/LC08_L1GT_001003_20170430_20170501_01_RT/LC08_L1GT_001003_20170430_20170501_01_RT_MTL.txt")
    output:
        "landsat-data.txt"
    resources:
        mem_mb=100
    run:
        shell("cp {input} {output}")

def get_pack_input(wildcards):
    output = checkpoints.copy.get().output[0]
    return output


rule pack:
    input:
        get_pack_input
    output:
        "landsat-data.txt.bz2"
    conda:
        "env.yml"
    log:
        "logs/pack.log"
    shell:
        "bzip2 -c {input} > {output}; echo successful > {log}"



================================================
FILE: tests/test_cloud_checkpoints_issue574/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_cluster_cancelscript/sbatch
================================================
#!/bin/bash
set -x
echo --sbatch-- >> sbatch.log
echo `date` >> sbatch.log
tail -n1 $1 >> sbatch.log
cat $1 >> sbatch.log
# daemonize job script
nohup sh $1 0<&- &>/dev/null &
# print PID for job number
echo $!



================================================
FILE: tests/test_cluster_cancelscript/scancel.sh
================================================
#!/bin/bash
set -x
echo `date`
echo cancel $* >>scancel.txt

list_descendants ()
{
  local children=$(ps -o pid= --ppid "$1")

  for pid in $children
  do
    list_descendants "$pid"
  done

  echo "$children"
}

for x in $*; do
    kill $(list_descendants $x)
done



================================================
FILE: tests/test_cluster_cancelscript/Snakefile.nonstandard
================================================



rule all:
	input: 'f.1', 'f.2'

rule one:
	output: 'f.1'
	shell: "sleep 120s; touch {output}"

rule two:
	output: 'f.2'
	shell: "sleep 120s; touch {output}"



================================================
FILE: tests/test_cluster_cancelscript/status.sh
================================================
echo running



================================================
FILE: tests/test_cluster_cancelscript/test.in
================================================
testz0r



================================================
FILE: tests/test_cluster_cancelscript/expected-results/scancel.txt
================================================
cancel
cancel



================================================
FILE: tests/test_conda/Snakefile
================================================
shell.executable("bash")
conda_env = Path("test-env.yaml")


rule all:
    input:
        expand("test{i}.out2", i=range(3)),


rule a:
    output:
        "test{i}.out",
    conda:
        "test-env.yaml"
    shell:
        "Tm -h > {output} || true"


rule b:
    input:
        "test{i}.out",
    output:
        "test{i}.out2",
    conda:
        conda_env
    shell:
        "Tm -h > {output} || true"



================================================
FILE: tests/test_conda/test-env.yaml
================================================
channels:
  - bioconda
  - conda-forge
dependencies:
  - melt ==1.0.3
  - python <3.10



================================================
FILE: tests/test_conda_cmd_exe/Snakefile
================================================

rule all:
    input:
        expand("test{i}.out2", i=range(3))

rule a:
    output:
        "test{i}.out"
    conda:
        "test-env.yaml"
    shell:
        'make --version > {output} || VER>NUL'


rule b:
    input:
        "test{i}.out"
    output:
        "test{i}.out2"
    conda:
        "test-env.yaml"
    shell:
        "make --version > {output} || VER>NUL"



================================================
FILE: tests/test_conda_cmd_exe/test-env.yaml
================================================
channels:
  - conda-forge
dependencies:
  - make   ==4.3



================================================
FILE: tests/test_conda_custom_prefix/Snakefile
================================================
import platform
shell.executable("bash")


if platform.system() == "Windows":
    custom_path = "custom/*/Scripts/snakemake.exe"
else:
    custom_path = "custom/*/bin/snakemake"


rule all:
    input:
        expand("test{i}.out", i=range(3))

rule a:
    output:
        "test{i}.out"
    conda:
        "test-env.yaml"
    params:
        custom_path=custom_path
    shell:
        "test -e {params.custom_path} && "
        "snakemake --version > {output}"



================================================
FILE: tests/test_conda_custom_prefix/test-env.yaml
================================================
channels:
  - conda-forge
  - bioconda
  - nodefaults
dependencies:
  - snakemake-minimal ==7.28.3



================================================
FILE: tests/test_conda_function/Snakefile
================================================
def conda_func(wildcards, params):
    env_name = f"{params.name_prefix}-test-env-{wildcards.version}"
    shell(
        f"conda create -y -n {env_name} -c conda-forge --override-channels ripgrep=={wildcards.version}"
    )
    return env_name


onsuccess:
    shell("conda env remove -y -n foo-test-env-12.1.1")
    shell("conda env remove -y -n foo-test-env-13.0.0")


onerror:
    shell("conda env remove -y -n foo-test-env-12.1.1")
    shell("conda env remove -y -n foo-test-env-13.0.0")


rule all:
    input:
        "test_13.0.0_v.out",
        "test_12.1.1_v.out",


rule a:
    output:
        "test_{version}_v.out",
    log:
        err="test_{version}.log",
    params:
        name_prefix="foo",
    conda:
        conda_func
    shell:
        r"(rg --version | grep -o 'ripgrep [0-9]*\.[0-9]*\.[0-9]*' > {output}) 2> {log.err}"



================================================
FILE: tests/test_conda_global/env.yaml
================================================
channels:
  - conda-forge
dependencies:
  - ripgrep
  - pillow =10.0


================================================
FILE: tests/test_conda_global/Snakefile
================================================
import sys
import os

conda:
    "env.yaml"

print(sys.path, file=sys.stderr)
print(os.environ["PATH"], file=sys.stderr)

import PIL
from snakemake.shell import shell
shell("which rg")



================================================
FILE: tests/test_conda_global/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_conda_named/Snakefile
================================================
import subprocess as sp
import sys


try:
    shell(
        "conda create -y -n xxx-test-env -c conda-forge --override-channels ripgrep==13.0.0"
    )
    print("created conda env", file=sys.stderr)
except sp.CalledProcessError as e:
    print(e.stderr)
    raise e


onsuccess:
    shell("conda env remove -y -n xxx-test-env")


onerror:
    shell("conda env remove -y -n xxx-test-env")


rule a:
    output:
        "test.out",
    log:
        err="test.log",
    conda:
        "xxx-test-env"
    shell:
        r"(rg --version | grep -o 'ripgrep [0-9]*\.[0-9]*\.[0-9]*' > {output}) 2> {log.err}"



================================================
FILE: tests/test_conda_pin_file/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.txt"
    conda:
        "test-env.yaml"
    shell:
        "rg --version > version.txt; head -n1 version.txt | cut -f2 -d' ' > {output}"




================================================
FILE: tests/test_conda_pin_file/test-env.linux-64.pin.txt
================================================
# This file may be used to create an environment using:
# $ conda create --name <env> --file <this file>
# platform: linux-64
@EXPLICIT
https://conda.anaconda.org/conda-forge/linux-64/_libgcc_mutex-0.1-conda_forge.tar.bz2#d7c89558ba9fa0495403155b64376d81
https://conda.anaconda.org/conda-forge/linux-64/libgomp-12.1.0-h8d9b700_16.tar.bz2#f013cf7749536ce43d82afbffdf499ab
https://conda.anaconda.org/conda-forge/linux-64/_openmp_mutex-4.5-2_gnu.tar.bz2#73aaf86a425cc6e73fcf236a5a46396d
https://conda.anaconda.org/conda-forge/linux-64/libgcc-ng-12.1.0-h8d9b700_16.tar.bz2#4f05bc9844f7c101e6e147dab3c88d5c
https://conda.anaconda.org/conda-forge/linux-64/ripgrep-13.0.0-h2f28480_2.tar.bz2#15a0bf4a1260b0a08198eb683eb272fb



================================================
FILE: tests/test_conda_pin_file/test-env.yaml
================================================
channels:
  - conda-forge
dependencies:
  - ripgrep =12.0



================================================
FILE: tests/test_conda_pin_file/expected-results/test.txt
================================================
13.0.0



================================================
FILE: tests/test_conda_python_3_7_script/Snakefile
================================================
rule random_python_conda_script:
	output:
		"version.txt"
	conda:
		"test_python_env.yaml"
	script:
		"test_script_python_3_7.py"



================================================
FILE: tests/test_conda_python_3_7_script/test_python_env.yaml
================================================
channels:
  - conda-forge
  - defaults
dependencies:
  - python =3.7.12
  # add a dependency that is not used by Snakemake itself,
  # to simulate the case where the Python script needs 3.7 and
  # that dependency
  - pillow =9.2



================================================
FILE: tests/test_conda_python_3_7_script/test_script_python_3_7.py
================================================
import platform
import PIL

with open("version.txt", "w") as f:
    f.write(platform.python_version())



================================================
FILE: tests/test_conda_python_3_7_script/expected-results/version.txt
================================================
3.7.12


================================================
FILE: tests/test_conda_python_script/Snakefile
================================================
rule random_python_conda_script:
	output:
		"version.txt"
	conda:
		"test_python_env.yaml"
	script:
		"test_script.py"



================================================
FILE: tests/test_conda_python_script/test_python_env.yaml
================================================
channels:
  - conda-forge
  - defaults
dependencies:
  - numpy ==1.21.4
  - python <3.10



================================================
FILE: tests/test_conda_python_script/test_script.py
================================================
import numpy

with open('version.txt', 'w') as f:
    f.write(numpy.__version__)



================================================
FILE: tests/test_conda_python_script/expected-results/version.txt
================================================
1.21.4


================================================
FILE: tests/test_conda_run/Snakefile
================================================
rule random_python_conda_script:
    output:
        "test.txt"
    conda:
        "test_python_env.yaml"
    params: 
        script=workflow.source_path('test_script_run.py')
    run:
        # Calling a script like this is usually unnecessary with Snakemake
        # we use this pattern just for testing purposes here.
        # In a real workflow, use the script directive instead which
        # is cleaner and gives you many more benefits.
        shell("python {params.script}")



================================================
FILE: tests/test_conda_run/test_python_env.yaml
================================================
channels:
  - conda-forge
  - defaults
dependencies:
  - numpy
  - python



================================================
FILE: tests/test_conda_run/test_script_run.py
================================================
import numpy

with open("test.txt", "w") as f:
    f.write(str(numpy.log2(8)))



================================================
FILE: tests/test_conda_run/expected-results/test.txt
================================================
3.0


================================================
FILE: tests/test_conditional/Snakefile
================================================

CONDITION = False

if CONDITION:
	rule:
		output: "test.out"
		shell:  "echo bla; exit 1"
else:
	rule:
		output: "test.out"
		shell:  "touch {output}"


# the following is bad style and just for testing. You should rather write a rule that collects test{i}.out as input files and have ONE rule that generates any of the files with a wildcard.
for i in range(3):
	rule:
		output: "test.{i}.out".format(i=i)
		shell:  "touch {output}"



================================================
FILE: tests/test_config/Snakefile
================================================


include: "test.rules"


configfile: "test.json"


rule:
    output:
        config["outfile"]
    shell:
        "touch {output}"



================================================
FILE: tests/test_config/test.json
================================================
{
    "outfile": "test.out"
}



================================================
FILE: tests/test_config/test.rules
================================================
configfile: "test2.json"



================================================
FILE: tests/test_config/test2.json
================================================
{
    "outfile": "test2.out"
}



================================================
FILE: tests/test_config/test3.json
================================================
{
    "outfile": "test3.out"
}



================================================
FILE: tests/test_config_merging/config_cmdline_01.yaml
================================================
block:
    foo: cfg01_foo
    bar: cfg01_bar
    baz: cfg01_baz
    foobar:
        cfg01_key: cfg01_val



================================================
FILE: tests/test_config_merging/config_cmdline_02.yaml
================================================
block:
    foo: cfg02_foo
    qux: cfg02_qux
    bowser: cfg02_bowser



================================================
FILE: tests/test_config_merging/config_snakefile.yaml
================================================
block:
    bar: snake_bar
    fubar: snake_fubar
    foobar: null



================================================
FILE: tests/test_config_merging/Snakefile
================================================
configfile: "config_snakefile.yaml"


rule dump_config:
    output:
        "test.out",
    run:
        import json

        with open(output[0], "w") as fd:
            json.dump(config, fd, sort_keys=True)



================================================
FILE: tests/test_config_replacing/cli-config.yaml
================================================
id: 'id'
value: 'value1'



================================================
FILE: tests/test_config_replacing/Snakefile
================================================
configfile: "workflow-config.yaml"

name = config['name'] if 'name' in config else 'noname'
id = config['id'] if 'id' in config else 'noid'
value = config['value'] if 'value' in config else 'novalue'

rule all:
    input:
        'result.txt'

rule create:
    output:
        'result.txt'
    shell:
        'echo {name}-{id}-{value} > {output}'




================================================
FILE: tests/test_config_replacing/workflow-config.yaml
================================================
name: 'name'



================================================
FILE: tests/test_config_replacing/expected-results/result.txt
================================================
noname-id-value2



================================================
FILE: tests/test_config_replacing_nocli/Snakefile
================================================
configfile: "workflow-config.yaml"

name = config['name'] if 'name' in config else 'noname'
id = config['id'] if 'id' in config else 'noid'
value = config['value'] if 'value' in config else 'novalue'

rule all:
    input:
        'result.txt'

rule create:
    output:
        'result.txt'
    shell:
        'echo {name}-{id}-{value} > {output}'




================================================
FILE: tests/test_config_replacing_nocli/workflow-config.yaml
================================================
name: 'name'



================================================
FILE: tests/test_config_replacing_nocli/expected-results/result.txt
================================================
name-noid-novalue



================================================
FILE: tests/test_config_yte/config.yaml
================================================
__use_yte__: true

test: ?5 + 5



================================================
FILE: tests/test_config_yte/Snakefile
================================================
configfile: "config.yaml"

rule a:
    output:
        "test.out"
    shell:
        "echo {config[test]} > {output}"



================================================
FILE: tests/test_container/Snakefile
================================================


rule a:
    input:
        "test.txt"
    output:
        "test.out"
    container:
        "docker://bash"
    shell:
        'ls {input}; echo "test" > {output}'


rule b:
    output:
        "test.txt"
    shell:
        "touch {output}"


rule c:
    output:
        "invalid.txt"
    container:
        "docker://invalid/image url even with whitespace"
    shell:
        "touch {output}"



================================================
FILE: tests/test_container/test.txt
================================================
[Empty file]


================================================
FILE: tests/test_containerized/Dockerfile
================================================
FROM condaforge/miniforge3:latest
LABEL io.github.snakemake.containerized="true"
LABEL io.github.snakemake.conda_env_hash="cb075ce12360612250e0065299b188ea3547727b9abc02b9927671af50d26bca"

# Step 2: Retrieve conda environments

# Conda environment:
#   source: env.yaml
#   prefix: /conda-envs/2a132eb0d84044eaf02032bd9f9851fd
#   channels:
#     - conda-forge
#     - bioconda
#     - nodefaults
#   dependencies:
#     - bcftools =1.21
RUN mkdir -p /conda-envs/2a132eb0d84044eaf02032bd9f9851fd
COPY env.yaml /conda-envs/2a132eb0d84044eaf02032bd9f9851fd/environment.yaml

# Step 3: Generate conda environments

RUN conda env create --prefix /conda-envs/2a132eb0d84044eaf02032bd9f9851fd --file /conda-envs/2a132eb0d84044eaf02032bd9f9851fd/environment.yaml && \
    conda clean --all -y



================================================
FILE: tests/test_containerized/env.yaml
================================================
channels:
  - conda-forge
  - bioconda
  - nodefaults
dependencies:
  - bcftools =1.21



================================================
FILE: tests/test_containerized/Snakefile
================================================
containerized: "docker://snakemake/containerize-testimage:1.2"

rule a:
    output:
        "test.out"
    conda:
        "env.yaml"
    shell:
        "bcftools 2> {output} || true"



================================================
FILE: tests/test_convert_to_cwl/Snakefile
================================================
import os
print(os.listdir("."))
print(os.getcwd())

TEST = "abc"


onstart:
    print("Workflow starting")
    print("Log:")
    print(log)

onsuccess:
    print("Workflow finished")
    print("Log:")
    print(log)


onerror:
    print("Workflow failed")
    print("Log:")
    print(log)


ruleorder: rule2 > rule4

def testin(wildcards):
	return "test.in"

def version():
	return "3.3"

rule rule1:
	# This rule creates an intermediate file.
	input: 
		'dir/a/test.inter'
	output: 'dir/test.out'
	log:    a='log/logfile.log'
	version: version()
	shell: 
		'echo {TEST}; echo {version}; cp {input[0]} {output[0]}; ' # append a comment
		'echo test > {log.a}'

rule rule2:
	input: testin
	output: 'dir/a/test.inter'
	shell: 
		'''
		cp {input[0]} {output[0]}
		'''

rule rule4:
	input: "test.in"
	output: "dir/a/test.inter"
	shell: "cp {input} {output}"


# this should be ignored since test.in is present
rule rule3:
	input: "dir/{a}.out"
	output: "{a}.in"
	shell: "cp {input} {output}"




================================================
FILE: tests/test_convert_to_cwl/test.in
================================================
testz0r



================================================
FILE: tests/test_converting_path_for_r_script/env.yaml
================================================
channels:
  - conda-forge
dependencies:
  - r-base



================================================
FILE: tests/test_converting_path_for_r_script/r-script.R
================================================

infile <- snakemake@input[["text_file"]]

# Check the read value is as expected in R.
param_dir <- snakemake@params[["param_dir"]]
stopifnot(param_dir == "dir")

outfile <- snakemake@output[["out_file"]]
file.create(outfile)



================================================
FILE: tests/test_converting_path_for_r_script/Snakefile
================================================

from pathlib import Path


rule all:
  input:
    out_file="out-file.txt"


rule step1:
  input:
    text_file=Path("text-file.txt")
  params:
    param_dir=Path("dir")
  output:
    out_file=Path("out-file.txt")
  conda:
    "env.yaml"
  script:
    "r-script.R"



================================================
FILE: tests/test_converting_path_for_r_script/text-file.txt
================================================
[Empty file]


================================================
FILE: tests/test_converting_path_for_r_script/expected-results/.github_touch
================================================
[Empty file]


================================================
FILE: tests/test_core_dependent_threads/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.out"
    threads: workflow.cores * 0.75
    shell:
        "echo {threads} > {output}"



================================================
FILE: tests/test_cwl/Snakefile
================================================
rule a:
    input:
        input=["test.txt"]
    output:
        output="sorted.txt"
    params:
        key=["2"]
    cwl:
        "https://github.com/common-workflow-language/workflows/raw/master/tools/linux-sort.cwl"



================================================
FILE: tests/test_cwl/test.txt
================================================
c	3
a	2
b	1



================================================
FILE: tests/test_cwl/expected-results/sorted.txt
================================================
b	1
a	2
c	3



================================================
FILE: tests/test_default_flags/Snakefile
================================================
inputflags:
    access.sequential


outputflags:
    temp


rule all:
    input:
        "test3.out"


rule a:
    output:
        "test1.out"
    shell:
        "echo test > {output}"


rule b:
    input:
        "test1.out"
    output:
        "test2.out"
    group: "test"
    shell:
        "cp {input} {output}"


rule c:
    input:
        "test2.out"
    output:
        "test3.out"
    group: "test"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/test_default_remote/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "test.3.txt", 'test.4.txt', 'test.5.txt', 'test.6.txt'


def check_remote(f):
    if not f.startswith(".snakemake/storage/"):
        raise ValueError("Input and output are not remote files.")


def list_function(wildcards):
    return ["test.2.txt", "test.txt"]


def dict_function(wildcards):
    return {'a': "test.2.txt", 'b': "test.txt"}


rule a:
    input:
        "test.txt"
    output:
        "{sample}.2.txt"
    run:
        check_remote(input[0])
        check_remote(output[0])
        shell("cp {input} {output}")

rule b:
    input:
        list_function
    output:
        "test.3.txt"
    run:
        for e in input:
            check_remote(e)
        check_remote(output[0])
        shell("cp {input[0]} {output}")

rule c:
    input:
        **dict_function(None)
    output:
        "test.4.txt"
    run:
        for e in input:
            check_remote(e)
        check_remote(output[0])
        shell("cp {input.a} {output}")

rule d:
    input:
        unpack(dict_function)
    output:
        "test.5.txt"
    run:
        for e in input:
            check_remote(e)
        check_remote(output[0])
        shell("cp {input.a} {output}")

rule e:
    input:
        rules.a.output[0]
    output:
        "{sample}.6.txt"
    run:
        for e in input:
            check_remote(e)
        check_remote(output[0])
        shell("cp {input} {output}")


rule base:
    output:
        "test.txt"
    log:
        "logs/base.log"
    shell:
        "touch {output} 2> {log}"



================================================
FILE: tests/test_default_resources/Snakefile
================================================

rule a:
    input:
        "test-remote-bucket/test.in"
    output:
        "test.out"
    group: "test"
    shell:
        "[[ '{resources.mem_mb}' == '1000' ]] && touch {output}"

rule b:
    output:
        "test-remote-bucket/test.in"
    group: "test"
    shell:
        "[[ '{resources.mem_mb}' == '1000' ]] && touch {output}"



================================================
FILE: tests/test_default_resources/test.txt
================================================
test



================================================
FILE: tests/test_default_storage_local_job/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_default_storage_local_job/Snakefile
================================================
rule all:
    input:
        "c.txt"


rule a:
    output:
        local("a.txt")
    shell:
        "echo a > a.txt"


rule b:
    input:
        local("a.txt")
    output:
        "b.txt"
    shell:
        "cat {input} > {output}"

rule c:
    input:
        "b.txt"
    output:
        "c.txt"
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_default_storage_local_job/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_default_target/Snakefile
================================================
rule a:
    output:
        "{sample}.txt"
    shell:
        "echo test > {output}"


rule b:
    input:
        expand("{sample}.txt", sample=[1, 2])
    default_target: True


================================================
FILE: tests/test_default_target/expected-results/1.txt
================================================
test



================================================
FILE: tests/test_default_target/expected-results/2.txt
================================================
test



================================================
FILE: tests/test_deferred_func_eval/Snakefile
================================================
import os

shell.executable("bash")


def get_mem_gb(wildcards, input):
    size = 0
    if os.path.exists(input[0]):
        size = int(os.path.getsize(input[0]) / (1024 ** 3))
    return max(size, 1)


rule a:
    input:
        "test1.in",
        "test2.in"
    output:
        "test.out"
    params:
        a=lambda wildcards, input, resources: "+".join(input)
    resources:
        mem_gb=get_mem_gb
    shell:
        "echo {params.a} > {output}"



================================================
FILE: tests/test_deferred_func_eval/test1.in
================================================
[Empty file]


================================================
FILE: tests/test_deferred_func_eval/test2.in
================================================
[Empty file]


================================================
FILE: tests/test_delete_all_output/infile
================================================
[Empty file]


================================================
FILE: tests/test_delete_all_output/Snakefile
================================================
shell.executable("bash")

rule all:
    input: "delete_all_output", "delete_temp_output"
    output: touch("all_ok")

rule delete_all_output:
    output: touch("delete_all_output")
    shell:
        """
        echo $PATH
        ls
        python -m snakemake --cores 1 -s Snakefile_inner all && \
        rm nothere && \
        python -m snakemake --cores 1 -s Snakefile_inner --delete-all-output all && \
        test -f infile && test ! -f intermediate && test ! -f some_dir/final && \
        test ! -d empty_dir && test ! -L dangling && test ! -d full_dir
        """

rule delete_temp_output:
    output: touch("delete_temp_output")
    shell:
        """
        python -m snakemake --cores 1 -s Snakefile_inner --notemp temp && \
        python -m snakemake --cores 1 -s Snakefile_inner --delete-temp-output temp && \
        test -f infile && test ! -f temp_intermediate && \
        test ! -d temp_empty_dir && test ! -d temp_full_dir && test -f temp_keep
        """



================================================
FILE: tests/test_delete_all_output/Snakefile_inner
================================================
shell.executable("bash")

rule all:
    input:
        "some_dir/final",
        "empty_dir",
        "full_dir"

rule a:
    input: "infile"
    output: "intermediate", "dangling", touch(protected("protected"))
    shell: "ln -s {input} {output[0]} && touch nothere && ln -s nothere {output[1]}"

rule b:
    input: "intermediate"
    output: touch("some_dir/final")

rule c:
    output: directory("empty_dir")
    shell: "mkdir empty_dir"

rule d:
    output: directory("full_dir")
    shell: "mkdir full_dir && touch full_dir/somefile"

rule e:
    input: "infile"
    output: temp("temp_intermediate")
    shell: "touch {output}"

rule f:
    input: "temp_intermediate"
    output: directory(temp("temp_empty_dir"))
    shell: "mkdir temp_empty_dir"

rule g:
    input: "temp_intermediate"
    output: directory(temp("temp_full_dir"))
    shell: "mkdir temp_full_dir && touch temp_full_dir/somefile"

rule temp:
    input: "temp_empty_dir", "temp_full_dir"
    output: touch("temp_keep")



================================================
FILE: tests/test_delete_all_output/expected-results/all_ok
================================================
[Empty file]


================================================
FILE: tests/test_delete_output/nosuchfile
================================================
[Empty file]


================================================
FILE: tests/test_delete_output/Snakefile
================================================
# See bug #300. This tests that output files really are cleaned up
# before running a rule, and touched afterwards.
#
# The output should be deleted before the job starts.
# (The output should be deleted on the the head node for cluster jobs.)
# The path to the output should be created
# The output should be touch'd on the head node to always be new.
#
# Additionally this should work for directories, symlinks and symlinks
# to directories.
#
# TODO - consider adding a cluster-based test for point 2 above.
shell.executable("bash")


# Setup - touch a mock input file and an out-of-date output file.
shell("touch -t 201604010000 output.file")
shell("touch input.file")

# An empty directory
shell("mkdir -p output.dir ; touch -ch -t 201604010000 output.dir")
# A dangling symlink
shell("ln -fs nosuchfile output.link ; touch -ch -t 201604010000 output.link")
# A symlink to an empty directory
shell("mkdir -p an_empty_dir; ln -fs an_empty_dir output.dirlink ; touch -ch -t 201604010000 an_empty_dir output.dirlink")


rule main:
    input: "output.file", "output.dir", "output.link", "output.dirlink"

rule make_the_file:
    output: "output.file", "foo/output.foo.file"
    input: "input.file"
    # Rule fails if any output.file is already present
    run:
        shell("test ! -e output.file")
        shell("test -d foo")
        shell("test ! -e foo/*")
        shell("touch -t 201604010000 output.file")
        shell("touch foo/output.foo.file")

rule make_the_dir:
    output: directory("output.dir")
    input: "input.file"
    #mkdir fails if the dir is already present
    run:
        shell("mkdir output.dir")
        shell("touch output.dir/foo")

rule make_the_links:
    output: "output.link", directory("output.dirlink")
    input: "input.file"
    # Both links should be gone, but an_empty_dir should not have been removed
    # as it's not a direct target of the rule.
    run:
        shell("touch arealfile")
        shell("ln -s arealfile output.link")
        shell("test -d an_empty_dir")
        shell("mkdir empty_dir2")
        shell("ln -s empty_dir2 output.dirlink")



================================================
FILE: tests/test_delete_output/expected-results/output.file
================================================
[Empty file]


================================================
FILE: tests/test_delete_output/expected-results/foo/output.foo.file
================================================
[Empty file]


================================================
FILE: tests/test_deploy_hashing/a.post-deploy.sh
================================================
#!/bin/bash

echo "test" > $CONDA_PREFIX/a.txt


================================================
FILE: tests/test_deploy_hashing/a.yaml
================================================
channels:
  - bioconda
  - conda-forge
dependencies:
  - python =3.10



================================================
FILE: tests/test_deploy_hashing/b.yaml
================================================
channels:
  - bioconda
  - conda-forge
dependencies:
  - python =3.10



================================================
FILE: tests/test_deploy_hashing/Snakefile
================================================
rule all:
    input:
        expand("{s}.txt", s=["a", "b"])

rule a:
    output:
        "a.txt"
    conda:
        "a.yaml"
    shell:
        "touch {output}"

rule b:
    output:
        "b.txt"
    conda:
        "b.yaml"
    shell:
        "touch {output}"


================================================
FILE: tests/test_deploy_hashing/expected-results/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_deploy_hashing/expected-results/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_deploy_script/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "test.out"

rule a:
    output:
        "test.out"
    conda:
        "test-env.yaml"
    shell:
        """
        if [ -f $CONDA_PREFIX/test.txt ] ;then
            touch {output}
        fi
        """


================================================
FILE: tests/test_deploy_script/test-env.post-deploy.sh
================================================
#!/bin/bash

touch $CONDA_PREFIX/test.txt


================================================
FILE: tests/test_deploy_script/test-env.yaml
================================================
channels:
  - bioconda
  - conda-forge
dependencies:
  - python <3.10



================================================
FILE: tests/test_deploy_sources/Snakefile
================================================
rule a:
    output:
        "test.out"
    script:
        "scripts/test.py"


================================================
FILE: tests/test_deploy_sources/scripts/test.py
================================================
[Empty file]


================================================
FILE: tests/test_directory/Snakefile
================================================
shell.executable("bash")


rule downstream:
    input: "some/dir"
    output: touch(directory("some/other_dir"))

rule normal:
    input: "input_dir"
    output: directory("some/dir")
    shell:
        "mkdir -p {output} && touch some/dir/some_file"

rule symlinked_output:
    output: directory("symlinked_dir")
    shell: "mkdir -p dir_to_link_to && ln -s dir_to_link_to symlinked_dir"

rule symlinked_input:
    input: "symlinked_dir"

# Check that there is an error if the output isn't a directory
rule file_expecting_dir:
    output: directory("not_a_dir")
    shell: "touch not_a_dir"

# Check that there is an error if an output directory isn't flagged with directory()
rule dir_expecting_file:
    output: "is_a_dir"
    shell: "mkdir is_a_dir"

# Should fail due to being a child to some/dir
rule child_to_other:
    input: "some/other_dir"
    output: touch("some/dir/child")

# Shouldn't fail, even though some/dir is a shared prefix
rule not_child_to_other:
    input: "some/other_dir"
    output: touch("some/dir-child")

# Shouldn't fail since children to inputs are currently allowed.
rule child_to_input:
    input: "input_dir/child"
    output: touch("child_to_input")

rule shadow:
    input: "some/dir"
    output: directory("some/shadow")
    shadow: "shallow"
    shell: "mkdir -p {output}"



================================================
FILE: tests/test_directory/expected-results/child_to_input
================================================
[Empty file]


================================================
FILE: tests/test_directory/expected-results/dir_to_link_to/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_directory/expected-results/some/dir-child
================================================
[Empty file]


================================================
FILE: tests/test_directory/expected-results/some/shadow/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_directory/expected-results/some_other_dir/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_directory/input_dir/child
================================================
[Empty file]


================================================
FILE: tests/test_dup_out_patterns/Snakefile
================================================
rule default:
    input:
        "out.txt"

rule my_rule:
    output:
        "{name}.txt",
        "{name}.txt",
    shell:
        r"""
        touch {output}
        """



================================================
FILE: tests/test_dup_out_patterns/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_dynamic_container/Snakefile
================================================
shell.executable("sh")
IMAGE = ["bash", "alpine"]

rule a:
    input:
        expand("test_str_{image}.out", image=IMAGE),
        expand("test_func_{image}.out", image=IMAGE)
 
rule b:
    input:
        "test_str_{image}.txt"
    output:
        "test_str_{image}.out"
    container:
        "docker://{image}"
    shell:
        'ls {input}; echo "test" > {output}'

rule c:
    input:
        "test_func_{image}.txt"
    output:
        "test_func_{image}.out"
    container:
        lambda wildcards: f"docker://{wildcards.image}"
    shell:
        'ls {input}; echo "test" > {output}'

rule d:
    output:
        expand("test_str_{image}.txt", image=IMAGE),
        expand("test_func_{image}.txt", image=IMAGE)
    shell:
        'touch {output}'




================================================
FILE: tests/test_empty_include/include.rules
================================================
[Empty file]


================================================
FILE: tests/test_empty_include/Snakefile
================================================
include: "include.rules"



================================================
FILE: tests/test_ensure/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        ensure("test.txt", non_empty=True)
    shell:
        "touch {output}"


rule b:
    output:
        ensure("test2.txt", non_empty=True)
    shell:
        "echo test > {output}"


sha256 = "9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08"


rule c:
    output:
        ensure("test3.txt", sha256=sha256)
    shell:
        "echo -n test > {output}"


rule d:
    output:
        ensure("test4.txt", sha256=lambda w: sha256)
    shell:
        "echo -n test2 > {output}"



================================================
FILE: tests/test_ensure/expected-results/test2.txt
================================================
test



================================================
FILE: tests/test_env_modules/Snakefile
================================================
rule a:
    output:
        "test.out"
    envmodules:
        "dot"
    shell:
        "test-env-modules.sh {output}"



================================================
FILE: tests/test_env_modules/test-env-modules.sh
================================================
echo test > $1



================================================
FILE: tests/test_envvars/Snakefile
================================================
shell.executable("bash")

envvars:
    "TEST_ENV_VAR"

rule all:
    input:
        "test.out",
        "test2.out"

rule a:
    output:
        "test.out"
    shell:
        "echo $TEST_ENV_VAR > {output}"


rule b:
    output:
        "test2.out"
    shell:
        "echo $TEST_ENV_VAR > {output}"



================================================
FILE: tests/test_exists/Snakefile
================================================
def foo():
    if not exists("test.txt"):
        raise ValueError("File does not exist")

foo()



================================================
FILE: tests/test_exists/test.txt
================================================
[Empty file]


================================================
FILE: tests/test_expand_flag/Snakefile
================================================
rule a:
    output:
        expand(directory("{sample}/dir"), sample=["a"])
    shell:
        "mkdir {output}"



================================================
FILE: tests/test_expand_list_of_functions/Snakefile
================================================
def get_i1(wildcards):
    return 0


def get_i2(wildcards):
    return 1


rule all:
    input:
        expand("test.{i}.out", i=[get_i1, get_i2])
    output:
        "test.out"
    shell:
        "touch {output}"


================================================
FILE: tests/test_failed_intermediate/Snakefile
================================================
rule all:
    input:
        "test3.txt"

rule a:
    output:
        "test2.txt"
    params:
        a=1,
        fail=config["fail"]
    shell:
        "if [ {params.fail} = true ]; then exit 1; fi; echo '{params.fail}' > {output}"

rule b:
    input:
        "test2.txt"
    output:
        "test3.txt"
    params:
        a=2
    shell:
        "cp {input} {output}"


================================================
FILE: tests/test_failed_intermediate/expected-results/test3.txt
================================================
false



================================================
FILE: tests/test_filegraph/Snakefile
================================================
import os

shell.executable("bash")

rule all:
    input:
        expand("folder/file_{v1}_{v2}.dat", v1=[1, 2], v2=["a", "b"]),
        expand("visualization_of_1_a.pdf", v1=[1, 2])


rule data:
    output:
        "folder/file_{v1}_{v2}.dat"
    shell:
        "touch {output}"


rule visualization:
    input:
        "folder/file_1_a.dat",
        lambda x: os.path.join("scripts", "vis.py"),
    output:
        "visualization_of_1_a.pdf"
    shell:
        "touch {output}"



================================================
FILE: tests/test_filegraph/scripts/vis.py
================================================
[Empty file]


================================================
FILE: tests/test_filesep_windows/Snakefile
================================================

#shell.executable("bash")

# This tests that snakemake handles input/output 
# defined with both forward and backwards slashes
# on Windows. 

rule all:
    input:
        expand("subfolder\\test{i}.out2", i=range(3))

rule a:
    output:
        "subfolder\\test{i}.out"
    shell:
        'echo hello > {output}'


rule b:
    input:
        "subfolder\\test{i}.out"
    output:
        "subfolder/test{i}.out2"
    shell:
        "echo world> {output}"



================================================
FILE: tests/test_filesep_windows/expected-results/subfolder/test2.out2
================================================
world



================================================
FILE: tests/test_filesep_windows/subfolder/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_format_params/Snakefile
================================================
shell.executable("bash")

rule a:
    output: "test.out"
    params: [1, 2, 3]
    shell:
        "echo {params[0]} > {output}"



================================================
FILE: tests/test_format_wildcards/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "foo.txt"

rule a:
    output:
        "{test}.txt"
    shell:
        "echo {rule} {wildcards} > {output}"



================================================
FILE: tests/test_format_wildcards/expected-results/foo.txt
================================================
a test=foo



================================================
FILE: tests/test_fstring/Snakefile
================================================
shell.executable("bash")

PREFIX = "SID23454678"
mid = ".t"

rule unit1:
    output:
        f"{PREFIX}{mid}xt",
    shell:
        "echo '>'{output}'<'; touch {output}; sleep 1"


rule unit2:
    shell:
        f"ls"

assert (
    f"""
{
  "hello, snakemake"
}
"""
    == """
hello, snakemake
"""
)
assert (
    f"""
    {
  "hello, snakemake"
}
"""
    == """
    hello, snakemake
"""
)

if 1:
    assert (
        f"""
{
  "hello, snakemake"
}
"""
        == """
hello, snakemake
"""
    )

assert f"FORMAT['{PREFIX}']['{{}}']" == "FORMAT['SID23454678']['{}']"
assert f"FORMAT['{PREFIX}'][}}'{{'{{]" == "FORMAT['SID23454678'][}'{'{]"

assert f"works in new version: {[(x, y) for x in [1,2,3] for y in [3,1,4]]}" == "works in new version: [(1, 3), (1, 1), (1, 4), (2, 3), (2, 1), (2, 4), (3, 3), (3, 1), (3, 4)]"


part_one = "foo"
an_fstring = f'{part_one}/{"bar" if True else "baz"}'
another_fstring = f"{part_one.replace('foo', 'bar')}/{{escaped}}"
multiple_parts_fstring = f"no_replacement_" "regular_string_" f"{'bar' if True else 'baz'}"



================================================
FILE: tests/test_fstring/expected-results/SID23454678.txt
================================================
[Empty file]


================================================
FILE: tests/test_ftp_immediate_close/Snakefile
================================================
from snakemake.remote.FTP import RemoteProvider
FTP = RemoteProvider()

rule a:
    input:
        FTP.remote("ftp://ftp.sra.ebi.ac.uk/vol1/ERA651/ERA651425/fastq/RAG16_R1.fastq.gz", immediate_close=True)
    output:
        "size.txt"
    run:
        shell("du -h {input} > {output}")




================================================
FILE: tests/test_ftp_immediate_close/expected-results/size.txt
================================================
9.9M	ftp.sra.ebi.ac.uk/vol1/ERA651/ERA651425/fastq/RAG16_R1.fastq.gz



================================================
FILE: tests/test_generate_unit_tests/Snakefile
================================================


configfile: "config/config.json"


rule all:
    input:
        expand("test/{sample}.tsv", sample=range(3))

rule a:
    input:
        config["in_file"],
    output:
        temp("test/{sample}.txt"),
        temp("test/{sample}.tmp"),
    shell:
        "touch {output}"


rule b:
    input:
        "test/{sample}.txt"
    output:
        "test/{sample}.tsv"
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/common.py
================================================
"""
Common code for unit testing of rules generated with Snakemake 9.8.2.dev50.
"""

import os
from pathlib import Path
from subprocess import check_output


class OutputChecker:
    def __init__(self, data_path, expected_path, workdir):
        self.data_path = data_path
        self.expected_path = expected_path
        self.workdir = workdir

    def check(self):
        # Input files
        input_files = set(
            (Path(path) / f).relative_to(self.data_path)
            for path, subdirs, files in os.walk(self.data_path)
            for f in files
        )
        print(f"input: {input_files}")  # DEBUG
        # Workdir files
        workdir_files = set(
            (Path(path) / f).relative_to(self.workdir)
            for path, subdirs, files in os.walk(self.workdir)
            for f in files
        )
        print(f"workdir: {workdir_files}")  # DEBUG
        # Expected files
        expected_files = set(
            (Path(path) / f).relative_to(self.expected_path)
            for path, subdirs, files in os.walk(self.expected_path)
            for f in files
        )
        print(f"expected: {expected_files}")  # DEBUG

        assert expected_files.issubset(
            workdir_files
        ), f"Output files missing: {expected_files - workdir_files}"

        # Compare output and expected files
        for f in expected_files:
            self.compare_files(self.expected_path / f, self.workdir / f)

    def compare_files(self, expected_file, generated_file):
        if expected_file.suffix == ".gz":
            cmp = "zcmp"
        elif expected_file.suffix == ".bz2":
            cmp = "bzcmp"
        elif expected_file.suffix == ".xz":
            cmp = "xzcmp"
        else:
            cmp = "cmp"

        check_output([cmp, expected_file, generated_file])



================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/conftest.py
================================================
"""
conftest.py for unit testing of rules generated with Snakemake 9.8.2.dev50.
"""

from pytest import fixture


def pytest_addoption(parser):
    parser.addoption("--conda-prefix", action="store", default=None)


@fixture()
def conda_prefix(request):
    conda_prefix = request.config.option.conda_prefix
    if conda_prefix:
        return ["--conda-prefix", conda_prefix]
    else:
        return []



================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/test_a.py
================================================
"""
Rule test code for unit testing of rules generated with Snakemake 9.8.2.dev50.
"""


import os
import sys
import shutil
import tempfile
from pathlib import Path
from subprocess import check_output

sys.path.insert(0, os.path.dirname(__file__))


def test_a(conda_prefix):

    with tempfile.TemporaryDirectory() as tmpdir:
        workdir = Path(tmpdir) / "workdir"
        config_path = Path(".tests/units/a/config")
        data_path = Path(".tests/units/a/data")
        expected_path = Path(".tests/units/a/expected")

        # Copy config to the temporary workdir.
        shutil.copytree(config_path, workdir)

        # Copy data to the temporary workdir.
        shutil.copytree(data_path, workdir, dirs_exist_ok=True)

        # Run the test job.
        check_output(
            [
                "python",
                "-m",
                "snakemake",
                "test/0.txt",
                "test/0.tmp",
                "--snakefile",
                "Snakefile",
                "-f",
                "--notemp",
                "-j1",
                "--target-files-omit-workdir-adjustment",
                "--configfile",
                ".tests/integration/config/config.json",
                "--directory",
                workdir,
            ]
            + conda_prefix
        )

        # Check the output byte by byte using cmp/zmp/bzcmp/xzcmp.
        # To modify this behavior, you can inherit from common.OutputChecker in here
        # and overwrite the method `compare_files(generated_file, expected_file), 
        # also see common.py.
        import common
        common.OutputChecker(data_path, expected_path, workdir).check()



================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/test_b.py
================================================
"""
Rule test code for unit testing of rules generated with Snakemake 9.8.2.dev50.
"""


import os
import sys
import shutil
import tempfile
from pathlib import Path
from subprocess import check_output

sys.path.insert(0, os.path.dirname(__file__))


def test_b(conda_prefix):

    with tempfile.TemporaryDirectory() as tmpdir:
        workdir = Path(tmpdir) / "workdir"
        config_path = Path(".tests/units/b/config")
        data_path = Path(".tests/units/b/data")
        expected_path = Path(".tests/units/b/expected")

        # Copy config to the temporary workdir.
        shutil.copytree(config_path, workdir)

        # Copy data to the temporary workdir.
        shutil.copytree(data_path, workdir, dirs_exist_ok=True)

        # Run the test job.
        check_output(
            [
                "python",
                "-m",
                "snakemake",
                "test/0.tsv",
                "--snakefile",
                "Snakefile",
                "-f",
                "--notemp",
                "-j1",
                "--target-files-omit-workdir-adjustment",
                "--configfile",
                ".tests/integration/config/config.json",
                "--directory",
                workdir,
            ]
            + conda_prefix
        )

        # Check the output byte by byte using cmp/zmp/bzcmp/xzcmp.
        # To modify this behavior, you can inherit from common.OutputChecker in here
        # and overwrite the method `compare_files(generated_file, expected_file), 
        # also see common.py.
        import common
        common.OutputChecker(data_path, expected_path, workdir).check()



================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/a/config/config/config.json
================================================
{
    "in_file": "input.csv"
}



================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/a/data/input.csv
================================================
input



================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/a/expected/test/0.txt
================================================
[Empty file]


================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/b/config/config/config.json
================================================
{
    "in_file": "input.csv"
}



================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/b/data/test/0.txt
================================================
[Empty file]


================================================
FILE: tests/test_generate_unit_tests/expected-results/.tests/units/b/expected/test/0.tsv
================================================
[Empty file]


================================================
FILE: tests/test_generate_unit_tests/.tests/integration/input.csv
================================================
input



================================================
FILE: tests/test_generate_unit_tests/.tests/integration/config/config.json
================================================
{
    "in_file": "input.csv"
}



================================================
FILE: tests/test_generate_unit_tests/.tests/integration/test/0.tsv
================================================
[Empty file]


================================================
FILE: tests/test_generate_unit_tests/.tests/integration/test/0.txt
================================================
[Empty file]


================================================
FILE: tests/test_generate_unit_tests/.tests/integration/test/1.tsv
================================================
[Empty file]


================================================
FILE: tests/test_generate_unit_tests/.tests/integration/test/1.txt
================================================
[Empty file]


================================================
FILE: tests/test_generate_unit_tests/.tests/integration/test/2.tsv
================================================
[Empty file]


================================================
FILE: tests/test_generate_unit_tests/.tests/integration/test/2.txt
================================================
[Empty file]


================================================
FILE: tests/test_get_log_both/environment.yaml
================================================
channels:
  - conda-forge
dependencies: []


================================================
FILE: tests/test_get_log_both/Snakefile
================================================
rule:
    input: "test.in"
    output: "test.out"
    log: "test.log"
    wrapper:
        'file://wrapper.py'



================================================
FILE: tests/test_get_log_both/test.in
================================================
foo
bar



================================================
FILE: tests/test_get_log_both/wrapper.py
================================================
from snakemake.shell import shell
shell.executable("bash")

log = snakemake.log_fmt_shell(append=True)
shell(
    """
      cat {snakemake.input} > {snakemake.output}
      (>&2 echo "a stderr message") {log}
      (echo "a stdout message") {log}
      """
)



================================================
FILE: tests/test_get_log_complex/environment.yaml
================================================
channels:
  - conda-forge
dependencies: []


================================================
FILE: tests/test_get_log_complex/Snakefile
================================================
rule:
    input: "test.in"
    output: "test.out"
    log: "test.log"
    wrapper:
        'file://wrapper.py'



================================================
FILE: tests/test_get_log_complex/test.in
================================================
foo
bar



================================================
FILE: tests/test_get_log_complex/wrapper.py
================================================
from snakemake.shell import shell
shell.executable("bash")
initial_log = snakemake.log_fmt_shell()
stdout_log = snakemake.log_fmt_shell(stderr=False, append=True)
stderr_log = snakemake.log_fmt_shell(stdout=False, append=True)
shell(
    """
      cat {snakemake.input} > {snakemake.output}
      echo "should not appear since next line truncates" {initial_log}
      echo "first line" {initial_log}
      (>&2 echo "a stderr message") {stderr_log}
      (echo "a stdout message") {stdout_log}
      """
)



================================================
FILE: tests/test_get_log_none/environment.yaml
================================================
channels:
  - conda-forge
dependencies: []


================================================
FILE: tests/test_get_log_none/Snakefile
================================================
rule:
    input: "test.in"
    output: "test.out"
    wrapper:
        'file:wrapper.py'



================================================
FILE: tests/test_get_log_none/test.in
================================================
foo
bar



================================================
FILE: tests/test_get_log_none/wrapper.py
================================================
from snakemake.shell import shell

log = snakemake.log_fmt_shell(stdout=False)
shell(
    """
      cat {snakemake.input} > {snakemake.output}
      (>&2 echo "a stderr message") {log}
      (echo "a stdout message") {log}
      """
)



================================================
FILE: tests/test_get_log_stderr/environment.yaml
================================================
channels:
  - conda-forge
dependencies: []


================================================
FILE: tests/test_get_log_stderr/Snakefile
================================================
rule:
    input: "test.in"
    output: "test.out"
    log: "test.log"
    wrapper:
        'file://wrapper.py'



================================================
FILE: tests/test_get_log_stderr/test.in
================================================
foo
bar



================================================
FILE: tests/test_get_log_stderr/wrapper.py
================================================
from snakemake.shell import shell
shell.executable("bash")
log = snakemake.log_fmt_shell(stdout=False, append=True)
shell(
    """
      cat {snakemake.input} > {snakemake.output}
      (>&2 echo "a stderr message") {log}
      (echo "a stdout message") {log}
      """
)



================================================
FILE: tests/test_get_log_stdout/environment.yaml
================================================
channels:
  - conda-forge
dependencies: []


================================================
FILE: tests/test_get_log_stdout/Snakefile
================================================
rule:
    input: "test.in"
    output: "test.out"
    log: "test.log"
    wrapper:
        'file://wrapper.py'



================================================
FILE: tests/test_get_log_stdout/test.in
================================================
foo
bar



================================================
FILE: tests/test_get_log_stdout/wrapper.py
================================================
from snakemake.shell import shell
shell.executable("bash")
log = snakemake.log_fmt_shell(stderr=False, append=True)
shell(
    """
      cat {snakemake.input} > {snakemake.output}
      (>&2 echo "a stderr message") {log}
      (echo "a stdout message") {log}
      """
)



================================================
FILE: tests/test_github_issue105/Snakefile
================================================
rule all:
    input:
        "b.txt"

checkpoint a:
    output:
        "a.txt"
    shell:
        "touch {output}"

def aggregate(wildcards):
    # files = {'inp1': "Snakefile"}
    files = {'inp1': checkpoints.a.get(**wildcards).output[0]}
    return files

rule b:
    input:
        unpack(aggregate)
    output:
        "b.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_github_issue105/expected-results/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue105/expected-results/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1062/Snakefile
================================================
from snakemake.remote.FTP import RemoteProvider as FTPRemoteProvider

FTP = FTPRemoteProvider(username="demo", password="password")


rule all:
    input:
        "readme.txt",


FTP_URL = "ftp://test.rebex.net/readme.txt"


rule get_fasta_ftp:
    input:
        FTP.remote(FTP_URL, keep_local=True),
    output:
        "readme.txt",
    shell:
        "mv {input:q} {output:q}"



================================================
FILE: tests/test_github_issue1062/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1069/Snakefile
================================================
rule a:
    output:
        "test.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_github_issue1069/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1158/qsub.py
================================================
#!/usr/bin/env python3
import sys
import os
import random
import re

jobscript = sys.argv[1]

os.system("strace -f -o trace {}".format(jobscript))

with open("trace", "r") as fp:
    lines = fp.readlines()

regex = re.compile('.*openat\(.*.snakemake/incomplete".*')
matches = list(filter(lambda l: regex.match(l), lines))
if len(matches) > 0:
    sys.stderr.write(repr(matches))
    sys.exit(1)

print(random.randint(1, 100))



================================================
FILE: tests/test_github_issue1158/Snakefile
================================================
rule Test:
    output: "output"
    shell: "touch {output}"



================================================
FILE: tests/test_github_issue1158/expected-results/output
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1261/Snakefile
================================================
import time

rule all:
    input:
        ['out/A.txt', 'out/B.txt'],

rule A:
    output:
        out= 'out/A.txt',
    run:
        time.sleep(10)
        with open(output.out, 'w') as fout:
            fout.write('done')

rule B:
    output:
        'out/B.txt',
    run:
        time.sleep(5)
        raise Exception("Artificially failing rule B, A should go on and finalize properly.")



================================================
FILE: tests/test_github_issue1384/Snakefile
================================================
rule all:
    input:
        "results/checksum.txt"

rule foo:
    output:
        directory("results/dir")
    shell:
        """
        mkdir results/dir
        echo hi >> results/dir/something
        """

rule bar:
    input:
        rules.foo.output
    output:
        "results/checksum.txt"
    shell:
        "ls {input}/* >> {output}"



================================================
FILE: tests/test_github_issue1384/expected-results/.gitempty
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1389/Snakefile
================================================
rule all:
    input:
        "test1.txt",
        "test2.txt",


rule a:
    output:
        "test1.txt",
    resources:
        foo=1,
    shell:
        "touch {output}"


rule b:
    output:
        "test2.txt",
    resources:
        foo="bar",
    shell:
        "touch {output}"



================================================
FILE: tests/test_github_issue1389/expected-results/test1.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1389/expected-results/test2.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1396/Snakefile
================================================
def get_files(wildcards):
    files_1 = expand("file_{i}", i=list(range(1, 5)))
    files_2 = expand("file_{i}", i=list(range(5, 9)))
    return {"files_1": files_1, "files_2": files_2}


rule all:
    input:
        unpack(get_files),


rule make_files:
    output:
        expand("file_{i}", i=list(range(1, 9))),
    shell:
        "touch {output}"



================================================
FILE: tests/test_github_issue1396/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1460/Snakefile
================================================
rule all:
    input:
        "blob.txt",
        "test.txt"


rule intermediate:
    input:
        "preblob.txt",
        "pretest.txt",
    output:
        "blob.txt",
        "test.txt",
    shell:
        """
        cp {input[0]} {output[0]}
        cp {input[1]} {output[1]}
        """


rule create:
    output:
        "preblob.txt",
        "pretest.txt"
    shell:
        '''
        echo "test file" > {output[0]}
        echo "test file" > {output[1]}
        '''



================================================
FILE: tests/test_github_issue1460/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1469/Snakefile
================================================
ruleorder: single_file > both_files


rule all:
    input:
        "foo.txt",
        "bar.txt",


rule both_files:
    output:
        "foo.txt",
        "bar.txt",
    shell:
        "if [[ -e foo.txt ]]; then exit 1; fi; touch {output}"


rule single_file:
    output:
        "foo.txt",
    shell:
        "if [[ -e foo.txt ]]; then exit 1; fi; touch {output}"



================================================
FILE: tests/test_github_issue1498/module.smk
================================================
rule a:
    output:
        a=touch("results/a.txt"),


def unpack_a(wildcards):
    return {"a": rules.a.output.a}


def lambda_a(wildcards):
    return rules.a.output.a


rule b:
    input:
        unpack(unpack_a),
        # a = lambda_a,
    output:
        b="results/b.txt",
    shell:
        "cp {input.a} {output}"



================================================
FILE: tests/test_github_issue1498/Snakefile
================================================
rule all:
    input:
        "results/all.done",


module other:
    snakefile:
        "module.smk"
    prefix:
        "results/other"


use rule * from other as other_*


rule all_impl:
    output:
        out=touch("results/all.done"),
    input:
        b=rules.other_b.output.b,
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_github_issue1498/expected-results/results/all.done
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1498/expected-results/results/other/results/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1498/expected-results/results/other/results/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1500/Snakefile
================================================
shell.executable("bash")

configfile: "config/config.yaml"


module test1:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix: 
        {"results/": "results/testmodule1/"}




module test2:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix: 
        {"results/": "results/testmodule2/"}


use rule * from test1 as test1_*

use rule * from test2 as test2_*



rule all:
    default_target: True
    input:
        rules.test1_a.output,
        rules.test2_a.output


assert test1.some_func() == 15
assert test2.some_func() == 15



================================================
FILE: tests/test_github_issue1500/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_github_issue1500/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1500/module-test/Snakefile
================================================
configfile: "config.yaml" # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    output:
        "results/test.out"
    shell:
        "echo {config[test]} > {output}"


================================================
FILE: tests/test_github_issue1542/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1542/Snakefile
================================================
# Snakefile
rule all:
    input:
        "b.txt",


rule a:
    output:
        temp("a.txt"),
    shell:
        "touch a.txt"


rule b:
    input:
        "a.txt",
    output:
        "b.txt",
    shell:
        "touch b.txt"



================================================
FILE: tests/test_github_issue1542/expected-results/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1550/Snakefile
================================================
# Snakefile
rule all:
    input:
        "a.out",


rule a:
    output:
        "a.out",
    resources:
        mem_mb=2000,  # Note that resources are specified here, but not in all.
    shell:
        "touch {output}"



================================================
FILE: tests/test_github_issue1618/Snakefile
================================================
rule all:
    input:
        "test2.out",


rule a:
    output:
        "test.out",
    threads: 4
    shell:
        """
        echo {threads} > {output}
        """


use rule a as b with:
    threads: 5
    output:
        "test2.out",



================================================
FILE: tests/test_github_issue1818/aggregated.txt
================================================
AGGREGATED



================================================
FILE: tests/test_github_issue1818/processed.txt
================================================
PROCESSED



================================================
FILE: tests/test_github_issue1818/processed2.txt
================================================
PROCESSED2



================================================
FILE: tests/test_github_issue1818/Snakefile
================================================
# a target rule to define the desired final output
rule all:
    input:
        "processed2.txt",


# the checkpoint that shall trigger re-evaluation of the DAG
# an number of file is created in a defined directory
checkpoint somestep:
    output:
        directory("my_directory/"),
    shell:
        """
        mkdir my_directory/
        cd my_directory
        for i in 1 2 3; do touch $i.txt; done
        """


# input function for rule aggregate, return paths to all files produced by the checkpoint 'somestep'
def aggregate_input(wildcards):
    checkpoint_output = checkpoints.somestep.get(**wildcards).output[0]
    return expand(
        "my_directory/{i}.txt",
        i=glob_wildcards(os.path.join(checkpoint_output, "{i}.txt")).i,
    )


rule aggregate:
    input:
        aggregate_input,
    output:
        "aggregated.txt",
    shell:
        "echo AGGREGATED > {output}"


# Fail here if the job runs again, as we want to ensure that snakemake does not false trigger a rerun
# as reported in issue #1818.
rule process:
    input:
        "aggregated.txt",
    output:
        "processed.txt",
    shell:
        "exit 1; echo PROCESSED > {output}"


rule process2:
    input:
        "processed.txt",
    output:
        "processed2.txt",
    shell:
        "echo PROCESSED2 > {output}"



================================================
FILE: tests/test_github_issue1818/expected-results/aggregated.txt
================================================
AGGREGATED



================================================
FILE: tests/test_github_issue1818/expected-results/processed.txt
================================================
PROCESSED



================================================
FILE: tests/test_github_issue1818/expected-results/processed2.txt
================================================
PROCESSED2



================================================
FILE: tests/test_github_issue1818/expected-results/my_directory/1.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1818/expected-results/my_directory/2.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1818/expected-results/my_directory/3.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1818/expected-results/my_directory/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1818/my_directory/1.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1818/my_directory/2.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1818/my_directory/3.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1818/my_directory/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_github_issue1818/.snakemake/metadata/bXlfZGlyZWN0b3J5
================================================
{"version": null, "code": "gASV9gEAAAAAAAAoQxJ0AGQBfA58EWQCjQMBAGQAUwCUKIwFaW5wdXSUjAZvdXRwdXSUjAZwYXJhbXOUjAl3aWxkY2FyZHOUjAd0aHJlYWRzlIwJcmVzb3VyY2VzlIwDbG9nlIwHdmVyc2lvbpSMBHJ1bGWUjAljb25kYV9lbnaUjA1jb250YWluZXJfaW1nlIwQc2luZ3VsYXJpdHlfYXJnc5SMD3VzZV9zaW5ndWxhcml0eZSMC2Vudl9tb2R1bGVzlIwMYmVuY2hfcmVjb3JklIwFam9iaWSUjAhpc19zaGVsbJSMD2JlbmNoX2l0ZXJhdGlvbpSMD2NsZWFudXBfc2NyaXB0c5SMCnNoYWRvd19kaXKUjA1lZGl0X25vdGVib29rlIwPY29uZGFfYmFzZV9wYXRolIwHYmFzZWRpcpSMGHJ1bnRpbWVfc291cmNlY2FjaGVfcGF0aJSMGF9faXNfc25ha2VtYWtlX3J1bGVfZnVuY5R0lF2UKE6MawogICAgICAgIG1rZGlyIG15X2RpcmVjdG9yeS8KICAgICAgICBjZCBteV9kaXJlY3RvcnkKICAgICAgICBmb3IgaSBpbiAxIDIgMzsgZG8gdG91Y2ggJGkudHh0OyBkb25lCiAgICAgICAglGgPaBKGlGWMBXNoZWxslIWUdJQu", "rule": "somestep", "input": [], "log": [], "params": [], "shellcmd": "\n        mkdir my_directory/\n        cd my_directory\n        for i in 1 2 3; do touch $i.txt; done\n        ", "incomplete": false, "starttime": 1665672954.5860517, "endtime": 1665672954.6027184, "job_hash": 8729438582497, "conda_env": null, "container_img_url": null, "input_checksums": {}}


================================================
FILE: tests/test_github_issue1818/.snakemake/metadata/cHJvY2Vzc2VkLnR4dA==
================================================
{"version": null, "code": "gASVpAEAAAAAAAAoQxJ0AGQBfA58EWQCjQMBAGQAUwCUKIwFaW5wdXSUjAZvdXRwdXSUjAZwYXJhbXOUjAl3aWxkY2FyZHOUjAd0aHJlYWRzlIwJcmVzb3VyY2VzlIwDbG9nlIwHdmVyc2lvbpSMBHJ1bGWUjAljb25kYV9lbnaUjA1jb250YWluZXJfaW1nlIwQc2luZ3VsYXJpdHlfYXJnc5SMD3VzZV9zaW5ndWxhcml0eZSMC2Vudl9tb2R1bGVzlIwMYmVuY2hfcmVjb3JklIwFam9iaWSUjAhpc19zaGVsbJSMD2JlbmNoX2l0ZXJhdGlvbpSMD2NsZWFudXBfc2NyaXB0c5SMCnNoYWRvd19kaXKUjA1lZGl0X25vdGVib29rlIwPY29uZGFfYmFzZV9wYXRolIwHYmFzZWRpcpSMGHJ1bnRpbWVfc291cmNlY2FjaGVfcGF0aJSMGF9faXNfc25ha2VtYWtlX3J1bGVfZnVuY5R0lF2UKE6MGWVjaG8gUFJPQ0VTU0VEID4ge291dHB1dH2UaA9oEoaUZYwFc2hlbGyUhZR0lC4=", "rule": "process", "input": ["aggregated.txt"], "log": [], "params": [], "shellcmd": "echo PROCESSED > processed.txt", "incomplete": false, "starttime": 1665672954.6227183, "endtime": 1665672954.6327186, "job_hash": 8729438582463, "conda_env": null, "container_img_url": null, "input_checksums": {"aggregated.txt": "a144e9df267c10532db93a5e1625d6362d934c9973be60a6c07718a1f1942f12"}}


================================================
FILE: tests/test_github_issue1818/.snakemake/metadata/cHJvY2Vzc2VkMi50eHQ=
================================================
{"version": null, "code": "gASVpQEAAAAAAAAoQxJ0AGQBfA58EWQCjQMBAGQAUwCUKIwFaW5wdXSUjAZvdXRwdXSUjAZwYXJhbXOUjAl3aWxkY2FyZHOUjAd0aHJlYWRzlIwJcmVzb3VyY2VzlIwDbG9nlIwHdmVyc2lvbpSMBHJ1bGWUjAljb25kYV9lbnaUjA1jb250YWluZXJfaW1nlIwQc2luZ3VsYXJpdHlfYXJnc5SMD3VzZV9zaW5ndWxhcml0eZSMC2Vudl9tb2R1bGVzlIwMYmVuY2hfcmVjb3JklIwFam9iaWSUjAhpc19zaGVsbJSMD2JlbmNoX2l0ZXJhdGlvbpSMD2NsZWFudXBfc2NyaXB0c5SMCnNoYWRvd19kaXKUjA1lZGl0X25vdGVib29rlIwPY29uZGFfYmFzZV9wYXRolIwHYmFzZWRpcpSMGHJ1bnRpbWVfc291cmNlY2FjaGVfcGF0aJSMGF9faXNfc25ha2VtYWtlX3J1bGVfZnVuY5R0lF2UKE6MGmVjaG8gUFJPQ0VTU0VEMiA+IHtvdXRwdXR9lGgPaBKGlGWMBXNoZWxslIWUdJQu", "rule": "process2", "input": ["processed.txt"], "log": [], "params": [], "shellcmd": "echo PROCESSED2 > processed2.txt", "incomplete": false, "starttime": 1665672954.636052, "endtime": 1665672954.6493852, "job_hash": 8729438582446, "conda_env": null, "container_img_url": null, "input_checksums": {"processed.txt": "49f193a0dae7bdb2368a9f113cf2e1f3827a787dde3808a5d7838bc8ff63164c"}}


================================================
FILE: tests/test_github_issue1818/.snakemake/metadata/YWdncmVnYXRlZC50eHQ=
================================================
{"version": null, "code": "gASVpQEAAAAAAAAoQxJ0AGQBfA58EWQCjQMBAGQAUwCUKIwFaW5wdXSUjAZvdXRwdXSUjAZwYXJhbXOUjAl3aWxkY2FyZHOUjAd0aHJlYWRzlIwJcmVzb3VyY2VzlIwDbG9nlIwHdmVyc2lvbpSMBHJ1bGWUjAljb25kYV9lbnaUjA1jb250YWluZXJfaW1nlIwQc2luZ3VsYXJpdHlfYXJnc5SMD3VzZV9zaW5ndWxhcml0eZSMC2Vudl9tb2R1bGVzlIwMYmVuY2hfcmVjb3JklIwFam9iaWSUjAhpc19zaGVsbJSMD2JlbmNoX2l0ZXJhdGlvbpSMD2NsZWFudXBfc2NyaXB0c5SMCnNoYWRvd19kaXKUjA1lZGl0X25vdGVib29rlIwPY29uZGFfYmFzZV9wYXRolIwHYmFzZWRpcpSMGHJ1bnRpbWVfc291cmNlY2FjaGVfcGF0aJSMGF9faXNfc25ha2VtYWtlX3J1bGVfZnVuY5R0lF2UKE6MGmVjaG8gQUdHUkVHQVRFRCA+IHtvdXRwdXR9lGgPaBKGlGWMBXNoZWxslIWUdJQu", "rule": "aggregate", "input": ["my_directory/1.txt", "my_directory/2.txt", "my_directory/3.txt"], "log": [], "params": [], "shellcmd": "echo AGGREGATED > aggregated.txt", "incomplete": false, "starttime": 1665672954.609385, "endtime": 1665672954.619385, "job_hash": 8729438519112, "conda_env": null, "container_img_url": null, "input_checksums": {"my_directory/1.txt": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "my_directory/2.txt": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "my_directory/3.txt": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}}


================================================
FILE: tests/test_github_issue1882/Snakefile
================================================
rule all:
    output:
        "foo.txt"
    run:
        with open('foo.txt', 'w') as f: pass



================================================
FILE: tests/test_github_issue1882/expected-results/foo.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue2142/Snakefile
================================================
rule all:
    input: "data.txt"
    log:
        "logs/all.log"

rule A:
    output: "data.txt"
    shell: "touch {output}"


================================================
FILE: tests/test_github_issue2142/expected-results/data.txt
================================================
[Empty file]


================================================
FILE: tests/test_github_issue2154/Snakefile
================================================
# Snakemake version > 7.19.0

rule all:
	input: 
		"test.out"

rule test:
	input:
		testIN="test.in",
	resources:
		runtime="00:30:00",
	output: 
		testOUT="test.out",
	shell:
		"""
		cp {input.testIN} {output.testOUT}
		"""



================================================
FILE: tests/test_github_issue2154/test.in
================================================
TestFile for issue 2154



================================================
FILE: tests/test_github_issue261/Snakefile
================================================
import random

checkpoint random:
    output:
        target = 'test1/{target}/{config}.txt'

    run:
        with open(output.target, 'w') as ftg:
            for i in range(2, 10):
                ftg.write(f'test1/{wildcards.target}/{wildcards.config}/v{i}\n')

rule process:
    output:
        touch('test1/{target}/{config}/v{index}')

def genetate_inputs(wildcards):
    with checkpoints.random.get(**wildcards).output[0].open() as fran:
        for line in fran.readlines():
            yield line.rstrip()

rule generate:
    input:
        genetate_inputs
    output:
        touch('test1/{target}/{config}.done')



================================================
FILE: tests/test_github_issue261/expected-results/test1/target1/config1.done
================================================
[Empty file]


================================================
FILE: tests/test_github_issue2732/Snakefile
================================================
rule a:
    output:
        temp("{name}.txt"),
    shell:
        "touch {output}"

rule a1:
    input:
        "{name}.txt",
    output:
        temp("{name}.a1.out"),
    shell:
        "touch {output}"

rule b1:
    input:
        "A.txt",
    output:
        temp("A.b1.out"),
    shell:
        "touch {output}"

checkpoint b2:
    input:
        "A.b1.out"
    output:
        "A.b2.out"
    shell:
        "echo A > {output}"

def in_func1(w):
    file = checkpoints.b2.get().output[0]
    return open(file, "r").read().strip()

rule all:
    input:
        lambda w: expand(rules.a1.output, name=in_func1(w)),
    default_target: True


================================================
FILE: tests/test_github_issue3271/Snakefile
================================================
rule a:
    output:
        "output.txt"
    resources:
        mem_mb = lambda wildcards, attempt: attempt * 1000,
    params:
        mem_gb = lambda wildcards, resources: int(resources.mem_mb) / 1000,
    shell:
        r"""
        echo {params.mem_gb} > {output}
        """


================================================
FILE: tests/test_github_issue3271/Snakefile_should_fail
================================================
rule b:
    output:
        "output.txt"
    input:
        "input.txt"
    resources:
        mem_mb = lambda wildcards, attempt, input: attempt * 1000,
    params:
        mem_gb = lambda wildcards, resources: int(resources.mem_mb) / 1000,
    shell:
        r"""
        echo {params.mem_gb} > {output}
        """
rule a:
    output:
        "input.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_github_issue3556/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "d.out",

rule A:
    output:
        "a.out",
    shell:
        "echo 'text' > {output}"

rule B:
    input:
        "a.out",
    output:
        "b_{i}.out",
    shell:
        "cat {input} > {output}"

# rule C is required for #946
#  use `range(20)` so test will pass in < 5% of cases where issue is present
rule C:
    input:
        expand("b_{i}.out", i=list(range(4))),
    output:
        "c.out",
    shell:
        "cat {input} > {output}"

# For #946, 'a.out' is required as input, but does not need to be `ancient()`
rule D:
    input:
        "a.out",
        "c.out",
    output:
        "d.out",
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_github_issue3556/expected-results/dag.mmd
================================================
---
title: DAG
---
flowchart TB
	id0[all]
	id1[D]
	id2[A]
	id3[C]
	id4[B - i: 0]
	id5[B - i: 1]
	id6[B - i: 2]
	id7[B - i: 3]
	style id0 fill:#57BFD9,stroke-width:2px,color:#333333
	style id1 fill:#57D98B,stroke-width:2px,color:#333333
	style id2 fill:#D95757,stroke-width:2px,color:#333333
	style id3 fill:#8BD957,stroke-width:2px,color:#333333
	style id4 fill:#D9BF57,stroke-width:2px,color:#333333
	style id5 fill:#D9BF57,stroke-width:2px,color:#333333
	style id6 fill:#D9BF57,stroke-width:2px,color:#333333
	style id7 fill:#D9BF57,stroke-width:2px,color:#333333
	id1 --> id0
	id2 --> id1
	id3 --> id1
	id4 --> id3
	id5 --> id3
	id6 --> id3
	id7 --> id3
	id2 --> id4
	id2 --> id5
	id2 --> id6
	id2 --> id7



================================================
FILE: tests/test_github_issue413/Snakefile
================================================
"""
Issue 413:
 When an output file was a link to a previous output,
 AND the output link already exists, there would be a 
 false ChildIOException thrown.

 So test.txt and test.link already exist and we have a third
 output to trigger a build
 """

rule all:
    input:
        "test_dir/other_file"

rule make_test_dir_and_test_file:
    output:
        directory("test_dir/subdir"),
    shell:
        """
        mkdir -p {output[0]}/
        echo "test" > {output[0]}/test_file
        """
        
rule make_other_file:
    input:
        "test_dir/subdir"
    output:
        "test_dir/other_file"
    shell:
        """
        ln -s subdir/test_file {output[0]}
        """



================================================
SYMLINK: tests/test_github_issue413/expected-results/test_dir/other_file -> test_file
================================================



================================================
FILE: tests/test_github_issue413/expected-results/test_dir/subdir/test_file
================================================
test



================================================
SYMLINK: tests/test_github_issue413/test_dir/other_file -> test_file
================================================



================================================
FILE: tests/test_github_issue413/test_dir/subdir/test_file
================================================
test



================================================
FILE: tests/test_github_issue456/Snakefile
================================================
shell.executable("bash")

rule all:
   input:
      gff_dir='references/gffs/'
   output:
      test_file='test.txt'
   shell:
      "echo b0rk > {output.test_file}"



================================================
FILE: tests/test_github_issue456/expected-results/test.txt
================================================
b0rk



================================================
FILE: tests/test_github_issue456/references/gffs/test.gff
================================================
[Empty file]


================================================
FILE: tests/test_github_issue52/B
================================================
[Empty file]


================================================
FILE: tests/test_github_issue52/other.smk
================================================
shell.executable("bash")

rule All: input: "C"

rule A:
    output: "outputA/A1", "outputA/A2", directory("outputA")
    shell: "mkdir -p outputA; touch outputA/A1; touch outputA/A2"

rule B:
    input: "outputA"
    output: "B"
    shell: "touch B"

rule C:
    input: test="outputA/A1", testb="B"
    output: "C"
    shell: "cp {input.test} C"



================================================
FILE: tests/test_github_issue52/Snakefile
================================================
shell.executable("bash")

rule All: input: "C"

rule A:
    output: "outputA/A1", "outputA/A2", directory("outputA")
    shell: "mkdir -p outputA; touch outputA/A1; touch outputA/A2"

rule C:
    input: test="outputA/A1", testb="B"
    output: "C"
    shell: "cp {input.test} C"



================================================
FILE: tests/test_github_issue52/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_github_issue627/config.yaml
================================================
cluster: "./run_locally.sh"
cluster-status: "./local_job_status.sh"



================================================
FILE: tests/test_github_issue627/job.py
================================================
#!/usr/bin/env python

import time

file = open("job.txt", "w")

for i in range(300):
    file.write(str(i) + "\n")
    file.flush()
    time.sleep(1)



================================================
FILE: tests/test_github_issue627/local_job_status.sh
================================================
#!/bin/bash

STATUS=$(ps aux | awk '{print $2}' | grep "$1")
if [ -z "$STATUS" ]; then
  echo "failed"
else
  echo "running"
fi


================================================
FILE: tests/test_github_issue627/run_locally.sh
================================================
#!/bin/bash

PID=$(bash -c "setsid $1 > /dev/null 2>&1 & echo \"\$!\"")
echo "$PID"

bash -c "sleep 2; kill -SIGKILL \"$PID\"" > /dev/null 2>&1 &


================================================
FILE: tests/test_github_issue627/Snakefile
================================================
rule:
    shell:
        """
        set +e
        python -m snakemake --cores 1 -p --profile . -s Snakefile.internal
        RETVAL=$?
        set -e

        if [ "$RETVAL" == 0 ]; then
            echo "Snakemake did not correctly discover that the job failed"
            exit 1
        fi

        if [ -f "./job.txt" ]; then
            echo "Output file of failed job exists"
            exit 1
        fi
        """



================================================
FILE: tests/test_github_issue627/Snakefile.internal
================================================
rule:
    output: "job.txt"
    shell:
        "./job.py"



================================================
FILE: tests/test_github_issue627/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_github_issue640/setup.sh
================================================
#!/bin/bash
mkdir -p DistantDir/WithOUTRights
mkdir -p DistantDir/WithRights
touch DistantDir/WithOUTRights/File
touch DistantDir/WithRights/File
chmod 000 DistantDir/WithOUTRights/
mkdir Input
ln -s ../DistantDir/WithOUTRights/File Input/FileWithOUTRights
ln -s ../DistantDir/WithRights/File Input/FileWithRights



================================================
FILE: tests/test_github_issue640/Snakefile
================================================
import subprocess

try:
    shell("./setup.sh")
except subprocess.CalledProcessError:
    pass

rule all:
    input:
        'Input/{file}'
    output:
        'Output/{file}'
    shell:
        'cp {input} {output}'



================================================
FILE: tests/test_github_issue727/Snakefile
================================================
INPUT_FILE = 'Snakefile'
OUTPUT_ROOT = 'checkpoint-test'

BINS_DIR                    = Path(OUTPUT_ROOT) / 'bins'
BIN_DIR_TEMPLATE            = BINS_DIR / '{bin_id}'
BIN_FILE_TEMPLATE           = BINS_DIR / '{bin_id}' / 'read_list.txt'
BIN_FILE2_TEMPLATE          = OUTPUT_ROOT + '.bin.{bin_id}.clust.info.csv'

rule all:
    input: OUTPUT_ROOT + '.final_file'

rule rule1:
    input: INPUT_FILE
    output: OUTPUT_ROOT + '.file1'
    shell:
        'touch {output}'

checkpoint checkpoint1:
    " defines some bins "
    input: OUTPUT_ROOT + '.file1'
    output:
        bins_dir=directory(BINS_DIR)
    run:
        # 20 bins
        for bin_id in range(1,20):
            # create bin specific subdir of BINS_DIR
            bin_dir = str(BIN_DIR_TEMPLATE).format(bin_id=bin_id)
            os.makedirs(bin_dir, exist_ok=True)

            # create a bin specific output file
            rlist_fn = str(BIN_FILE_TEMPLATE).format(bin_id=bin_id)
            with open(rlist_fn, 'w') as fh:
                fh.write(str(bin_id) + '\n')

def expand_template_from_bins(wildcards, template):
    # get dir through checkpoints to throw Exception if checkpoint is pending
    checkpoint_dir = checkpoints.checkpoint1.get(**wildcards).output
    # get bins from files
    bins, = glob_wildcards(BIN_FILE_TEMPLATE)
    # expand template
    return expand(str(template), bin_id=bins)

# the second input is key here
rule rule2:
    input:
        bin_file=BIN_FILE_TEMPLATE,
        other=OUTPUT_ROOT + '.file1'
    output: BIN_FILE2_TEMPLATE
    shell:
        'touch {output}'

rule final_rule:
    """ access all bin files """
    input: lambda w: expand_template_from_bins(w, BIN_FILE2_TEMPLATE)
    output: OUTPUT_ROOT + '.final_file'
    shell: 'touch {output}'



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.1.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.10.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.11.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.12.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.13.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.14.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.15.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.16.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.17.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.18.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.19.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.2.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.3.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.4.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.5.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.6.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.7.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.8.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.bin.9.clust.info.csv
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.file1
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test.final_file
================================================
[Empty file]


================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/1/read_list.txt
================================================
1



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/10/read_list.txt
================================================
10



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/11/read_list.txt
================================================
11



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/12/read_list.txt
================================================
12



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/13/read_list.txt
================================================
13



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/14/read_list.txt
================================================
14



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/15/read_list.txt
================================================
15



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/16/read_list.txt
================================================
16



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/17/read_list.txt
================================================
17



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/18/read_list.txt
================================================
18



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/19/read_list.txt
================================================
19



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/2/read_list.txt
================================================
2



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/3/read_list.txt
================================================
3



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/4/read_list.txt
================================================
4



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/5/read_list.txt
================================================
5



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/6/read_list.txt
================================================
6



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/7/read_list.txt
================================================
7



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/8/read_list.txt
================================================
8



================================================
FILE: tests/test_github_issue727/expected-results/checkpoint-test/bins/9/read_list.txt
================================================
9



================================================
FILE: tests/test_github_issue78/Snakefile
================================================
rule all:
    input: "test.out"

rule singularity_ok:
    input:
        "test_a.out",
        "test_b.out",
        "test_c.out"
    output:
        touch("test.out")
    shell:
        """
        test ! -f junk_a.out
        test ! -f junk_b.out
        test ! -f junk_c.out
        """

rule a:
    output:
        "test_a.out"
    singularity:
        "docker://bash"
    shadow:
        "minimal"
    shell:
        'echo 1 > junk_a.out; echo "test" > {output}'

rule b:
    output:
        "test_b.out"
    shadow:
        "minimal"
    shell:
        'echo 1 > junk_b.out; echo "test" > {output}'

rule c:
    output:
        "test_c.out"
    shadow:
        "minimal"
    run:
        with open(output[0], 'w') as fp:
            print("test", file=fp)
        with open('junk_c.out', 'w') as fp:
            print("junk", file=fp)



================================================
FILE: tests/test_github_issue806/Snakefile
================================================
shell.executable("bash")

from itertools import product

src_lang = config['src_lang']
trg_lang = config['trg_lang']

rule all:                        
    input: "data/corpus.txt", f"data/batches.{src_lang}", f"data/batches.{trg_lang}"

checkpoint shard:                    
    output: "data/batches.{lang}"    
    shell: '''                     
        for i in $(seq 0 $(( 2 % 5 + 1))); do # 0 .. 3 -> 4
            for j in $(seq 0 $(( 1 % 2 + 1))); do # 0 .. 2 -> 3
                mkdir -p data/{wildcards.lang}/$i/$j
                echo 'hello {wildcards.lang}' > data/{wildcards.lang}/$i/$j/text
                if [[ "{wildcards.lang}" == "{src_lang}" ]]; then
                    sleep 1
                else
                    sleep 1.5
                fi
            done
        done
        ls -d data/{wildcards.lang}/*/* > {output}  
        '''
                                                                
rule combine:                                              
    input:                                                
        l1='data/{src_lang}/{shard}/{src_batch}/text',
        l2='data/{trg_lang}/{shard}/{trg_batch}/text'
    output: 'data/{src_lang}_{trg_lang}/{shard}.{src_batch}_{trg_batch}.combined'
    shell: ''' paste {input.l1} {input.l2} > {output} '''
                                                                                                
def get_batches_pairs(src_lang, trg_lang):                                 
    src_batches = []      
    trg_batches = []                                        
    with checkpoints.shard.get(lang=src_lang).output[0].open() as src_f, \
            checkpoints.shard.get(lang=trg_lang).output[0].open() as trg_f:
        for line in src_f:                      
            src_batches.append(line.strip().split('/')[-2:])                                                                                
        for line in trg_f:
           trg_batches.append(line.strip().split('/')[-2:])
    iterator = product(src_batches, trg_batches)
    return [(src_shard, (src_batch, trg_batch)) for ((src_shard, src_batch), (trg_shard, trg_batch)) in iterator if src_shard == trg_shard]

rule corpus:           
    input: lambda wildcards: [f'data/{src_lang}_{trg_lang}/{shard}.{src_batch}_{trg_batch}.combined' for (shard, (src_batch, trg_batch)) in get_batches_pairs(src_lang, trg_lang)]
    output: 'data/corpus.txt'
    shell: ''' cat {input} > {output} '''



================================================
FILE: tests/test_github_issue806/expected-results/data/batches.en
================================================
data/en/0/0
data/en/0/1
data/en/0/2
data/en/1/0
data/en/1/1
data/en/1/2
data/en/2/0
data/en/2/1
data/en/2/2
data/en/3/0
data/en/3/1
data/en/3/2



================================================
FILE: tests/test_github_issue806/expected-results/data/batches.es
================================================
data/es/0/0
data/es/0/1
data/es/0/2
data/es/1/0
data/es/1/1
data/es/1/2
data/es/2/0
data/es/2/1
data/es/2/2
data/es/3/0
data/es/3/1
data/es/3/2



================================================
FILE: tests/test_github_issue806/expected-results/data/corpus.txt
================================================
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/0/0/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/0/1/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/0/2/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/1/0/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/1/1/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/1/2/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/2/0/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/2/1/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/2/2/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/3/0/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/3/1/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/en/3/2/text
================================================
hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es/0/0/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/0/1/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/0/2/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/1/0/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/1/1/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/1/2/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/2/0/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/2/1/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/2/2/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/3/0/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/3/1/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es/3/2/text
================================================
hello es



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.0_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.0_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.0_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.1_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.1_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.1_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.2_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.2_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/0.2_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.0_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.0_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.0_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.1_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.1_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.1_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.2_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.2_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/1.2_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.0_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.0_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.0_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.1_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.1_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.1_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.2_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.2_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/2.2_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.0_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.0_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.0_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.1_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.1_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.1_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.2_0.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.2_1.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue806/expected-results/data/es_en/3.2_2.combined
================================================
hello es	hello en



================================================
FILE: tests/test_github_issue929/Snakefile
================================================
rule baserule:
    params:
        mymessage="I am the base rule",
    output:
        "test.file",
    shell:
        "echo {params.mymessage} > {output}"


use rule baserule as childrule_1 with:
    params:
        mymessage="I am childrule_1",


use rule baserule as childrule_2 with:
    output:
        "test.file",



================================================
FILE: tests/test_github_issue929/expected-results/test.file
================================================
I am the base rule



================================================
FILE: tests/test_github_issue988/Snakefile
================================================
"""
A test case for this bug
"""

n_files = 4

rule all:
    input:
        faa='all.faa',
        fasta='all.fasta'

checkpoint make_files:
    output: 
        files=directory('files')
    run:
        os.makedirs(output.files)
        for i in range(n_files):
            filename = f'{output.files}/f{i}.fasta'
            with open(filename, 'wt') as fasta_out:
                fasta_out.write('>blank\nACTGACTG\n')

def get_report_files(wildcards):
    " get the names of the faa files to be created "
    files_dir = checkpoints.make_files.get().output.files
    files, = glob_wildcards(f'{files_dir}/{{file}}.fasta')
    return expand(f'{files_dir}/{{file}}.txt', file=files)
    
rule pattern:
    input: "{any_file}.fasta"
    output: "{any_file}.faa"
    shell: "cp {input} {output}"
        

rule multiple:
    """ in my original example,  compiled a bunch of sets of report, fasta and faa files
    into a master file of each type.
    
    only the reports were the actual inputs. Neitheer the fasta or faa were listed 
    """
    input: 
        reports=get_report_files
    output:
        fasta='all.fasta',
        faa='all.faa'
    shell:
        "touch {output}"

rule report:
    input:
        "f{n}.faa"
    output:
        "f{n}.txt"
    shell: "wc {input} > {output}"
        
rule make_fasta:
    " just create a placehlder"
    output: "f{n}.fasta"
    shell: "echo {wildcards.n} > {output}"



================================================
FILE: tests/test_github_issue988/expected-results/all.faa
================================================
[Empty file]


================================================
FILE: tests/test_github_issue988/expected-results/all.fasta
================================================
[Empty file]


================================================
FILE: tests/test_github_issue988/expected-results/files/f0.faa
================================================
>blank
ACTGACTG



================================================
FILE: tests/test_github_issue988/expected-results/files/f0.fasta
================================================
>blank
ACTGACTG



================================================
FILE: tests/test_github_issue988/expected-results/files/f0.txt
================================================
 2  2 16 files/f0.faa



================================================
FILE: tests/test_github_issue988/expected-results/files/f1.faa
================================================
>blank
ACTGACTG



================================================
FILE: tests/test_github_issue988/expected-results/files/f1.fasta
================================================
>blank
ACTGACTG



================================================
FILE: tests/test_github_issue988/expected-results/files/f1.txt
================================================
 2  2 16 files/f1.faa



================================================
FILE: tests/test_github_issue988/expected-results/files/f2.faa
================================================
>blank
ACTGACTG



================================================
FILE: tests/test_github_issue988/expected-results/files/f2.fasta
================================================
>blank
ACTGACTG



================================================
FILE: tests/test_github_issue988/expected-results/files/f2.txt
================================================
 2  2 16 files/f2.faa



================================================
FILE: tests/test_github_issue988/expected-results/files/f3.faa
================================================
>blank
ACTGACTG



================================================
FILE: tests/test_github_issue988/expected-results/files/f3.fasta
================================================
>blank
ACTGACTG



================================================
FILE: tests/test_github_issue988/expected-results/files/f3.txt
================================================
 2  2 16 files/f3.faa



================================================
FILE: tests/test_github_issue988/expected-results/files/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_github_issue_14/local_script.py
================================================
contents = "blah blah blah"



================================================
FILE: tests/test_github_issue_14/pythonTest.py
================================================
#!/usr/bin/env python
import sys
import os
import local_script

# Ensure that the __real_file__ path ends in .snakemake
dname = os.path.dirname(__real_file__)
print(dname)
if not dname.endswith(os.path.join(".snakemake", "scripts")):
    sys.exit("We're not being written in the output directory!\n")

# Write out script to indicte success.
of = open(snakemake.output[0], "w")
of.write("local_script.contents {}\n".format(local_script.contents))
of.close()



================================================
FILE: tests/test_github_issue_14/Snakefile
================================================
rule all:
    input: "pythonTest"

rule test_github_issue_14:
    output: "pythonTest"
    script: "pythonTest.py"



================================================
FILE: tests/test_github_issue_14/expected-results/pythonTest
================================================
local_script.contents blah blah blah



================================================
FILE: tests/test_github_issue_3265_respect_dryrun_delete_all/infile
================================================
[Empty file]


================================================
FILE: tests/test_github_issue_3265_respect_dryrun_delete_all/Snakefile
================================================
shell.executable("bash")


rule all:
    input:
        "delete_all_output",
        "delete_temp_output",
    output:
        touch("all_ok"),


rule delete_all_output:
    output:
        touch("delete_all_output"),
    shell:
        """
        echo $PATH
        ls
        python -m snakemake --cores 1 -s Snakefile_inner all && \
        rm nothere && \
        python -m snakemake --cores 1 -s Snakefile_inner --dry-run --delete-all-output all && \
        test -f infile && test -f intermediate && test -f some_dir/final && \
        test -d empty_dir && test -L dangling && test -d full_dir
        """


rule delete_temp_output:
    output:
        touch("delete_temp_output"),
    shell:
        """
        python -m snakemake --cores 1 -s Snakefile_inner --notemp temp && \
        python -m snakemake --cores 1 -s Snakefile_inner --dry-run  --delete-temp-output temp && \
        test -f infile && test -f temp_intermediate && \
        test -d temp_empty_dir && test -d temp_full_dir && test -f temp_keep
        """



================================================
FILE: tests/test_github_issue_3265_respect_dryrun_delete_all/Snakefile_inner
================================================
shell.executable("bash")


rule all:
    input:
        "some_dir/final",
        "empty_dir",
        "full_dir",


rule a:
    input:
        "infile",
    output:
        "intermediate",
        "dangling",
        touch(protected("protected")),
    shell:
        "ln -s {input} {output[0]} && touch nothere && ln -s nothere {output[1]}"


rule b:
    input:
        "intermediate",
    output:
        touch("some_dir/final"),


rule c:
    output:
        directory("empty_dir"),
    shell:
        "mkdir empty_dir"


rule d:
    output:
        directory("full_dir"),
    shell:
        "mkdir full_dir && touch full_dir/somefile"


rule e:
    input:
        "infile",
    output:
        temp("temp_intermediate"),
    shell:
        "touch {output}"


rule f:
    input:
        "temp_intermediate",
    output:
        directory(temp("temp_empty_dir")),
    shell:
        "mkdir temp_empty_dir"


rule g:
    input:
        "temp_intermediate",
    output:
        directory(temp("temp_full_dir")),
    shell:
        "mkdir temp_full_dir && touch temp_full_dir/somefile"


rule temp:
    input:
        "temp_empty_dir",
        "temp_full_dir",
    output:
        touch("temp_keep"),



================================================
FILE: tests/test_github_issue_3265_respect_dryrun_delete_all/expected-results/all_ok
================================================
[Empty file]


================================================
FILE: tests/test_globwildcards/Snakefile
================================================
(IDS,) = glob_wildcards("test.{id}.txt")
(IDS2,) = glob_wildcards("test_{id}_{id}.txt")

assert set(IDS) == {"0", "1", "2"}
assert set(IDS2) == {"a", "b"}


rule all:
    input:
        expand("test.{id}.out", id=IDS),
        expand("test_{id}.out", id=IDS2),


rule:
    input:
        "test.{id}.txt",
    output:
        touch("test.{id}.out"),


rule:
    input:
        "test_{id}_{id}.txt",
    output:
        touch("test_{id}.out"),



================================================
FILE: tests/test_globwildcards/test.0.txt
================================================
[Empty file]


================================================
FILE: tests/test_globwildcards/test.1.txt
================================================
[Empty file]


================================================
FILE: tests/test_globwildcards/test.2.txt
================================================
[Empty file]


================================================
FILE: tests/test_globwildcards/test_a_a.txt
================================================
[Empty file]


================================================
FILE: tests/test_globwildcards/test_a_b.txt
================================================
[Empty file]


================================================
FILE: tests/test_globwildcards/test_b_b.txt
================================================
[Empty file]


================================================
FILE: tests/test_google_lifesciences/config.json
================================================
{"message": "hahaha"}



================================================
FILE: tests/test_google_lifesciences/env.yml
================================================
channels:
  - conda-forge
dependencies:
  - bzip2



================================================
FILE: tests/test_google_lifesciences/Snakefile
================================================
import os

# This test file is taken from Kubernetes - both use Google Cloud
from snakemake.remote.GS import RemoteProvider as GSRemoteProvider
GS = GSRemoteProvider()

rule all:
    input:
        "landsat-data.txt.bz2",
        "testdir"

rule copy:
    input:
        GS.remote("gcp-public-data-landsat/LC08/01/001/003/LC08_L1GT_001003_20170430_20170501_01_RT/LC08_L1GT_001003_20170430_20170501_01_RT_MTL.txt")
    output:
        "landsat-data.txt"
    resources:
        mem_mb=100
    run:
        # we could test volume size like this but it is currently unclear what f1-micro instances provide as boot disk size
        #stats = os.statvfs('.')
        #volume_gib = stats.f_bsize * stats.f_blocks / 1.074e9
        #assert volume_gib > 90
        shell("cp {input} {output}")

rule pack:
    input:
        "landsat-data.txt"
    output:
        "landsat-data.txt.bz2"
    conda:
        "env.yml"
    log:
        "logs/pack.log"
    shell:
        "bzip2 -c {input} > {output}; echo successful > {log}"


rule directory:
    output:
        directory("testdir")
    log:
        "logs/directory.log"
    shell:
        "mkdir -p {output}; touch {output}/test.txt"



================================================
FILE: tests/test_google_lifesciences/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_group_job_fail/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1 || true



================================================
FILE: tests/test_group_job_fail/Snakefile
================================================
samples = [1,2,3,4,5]


rule all:
    input:
        "test.out"


rule a:
    output:
        "a/{sample}.out"
    group: 0
    shell:
        "touch {output}"


rule b:
    input:
        "a/{sample}.out"
    output:
        "b/{sample}.out"
    log:
        "logs/b/{sample}.log"
    group: 0
    shell:
        "touch {output}; echo failed > {log}; exit 1"


rule c:
    input:
        expand("b/{sample}.out", sample=samples)
    output:
        "test.out"
    group: 1
    shell:
        "touch {output}"



================================================
FILE: tests/test_group_jobs/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_group_jobs/Snakefile
================================================
samples = [1,2,3,4,5]


rule all:
    input:
        "test.out"


rule a:
    output:
        "a/{sample}.out"
    group: 0
    shell:
        "touch {output}"


rule b:
    input:
        "a/{sample}.out"
    output:
        "b/{sample}.out"
    group: 0
    shell:
        "touch {output}"


rule c:
    input:
        expand("b/{sample}.out", sample=samples)
    output:
        "test.out"
    group: 1
    shell:
        "touch {output}"



================================================
FILE: tests/test_group_jobs_attempts/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_group_jobs_attempts/Snakefile
================================================
rule a:
    output: "a"
    resources: mem_mb = lambda w, attempt: attempt*1000
    group: "mygroup"
    shell: '''
    if [ {resources.mem_mb} == 2000 ]; then
        touch {output}
    fi
    '''

# rule b:
#     input: rules.a.output
#     output: "b_{number}"
#     group: "mygroup"
#     shell: '''
#     touch -m {output}
#     '''


================================================
FILE: tests/test_group_jobs_attempts/expected-results/a
================================================
[Empty file]


================================================
FILE: tests/test_group_jobs_resources/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_group_jobs_resources/Snakefile
================================================
samples = [1,2,3,4,5]

resource_scopes:
    fake_res="local",
    mem_mb="global",

rule all:
    input:
        "test.out"


rule a:
    output:
        "a/{sample}.out"
    group: 0
    threads: 4
    resources:
        mem_mb=20000,
        fake_res=400,
        global_res=1000,
        runtime=80,
    shell:
        "touch {output}"

rule a_1:
    output:
        "a_1/{sample}.out"
    group: 0
    threads: 2
    resources:
        mem_mb=40000,
        runtime=40,
        fake_res=200,
        global_res=1000,
    shell:
        "touch {output}"


rule b:
    input:
        "a/{sample}.out",
        "a_1/{sample}.out"
    output:
        "b/{sample}.out"
    group: 0
    threads: 1
    resources:
        runtime=20,
        mem_mb=10000
    shell:
        "touch {output}"


rule c:
    input:
        expand("b/{sample}.out", sample=samples)
    output:
        "test.out"
    group: 0
    shell:
        "touch {output}"



================================================
FILE: tests/test_group_jobs_resources/status_failed
================================================
#!/bin/bash

echo failed



================================================
FILE: tests/test_group_parallel/Snakefile
================================================
import os.path
import os, sys
import time

rule all:
    group: "g1"
    input:
        "foo.txt",
        "bar.txt",

rule a:
    output:
        touch("bar.txt"),
    group: "g1"
    run:
        time.sleep(5)
        assert os.path.exists("b.txt")


rule b:
    output:
        touch("foo.txt"),
    group: "g1"
    run:
        with open("b.txt", "w") as out:
            print("b", file=out)
        time.sleep(10)
        os.remove("b.txt")


================================================
FILE: tests/test_group_parallel/expected-results/bar.txt
================================================
[Empty file]


================================================
FILE: tests/test_group_parallel/expected-results/foo.txt
================================================
[Empty file]


================================================
FILE: tests/test_group_with_pipe/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_group_with_pipe/Snakefile
================================================
samples = [0,1]

rule all:
    input:
        "test.out"

rule a:
    output:
        "a/{sample}.out"
    group: 0
    threads: 4
    resources:
        mem_mb=20000,
        runtime=80,
    shell:
        "touch {output}"

rule a_1:
    output:
        pipe("test.{sample}.txt")
    group: 0
    threads: 4
    resources:
        mem_mb=40000,
        runtime=40,
    shell:
        "for i in {{0..2}}; do echo {wildcards.sample} >> {output}; done"


rule b:
    input:
        a="a/{sample}.out",
        a_1="test.{sample}.txt"
    output:
        "test.{sample}.out"
    group: 0
    threads: 2
    resources:
        runtime=20,
        mem_mb=10000
    shell:
        "grep {wildcards.sample} < {input.a_1} > {output}"

rule c:
    input:
        expand("test.{sample}.out", sample=samples)
    output:
        "test.out"
    group: 0
    shell:
        "touch {output}"



================================================
FILE: tests/test_groupid_expand/Snakefile
================================================
shell.executable("bash")


rule all:
    input:
        expand("bar{i}.txt", i=range(3)),


rule grouplocal:
    output:
        "foo.{groupid}.txt",
    group:
        "foo"
    shell:
        "echo {wildcards.groupid} > {output}"


def get_input(wildcards, groupid):
    return f"foo.{groupid}.txt"


rule consumer:
    input:
        get_input,
    output:
        "bar{i}.txt",
    group:
        "foo"
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_groupid_expand/expected-results/bar0.txt
================================================
local



================================================
FILE: tests/test_groupid_expand/expected-results/bar1.txt
================================================
local



================================================
FILE: tests/test_groupid_expand/expected-results/bar2.txt
================================================
local



================================================
FILE: tests/test_groupid_expand/expected-results/foo.local.txt
================================================
local



================================================
FILE: tests/test_groupid_expand_cluster/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
cat $1 >> qsub.log
sh $1



================================================
FILE: tests/test_groupid_expand_cluster/Snakefile
================================================
rule all:
    input:
        expand("bar{i}.txt", i=range(3))


rule grouplocal:
    output:
        "foo.{groupid}.txt"
    group:
        "foo"
    shell:
        "echo test > {output}"


def get_input(wildcards, groupid):
    return f"foo.{groupid}.txt"


rule consumer:
    input:
        get_input
    output:
        "bar{i}.txt"
    group:
        "foo"
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_groupid_expand_cluster/expected-results/bar0.txt
================================================
test



================================================
FILE: tests/test_groupid_expand_cluster/expected-results/bar1.txt
================================================
test



================================================
FILE: tests/test_groupid_expand_cluster/expected-results/bar2.txt
================================================
test



================================================
FILE: tests/test_groupid_expand_cluster/expected-results/foo.412c59b9-e2fc-51ea-9a84-d055fb244f80.txt
================================================
test



================================================
FILE: tests/test_groupid_expand_cluster/expected-results/foo.5792af91-9d58-5430-a941-2d29860112e7.txt
================================================
test



================================================
FILE: tests/test_groupid_expand_cluster/expected-results/foo.e1a9b7a0-f48e-568a-bc42-d6c2078055be.txt
================================================
test



================================================
FILE: tests/test_groups_out_of_jobs/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_groups_out_of_jobs/Snakefile
================================================
rule all:
    input:
        "results/e.txt",


rule e:
    input:
        "results/d.txt",
        "results/a.txt"
    output:
        "results/e.txt"
    group:
        "group1"
    shell:
        "touch {output}"


rule c:
    input:
        "results/a.txt"
    output:
        "results/c.txt",
    shell:
        "touch {output}"


rule d:
    input:
        "results/c.txt"
    output:
        "results/d.txt"
    shell:
        "touch {output}"


rule a:
    output:
        "results/a.txt"
    group:
        "group1"
    shell:
        "touch {output}"


================================================
FILE: tests/test_groups_out_of_jobs/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_gs_requester_pays/Snakefile
================================================
from snakemake.remote import GS
import google.auth
try:
    GS = GS.RemoteProvider()

    rule copy:
        input:
            GS.remote(config["url"], user_project=config["project"])
        output:
            "landsat-data.txt"
        shell:
            "cp {input} {output}"
except google.auth.exceptions.DefaultCredentialsError:
    # ignore the test if not authenticated
    print("skipping test_remote_gs because we are not authenticated with gcloud")



================================================
FILE: tests/test_gs_requester_pays/expected-results/landsat-data.txt
================================================
GROUP = L1_METADATA_FILE
  GROUP = METADATA_FILE_INFO
    ORIGIN = "Image courtesy of the U.S. Geological Survey"
    REQUEST_ID = "0501705013406_00001"
    LANDSAT_SCENE_ID = "LC80010032017120LGN00"
    LANDSAT_PRODUCT_ID = "LC08_L1GT_001003_20170430_20170501_01_RT"
    COLLECTION_NUMBER = 01
    FILE_DATE = 2017-05-01T16:00:24Z
    STATION_ID = "LGN"
    PROCESSING_SOFTWARE_VERSION = "LPGS_2.7.0"
  END_GROUP = METADATA_FILE_INFO
  GROUP = PRODUCT_METADATA
    DATA_TYPE = "L1GT"
    COLLECTION_CATEGORY = "RT"
    ELEVATION_SOURCE = "GLS2000"
    OUTPUT_FORMAT = "GEOTIFF"
    SPACECRAFT_ID = "LANDSAT_8"
    SENSOR_ID = "OLI_TIRS"
    WRS_PATH = 1
    WRS_ROW = 3
    NADIR_OFFNADIR = "NADIR"
    TARGET_WRS_PATH = 1
    TARGET_WRS_ROW = 3
    DATE_ACQUIRED = 2017-04-30
    SCENE_CENTER_TIME = "14:07:16.5180850Z"
    CORNER_UL_LAT_PRODUCT = 80.21528
    CORNER_UL_LON_PRODUCT = -17.96312
    CORNER_UR_LAT_PRODUCT = 80.28798
    CORNER_UR_LON_PRODUCT = -3.45901
    CORNER_LL_LAT_PRODUCT = 77.79053
    CORNER_LL_LON_PRODUCT = -16.19251
    CORNER_LR_LAT_PRODUCT = 77.84841
    CORNER_LR_LON_PRODUCT = -4.56164
    CORNER_UL_PROJECTION_X_PRODUCT = 330600.000
    CORNER_UL_PROJECTION_Y_PRODUCT = 8918700.000
    CORNER_UR_PROJECTION_X_PRODUCT = 604200.000
    CORNER_UR_PROJECTION_Y_PRODUCT = 8918700.000
    CORNER_LL_PROJECTION_X_PRODUCT = 330600.000
    CORNER_LL_PROJECTION_Y_PRODUCT = 8645400.000
    CORNER_LR_PROJECTION_X_PRODUCT = 604200.000
    CORNER_LR_PROJECTION_Y_PRODUCT = 8645400.000
    PANCHROMATIC_LINES = 18221
    PANCHROMATIC_SAMPLES = 18241
    REFLECTIVE_LINES = 9111
    REFLECTIVE_SAMPLES = 9121
    THERMAL_LINES = 9111
    THERMAL_SAMPLES = 9121
    FILE_NAME_BAND_1 = "LC08_L1GT_001003_20170430_20170501_01_RT_B1.TIF"
    FILE_NAME_BAND_2 = "LC08_L1GT_001003_20170430_20170501_01_RT_B2.TIF"
    FILE_NAME_BAND_3 = "LC08_L1GT_001003_20170430_20170501_01_RT_B3.TIF"
    FILE_NAME_BAND_4 = "LC08_L1GT_001003_20170430_20170501_01_RT_B4.TIF"
    FILE_NAME_BAND_5 = "LC08_L1GT_001003_20170430_20170501_01_RT_B5.TIF"
    FILE_NAME_BAND_6 = "LC08_L1GT_001003_20170430_20170501_01_RT_B6.TIF"
    FILE_NAME_BAND_7 = "LC08_L1GT_001003_20170430_20170501_01_RT_B7.TIF"
    FILE_NAME_BAND_8 = "LC08_L1GT_001003_20170430_20170501_01_RT_B8.TIF"
    FILE_NAME_BAND_9 = "LC08_L1GT_001003_20170430_20170501_01_RT_B9.TIF"
    FILE_NAME_BAND_10 = "LC08_L1GT_001003_20170430_20170501_01_RT_B10.TIF"
    FILE_NAME_BAND_11 = "LC08_L1GT_001003_20170430_20170501_01_RT_B11.TIF"
    FILE_NAME_BAND_QUALITY = "LC08_L1GT_001003_20170430_20170501_01_RT_BQA.TIF"
    ANGLE_COEFFICIENT_FILE_NAME = "LC08_L1GT_001003_20170430_20170501_01_RT_ANG.txt"
    METADATA_FILE_NAME = "LC08_L1GT_001003_20170430_20170501_01_RT_MTL.txt"
    CPF_NAME = "LC08CPF_20170401_20170630_01.02"
    BPF_NAME_OLI = "LO8BPF20170430140614_20170430144404.01"
    BPF_NAME_TIRS = "LT8BPF20170426235522_20170427000359.01"
    RLUT_FILE_NAME = "LC08RLUT_20150303_20431231_01_12.h5"
  END_GROUP = PRODUCT_METADATA
  GROUP = IMAGE_ATTRIBUTES
    CLOUD_COVER = 24.32
    CLOUD_COVER_LAND = -1
    IMAGE_QUALITY_OLI = 9
    IMAGE_QUALITY_TIRS = 7
    TIRS_SSM_MODEL = "PRELIMINARY"
    TIRS_SSM_POSITION_STATUS = "ESTIMATED"
    TIRS_STRAY_LIGHT_CORRECTION_SOURCE = "TIRS"
    ROLL_ANGLE = -0.001
    SUN_AZIMUTH = -156.47580997
    SUN_ELEVATION = 24.99099132
    EARTH_SUN_DISTANCE = 1.0074392
    SATURATION_BAND_1 = "N"
    SATURATION_BAND_2 = "N"
    SATURATION_BAND_3 = "N"
    SATURATION_BAND_4 = "N"
    SATURATION_BAND_5 = "N"
    SATURATION_BAND_6 = "N"
    SATURATION_BAND_7 = "N"
    SATURATION_BAND_8 = "N"
    SATURATION_BAND_9 = "N"
    TRUNCATION_OLI = "UPPER"
  END_GROUP = IMAGE_ATTRIBUTES
  GROUP = MIN_MAX_RADIANCE
    RADIANCE_MAXIMUM_BAND_1 = 748.87909
    RADIANCE_MINIMUM_BAND_1 = -61.84268
    RADIANCE_MAXIMUM_BAND_2 = 766.86133
    RADIANCE_MINIMUM_BAND_2 = -63.32766
    RADIANCE_MAXIMUM_BAND_3 = 706.65613
    RADIANCE_MINIMUM_BAND_3 = -58.35589
    RADIANCE_MAXIMUM_BAND_4 = 595.89227
    RADIANCE_MINIMUM_BAND_4 = -49.20898
    RADIANCE_MAXIMUM_BAND_5 = 364.65634
    RADIANCE_MINIMUM_BAND_5 = -30.11344
    RADIANCE_MAXIMUM_BAND_6 = 90.68671
    RADIANCE_MINIMUM_BAND_6 = -7.48894
    RADIANCE_MAXIMUM_BAND_7 = 30.56628
    RADIANCE_MINIMUM_BAND_7 = -2.52417
    RADIANCE_MAXIMUM_BAND_8 = 674.38605
    RADIANCE_MINIMUM_BAND_8 = -55.69102
    RADIANCE_MAXIMUM_BAND_9 = 142.51598
    RADIANCE_MINIMUM_BAND_9 = -11.76902
    RADIANCE_MAXIMUM_BAND_10 = 22.00180
    RADIANCE_MINIMUM_BAND_10 = 0.10033
    RADIANCE_MAXIMUM_BAND_11 = 22.00180
    RADIANCE_MINIMUM_BAND_11 = 0.10033
  END_GROUP = MIN_MAX_RADIANCE
  GROUP = MIN_MAX_REFLECTANCE
    REFLECTANCE_MAXIMUM_BAND_1 = 1.210700
    REFLECTANCE_MINIMUM_BAND_1 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_2 = 1.210700
    REFLECTANCE_MINIMUM_BAND_2 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_3 = 1.210700
    REFLECTANCE_MINIMUM_BAND_3 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_4 = 1.210700
    REFLECTANCE_MINIMUM_BAND_4 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_5 = 1.210700
    REFLECTANCE_MINIMUM_BAND_5 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_6 = 1.210700
    REFLECTANCE_MINIMUM_BAND_6 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_7 = 1.210700
    REFLECTANCE_MINIMUM_BAND_7 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_8 = 1.210700
    REFLECTANCE_MINIMUM_BAND_8 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_9 = 1.210700
    REFLECTANCE_MINIMUM_BAND_9 = -0.099980
  END_GROUP = MIN_MAX_REFLECTANCE
  GROUP = MIN_MAX_PIXEL_VALUE
    QUANTIZE_CAL_MAX_BAND_1 = 65535
    QUANTIZE_CAL_MIN_BAND_1 = 1
    QUANTIZE_CAL_MAX_BAND_2 = 65535
    QUANTIZE_CAL_MIN_BAND_2 = 1
    QUANTIZE_CAL_MAX_BAND_3 = 65535
    QUANTIZE_CAL_MIN_BAND_3 = 1
    QUANTIZE_CAL_MAX_BAND_4 = 65535
    QUANTIZE_CAL_MIN_BAND_4 = 1
    QUANTIZE_CAL_MAX_BAND_5 = 65535
    QUANTIZE_CAL_MIN_BAND_5 = 1
    QUANTIZE_CAL_MAX_BAND_6 = 65535
    QUANTIZE_CAL_MIN_BAND_6 = 1
    QUANTIZE_CAL_MAX_BAND_7 = 65535
    QUANTIZE_CAL_MIN_BAND_7 = 1
    QUANTIZE_CAL_MAX_BAND_8 = 65535
    QUANTIZE_CAL_MIN_BAND_8 = 1
    QUANTIZE_CAL_MAX_BAND_9 = 65535
    QUANTIZE_CAL_MIN_BAND_9 = 1
    QUANTIZE_CAL_MAX_BAND_10 = 65535
    QUANTIZE_CAL_MIN_BAND_10 = 1
    QUANTIZE_CAL_MAX_BAND_11 = 65535
    QUANTIZE_CAL_MIN_BAND_11 = 1
  END_GROUP = MIN_MAX_PIXEL_VALUE
  GROUP = RADIOMETRIC_RESCALING
    RADIANCE_MULT_BAND_1 = 1.2371E-02
    RADIANCE_MULT_BAND_2 = 1.2668E-02
    RADIANCE_MULT_BAND_3 = 1.1674E-02
    RADIANCE_MULT_BAND_4 = 9.8438E-03
    RADIANCE_MULT_BAND_5 = 6.0239E-03
    RADIANCE_MULT_BAND_6 = 1.4981E-03
    RADIANCE_MULT_BAND_7 = 5.0494E-04
    RADIANCE_MULT_BAND_8 = 1.1140E-02
    RADIANCE_MULT_BAND_9 = 2.3543E-03
    RADIANCE_MULT_BAND_10 = 3.3420E-04
    RADIANCE_MULT_BAND_11 = 3.3420E-04
    RADIANCE_ADD_BAND_1 = -61.85505
    RADIANCE_ADD_BAND_2 = -63.34032
    RADIANCE_ADD_BAND_3 = -58.36757
    RADIANCE_ADD_BAND_4 = -49.21882
    RADIANCE_ADD_BAND_5 = -30.11946
    RADIANCE_ADD_BAND_6 = -7.49044
    RADIANCE_ADD_BAND_7 = -2.52468
    RADIANCE_ADD_BAND_8 = -55.70216
    RADIANCE_ADD_BAND_9 = -11.77137
    RADIANCE_ADD_BAND_10 = 0.10000
    RADIANCE_ADD_BAND_11 = 0.10000
    REFLECTANCE_MULT_BAND_1 = 2.0000E-05
    REFLECTANCE_MULT_BAND_2 = 2.0000E-05
    REFLECTANCE_MULT_BAND_3 = 2.0000E-05
    REFLECTANCE_MULT_BAND_4 = 2.0000E-05
    REFLECTANCE_MULT_BAND_5 = 2.0000E-05
    REFLECTANCE_MULT_BAND_6 = 2.0000E-05
    REFLECTANCE_MULT_BAND_7 = 2.0000E-05
    REFLECTANCE_MULT_BAND_8 = 2.0000E-05
    REFLECTANCE_MULT_BAND_9 = 2.0000E-05
    REFLECTANCE_ADD_BAND_1 = -0.100000
    REFLECTANCE_ADD_BAND_2 = -0.100000
    REFLECTANCE_ADD_BAND_3 = -0.100000
    REFLECTANCE_ADD_BAND_4 = -0.100000
    REFLECTANCE_ADD_BAND_5 = -0.100000
    REFLECTANCE_ADD_BAND_6 = -0.100000
    REFLECTANCE_ADD_BAND_7 = -0.100000
    REFLECTANCE_ADD_BAND_8 = -0.100000
    REFLECTANCE_ADD_BAND_9 = -0.100000
  END_GROUP = RADIOMETRIC_RESCALING
  GROUP = TIRS_THERMAL_CONSTANTS
    K1_CONSTANT_BAND_10 = 774.8853
    K2_CONSTANT_BAND_10 = 1321.0789
    K1_CONSTANT_BAND_11 = 480.8883
    K2_CONSTANT_BAND_11 = 1201.1442
  END_GROUP = TIRS_THERMAL_CONSTANTS
  GROUP = PROJECTION_PARAMETERS
    MAP_PROJECTION = "UTM"
    DATUM = "WGS84"
    ELLIPSOID = "WGS84"
    UTM_ZONE = 29
    GRID_CELL_SIZE_PANCHROMATIC = 15.00
    GRID_CELL_SIZE_REFLECTIVE = 30.00
    GRID_CELL_SIZE_THERMAL = 30.00
    ORIENTATION = "NORTH_UP"
    RESAMPLING_OPTION = "CUBIC_CONVOLUTION"
  END_GROUP = PROJECTION_PARAMETERS
END_GROUP = L1_METADATA_FILE
END



================================================
FILE: tests/test_handle_storage_multi_consumers/Snakefile
================================================
rule all:
    input:
        "test.a.txt",
        "test.b.txt",


rule a:
    input:
        "test.txt"
    output:
        "test.a.txt"
    shell:
        "cp {input} {output}"

rule b:
    input:
        "test.txt"
    output:
        "test.b.txt"
    shell:
        "cp {input} {output}"


================================================
FILE: tests/test_handle_storage_multi_consumers/expected-results/storage/test.a.txt
================================================
[Empty file]


================================================
FILE: tests/test_handle_storage_multi_consumers/expected-results/storage/test.b.txt
================================================
[Empty file]


================================================
FILE: tests/test_handle_storage_multi_consumers/storage/test.txt
================================================
[Empty file]


================================================
FILE: tests/test_handover/script.py
================================================
if not snakemake.resources.mem_mb == 20:
    raise ValueError("Handover of all resources did not work.")
with open(snakemake.output[0], "w") as out:
    print("test", file=out)


================================================
FILE: tests/test_handover/Snakefile
================================================
rule a:
    output:
        "test.out"
    handover: True
    script:
        "script.py"


================================================
FILE: tests/test_immediate_submit/README.md
================================================
```bash
snakemake --profile slurm all
touch output/output.touch
```
- only after create `output/output.touch`, snakemake can run as expected.



================================================
FILE: tests/test_immediate_submit/Snakefile
================================================
rule all:
    input:
        echo_helps=[f"output/{i}.touch" for i in ("a", "b")],
    output:
        "output/all"
    resources:
        partition="debug",
        threads=1,
    threads:
        1
    shell:
        """
        date +%F\ %T
        ls {input} > {output}
        """

rule test_array:
    input:
        dirtouch="output/output.touch",
    output:
        echo_help="output/{i}.touch",
    params:
        watch="output"
    threads: 40
    params:
        echo="{i}",
    shell:
        """
        date +%F\ %T
        sleep 3

        ls -lrt {params.watch} > {output}
        date +%F\ %T
        """


rule make_output:
    output:
        dirtouch="output/output.touch",
    resources:
        partition="debug",
        threads=1,
    threads:
        1
    shell:
        """
        date +%F\ %T
        while [ ! -f {output.dirtouch} ]; do sleep 5; done

        touch {output.dirtouch}
        date +%F\ %T
        """



================================================
FILE: tests/test_immediate_submit/expected-results/output/all
================================================
output/a.touch
output/b.touch



================================================
FILE: tests/test_immediate_submit/slurm/config.yaml
================================================
cluster:
  mkdir -p logs/{rule} &&
  bash slurm/get_jobid.sh
  'logs/{rule}/{rule}-{wildcards}-%j'
  '{dependencies}'
  #  '--partition={resources.partition}
  #  --ntasks-per-node={resources.threads}
  #  --job-name=smk-{rule}-{wildcards}
  #  --output=logs/{rule}/{rule}-{wildcards}-%j.out
  #  --error=logs/{rule}/{rule}-{wildcards}-%j.err'
default-resources:
  - partition=cpu
  - threads=40
local-cores: 1
jobs: 500
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
immediate-submit: True
notemp: True
# jobscript: "slurm-jobscript.sh"

# Example resource configuration
# default-resources:
#   - runtime=100
#   - mem_mb=6000
#   - disk_mb=1000000
# # set-threads: map rule names to threads
# set-threads:
#   - single_core_rule=1
#   - multi_core_rule=10
# # set-resources: map rule names to resources in general
# set-resources:
#   - high_memory_rule:mem_mb=12000
#   - long_running_rule:runtime=1200



================================================
FILE: tests/test_immediate_submit/slurm/get_jobid.sh
================================================
#!/bin/bash
set -e && echo "$0 $*" >&2

set -vx

sbatch_params=$1
dependencies=$2
script=$3

echo `bash slurm/sbatch.sh $sbatch_params $script "$dependencies"`



================================================
FILE: tests/test_immediate_submit/slurm/sbatch.sh
================================================
#!/bin/bash
set -e && echo "$0 $*" >&2

set -vx

sbatch_params=$1
script=$2
dependencies=$3
flag=tmp/slurm-signal/dependencies-$(basename $script)-finish

mkdir -p tmp/slurm-signal

echo "#!/bin/bash"                      > $flag.sh
echo ''                                 >> $flag.sh
echo "for i in ${dependencies//,/ }"    >> $flag.sh
echo 'do'                               >> $flag.sh
echo '    while [ ! -f $i ]'            >> $flag.sh
echo '    do'                           >> $flag.sh
echo '        sleep 3'                  >> $flag.sh
echo '    done'                         >> $flag.sh
echo 'done'                             >> $flag.sh
echo ''                                 >> $flag.sh
cat $script                             >> $flag.sh
echo ''                                 >> $flag.sh
echo "touch $flag"                      >> $flag.sh

nohup bash $flag.sh > $sbatch_params.log 2>&1 &

echo $flag



================================================
FILE: tests/test_incomplete_params/Snakefile
================================================
def get_x(wildcards, input):
    with open(input[0]) as infile:
        return {"foo": infile.read()}


def get_mem_mb(wildcards, input):
    return os.path.getsize(input[0]) / 1024.0


rule a:
    input:
        "test.in",
    output:
        "test.out",
    params:
        x=get_x,
    resources:
        mem_mb=get_mem_mb,
    shell:
        "echo {params.x[foo]} > {output}"


rule b:
    output:
        "test.in",
    shell:
        "touch {output}"



================================================
FILE: tests/test_incomplete_params/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_inferred_resources/Snakefile
================================================
rule a:
    output:
        "test.txt"
    resources:
        mem="5MB",
        disk="5MB",
        runtime="1h",
    shell:
        "echo {resources.mem_mb} {resources.disk_mb} {resources.runtime} > {output}"



================================================
FILE: tests/test_inferred_resources/expected-results/test.txt
================================================
5 5 60



================================================
FILE: tests/test_inferred_resources_mib/Snakefile
================================================
rule a:
    output:
        "test1.out",
    resources:
        mem="5000MB",
        disk="5000MB",
    shell:
        "echo {resources.mem_mib} {resources.disk_mib} {resources.mem_mb} {resources.disk_mb} > {output}"


rule b:
    output:
        "test2.out",
    resources:
        mem="50G",
        disk="50G",
    shell:
        "echo {resources.mem_mib} {resources.disk_mib} {resources.mem_mb} {resources.disk_mb} > {output}"


rule c:
    output:
        "test3.out",
    resources:
        mem="5TB",
        disk="5TB",
    shell:
        "echo {resources.mem_mib} {resources.disk_mib} {resources.mem_mb} {resources.disk_mb} > {output}"



================================================
FILE: tests/test_inner_call/a.in
================================================
[Empty file]


================================================
FILE: tests/test_inner_call/Snakefile
================================================
def some_b():
    def inner(wildcards):
        return {wildcards.x}
    return inner

def some_a():
    def inner(wildcards):
        return expand("{x}.in", x=some_b())
    return inner

rule all:
    input:
        "a.txt"

rule b:
    input:
        some_a()
    output:
        "{x}.txt"
    shell:
        "touch {output}"




================================================
FILE: tests/test_inner_call/expected-results/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_inoutput_is_path/Snakefile
================================================
from pathlib import Path

rule:
	input: Path("test.in").resolve() # absolute, must exist
	output: Path("test.out") # relative, does not have to exist (yet)
	run:
		test = shell("echo 42;", read=True)
		assert int(test) == 42
		with open(output[0], "w") as f:
			for l in shell("cat {input}", iterable=True):
				print(l, file=f)



================================================
FILE: tests/test_inoutput_is_path/test.in
================================================
foo
bar



================================================
FILE: tests/test_input_generator/foo.1.in
================================================
1



================================================
FILE: tests/test_input_generator/foo.2.in
================================================
2



================================================
FILE: tests/test_input_generator/foo.3.in
================================================
3



================================================
FILE: tests/test_input_generator/Snakefile
================================================
# What if an input function returns a generator rather than a list?
# Pythonically, generators are preferred to lists.

#This always worked fine:
def input_func1(wildcards):
    return [ "%s.%d.in" % (wildcards.base, n) for n in range(1,4) ]

#This fails:
def input_func2(wildcards):
    return ( "%s.%d.in" % (wildcards.base, n) for n in range(1,4) )

#As does this
def input_func3(wildcards):
    for n in range(1,4):
        yield "%s.%d.in" % (wildcards.base, n)

rule main:
    input: "foo.out1", "foo.out2", "foo.out3"

rule test1:
    output: "{base}.out1"
    input: input_func1
    shell: "cat {input} > {output}"

rule test2:
    output: "{base}.out2"
    input: input_func2
    shell: "cat {input} > {output}"

rule test3:
    output: "{base}.out3"
    input: input_func3
    shell: "cat {input} > {output}"



================================================
FILE: tests/test_input_generator/expected-results/foo.out1
================================================
1
2
3



================================================
FILE: tests/test_input_generator/expected-results/foo.out2
================================================
1
2
3



================================================
FILE: tests/test_input_generator/expected-results/foo.out3
================================================
1
2
3



================================================
FILE: tests/test_ioutils/config.yaml
================================================
switches:
  someswitch: true



================================================
FILE: tests/test_ioutils/dummy1.tsv
================================================
[Empty file]


================================================
FILE: tests/test_ioutils/in.txt
================================================
[Empty file]


================================================
FILE: tests/test_ioutils/samples.md5
================================================
5695eb6f38992d796551f4cb20d7d138  2.tsv
6695eb6f38992d796551f4cb20d7d138  3.tsv
7695eb6f38992d796551f4cb20d7d138  prefix_1.tsv
8695eb6f38992d796551f4cb20d7d138  1_suffix.tsv
9695eb6f38992d796551f4cb20d7d138  1.tsv



================================================
FILE: tests/test_ioutils/samples.tsv
================================================
sample	val
1	1
2	1
3	1


================================================
FILE: tests/test_ioutils/Snakefile
================================================
import pandas as pd

samples = pd.read_csv("samples.tsv", sep="\t")


configfile: "config.yaml"


assert lookup(dpath="does/not/exist", within=config, default=None) is None
assert lookup(dpath="does/not/exist", within=config, default=5) == 5
assert (
    extract_checksum("samples.md5", file="1.tsv") == "9695eb6f38992d796551f4cb20d7d138"
)
assert flatten([1, "a", [2, "b"], ["c", "d", ["e", 3]]]) == [
    "1",
    "a",
    "2",
    "b",
    "c",
    "d",
    "e",
    "3",
]


rule all:
    input:
        "results/switch~someswitch.column~sample.txt",


rule a:
    input:
        "dummy1.tsv",
        checksum="samples.md5",
    output:
        "a/{sample}.txt",
    params:
        checksum1=parse_input(
            input[1], parser=extract_checksum, file="1.tsv"
        ),
        checksum2=parse_input(
            input.checksum, parser=extract_checksum, file="1.tsv"
        ),
        checksum3=lambda w, input: parse_input(
            input.checksum, parser=extract_checksum, file=f"{w.sample}.tsv"
        ),
    shell:
        "echo -e '{params.checksum1}\n{params.checksum2}\n{params.checksum3}' | sort -u > {output}"


rule b:
    input:
        branch(evaluate("{sample} == '100'"), then="a/{sample}.txt"),
    output:
        "b/{sample}.txt",
    shell:
        "echo b > {output}"


rule c:
    input:
        branch(
            evaluate("{sample} == '1'"),
            then="a/{sample}.txt",
            otherwise="b/{sample}.txt",
        ),
    output:
        "c/{sample}.txt",
    shell:
        "cat {input} > {output}"


rule item_access:
    input:
        txt="in.txt",
    output:
        txt="test.txt",
    params:
        output_item=output.txt,
        output_item2=output[0],
        input_item=input.txt,
        mem=resources.mem,
    resources:
        mem="1GB",
    run:
        assert params.output_item == "test.txt"
        assert params.output_item2 == "test.txt"
        assert params.input_item == "in.txt"
        assert resources.mem == "1GB"

        with open(output[0], "w") as outfile:
            print("d", file=outfile)


rule e:
    input:
        collect("c/{item.sample}.txt", item=lookup(query="{col} <= 2", within=samples)),
        branch(lookup(dpath="switches/{switch}", within=config), then="test.txt"),
    output:
        "results/switch~{switch}.column~{col}.txt",
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_ioutils/expected-results/test.txt
================================================
d



================================================
FILE: tests/test_ioutils/expected-results/b/2.txt
================================================
b



================================================
FILE: tests/test_ioutils/expected-results/c/1.txt
================================================
9695eb6f38992d796551f4cb20d7d138



================================================
FILE: tests/test_ioutils/expected-results/c/2.txt
================================================
b



================================================
FILE: tests/test_ioutils/expected-results/results/switch~someswitch.column~sample.txt
================================================
9695eb6f38992d796551f4cb20d7d138
b
d



================================================
FILE: tests/test_issue1037/Foo_A.start
================================================
[Empty file]


================================================
FILE: tests/test_issue1037/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
cat $1 >> qsub.log
sh $1



================================================
FILE: tests/test_issue1037/Snakefile
================================================
ruleorder: done2 > done1

rule step1:
    input: '{sample}.start'
    output: '{sample}.step1'
    group: 'mygroup'
    shell: 'touch {output}'

rule done1:
    input: '{sample}.step1'
    output: '{sample}.done'
    group: 'mygroup'
    shell: 'touch {output}'

rule done2:
    input: '{sample}.step1'
    output: '{sample}.done'
    wildcard_constraints: sample = 'Foo_.+'
    group: 'mygroup'
    shell: 'touch {output}'



================================================
FILE: tests/test_issue1041/Snakefile
================================================
CHUNKS = range(3)

rule all:
    input:
        expand('raw.split_{chunk}.txt', chunk=CHUNKS)


rule start:
    group:
        'group1'
    output:
        'raw.txt'
    shell:
        'echo "hi" > {output}'


rule split:
    input:
        txt='raw.txt'
    output:
        expand('raw.split_{chunk}.txt', chunk=CHUNKS)
    group:
        'group1'
    shell:
        '''
        sleep 3
        echo 0 > {output[0]}
        echo 1 > {output[1]}
        echo 2 > {output[2]}
        '''



================================================
FILE: tests/test_issue1041/expected-results/raw.split_0.txt
================================================
0



================================================
FILE: tests/test_issue1041/expected-results/raw.split_1.txt
================================================
1



================================================
FILE: tests/test_issue1041/expected-results/raw.split_2.txt
================================================
2



================================================
FILE: tests/test_issue1046/Snakefile
================================================
rule all:
  input:
    "test_report.html"

rule test:
  input:
    "{name}.csv"
  output:
    "{name}_report.html"
  params:
  # DOES NOT WORK!
    title = lambda wildcards, input: input[0]
  # WORKS!!
  #  title = lambda wildcards: wildcards.name
  wrapper:
    "file:my_wrapper"



================================================
FILE: tests/test_issue1046/test.csv
================================================
[Empty file]


================================================
FILE: tests/test_issue1046/expected-results/test_report.html
================================================
test.csv



================================================
FILE: tests/test_issue1046/my_wrapper/environment.yaml
================================================
channels:
  - conda-forge
dependencies: []



================================================
FILE: tests/test_issue1046/my_wrapper/wrapper.py
================================================
from snakemake import shell
shell.executable("bash")

shell("echo {} > {}".format(snakemake.params["title"], snakemake.output[0]))



================================================
FILE: tests/test_issue1083/bar.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue1083/Snakefile
================================================
rule:
    output:
        "foo.txt"
    input:
        "bar.txt"
    singularity: "docker://bash"
    shadow: "minimal"
    shell:
        "mount; " 
        "pwd; "
        "ls -l {input}; "
        "cat {input} > {output}"




================================================
FILE: tests/test_issue1083/expected-results/foo.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue1085/Snakefile
================================================
shell.executable("bash")

def get_params(wildcards):
    return {'test': 'oo', 'norm': 'oo'} 

rule test: 
    output: "aa"
    params: unpack(get_params)
    shell:  "echo {params}"    




================================================
FILE: tests/test_issue1092/Snakefile
================================================
shell.executable("bash")

# a target rule to define the desired final output
rule all:
    input:
        "aggregated.txt",


# the checkpoint that shall trigger re-evaluation of the DAG
checkpoint clustering:
    output:
        clusters=directory("clustering")
    shell:
        "mkdir clustering; "
        "for i in 1 2 3; do echo $i > clustering/$i.txt; done"


def aggregate_input(wildcards):
    checkpoint_output = checkpoints.clustering.get(**wildcards).output[0]
    print("beyond")
    return sorted(
            expand("post/{i}.txt",
            i=glob_wildcards(os.path.join(checkpoint_output, "{i}.txt")).i) )


# an aggregation over all produced clusters
rule aggregate:
    input:
        aggregate_input
    output:
        "aggregated.txt"
    # shell:
    #     "cat {input} > {output}"
    run:
        shell("cat {input} > {output}")

# an intermediate rule
rule intermediate:
    input:
        "clustering/{i}.txt"
    output:
        "post/{i}.txt"
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_issue1092/expected-results/aggregated.txt
================================================
1
2
3



================================================
FILE: tests/test_issue1092/expected-results/aggregated.txt_WIN
================================================
1
2
3



================================================
FILE: tests/test_issue1093/condaenv.yaml
================================================
channels:
    - conda-forge
    - bioconda
dependencies:
    - bwa



================================================
FILE: tests/test_issue1093/Snakefile
================================================
# a target rule to define the desired final output
rule all:
    input:
        "aggregated.txt",


# the checkpoint that shall trigger re-evaluation of the DAG
checkpoint clustering:
    output:
        clusters=directory("clustering")
    shell:
        "mkdir clustering; "
        "for i in 1 2 3; do echo $i > clustering/$i.txt; done"


def aggregate_input(wildcards):
    checkpoint_output = checkpoints.clustering.get(**wildcards).output[0]
    return expand("post/{i}.txt",
           i=sorted(glob_wildcards(os.path.join(checkpoint_output, "{i}.txt")).i))


# an aggregation over all produced clusters
rule aggregate:
    input:
        aggregate_input
    output:
        "aggregated.txt"
    shell:
        "cat {input} > {output}"

# an intermediate rule
rule intermediate:
    input:
        "clustering/{i}.txt"
    output:
        "post/{i}.txt"
    conda:
        "condaenv.yaml"
    shell:
        "which bwa; "
        "cp {input} {output}"




================================================
FILE: tests/test_issue1093/expected-results/aggregated.txt
================================================
1
2
3



================================================
FILE: tests/test_issue1256/Snakefile
================================================
rule all:
    input:
        'done.txt',
        
rule trim_transcripts:
    output:
        done.txt,
    run:
        if 1 = 2:  # Intentional syntax error
            print('foo')



================================================
FILE: tests/test_issue1281/Snakefile
================================================
# using dictionary expansion

mydictionary={
'apple': 'crunchy fruit',
'banana': 'mushy and yellow'
}

rule all:
    input:
        expand('{key}.sh', key=mydictionary.keys())

rule test:
    output: temp('{f}.txt')
    params:
        keyval=lambda wildcards: mydictionary[wildcards.f]
    shell:
        """
        echo {params.keyval} > {output}
        cat {output}
        """

rule test2:
    input: rules.test.output
    output: '{f}.sh'
    shell: "touch {output}"

rule checkme:
    input: expand(rules.test2.output, f=mydictionary.keys())



================================================
FILE: tests/test_issue1281/expected-results/apple.sh
================================================
[Empty file]


================================================
FILE: tests/test_issue1281/expected-results/banana.sh
================================================
[Empty file]


================================================
FILE: tests/test_issue1284/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        'result.r1',
        'result.c1',  # targeting checkpoint output overrides input
        'result.c2'


def rule1_input(wc):
    checkpoints.check1.get(**wc).output[0]
    return "result.r2"

rule r1:
    input:
        rule1_input
    output:
        'result.r1',
    shell:
        'echo {input} > {output}'


rule r2:
    output:
        'result.r2'
    shell:
        'echo r2 > {output}'


def check1_input(wc):
    checkpoints.check2.get(**wc).output[0]
    return 'result.r3'

checkpoint check1:
    input:
        check1_input
    output:
        'result.c1'
    shell:
        'echo c1 > {output}'


checkpoint check2:
    output:
        'result.c2'
    shell:
        'echo c2 > {output}'


rule r3:
    output:
        'result.r3'
    shell:
        'echo r3 > {output}'



================================================
FILE: tests/test_issue1284/expected-results/result.c1
================================================
c1



================================================
FILE: tests/test_issue1284/expected-results/result.c2
================================================
c2



================================================
FILE: tests/test_issue1284/expected-results/result.r1
================================================
result.r2



================================================
FILE: tests/test_issue1284/expected-results/result.r2
================================================
r2



================================================
FILE: tests/test_issue1284/expected-results/result.r3
================================================
r3



================================================
FILE: tests/test_issue1331/Snakefile
================================================
rule all:
    input:
        [
         "aligned_and_sort/1.txt",
         "aligned_and_sort/2.txt",
         "aligned_and_sort/3.txt",
         "aligned_and_sort/4.txt",
         "aligned_and_sort/5.txt",
         "aligned_and_sort/6.txt",
        ]


checkpoint trimming:
    output:
        "trimmed/{sample}.txt"
    shell:
        "touch {output}; sleep 1"


rule align:
    input:
        "trimmed/{sample}.txt"
    output:
        pipe("aligned/{sample}.txt")
    shell:
        "touch {output}; sleep 1"


rule sort:
    input:
        "aligned/{sample}.txt"
    output:
        "aligned_and_sort/{sample}.txt"
    shell:
        "touch {output}; sleep 1"



================================================
FILE: tests/test_issue1331/expected-results/aligned_and_sort/1.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue1331/expected-results/aligned_and_sort/2.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue1331/expected-results/aligned_and_sort/3.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue1331/expected-results/aligned_and_sort/4.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue1331/expected-results/aligned_and_sort/5.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue1331/expected-results/aligned_and_sort/6.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue2574/config.yaml
================================================
stuff:
  - key: value



================================================
FILE: tests/test_issue2574/Snakefile
================================================
configfile: "config.yaml"


X = config["unexisting_key"]

print(1)



================================================
FILE: tests/test_issue2685/Snakefile
================================================
import pandas


samples = pandas.DataFrame.from_records(
    (
        {"sample_id": "a", "use_a": True},
        {"sample_id": "b", "use_a": False},
        {"sample_id": "c", "use_a": False},
    ),
)

rule b:
    input:
        collect(
            "test/{sample.sample_id}.txt",
            sample=lookup(
                query="use_a",
                within=samples
            )
        )
    output:
        "test/complete.txt"
    shell:
        "cat {input} > {output}"

rule a:
    output:
        "test/{i}.txt"
    shell:
        "echo '{wildcards.i}' > {output}"




================================================
FILE: tests/test_issue2685/expected-results/test/a.txt
================================================
a



================================================
FILE: tests/test_issue2685/expected-results/test/complete.txt
================================================
a



================================================
FILE: tests/test_issue2826_failed_binary_logs/Snakefile
================================================
rule:
    log:
        'log.bin'
    run:
        import pickle
        with open(log[0], 'wb') as f:
            # Write anything binary to the file
            # that cannot be interpreted as unicode
            # dumping the log object was an easy way to do this 
            pickle.dump(log, f)
        # make this test fail
        a


================================================
FILE: tests/test_issue3192/a.txt
================================================
some text



================================================
FILE: tests/test_issue3192/script.py
================================================
import shutil

shutil.copyfile(
    snakemake.input[0],
    snakemake.output[0])



================================================
FILE: tests/test_issue3192/Snakefile
================================================
conda_env = Path(os.environ['CONDA_PREFIX']) / 'envs' / 'test_issue3192'

rule all:
    input:
        "c.txt"

rule a2b:
    input:
        "a.txt"
    output:
        "b.txt"
    conda:
        conda_env
    script:
        "script.py"

rule b2c:
    input:
        "b.txt"
    output:
        "c.txt"
    conda:
        conda_env
    script:
        "script.py"



================================================
FILE: tests/test_issue3192/expected-results/c.txt
================================================
some text



================================================
FILE: tests/test_issue3338/other_workflow.smk
================================================
rule all:
    input:
        "test_file.tsv"

rule test:
    output:
        file = "test_file.tsv"
    params:
        "pos1",
        "pos2",
        a = 3,
        b = 6
    shell:
        "echo {params} > {output.file}"




================================================
FILE: tests/test_issue3338/Snakefile
================================================
rule all:
    input:
        "test_file.tsv",
        "test_file_no_pos.tsv",
        "test.out",
        "test2.out",
        "test2_pos.out"

# Test that the change works as intended for workflows imported as modules

module other_workflow:
    snakefile:
        "other_workflow.smk"

use rule test from other_workflow as other_test with:
    # here we test if the positional parameters from other workflow are overwritten
    params:
        "pos3",
        b = 9


use rule test from other_workflow as other_test_no_pos with:
    # here we test, if the positional parameters from other workflow are imported
    params:
        b = 9
    output:
        file = "test_file_no_pos.tsv"

# Test that the change works for regular rule inheritance
rule a:
    params:
        a = 3,
        b = 6
    output:
        "test.out"
    shell:
        "echo {params} > {output}"

use rule a as b with:
    # here we test, if the positional parameters from inherited rule are imported
    params:
        a = 9
    output:
        "test2.out"


use rule a as b_pos with:
    # here we test if the positional parameters from the inherited rule are overwritten
    params:
        "pos1",
        a = 9
    output:
        "test2_pos.out"



================================================
FILE: tests/test_issue3361_fail/input.txt
================================================
test



================================================
FILE: tests/test_issue3361_fail/Snakefile
================================================
rule all:
    input: 
        "output.txt"

rule copy_in_container:
    input: 
        "input.txt"
    output: 
        "output.txt"
    params:
        workflow.source_path("some_dir/some_other_input.txt")
    container: 
        "docker://ubuntu:latest"
    shell:
        "touch ../../test.txt && cat {input} {params} > {output}"



================================================
FILE: tests/test_issue3361_fail/expected-results/output.txt
================================================
test
another test



================================================
FILE: tests/test_issue3361_fail/some_dir/some_other_input.txt
================================================
another test



================================================
FILE: tests/test_issue3361_pass/input.txt
================================================
test



================================================
FILE: tests/test_issue3361_pass/Snakefile
================================================
rule all:
    input: 
        "output.txt"

rule copy_in_container:
    input: 
        "input.txt"
    output: 
        "output.txt"
    params:
        workflow.source_path("some_dir/some_other_input.txt")
    container: 
        "docker://ubuntu:latest"
    shell:
        "cat {input} {params} > {output}"



================================================
FILE: tests/test_issue3361_pass/expected-results/output.txt
================================================
test
another test



================================================
FILE: tests/test_issue3361_pass/some_dir/some_other_input.txt
================================================
another test



================================================
FILE: tests/test_issue381/a.in
================================================
[Empty file]


================================================
FILE: tests/test_issue381/Snakefile
================================================

rule all:
    input:
        "b.out"

rule a:
    input:
        "a.in"
    output:
        "a.out"
    shell:
        "touch {output}"


rule b:
    input:
        rules.a.input
    output:
        "b.out"
    shell:
        "touch {output}"



================================================
FILE: tests/test_issue471/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "test1.2.out"


def process_params(wildcards) :

   e,a = wildcards.ea.split(".")

   return { 'epsilon' : '0' + e, 'alpha' : '0.' + a }


rule a:
    output:
        "test{ea}.out"
    params:
        p=process_params
    shell:
        "echo {params.p[epsilon]} > {output}"



================================================
FILE: tests/test_issue584/Snakefile
================================================
shell.executable("bash")

rule all: 
    input: 
        "out1"

rule generate:
    output: 
        "out1"
    params:
        test=lambda wildcards: "{test}"
    shell: "echo "" > out1"



================================================
FILE: tests/test_issue584/expected-results/out1
================================================




================================================
FILE: tests/test_issue612/Snakefile
================================================
rule A:
    run: print("A")

rule B:
   run: print("B")


rule C:
   run:

       print("C")



================================================
FILE: tests/test_issue635/input.txt
================================================
1	2	3



================================================
FILE: tests/test_issue635/Snakefile
================================================

rule all:
    input: "report.html"

rule rmd:
    input:
        "input.txt"
    output:
        "report.html"
    conda:
        "envs/rmarkdown.yaml"
    script:
        "scripts/report.Rmd"



================================================
FILE: tests/test_issue635/envs/rmarkdown.yaml
================================================
channels:
  - conda-forge
dependencies:
  - r-base =3.6.1
  - r-rmarkdown =1.17



================================================
FILE: tests/test_issue635/scripts/report.Rmd
================================================
---
title: "Test Report"
author:
    - "Your Name"
date: "`r format(Sys.time(), '%d %B, %Y')`"
params:
   rmd: "report.Rmd"
output:
  html_document:
  highlight: tango
  number_sections: no
  theme: default
  toc: yes
  toc_depth: 3
  toc_float:
    collapsed: no
    smooth_scroll: yes
---

## R Markdown

This is an R Markdown document.

Test include from snakemake ``r snakemake@input``.


```{r}
print(getwd())
```

```{r}
# This fails
data <- read.table(snakemake@input[[1]])
data
```

## Source
<a download="report.Rmd" href="`r base64enc::dataURI(file = params$rmd, mime = 'text/rmd', encoding = 'base64')`">R Markdown source file (to produce this document)</a>



================================================
FILE: tests/test_issue805/Snakefile
================================================
rule a:
    output:
        "test.out"
    shell:
        "echo {params.b} > {output}"
    params:
        b=1


rule b:
    input:
        "test.out"
    output:
        "test2.out"
    shell:
        "touch {output}"



================================================
FILE: tests/test_issue823_1/Snakefile
================================================
rule all:
    input:
        "b.txt"

checkpoint a:
    output:
        # to reproduce the bug:
        # >1 output required
        # one or more temp() outputs required
        # exception: will work if only the first output is temp()
        temp('a1.txt'),
        temp('a2.txt'),
    shell:
        "touch {output}"

def _checkpoint_output(wildcards):
    out = checkpoints.a.get(**wildcards).output
    return out

rule b:
    input:
        _checkpoint_output,
    output:
        'b.txt'
    shell:
        "touch {output}"




================================================
FILE: tests/test_issue823_1/expected-results/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue823_2/Snakefile
================================================
rule all:
    input:
        "c.txt",

checkpoint a:
    output:
        "a.txt"
    shell:
        "touch {output}"

rule b1:
    output:
        pipe("b.pipe")
    shell:
        "echo test > {output}"

rule b2:
    input:
        "b.pipe"
    output:
        "b.txt"
    shell:
        """
        cat {input} > /dev/null
        touch {output}
        """

rule c:
    input:
        "a.txt",
        "b.txt",
    output:
        "c.txt"
    shell:
        "touch {output}"




================================================
FILE: tests/test_issue823_2/expected-results/c.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue823_3/Snakefile
================================================
rule all:
    input:
        "a.txt"

checkpoint a:
    output:
        "a.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_issue823_3/expected-results/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue850/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
cat $1 >> qsub.log
sh $1



================================================
FILE: tests/test_issue850/Snakefile
================================================

lists = [["1.bam", "2.bam"], ["3.bam", "4.bam"]]

rule done:
    input: ["-".join(l) for l in lists], "2.qc2"

rule a:
    output: touch("{sample}.bam")
    group: "cell"

rule b:
    input: "{sample}.bam"
    output: touch("{sample}.qc")
    group: "cell"

rule c:
    input: "{sample}.qc"
    output: touch("{sample}.qc2")
    group: "cell"

for l in lists:
    rule:
        input:
            l
        output: temp(touch(str("-".join(l))))
        group: "cell"





================================================
FILE: tests/test_issue850/expected-results/2.qc2
================================================
[Empty file]


================================================
FILE: tests/test_issue854/Snakefile
================================================

rule a:
    output:
        "test.{x}.txt"
    benchmark:
        "benchmarks/test.txt"
    shell:
        "touch {output}"






================================================
FILE: tests/test_issue860/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
cat $1 >> qsub.log
echo $1
sh $1



================================================
FILE: tests/test_issue860/Snakefile
================================================
rule a:
    output: "{sample}.bam"
    group: "cell"
    shell: "touch {output}"

rule b:
    input: "{sample}.bam"
    output: "{sample}.qc"
    group: "cell"
    shadow: "minimal"
    shell: "touch {output}"

rule c:
    input: "{sample}.qc"
    output: "{sample}.qc2"
    group: "cell"
    shell: "touch {output}"

lists = [["1.bam", "2.bam"], ["3.bam", "4.bam"]]
for l in lists:
    rule:
        input:
            l
        output: temp(touch(str("-".join(l))))
        group: "cell"

rule done:
    input: [str("-".join(l)) for l in lists], "2.qc2"



================================================
FILE: tests/test_issue860/expected-results/1.bam
================================================
[Empty file]


================================================
FILE: tests/test_issue860/expected-results/2.bam
================================================
[Empty file]


================================================
FILE: tests/test_issue860/expected-results/2.qc
================================================
[Empty file]


================================================
FILE: tests/test_issue860/expected-results/2.qc2
================================================
[Empty file]


================================================
FILE: tests/test_issue860/expected-results/3.bam
================================================
[Empty file]


================================================
FILE: tests/test_issue860/expected-results/4.bam
================================================
[Empty file]


================================================
FILE: tests/test_issue894/Snakefile
================================================
onsuccess:
    assert not os.path.exists("b")

rule a:
    output: "a", temp("b")
    shell: "touch a b"



================================================
FILE: tests/test_issue894/expected-results/a
================================================
[Empty file]


================================================
FILE: tests/test_issue912/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
cat $1 >> qsub.log
sh $1



================================================
FILE: tests/test_issue912/Snakefile
================================================
# Create the directory fake/test and the file fake/test/sample.Mutect2.snpSift.vcf.gz
# Run on cluster, example:
# snakemake --cluster 'bash ' -j1 -p

rule all:
    input:
        expand(
            "fake/test/{t}.Mutect2.snpSift.hardfilter.vcf.gz",
            t=['sample']),

rule rule1:
    input:
        vcf = ancient('fake/{y}/{x}.vcf.gz'),
    output:
        indel = temp('fake/{y}/{x}.indels.raw.vcf.gz'),
    log:
        'fake/logs/rule1.{y}.{x}.log'
    params:
        jobname = 'rule1.{x}'
    shell:
        'touch {output.indel}'


rule rule2:
    input:
        indel = 'fake/{y}/{x}.indels.raw.vcf.gz',
    output:
        'fake/{y}/{x}.hardfilter.vcf.gz'
    log:
        'fake/logs/rule2.{y}.{x}.log'
    params:
        jobname = 'rule2',
    shell:
        'touch {output}'



================================================
FILE: tests/test_issue912/expected-results/fake/test/sample.Mutect2.snpSift.hardfilter.vcf.gz
================================================
[Empty file]


================================================
FILE: tests/test_issue912/fake/test/sample.Mutect2.snpSift.vcf.gz
================================================
[Empty file]


================================================
FILE: tests/test_issue916/local_script.py
================================================
contents = "blah blah blah"



================================================
FILE: tests/test_issue916/pythonTest.py
================================================
#!/usr/bin/env python
import sys
import os
import local_script
from pathlib import Path

# Ensure that the __real_file__ path ends in .snakemake
dname = Path(__real_file__).parent
print(dname)
if not dname.parts[-2:] == (".snakemake", "scripts"):
    sys.exit("We're not being written in the output directory!\n")

# Write out script to indicte success.
of = open(snakemake.output[0], "w")
of.write("local_script.contents {}\n".format(local_script.contents))
of.close()



================================================
FILE: tests/test_issue916/Snakefile
================================================
rule all:
    input: "pythonTest"

rule testIssue916:
    output: "pythonTest"
    script: "pythonTest.py"



================================================
FILE: tests/test_issue916/expected-results/pythonTest
================================================
local_script.contents blah blah blah



================================================
FILE: tests/test_issue930/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
cat $1 >> qsub.log
sh $1



================================================
FILE: tests/test_issue930/Snakefile
================================================
samples = ["0","1"]

rule all:
    input:
        "test.out"

rule build_index:
    output:
        "large_reference_index"
    shell:
        "touch {output}"

rule a:
    output:
        "a/{sample}.out"
    group:
        "sample_group"
    shell:
        "touch {output}"

rule b:
    input:
        rules.a.output,
        rules.build_index.output
    output:
        "b/{sample}.out"
    group:
        "sample_group"
    shell:
        "touch {output}"

rule c:
    input:
        expand("a/{sample}.out", sample=samples),
        expand("b/{sample}.out", sample=samples)
    output:
        "test.out"
    shell:
        "touch {output}"



================================================
FILE: tests/test_issue956/Snakefile
================================================
shell.executable("bash")


rule all:
    input:
        expand(
            "f_{result_type}.txt",
            result_type=["A", "B"]),


rule make_A:
    output:
        result_A = "A_{result_type}.txt",
    shell:
        """
        echo "A" > {output.result_A}
        """


rule make_B:
    output:
        result_B = "B_{result_type}.txt",
    shell:
        """
        echo "B" > {output.result_B}
        """


def source_result_type(wildcards):
    if wildcards.result_type == "A":
        return rules.make_A.output.result_A
    elif wildcards.result_type == "B":
        return rules.make_B.output.result_B
    else:
        raise NotImplementedError(f"{wildcards.result_type} not possible.\n")


rule generate_final_results:
    input:
        file_in = source_result_type,
    output:
        file_out = "f_{result_type}.txt",
    shell:
        """
        cat {input.file_in} > {output.file_out}
        """



================================================
FILE: tests/test_issue956/expected-results/f_A.txt
================================================
A



================================================
FILE: tests/test_issue956/expected-results/f_B.txt
================================================
B



================================================
FILE: tests/test_issue958/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_issue958/Snakefile
================================================
rule all:
    input: 'b.txt'

rule g_touch_a:
    output: touch('a.txt')
    group: 'g'

rule g_make_b_from_a:
    input: 'a.txt'
    output: touch('b.txt')
    group: 'g'



================================================
FILE: tests/test_issue_3202/Snakefile
================================================
shell.executable("bash")
conda_env = Path("test-env.yaml")

rule all:
    input: 
        "output.txt"

# Test that rule can be run idenpently in conda and apptainer
rule run_in_container_and_conda:
    output: 
        "output.txt"
    container: 
        "docker://bash"
    conda:
        "test-env.yaml"
    shell:
        "echo foo > {output}"


================================================
FILE: tests/test_issue_3202/test-env.yaml
================================================
channels:
  - bioconda
  - conda-forge
dependencies:
  - melt ==1.0.3
  - python <3.10


================================================
FILE: tests/test_issue_3202/expected-results/output.txt
================================================
foo


================================================
FILE: tests/test_job_properties/qsub.py
================================================
#!/usr/bin/env python3
import sys
import os
import random

from snakemake.utils import read_job_properties

jobscript = sys.argv[1]
job_properties = read_job_properties(jobscript)
with open("qsub.log", "a") as log:
    print(job_properties, file=log)

print(random.randint(1, 100))
os.system("sh {}".format(jobscript))



================================================
FILE: tests/test_job_properties/Snakefile
================================================


rule all:
    input:
        "test2.out"


rule a:
    output:
        "test1.in"
    shell:
        "touch {output}"


rule b:
    input:
        "test1.in"
    output:
        "test1.out"
    group: "x"
    shell:
        "touch {output}"


rule c:
    input:
        "test1.out"
    output:
        "test2.out"
    group: "x"
    shell:
        "touch {output}"



================================================
FILE: tests/test_jupyter_notebook/env.yaml
================================================
channels:
  - conda-forge
  - bioconda
dependencies:
  - python >=3.5
  - jupyter
  - papermill



================================================
FILE: tests/test_jupyter_notebook/Notebook.ipynb
================================================
# Jupyter notebook converted to Python script.

with open(snakemake.input.infile) as fd:
    data = fd.read()

new_results = data + '!!!'

with open(snakemake.output.outfile, 'w') as fd:
    fd.write(new_results)



================================================
FILE: tests/test_jupyter_notebook/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        'result_final.txt',
        'book.result_final.txt',

rule foo:
    output:
        fname = 'data.txt'
    run:
        with open(output.fname, 'w') as fd:
            fd.write('result of serious computation')

rule bar:
    input:
        infile = 'data.txt'
    output:
        outfile = 'result_intermediate.txt'
    conda:
        'env.yaml'
    notebook:
        'Notebook.ipynb'

rule baz:
    input:
        infile = 'result_intermediate.txt'
    output:
        outfile = 'result_final.txt'
    log:
        notebook = 'Notebook_Processed.ipynb'
    conda:
        'env.yaml'
    notebook:
        'Notebook.ipynb'


rule wild:
    input:
        infile = 'result_intermediate.txt'
    output:
        outfile = '{what}.result_final.txt'
    conda:
        'env.yaml'
    notebook:
        'Note{wildcards.what}.ipynb'



================================================
FILE: tests/test_jupyter_notebook/expected-results/result_final.txt
================================================
result of serious computation!!!!!!


================================================
FILE: tests/test_jupyter_notebook/expected-results/result_intermediate.txt
================================================
result of serious computation!!!


================================================
FILE: tests/test_jupyter_notebook_draft/data.txt
================================================
result of serious computation


================================================
FILE: tests/test_jupyter_notebook_draft/env.yaml
================================================
channels:
  - conda-forge
  - bioconda
dependencies:
  - python >=3.5
  - jupyter



================================================
FILE: tests/test_jupyter_notebook_draft/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        'result_final.txt',
        'book.result_final.txt',

rule foo:
    output:
        fname = 'data.txt'
    run:
        with open(output.fname, 'w') as fd:
            fd.write('result of serious computation')

rule bar:
    input:
        infile = 'data.txt'
    output:
        outfile = 'results/result_intermediate.txt'
    conda:
        'env.yaml'
    notebook:
        'Notebook.py.ipynb'

rule baz:
    input:
        infile = 'results/result_intermediate.txt'
    output:
        outfile = 'result_final.txt'
    log:
        notebook = 'Notebook_Processed.ipynb'
    conda:
        'env.yaml'
    notebook:
        'Notebook.py.ipynb'


rule wild:
    input:
        infile = 'results/result_intermediate.txt'
    output:
        outfile = '{what}.result_final.txt'
    conda:
        'env.yaml'
    notebook:
        'Note{wildcards.what}.py.ipynb'



================================================
FILE: tests/test_jupyter_notebook_draft/expected-results/Notebook.py.ipynb
================================================
# Jupyter notebook converted to Python script.


######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/Users/johannes/scms/snakemake', '/Users/johannes/scms/snakemake/tests/test_jupyter_notebook_draft']); import pickle; snakemake = pickle.loads(b'\x80\x03csnakemake.script\nSnakemake\nq\x00)\x81q\x01}q\x02(X\x05\x00\x00\x00inputq\x03csnakemake.io\nInputFiles\nq\x04)\x81q\x05X\x08\x00\x00\x00data.txtq\x06a}q\x07(X\x06\x00\x00\x00_namesq\x08}q\tX\x06\x00\x00\x00infileq\nK\x00N\x86q\x0bsX\x12\x00\x00\x00_allowed_overridesq\x0c]q\r(X\x05\x00\x00\x00indexq\x0eX\x04\x00\x00\x00sortq\x0feh\x0ecfunctools\npartial\nq\x10cbuiltins\ngetattr\nq\x11csnakemake.io\nNamedlist\nq\x12X\x0f\x00\x00\x00_used_attributeq\x13\x86q\x14Rq\x15\x85q\x16Rq\x17(h\x15)}q\x18X\x05\x00\x00\x00_nameq\x19h\x0esNtq\x1abh\x0fh\x10h\x15\x85q\x1bRq\x1c(h\x15)}q\x1dh\x19h\x0fsNtq\x1ebh\nh\x06ubX\x06\x00\x00\x00outputq\x1fcsnakemake.io\nOutputFiles\nq )\x81q!X\x17\x00\x00\x00result_intermediate.txtq"a}q#(h\x08}q$X\x07\x00\x00\x00outfileq%K\x00N\x86q&sh\x0c]q\'(h\x0eh\x0feh\x0eh\x10h\x15\x85q(Rq)(h\x15)}q*h\x19h\x0esNtq+bh\x0fh\x10h\x15\x85q,Rq-(h\x15)}q.h\x19h\x0fsNtq/bh%h"ubX\x06\x00\x00\x00paramsq0csnakemake.io\nParams\nq1)\x81q2}q3(h\x08}q4h\x0c]q5(h\x0eh\x0feh\x0eh\x10h\x15\x85q6Rq7(h\x15)}q8h\x19h\x0esNtq9bh\x0fh\x10h\x15\x85q:Rq;(h\x15)}q<h\x19h\x0fsNtq=bubX\t\x00\x00\x00wildcardsq>csnakemake.io\nWildcards\nq?)\x81q@}qA(h\x08}qBh\x0c]qC(h\x0eh\x0feh\x0eh\x10h\x15\x85qDRqE(h\x15)}qFh\x19h\x0esNtqGbh\x0fh\x10h\x15\x85qHRqI(h\x15)}qJh\x19h\x0fsNtqKbubX\x07\x00\x00\x00threadsqLK\x01X\t\x00\x00\x00resourcesqMcsnakemake.io\nResources\nqN)\x81qO(K\x01K\x01X0\x00\x00\x00/var/folders/l0/9bhq7fc12lgfknlx5gyxckv00000gp/TqPe}qQ(h\x08}qR(X\x06\x00\x00\x00_coresqSK\x00N\x86qTX\x06\x00\x00\x00_nodesqUK\x01N\x86qVX\x06\x00\x00\x00tmpdirqWK\x02N\x86qXuh\x0c]qY(h\x0eh\x0feh\x0eh\x10h\x15\x85qZRq[(h\x15)}q\\h\x19h\x0esNtq]bh\x0fh\x10h\x15\x85q^Rq_(h\x15)}q`h\x19h\x0fsNtqabhSK\x01hUK\x01hWhPubX\x03\x00\x00\x00logqbcsnakemake.io\nLog\nqc)\x81qd}qe(h\x08}qfh\x0c]qg(h\x0eh\x0feh\x0eh\x10h\x15\x85qhRqi(h\x15)}qjh\x19h\x0esNtqkbh\x0fh\x10h\x15\x85qlRqm(h\x15)}qnh\x19h\x0fsNtqobubX\x06\x00\x00\x00configqp}qqX\x04\x00\x00\x00ruleqrX\x03\x00\x00\x00barqsX\x0f\x00\x00\x00bench_iterationqtNX\t\x00\x00\x00scriptdirquX@\x00\x00\x00/Users/johannes/scms/snakemake/tests/test_jupyter_notebook_draftqvub.'); from snakemake.logging import logger; logger.printshellcmds = False; import os; os.chdir(r'/Users/johannes/scms/snakemake/tests/test_jupyter_notebook_draft');
######## snakemake preamble end #########


# start coding here



================================================
FILE: tests/test_jupyter_notebook_draft/expected-results/results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_jupyter_notebook_nbconvert/env.yaml
================================================
channels:
  - conda-forge
  - bioconda
dependencies:
  - python >=3.5
  - jupyter



================================================
FILE: tests/test_jupyter_notebook_nbconvert/Notebook.ipynb
================================================
# Jupyter notebook converted to Python script.

with open(snakemake.input.infile) as fd:
    data = fd.read()

new_results = data + '!!!'

with open(snakemake.output.outfile, 'w') as fd:
    fd.write(new_results)



================================================
FILE: tests/test_jupyter_notebook_nbconvert/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        'result_final.txt',
        'book.result_final.txt',

rule foo:
    output:
        fname = 'data.txt'
    run:
        with open(output.fname, 'w') as fd:
            fd.write('result of serious computation')

rule bar:
    input:
        infile = 'data.txt'
    output:
        outfile = 'result_intermediate.txt'
    conda:
        'env.yaml'
    notebook:
        'Notebook.ipynb'

rule baz:
    input:
        infile = 'result_intermediate.txt'
    output:
        outfile = 'result_final.txt'
    log:
        notebook = 'Notebook_Processed.ipynb'
    conda:
        'env.yaml'
    notebook:
        'Notebook.ipynb'


rule wild:
    input:
        infile = 'result_intermediate.txt'
    output:
        outfile = '{what}.result_final.txt'
    conda:
        'env.yaml'
    notebook:
        'Note{wildcards.what}.ipynb'



================================================
FILE: tests/test_jupyter_notebook_nbconvert/expected-results/result_final.txt
================================================
result of serious computation!!!!!!


================================================
FILE: tests/test_jupyter_notebook_nbconvert/expected-results/result_intermediate.txt
================================================
result of serious computation!!!


================================================
FILE: tests/test_keyword_list/Snakefile
================================================
rule:
	input: bla="test.in1 test.in2".split()
	output: "test.out"
	run:
		print(input.bla)
		assert len(input.bla) == 2
		shell("touch {output}")



================================================
FILE: tests/test_keyword_list/test.in1
================================================
[Empty file]


================================================
FILE: tests/test_keyword_list/test.in2
================================================
[Empty file]


================================================
FILE: tests/test_kubernetes/README.md
================================================
# Executing this test case

To run this test, you need a running kubernetes setup.
For google cloud, see [here](https://snakemake.readthedocs.io/en/stable/executing/cloud.html#setup-kubernetes-on-google-cloud-engine).
With this, you can execute in case of google cloud:

    snakemake --kubernetes --use-conda --default-remote-provider GS --default-remote-prefix my-bucket

while replacing ``my-bucket`` with your storage bucket. The same test should also work on amazon (given that kubernetes is setup):

    snakemake --kubernetes --use-conda --default-remote-provider S3 --default-remote-prefix my-bucket



================================================
FILE: tests/test_kubernetes/Snakefile
================================================
import os

from snakemake.remote.GS import RemoteProvider as GSRemoteProvider

GS = GSRemoteProvider()


rule all:
    input:
        "landsat-data.txt.bz2",
        "testdir",


rule copy:
    input:
        GS.remote(
            "gcp-public-data-landsat/LC08/01/001/003/LC08_L1GT_001003_20170430_20170501_01_RT/LC08_L1GT_001003_20170430_20170501_01_RT_MTL.txt"
        ),
    output:
        "landsat-data.txt",
    resources:
        mem_mb=100,
        disk_mb=301,
    run:
        # Check whether there is enough storage attached
        stats = os.statvfs(".")
        volume_gb = stats.f_bsize * stats.f_blocks / 1.0e9
        assert volume_gb >= 0.3, "Volume size is too small: {}GB".format(volume_gb)

        shell("cp {input} {output}")


rule pack:
    input:
        "landsat-data.txt",
    output:
        "landsat-data.txt.bz2",
    conda:
        "envs/gzip.yaml"
    singularity:
        "docker://continuumio/miniconda3:4.4.10"
    log:
        "logs/pack.log",
    shell:
        "bzip2 -c {input} > {output}; echo successful > {log}"


rule directory:
    output:
        directory("testdir"),
    log:
        "logs/directory.log",
    shell:
        "mkdir -p {output}; touch {output}/test.txt"



================================================
FILE: tests/test_kubernetes/envs/gzip.yaml
================================================
channels:
  - conda-forge
dependencies:
  - bzip2



================================================
FILE: tests/test_lazy_resources/Snakefile
================================================
import os

rule all:
    input:
        "results/bar.txt",

rule foo:
    output:
        "results/foo.txt"
    shell:
        "touch {output}"

def get_resources(wc):
    # usually, anything that raises a FileNotFoundError if the file is not present is fine here, even in dryrun.
    # However, since this test case shall ensure that the function is evaluated just before the actual execution
    # of the job, we additionally assert that the file is present.
    assert os.path.isfile("results/foo.txt"), "bug: resource function is not evaluated in a lazy way, just before job execution"
    return os.path.getsize("results/foo.txt")

rule bar:
    input:
        "results/foo.txt"
    output:
        "results/bar.txt"
    resources:
        test=get_resources
    shell:
        "touch {output}"



================================================
FILE: tests/test_lazy_resources/expected-results/results/bar.txt
================================================
[Empty file]


================================================
FILE: tests/test_lazy_resources/expected-results/results/foo.txt
================================================
[Empty file]


================================================
FILE: tests/test_list_untracked/Snakefile
================================================
shell.executable("bash")

rule run_test:
    output: "leftover_files"
    shell:  "python -m snakemake -s Snakefile_inner --list-untracked > {output}"



================================================
FILE: tests/test_list_untracked/Snakefile_inner
================================================
shell.executable("bash")

shell("mkdir -p some_workdir/some_subdir && \
    touch some_workdir/used_input && \
    touch some_workdir/some_subdir/used_output && \
    touch some_workdir/some_subdir/not_used && \
    touch some_workdir/log && \
    touch some_workdir/.hiddenfile && \
    mkdir -p some_workdir/.hiddendir && \
    touch some_workdir/.hiddendir/not_used2")

workdir: "some_workdir"

rule a:
    input: "used_input"
    output: "some_subdir/used_output"
    log: "log"



================================================
FILE: tests/test_list_untracked/expected-results/leftover_files
================================================
some_subdir/not_used



================================================
FILE: tests/test_list_untracked/expected-results/leftover_files_WIN
================================================
some_subdir\not_used



================================================
FILE: tests/test_load_metawrapper/Snakefile
================================================
rule all:
    input:
        "b.fa",


module fusion_calling:
    meta_wrapper:
        "v2.5.0/meta/bio/star_arriba"


use rule * from fusion_calling


use rule star_index from fusion_calling with:
    input:
        fasta="data/genome.fa",
        gtf="data/a.gtf",


rule test:
    input:
        rules.star_index.output,
    output:
        "b.fa",



================================================
FILE: tests/test_load_metawrapper/data/a.gtf
================================================
[Empty file]


================================================
FILE: tests/test_load_metawrapper/data/genome.fa
================================================
[Empty file]


================================================
FILE: tests/test_load_metawrapper/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_local_and_retrieve/keep_local.smk
================================================
storage:
    provider="http"

storage http_local:
    provider="http",
    keep_local=True

rule keep_local_default:
    input: storage.http("https://github.com/snakemake/snakemake/blob/main/images/logo.png")
    output: "keep_local_default.flag"
    shell: "touch {output}"

rule keep_local_true:
    input: storage.http("https://github.com/snakemake/snakemake/blob/main/images/logo.png", keep_local=True)
    output: "keep_local_true.flag"
    shell: "touch {output}"

rule keep_local_false:
    input: storage.http("https://github.com/snakemake/snakemake/blob/main/images/logo.png", keep_local=False)
    output: "keep_local_false.flag"
    shell: "touch {output}"

rule keep_local_true_directive:
    input: storage.http_local("https://github.com/snakemake/snakemake/blob/main/images/logo.png")
    output: "keep_local_true_directive.flag"
    shell: "touch {output}"



================================================
FILE: tests/test_local_and_retrieve/retrieve.smk
================================================
storage:
    provider="http",
    keep_local=True

storage http_ret:
    provider="http",
    keep_local=True,
    retrieve=False,


rule retrieve_default:
    input: storage.http("https://github.com/snakemake/snakemake/blob/main/images/logo.png")
    output: "retrieve_default.flag"
    shell: "touch {output}"

rule retrieve_true:
    input: storage.http("https://github.com/snakemake/snakemake/blob/main/images/logo.png", retrieve=True)
    output: "retrieve_true.flag"
    shell: "touch {output}"

rule retrieve_false:
    input: storage.http("https://github.com/snakemake/snakemake/blob/main/images/logo.png", retrieve=False)
    output: "retrieve_false.flag"
    shell: "touch {output}"

rule retrieve_false_directive:
    input: storage.http_ret("https://github.com/snakemake/snakemake/blob/main/images/logo.png")
    output: "retrieve_false_directive.flag"
    shell: "touch {output}"



================================================
FILE: tests/test_local_import/bar.py
================================================
[Empty file]


================================================
FILE: tests/test_local_import/Snakefile
================================================
import bar
import foo

rule:
	output: "test.out"
	shell: "touch {output}"



================================================
FILE: tests/test_local_import/foo/__init__.py
================================================
[Empty file]


================================================
FILE: tests/test_localrule/Snakefile
================================================
rule a:
    output:
        "{sample}.txt"
    localrule: True
    shell:
        "echo test > {output}"


rule b:
    input:
        expand("{sample}.txt", sample=[1, 2])



================================================
FILE: tests/test_localrule/expected-results/1.txt
================================================
test



================================================
FILE: tests/test_localrule/expected-results/2.txt
================================================
test



================================================
FILE: tests/test_log_input/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "test.a.txt"


rule a:
    output:
        "unused.{sample}.txt"
    log:
        "logs/{sample}.txt"
    shell:
        "echo {wildcards.sample} > {log}; touch {output}"


rule b:
    input:
        "logs/{sample}.txt"
    output:
        "test.{sample}.txt"
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_log_input/expected-results/test.a.txt
================================================
a



================================================
FILE: tests/test_logfile/Snakefile
================================================
onsuccess:
    shell("echo {log}")

rule all:
    input:
        expand("results/{i}.txt", i=range(5)),


rule a:
    output:
        "results/{i}.txt",
    shell:
        "echo {wildcards.i} > {output}"



================================================
FILE: tests/test_long_shell/Snakefile
================================================
shell.executable("bash")

rule:
  input: "test.in"
  output: "test.out"
  shell: "cp {input} {output}; #" + "x" * 64 * 4096



================================================
FILE: tests/test_long_shell/test.in
================================================
foo
bar



================================================
FILE: tests/test_many_jobs/Snakefile
================================================

rule:
	input: expand("{sample}.out", sample=range(50000))
	

rule:
	input: "{sample}.inter2"
	output: "{sample}.out"
	shell: "touch {output}"

rule:
	input: "{sample}.inter1"
	output: "{sample}.inter2"
	shell: "touch {output}"

rule:
	input: "{sample}.in"
	output: "{sample}.inter1"
	shell: "touch {output}"

rule:
	output: "{sample}.in"
	shell: "touch {output}"



================================================
FILE: tests/test_match_by_wildcard_names/Snakefile
================================================
rule all:
    input:
        expand("test.{sample}.{read}.fq", sample=["a"], read=["r1"]),


rule a:
    input:
        "foo.{sample}{read}.fq",
    output:
        "test.{sample}.{read}.fq",
    shell:
        "touch {output}"


rule b:
    output:
        "foo.{sample}{read}.fq",
    run:
        if "r" in wildcards.sample:
            raise ValueError("sample name contains 'r'")
        with open(output[0], "w") as f:
            f.write("foo")



================================================
FILE: tests/test_match_by_wildcard_names/expected-results/test.a.r1.fq
================================================
[Empty file]


================================================
FILE: tests/test_metadata_migration/Snakefile
================================================
rule a:
    output:
        "some/veryveryveryveryveryveryvery/veryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryvery/veryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryvery/veryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryvery/veryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryvery/veryveryveryveryveryveryveryveryveryveryveryveryverylong/path.txt"
    shell:
        "echo updated > {output}"



================================================
FILE: tests/test_missing_file_dryrun/Snakefile
================================================
rule all:
    input:
        "out.txt"

def get_units_for_sample(sample):
    return units[units["sample_name"] == sample]["unit_name"]

rule merge_unit_basecalls:
    input:
        "foo"
    output:
        "out.txt"
    shell:
        "touch {output}"


================================================
FILE: tests/test_missing_file_dryrun/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_module_checkpoint/Snakefile
================================================
shell.executable("bash")

print(id(checkpoints))


module test_a:
    snakefile:
        "module-test/Snakefile"
    config:
        ["AAA"]
    prefix:
        "a"


use rule * from test_a as test_a_*


module test_b:
    snakefile:
        "module-test/Snakefile"
    config:
        ["BBB"]
    prefix:
        "b"


use rule * from test_b as test_b_*


assert test_a.config[0] == "AAA"


rule all:
    input:
        expand(rules.test_a_aggregate.output, sample="a"),
        expand(rules.test_b_aggregate.output, sample="b"),
    default_target: True


print(id(checkpoints))



================================================
FILE: tests/test_module_checkpoint/expected-results/a/aggregated/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_module_checkpoint/expected-results/a/post/a.txt
================================================
post_a



================================================
FILE: tests/test_module_checkpoint/expected-results/a/somestep/a.txt
================================================
AAA



================================================
FILE: tests/test_module_checkpoint/expected-results/b/aggregated/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_module_checkpoint/expected-results/b/alt/b.txt
================================================
alt_b



================================================
FILE: tests/test_module_checkpoint/expected-results/b/somestep/b.txt
================================================
BBB



================================================
FILE: tests/test_module_checkpoint/module-test/Snakefile
================================================
checkpoint somestep:
    output:
        "somestep/{sample}.txt",
    shell:
        # simulate some output vale
        "echo {config} > {output}"


rule intermediate:
    output:
        "post/{sample}.txt",
    shell:
        "echo post_{wildcards.sample} > {output}"


rule alt_intermediate:
    output:
        "alt/{sample}.txt",
    shell:
        "echo alt_{wildcards.sample} > {output}"


def aggregate_input(wildcards):
    # decision based on content of output file
    with checkpoints.somestep.get(**wildcards).output[0].open() as f:
        if f.read().strip() == "AAA":
            return "post/{sample}.txt"
        else:
            return "alt/{sample}.txt"


rule aggregate:
    input:
        aggregate_input,
    output:
        "aggregated/{sample}.txt",
    shell:
        "touch {output}"



================================================
FILE: tests/test_module_complex/config.yaml
================================================
samples: samples.tsv

units: units.tsv

igv_report:
  activate: true

ref:
  # Number of chromosomes to consider for calling.
  # The first n entries of the FASTA will be considered.
  n_chromosomes: 25
  # Ensembl species name
  species: homo_sapiens
  # Ensembl release
  release: 100
  # Genome build
  build: GRCh38

primers:
  trimming:
    activate: false
    # path to fasta files containg primer sequences
    primers_fa1: "path/to/primer-fa1"
    primers_fa2: "path/to/primer-fa2"
     # Library mean + error determines the maximum insert size between the outer primer ends.
    library_error: 0
    # Mean insert size between the outer primer ends.
    library_length: 0

# Estimation of tumor mutational burden.
tmb:
  activate: false
  # Size of the sequenced coding genome for TMB estimation
  # Attention: when doing panel sequencing, set this to the
  # CAPTURED coding genome, not the entire one!
  coding_genome_size: 3e7
  # Plotting modes - hist (stratified histogram)
  # or curve (stratified curve)
  mode:
    - hist
    - curve
  # Name of the tumor sample in the scenario.yaml.
  tumor_sample: tumor
  somatic_events:
    - SOMATIC_TUMOR_LOW
    - SOMATIC_TUMOR_MEDIUM
    - SOMATIC_TUMOR_HIGH

# printing of variants in a matrix, sorted by recurrence
report:
  # if stratificatio is deactivated, one oncoprint for all
  # samples will be created.
  activate: true
  max_read_depth: 250
  stratify:
    activate: false
    # select a sample sheet column for stratification
    by-column: condition

# printing of variants in a table format
tables:
  activate: true
  # vembrane expression to generate the table
  output:
    expression: "INDEX, CHROM, POS, REF, ALT[0], ANN['Consequence'], ANN['IMPACT'], ANN['SYMBOL'], ANN['Feature']"
    genotype: true
    coverage: true
    event_prob: true
  generate_excel: true

calling:
  delly:
    activate: true
  freebayes:
    activate: true
  # See https://varlociraptor.github.io/docs/calling/#generic-variant-calling
  scenario: scenario.yaml
  filter:
    # Filter candidate variants (this filter helps to keep the number of evaluated candidates small).
    # It should ideally generate a superset of all other filters defined below.
    # Annotation of candidate variants tries to be as fast as possible, only using VEP
    # default parameters.
    candidates: "ANN['IMPACT'] != 'LOW'"
    # Add any number of named filters here. They will be applied independenty,
    # and can be referred in FDR control below to generate calls for different events.
    # In particular, you can also filter by ID or dbsnp annotations here.
    # See http://snpeff.sourceforge.net/SnpSift.html#filter
    filtername: "ANN['IMPACT'] == 'HIGH'"
  fdr-control:
    threshold: 0.05
    events: 
      # Add any number of events here to filter for.
      somatic:
        varlociraptor: 
          - "SOMATIC_TUMOR_HIGH"
          - "SOMATIC_TUMOR_MEDIUM"
        filter: filtername

# Will be ignored if calc_consensus_reads is activated
remove_duplicates:
  activate: true

calc_consensus_reads:
  activate: true

annotations:
  vcfs:
    activate: true
    # annotate with known variants from ensembl
    known: resources/variation.vcf.gz
    # add more external VCFs as needed
    # cosmic: path/to/cosmic.vcf.gz
  dgidb:
    activate: true
    # List of datasources for filtering dgidb entries
    # Available sources can be found on http://dgidb.org/api/v2/interaction_sources.json
    datasources:
      - DrugBank
  vep:
    candidate_calls:
      params: --af_gnomade
      plugins: []
    final_calls:
      # Consider removing --everything if VEP is slow for you (e.g. for WGS), 
      # and think carefully about which annotations you need.
      params: --everything
      plugins:
        # Add any plugin from https://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html
        # Plugin args can be passed as well, e.g. "LoFtool,path/to/custom/scores.txt".
        - LoFtool

mutational_burden:
  activate: false
  # Plotting modes - hist (stratified histogram)
  # or curve (stratified curve)
  mode:
    - curve
    - hist
  events:
    - somatic_tumor_low
    - somatic_tumor_medium
    - somatic_tumor_high

params:
  cutadapt: ""
  picard:
    MarkDuplicates: "VALIDATION_STRINGENCY=LENIENT"
  gatk:
    BaseRecalibrator: ""
    applyBQSR: ""
  varlociraptor_preprocess:
    max_depth: 200 # Set when processing panel data with high coverage
  varlociraptor_call:
    omit_read_bias: false # Set true when processing panel data
  freebayes:
    min_alternate_fraction: 0.05 # Reduce for calling variants with lower VAFs
  varlociraptor:
    # add extra arguments for varlociraptor call
    # For example, in case of panel data consider to omit certain bias estimations
    # which might be misleading because all reads of an amplicon have the same start
    # position, strand etc. (--omit-strand-bias, --omit-read-position-bias,
    # --omit-softclip-bias, --omit-read-orientation-bias).
    call: ""
    # Add extra arguments for varlociraptor preprocess. By default, we limit the depth to 200.
    # Increase this value for panel sequencing!
    preprocess: "--max-depth 200"



================================================
FILE: tests/test_module_complex/samples.tsv
================================================
sample_name	alias	group	platform	purity
a	a	a	ILLUMINA	



================================================
FILE: tests/test_module_complex/scenario.yaml
================================================
[Empty file]


================================================
FILE: tests/test_module_complex/Snakefile
================================================
from snakemake.utils import min_version

min_version("6.1.1")

configfile: "config.yaml"

remote_snakefile = "https://raw.githubusercontent.com/snakemake-workflows/dna-seq-varlociraptor/v5.0.2/workflow/Snakefile"


# import 1
module first:
    snakefile:
        remote_snakefile
    replace_prefix:
        {
            "benchmarking/": "benchmarking/first/",
            "logs/": "logs/first/",
            "results/": "results/first/",
            "resources/": "resources/first/",
        }
    config:
        config

use rule * from first as first_*

rule all:
    input:
        rules.first_all.input



================================================
FILE: tests/test_module_complex/units.tsv
================================================
sample_name	unit_name	fq1	fq2	sra	adapters
a	a	data/a.1.fq	data/a.2.fq		



================================================
FILE: tests/test_module_complex/data/a.1.fq
================================================
[Empty file]


================================================
FILE: tests/test_module_complex/data/a.2.fq
================================================
[Empty file]


================================================
FILE: tests/test_module_complex/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_module_complex2/config.yaml
================================================
samples: samples.tsv

units: units.tsv

igv_report:
  activate: true

ref:
  # Number of chromosomes to consider for calling.
  # The first n entries of the FASTA will be considered.
  n_chromosomes: 25
  # Ensembl species name
  species: homo_sapiens
  # Ensembl release
  release: 100
  # Genome build
  build: GRCh38

primers:
  trimming:
    activate: false
    # path to fasta files containg primer sequences
    primers_fa1: "path/to/primer-fa1"
    primers_fa2: "path/to/primer-fa2"
     # Library mean + error determines the maximum insert size between the outer primer ends.
    library_error: 0
    # Mean insert size between the outer primer ends.
    library_length: 0

# Estimation of tumor mutational burden.
tmb:
  activate: false
  # Size of the sequenced coding genome for TMB estimation
  # Attention: when doing panel sequencing, set this to the
  # CAPTURED coding genome, not the entire one!
  coding_genome_size: 3e7
  # Plotting modes - hist (stratified histogram)
  # or curve (stratified curve)
  mode:
    - hist
    - curve
  # Name of the tumor sample in the scenario.yaml.
  tumor_sample: tumor
  somatic_events:
    - SOMATIC_TUMOR_LOW
    - SOMATIC_TUMOR_MEDIUM
    - SOMATIC_TUMOR_HIGH

# printing of variants in a matrix, sorted by recurrence
report:
  # if stratificatio is deactivated, one oncoprint for all
  # samples will be created.
  activate: true
  max_read_depth: 250
  stratify:
    activate: false
    # select a sample sheet column for stratification
    by-column: condition

# printing of variants in a table format
tables:
  activate: true
  # vembrane expression to generate the table
  output:
    expression: "INDEX, CHROM, POS, REF, ALT[0], ANN['Consequence'], ANN['IMPACT'], ANN['SYMBOL'], ANN['Feature']"
    genotype: true
    coverage: true
    event_prob: true
  generate_excel: true

calling:
  delly:
    activate: true
  freebayes:
    activate: true
  # See https://varlociraptor.github.io/docs/calling/#generic-variant-calling
  scenario: scenario.yaml
  filter:
    # Filter candidate variants (this filter helps to keep the number of evaluated candidates small).
    # It should ideally generate a superset of all other filters defined below.
    # Annotation of candidate variants tries to be as fast as possible, only using VEP
    # default parameters.
    candidates: "ANN['IMPACT'] != 'LOW'"
    # Add any number of named filters here. They will be applied independenty,
    # and can be referred in FDR control below to generate calls for different events.
    # In particular, you can also filter by ID or dbsnp annotations here.
    # See http://snpeff.sourceforge.net/SnpSift.html#filter
    filtername: "ANN['IMPACT'] == 'HIGH'"
  fdr-control:
    threshold: 0.05
    events: 
      # Add any number of events here to filter for.
      somatic:
        varlociraptor: 
          - "SOMATIC_TUMOR_HIGH"
          - "SOMATIC_TUMOR_MEDIUM"
        filter: filtername

# Will be ignored if calc_consensus_reads is activated
remove_duplicates:
  activate: true

calc_consensus_reads:
  activate: true

annotations:
  vcfs:
    activate: true
    # annotate with known variants from ensembl
    known: resources/variation.vcf.gz
    # add more external VCFs as needed
    # cosmic: path/to/cosmic.vcf.gz
  dgidb:
    activate: true
    # List of datasources for filtering dgidb entries
    # Available sources can be found on http://dgidb.org/api/v2/interaction_sources.json
    datasources:
      - DrugBank
  vep:
    candidate_calls:
      params: --af_gnomade
      plugins: []
    final_calls:
      # Consider removing --everything if VEP is slow for you (e.g. for WGS), 
      # and think carefully about which annotations you need.
      params: --everything
      plugins:
        # Add any plugin from https://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html
        # Plugin args can be passed as well, e.g. "LoFtool,path/to/custom/scores.txt".
        - LoFtool

mutational_burden:
  activate: false
  # Plotting modes - hist (stratified histogram)
  # or curve (stratified curve)
  mode:
    - curve
    - hist
  events:
    - somatic_tumor_low
    - somatic_tumor_medium
    - somatic_tumor_high

params:
  cutadapt: ""
  picard:
    MarkDuplicates: "VALIDATION_STRINGENCY=LENIENT"
  gatk:
    BaseRecalibrator: ""
    applyBQSR: ""
  varlociraptor_preprocess:
    max_depth: 200 # Set when processing panel data with high coverage
  varlociraptor_call:
    omit_read_bias: false # Set true when processing panel data
  freebayes:
    min_alternate_fraction: 0.05 # Reduce for calling variants with lower VAFs
  varlociraptor:
    # add extra arguments for varlociraptor call
    # For example, in case of panel data consider to omit certain bias estimations
    # which might be misleading because all reads of an amplicon have the same start
    # position, strand etc. (--omit-strand-bias, --omit-read-position-bias,
    # --omit-softclip-bias, --omit-read-orientation-bias).
    call: ""
    # Add extra arguments for varlociraptor preprocess. By default, we limit the depth to 200.
    # Increase this value for panel sequencing!
    preprocess: "--max-depth 200"



================================================
FILE: tests/test_module_complex2/samples.tsv
================================================
sample_name	alias	group	platform	purity
a	a	a	ILLUMINA	



================================================
FILE: tests/test_module_complex2/scenario.yaml
================================================
[Empty file]


================================================
FILE: tests/test_module_complex2/Snakefile
================================================
from snakemake.utils import min_version

min_version("6.0")

configfile: "config.yaml"

remote_workflow_directory = "https://raw.githubusercontent.com/snakemake-workflows/dna-seq-varlociraptor/v5.0.2"
workflow_snakefile = f"{remote_workflow_directory}/workflow/Snakefile"


# basic
## use basic workflow
module basic:
    snakefile:
        workflow_snakefile
    replace_prefix:
        {
            "benchmarking/": "benchmarking/basic/",
            "logs/": "logs/basic/",
            "results/": "results/basic/",
        }
    config:
        config


use rule * from basic as basic_*


rule all:
    input:
        rules.basic_all.input



================================================
FILE: tests/test_module_complex2/units.tsv
================================================
sample_name	unit_name	fq1	fq2	sra	adapters
a	a	data/a.1.fq	data/a.2.fq		



================================================
FILE: tests/test_module_complex2/data/a.1.fq
================================================
[Empty file]


================================================
FILE: tests/test_module_complex2/data/a.2.fq
================================================
[Empty file]


================================================
FILE: tests/test_module_complex2/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_module_input_func/Snakefile
================================================
module mod1:
    snakefile:
        "module1/Snakefile"
    prefix:
        "test/"


use rule * from mod1 as mod1_*


rule all:
    input:
        rules.mod1_all.input,
    default_target: True



================================================
FILE: tests/test_module_input_func/expected-results/test/results/C.txt
================================================
C



================================================
FILE: tests/test_module_input_func/module1/Snakefile
================================================
def txt_output(wildcards):
    return ["results/C.txt"]


rule all:
    input:
        txt_output


rule txt:
    output:
        "results/C.txt"
    shell:
        "echo 'C' "
        ">{output} "



================================================
FILE: tests/test_module_local_git/config.yaml
================================================
samples: samples.tsv

units: units.tsv



================================================
FILE: tests/test_module_local_git/module.tar.gz
================================================
[Binary file]


================================================
FILE: tests/test_module_local_git/samples.tsv
================================================
sample_name	alias	group	platform	purity
a	a	a	ILLUMINA	



================================================
FILE: tests/test_module_local_git/Snakefile
================================================
from snakemake.utils import min_version

min_version("6.1.1")

configfile: "config.yaml"

import os

module_path = os.path.join(os.getcwd(), "repo/module")

module local_git_module:
    snakefile:
        gitfile(module_path, path="workflow/Snakefile", tag="v0.0.1")
    config:
        config

use rule * from local_git_module as local_git_module_*

rule all:
    input:
        "test.txt"



================================================
FILE: tests/test_module_local_git/Snakefile_main_missing_rule_and_schema
================================================
from snakemake.utils import min_version
import os

min_version("6.1.1")

configfile: "config.yaml"

module_path = os.path.join(os.getcwd(), "repo/module")

module local_git_module:
    snakefile:
        gitfile(module_path, path="workflow/Snakefile", tag="main")
    config:
        config

use rule * from local_git_module as local_git_module_*

rule all:
    input:
        "test.txt"



================================================
FILE: tests/test_module_local_git/Snakefile_missing_rule
================================================
from snakemake.utils import min_version
import os

min_version("6.1.1")

configfile: "config.yaml"

module_path = os.path.join(os.getcwd(), "repo/module")

module local_git_module:
    snakefile:
        gitfile(module_path, path="workflow/Snakefile", tag="e0.0.1")
    config:
        config

use rule * from local_git_module as local_git_module_*

rule all:
    input:
        "test.txt"



================================================
FILE: tests/test_module_local_git/Snakefile_missing_schema
================================================
from snakemake.utils import min_version
import os

min_version("6.1.1")

configfile: "config.yaml"

module_path = os.path.join(os.getcwd(), "repo/module")

module local_git_module:
    snakefile:
        gitfile(module_path, path="workflow/Snakefile", tag="e0.0.2")
    config:
        config

use rule * from local_git_module as local_git_module_*

rule all:
    input:
        "test.txt"



================================================
FILE: tests/test_module_local_git/Snakefile_relative
================================================
from snakemake.utils import min_version

min_version("6.1.1")

configfile: "config.yaml"

module local_git_module:
    snakefile:
        gitfile("repo/module", path="workflow/Snakefile", tag="v0.0.1")
    config:
        config

use rule * from local_git_module as local_git_module_*

rule all:
    input:
        "test.txt"



================================================
FILE: tests/test_module_local_git/units.tsv
================================================
sample_name	unit_name	fq1	fq2	sra	adapters
a	a	data/a.1.fq	data/a.2.fq		



================================================
FILE: tests/test_module_local_git/data/a.1.fq
================================================
[Empty file]


================================================
FILE: tests/test_module_local_git/data/a.2.fq
================================================
[Empty file]


================================================
FILE: tests/test_module_local_git/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_module_nested/module_deep.smk
================================================
# deep_module.smk
rule all:
    input:
        "foo.txt",


rule work:
    output:
        "foo.txt",
    shell:
        "echo 'I was here' > {output}"


# rules.work has to work even if the rule is renamed in a parent module
# The rulename itself can be already modified.
assert hasattr(rules, "work"), f"bug: rule cannot be accessed as work: {dir(rules)}"



================================================
FILE: tests/test_module_nested/module_shallow.smk
================================================
module deep_module:
    snakefile:
        "module_deep.smk"


use rule work from deep_module as deep_work


rule all:
    input:
        "foo.txt",
    default_target: True


# rules.deep_work has to work even if the rule is renamed in a parent module
# The rulename itself can be already modified.
assert hasattr(
    rules, "deep_work"
), f"bug: rule cannot be accessed as deep_work: {dir(rules)}"



================================================
FILE: tests/test_module_nested/Snakefile
================================================
shell.executable("bash")


module shallow_module:
    snakefile:
        "module_shallow.smk"


use rule deep_work from shallow_module as shallow_work


rule all:
    input:
        "foo.txt",
    default_target: True


assert hasattr(
    rules, "shallow_work"
), f"bug: rule cannot be accessed as shallow_work: {dir(rules)}"



================================================
FILE: tests/test_module_nested/expected-results/foo.txt
================================================
I was here



================================================
FILE: tests/test_module_no_prefixing_modified_paths/Snakefile
================================================
module module1:
    snakefile: "module1/Snakefile"
    config: config


use rule * from module1 as module1_*


# provide a prefix for all paths in module2
module module2:
    snakefile: "module2/Snakefile"
    config: config
    prefix: "module2"


use rule * from module2 as module2_*

# overwrite the input to remove the module2 prefix specified above
use rule c from module2 as module2_c with:
    input:
        "test.txt"


rule joint_all:
    input:
        "module2/test_final.txt",
    default_target: True



================================================
FILE: tests/test_module_no_prefixing_modified_paths/expected-results/module2/test_final.txt
================================================
test_a
test_c



================================================
FILE: tests/test_module_no_prefixing_modified_paths/module1/Snakefile
================================================
rule a:
    output:
        "test.txt"
    shell:
        "echo test_a > {output}"



================================================
FILE: tests/test_module_no_prefixing_modified_paths/module2/Snakefile
================================================
rule b:
    output:
        "test.txt"
    shell:
        "echo test_b > {output}"


rule c:
    input:
        "test.txt"
    output:
        "test_final.txt"
    shell:
        """
        cp {input} {output};
        echo test_c >> {output}
        """


================================================
FILE: tests/test_module_report/custom-stylesheet.css
================================================
#brand {
  margin: auto;
  height: 30px;
  width: 311px;
  background-repeat: no-repeat;
  background-size: 311px 30px;
  background-image: url("data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8' standalone='no'%3F%3E%3C!-- Creator: CorelDRAW --%3E%3Csvg xmlns:dc='http://purl.org/dc/elements/1.1/' xmlns:cc='http://creativecommons.org/ns%23' xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns%23' xmlns:svg='http://www.w3.org/2000/svg' xmlns='http://www.w3.org/2000/svg' xmlns:sodipodi='http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd' xmlns:inkscape='http://www.inkscape.org/namespaces/inkscape' xml:space='preserve' width='1568.9631' height='150.92992' viewBox='0 0 1483.7169 142.72835' version='1.1' id='svg12' sodipodi:docname='uk-uni-white.svg' style='clip-rule:evenodd;fill-rule:evenodd;image-rendering:optimizeQuality;shape-rendering:geometricPrecision;text-rendering:geometricPrecision' inkscape:version='0.92.4 (5da689c313, 2019-01-14)'%3E%3Cmetadata id='metadata18'%3E%3Crdf:RDF%3E%3Ccc:Work rdf:about=''%3E%3Cdc:format%3Eimage/svg+xml%3C/dc:format%3E%3Cdc:type rdf:resource='http://purl.org/dc/dcmitype/StillImage' /%3E%3Cdc:title%3E%3C/dc:title%3E%3C/cc:Work%3E%3C/rdf:RDF%3E%3C/metadata%3E%3Cdefs id='defs16'%3E%3C/defs%3E%3Csodipodi:namedview pagecolor='%23ffffff' bordercolor='%23666666' borderopacity='1' objecttolerance='10' gridtolerance='10' guidetolerance='10' inkscape:pageopacity='0' inkscape:pageshadow='2' inkscape:window-width='1850' inkscape:window-height='1025' id='namedview14' showgrid='false' inkscape:pagecheckerboard='true' fit-margin-top='0' fit-margin-left='0' fit-margin-right='0' fit-margin-bottom='0' inkscape:zoom='1.2128168' inkscape:cx='136.25611' inkscape:cy='10.038152' inkscape:window-x='70' inkscape:window-y='27' inkscape:window-maximized='1' inkscape:current-layer='svg12' showguides='true' inkscape:guide-bbox='true'%3E%3Csodipodi:guide position='989.81019,142.98824' orientation='0,1' id='guide840' inkscape:locked='false' /%3E%3C/sodipodi:namedview%3E%3Cg id='g845' transform='matrix(0.77078623,0,0,0.77078623,1422.0842,-34.102505)' style='stroke-width:1.29737616'%3E%3Cpath d='m -427.2909,73.871082 c 0,2.94841 0.13153,6.70348 3.30574,9.69765 2.01076,1.875846 5.54175,2.732196 8.98239,2.732196 2.99413,0 5.80821,-0.66893 8.08768,-2.509216 3.35373,-2.72769 3.39663,-7.46386 3.39663,-9.4312 v -21.71651 h -7.28323 v 21.6736 c 0,5.98828 -3.21768,5.98828 -4.6041,5.98828 -4.42403,0 -4.42403,-4.06669 -4.42403,-6.03397 v -21.62786 h -7.46108 v 21.22708 z m 32.19369,11.573456 h 6.61433 V 70.070802 c 0,-0.84902 -0.18008,-5.00434 -0.22298,-5.94255 h 0.22298 c 1.69859,3.97807 1.7415,4.11242 2.45672,5.40737 l 8.71595,15.908866 h 5.89964 V 52.643952 h -6.61433 v 12.1995 c 0,1.24981 0,1.51852 0.13435,3.75452 l 0.17726,3.30574 h -0.22298 c -0.13435,-0.22298 -0.58088,-1.42989 -0.71523,-1.69859 -0.31161,-0.66951 -0.76038,-1.56424 -1.11772,-2.23318 l -8.445,-15.328 h -6.883 v 32.800526 z m 39.5255,0.007 V 52.643682 h -7.24032 v 32.807856 z m 23.25703,-0.007 9.78625,-32.800536 h -7.14945 l -4.87281,17.3382 c -0.53515,1.9673 -1.07256,3.93234 -1.56198,5.89963 h -0.17951 c -0.26871,-1.02965 -0.80386,-2.90551 -1.56424,-5.58805 l -5.09352,-17.64983 h -7.19519 l 10.00923,32.800536 h 7.82123 z m 14.67543,0 h 23.06058 v -6.122616 h -15.81965 v -7.72976 h 10.143 v -5.98828 h -10.23448 v -6.9722 h 14.88203 v -5.98773 h -22.03148 v 32.800536 z m 36.70918,-27.707576 c 4.02096,0 4.91571,0 6.03119,0.40306 1.42989,0.53515 2.14512,1.65286 2.14512,3.93233 0,1.78722 -0.17782,3.57445 -2.45673,4.3783 -1.02909,0.35733 -1.65287,0.35733 -5.71956,0.35733 v -9.07104 z m 0,14.30117 h 2.411 l 6.21405,13.406406 h 7.8184 l -7.14945,-14.569846 c 4.6041,-1.38416 6.25695,-4.55612 6.25695,-8.71367 0,-8.80233 -6.74866,-9.51697 -12.06516,-9.51697 h -10.59181 v 32.800536 h 7.10599 V 72.038182 Z m 43.14115,-14.16683 c -2.6797,-3.93234 -6.07688,-5.89908 -10.90399,-5.89908 -6.74639,0 -10.67874,3.88887 -10.67874,9.29625 0,6.47994 4.28967,8.66799 9.24885,10.58954 4.91571,1.92213 6.25695,2.45954 6.25695,4.91627 0,2.99413 -2.81462,3.70879 -4.37886,3.70879 -2.81631,0 -5.00659,-1.69859 -6.21349,-4.46692 l -5.58522,3.7088 c 2.94841,5.273586 6.3456,6.664556 12.1109,6.664556 1.65287,0 4.3783,-0.49168 6.29986,-1.659636 4.64983,-2.81688 4.64983,-7.19519 4.64983,-8.67082 0,-3.08276 -1.42989,-5.2279 -1.83295,-5.7653 -1.29553,-1.78722 -3.62017,-3.12792 -5.58522,-3.84314 l -4.15758,-1.51852 c -2.01076,-0.71523 -4.15531,-1.65287 -4.15531,-3.97807 0,-0.35733 0.0886,-3.17139 3.84315,-3.17139 2.54817,0 4.647,1.6986 5.67382,3.66364 l 5.40797,-3.57501 z m 14.45472,27.580556 V 52.644002 h -7.24093 v 32.807856 z m 5.96175,-32.807856 v 5.98773 h 8.35635 v 26.812856 h 7.50904 V 58.631732 h 8.17856 v -5.98773 z m 33.26626,-3.21767 h 5.67609 v -5.18439 h -5.67609 z m 8.71595,0 h 5.67382 v -5.18439 h -5.67382 z m 2.09939,23.90734 h -7.37243 l 3.57445,-14.8363 h 0.17782 l 3.62017,14.8363 z m -8.08768,-20.68969 -9.25112,32.800536 h 7.01511 l 1.6986,-6.525676 h 10.05496 l 1.65287,6.525676 h 7.01734 l -9.47411,-32.800536 h -8.71367 z m 20.88669,0 v 5.98773 h 8.35636 v 26.812856 h 7.50904 V 58.631712 h 8.17855 v -5.98773 z' id='path5' inkscape:connector-curvature='0' style='fill:%23ffffff;fill-rule:nonzero;stroke-width:1.29737616' /%3E%3Cpath d='m -425.94284,170.75254 c 8.35636,0 15.60855,0 17.05765,-0.13661 4.07629,-0.34548 7.18331,-0.69096 10.84415,-2.55495 5.59256,-2.83325 6.90504,-8.91073 6.97448,-14.64212 l 0.0694,-8.63412 c 0.0694,-11.39621 -0.82756,-14.91817 -3.03986,-18.16408 -2.90042,-4.2129 -7.24992,-6.55899 -19.61256,-6.55899 h -12.2932 v 50.69104 z m 11.32621,-41.71349 c 5.93805,0 8.35636,0 10.49639,3.10703 1.72739,2.48777 1.86625,6.70065 1.86625,12.77758 0,1.72739 -0.13887,4.48894 -0.13887,6.21632 0,9.73773 -4.55838,10.56583 -9.04734,10.56583 h -3.17647 v -32.66675 z m 59.49367,23.82546 c 0,4.55838 0.20604,10.35978 5.10989,14.98761 3.10703,2.90043 8.56523,4.22476 13.88118,4.22476 4.62837,0 8.98012,-1.03643 12.50208,-3.87928 5.17989,-4.21289 5.24706,-11.53282 5.24706,-14.57212 v -33.56374 h -11.25677 v 33.49375 c 0,9.25617 -4.97103,9.25617 -7.11331,9.25617 -6.83787,0 -6.83787,-6.28576 -6.83787,-9.32562 v -33.4243 h -11.53226 v 32.80281 z m 85.11614,17.89991 v -50.70267 h -11.18738 v 50.70267 z m 68.33672,-42.62236 c -4.14572,-6.07971 -9.39279,-9.11674 -16.85159,-9.11674 -10.42867,0 -16.5061,6.00744 -16.5061,14.36324 0,10.01655 6.62898,13.39907 14.2944,16.36949 7.59769,2.96986 9.6705,3.79742 9.6705,7.59769 0,4.62555 -4.35176,5.73199 -6.76786,5.73199 -4.35177,0 -7.73709,-2.62495 -9.60112,-6.90726 l -8.6324,5.73144 c 4.55837,8.14974 9.80772,10.30448 18.71557,10.30448 2.55551,0 6.76842,-0.76095 9.73828,-2.56737 7.18276,-4.35176 7.18276,-11.1196 7.18276,-13.39907 0,-4.76442 -2.20947,-8.0803 -2.831,-8.91013 -2.00286,-2.76156 -5.59421,-4.83442 -8.63412,-5.93804 l -6.42237,-2.34891 c -3.10704,-1.10586 -6.42238,-2.55495 -6.42238,-6.14632 0,-0.55152 0.13661,-4.90385 5.93972,-4.90385 3.93459,0 7.18108,2.62494 8.76901,5.66421 l 8.35863,-5.52482 z m 34.90954,42.61053 h 16.3667 c 6.42293,0 10.77472,-0.27604 14.43551,-4.00403 3.38308,-3.38534 3.9346,-7.18276 3.9346,-10.91355 0,-7.45881 -4.69499,-11.11738 -8.07975,-11.60226 v -0.34548 c 3.93686,-0.897 6.97615,-4.83442 6.97615,-10.98077 0,-4.21289 -1.72739,-8.49524 -5.45538,-10.77471 -2.90043,-1.79456 -7.11332,-2.07004 -11.46566,-2.07004 h -16.71215 v 50.69104 z m 10.84188,-22.23754 c 3.93685,0 7.87369,0 9.46223,0.62152 2.62438,1.10361 3.2459,3.93686 3.2459,6.35294 0,5.17988 -2.76381,7.25275 -7.18331,7.25275 h -5.52483 v -14.22723 z m 0,-20.65127 h 5.04272 c 2.27947,0 6.76787,0 6.76787,6.1486 0,6.42237 -2.90043,6.69842 -11.81059,6.69842 z m 58.943778,25.00078 c 0,4.55838 0.20661,10.35978 5.11045,14.98761 3.10646,2.90043 8.56468,4.22476 13.88118,4.22476 4.62781,0 8.97957,-1.03643 12.50153,-3.87928 5.17988,-4.21289 5.24705,-11.53282 5.24705,-14.57211 v -33.56375 h -11.25677 v 33.49375 c 0,9.25617 -4.97103,9.25617 -7.11331,9.25617 -6.83726,0 -6.83726,-6.28576 -6.83726,-9.32562 v -33.4243 h -11.53282 v 32.80281 z m 84.01234,-24.93134 c 6.2163199,0 7.5959699,0 9.3233399,0.62152 2.20947,0.82983 3.31534003,2.55495 3.31534003,6.07916 0,2.76156 -0.27604,5.52538 -3.79799003,6.76787 -1.59021,0.55208 -2.5572,0.55208 -8.8407299,0.55208 v -14.02062 z m 0,22.10093 h 3.7285499 l 9.60112003,20.71844 H 12.451478 L 1.4035379,148.23956 c 7.11331,-2.14286 9.6682801,-7.0467 9.6682801,-13.46908 0,-13.60512 -10.42866007,-14.70873 -18.6478501,-14.70873 H -23.943292 v 50.69104 h 10.98077 v -20.71844 z m 57.90821,2.48495 c 0,6.28576 0.69094,10.49866 3.45196,14.02062 3.31591,4.14347 8.01091,5.60665 13.12363,5.60665 7.80203,0 10.08095,-2.63624 11.53227,-4.36362 h 0.48266 l 1.45132,3.5338 h 4.97327 v -27.0838 h -16.99047 v 8.4258 h 5.94026 v 3.45251 c 0,1.86625 0,6.63126 -5.80143,6.63126 -5.45761,0 -6.70065,-4.07629 -6.70065,-10.70528 V 136.083 c 0,-2.96986 0.41494,-7.94091 6.21632,-7.94091 3.79746,0 6.07688,2.07004 6.28576,6.55898 h 11.05016 c 0,-9.80545 -6.83726,-15.67572 -16.43893,-15.67572 -4.90383,0 -9.4594,0.48209 -13.32963,4.00403 -5.2465,4.83442 -5.2465,11.25904 -5.2465,15.74799 v 13.74174 z' id='path7' inkscape:connector-curvature='0' style='fill:%23ffffff;fill-rule:nonzero;stroke-width:1.29737616' /%3E%3Cpath d='m -397.02371,227.62821 h 35.63661 v -9.46223 h -24.44701 v -11.94776 h 15.67572 v -9.25334 h -15.81516 v -10.77416 h 22.99847 v -9.25335 h -34.04868 v 50.69104 z m 99.01173,-42.61054 c -4.14572,-6.07971 -9.39223,-9.11673 -16.85159,-9.11673 -10.42866,0 -16.50555,6.00744 -16.50555,14.36324 0,10.01655 6.62843,13.39907 14.29385,16.36948 7.59825,2.96986 9.67111,3.79743 9.67111,7.59769 0,4.62556 -4.35233,5.73144 -6.76842,5.73144 -4.35177,0 -7.73708,-2.62438 -9.60112,-6.9067 l -8.63185,5.73143 c 4.55838,8.14975 9.80773,10.30449 18.71558,10.30449 2.55495,0 6.76787,-0.76095 9.73772,-2.56737 7.18331,-4.35177 7.18331,-11.1196 7.18331,-13.39907 0,-4.76442 -2.21003,-8.08031 -2.83099,-8.91013 -2.00342,-2.76156 -5.59482,-4.83442 -8.63468,-5.93804 l -6.42237,-2.34891 c -3.10704,-1.10587 -6.42238,-2.55495 -6.42238,-6.14632 0,-0.55209 0.13661,-4.90386 5.94027,-4.90386 3.9346,0 7.18048,2.62439 8.76846,5.66422 l 8.35863,-5.52483 z m 63.51163,0 c -4.14516,-6.07971 -9.39223,-9.11673 -16.85104,-9.11673 -10.42922,0 -16.5061,6.00744 -16.5061,14.36324 0,10.01655 6.62899,13.39907 14.29385,16.36948 7.59824,2.96986 9.67111,3.79743 9.67111,7.59769 0,4.62556 -4.35233,5.73144 -6.76787,5.73144 -4.35233,0 -7.73708,-2.62438 -9.60167,-6.9067 l -8.63185,5.73143 c 4.55838,8.14975 9.80772,10.30449 18.71557,10.30449 2.55495,0 6.76787,-0.76095 9.73829,-2.56737 7.18275,-4.35177 7.18275,-11.1196 7.18275,-13.39907 0,-4.76442 -2.20947,-8.08031 -2.83099,-8.91013 -2.00343,-2.76156 -5.59483,-4.83442 -8.63468,-5.93804 l -6.42238,-2.34891 c -3.10703,-1.10587 -6.42237,-2.55495 -6.42237,-6.14632 0,-0.55209 0.13661,-4.90386 5.94027,-4.90386 3.9346,0 7.18048,2.62439 8.76902,5.66422 l 8.35807,-5.52483 z m 31.87984,42.61054 h 35.63661 v -9.46223 h -24.44701 v -11.94776 h 15.67572 v -9.25334 h -15.81516 v -10.77416 h 22.99791 v -9.25335 h -34.04812 v 50.69104 z m 67.93341,0 h 10.22033 v -23.7583 c 0,-1.31021 -0.27604,-7.73486 -0.34548,-9.1839 h 0.34548 c 2.62438,6.14632 2.69382,6.35293 3.79743,8.3558 l 13.46907,24.58645 h 9.116738 v -50.69104 h -10.222608 v 18.85219 c 0,1.93343 0,2.3489 0.2083,5.80365 l 0.27604,5.10989 h -0.34547 c -0.20887,-0.34548 -0.897,-2.20947 -1.10587,-2.62494 -0.48209,-1.03644 -1.17304,-2.41778 -1.72738,-3.45421 l -13.05136,-23.68663 h -10.63527 v 50.69104 z' id='path9' inkscape:connector-curvature='0' style='fill:%23ffffff;fill-rule:nonzero;stroke-width:1.29737616' /%3E%3C/g%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path44' d='m 57.849497,14.92373 v 118.42303 l 8.333397,-0.0127 V 14.923159 h -8.333397 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:none;stroke:%23000000;stroke-width:0.53388023;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:3.86400008;stroke-dasharray:none;stroke-opacity:1' id='path48' d='m 57.849497,14.92373 v 118.42303 l 8.333397,-0.0127 V 14.923159 h -8.333397 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path52' d='M 13.340586,37.253319 0.2644434,50.29467 V 68.102403 H 26.614851 V 50.29467 Z M 0.2644434,80.614155 V 98.421888 L 13.340586,111.39704 26.614851,98.355688 V 80.614155 Z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23fffcfd;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path68' d='M 109.56251,37.253319 96.420323,50.22847 V 68.102403 H 122.77073 V 50.36087 Z M 96.420323,80.614155 V 98.421888 L 109.56251,111.39704 122.77073,98.355688 V 80.614155 Z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23fefefe;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path84' d='m 35.002074,106.82926 -13.010102,13.17375 13.010102,13.23995 h 17.699027 v -26.4137 z m 36.190541,0 v 26.4137 H 89.023718 L 101.96778,120.06921 88.957676,106.82926 Z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23fffffa;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path100' d='m 35.002074,15.076403 -13.010102,13.10755 13.010102,13.30615 h 17.699027 v -26.4137 z m 36.190541,0 v 26.3475 H 89.023718 L 101.96778,28.316353 88.957676,15.076403 Z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:none;stroke:%23000000;stroke-width:0.53388023;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:3.86400008;stroke-dasharray:none;stroke-opacity:1' id='path112' d='M 52.720437,15.060303 V 41.474342 H 35.011957 L 21.997076,28.215729 35.011957,15.060303 Z M 71.221652,41.436105 V 15.064561 H 88.930119 L 101.945,28.323195 88.99673,41.442185 71.221652,41.43583 Z m 0,91.817145 v -26.4201 h 17.708467 l 13.014881,13.2647 -12.94827,13.14932 -17.775078,0.006 z M 96.442915,80.605851 H 122.78153 V 98.344715 L 109.53047,111.39085 96.442915,98.423647 Z m 0,-12.51798 H 122.78153 V 50.330789 L 109.53047,37.28465 96.442915,50.257937 Z M 52.720437,106.83364 V 133.2416 H 35.011957 L 21.997076,119.98299 35.011957,106.83364 Z M 0.26662011,80.605851 26.599193,80.593141 V 98.344143 L 13.372344,111.3903 0.26662011,98.423075 V 80.605279 Z m 0,-12.51798 26.33257289,-0.0064 V 50.294077 L 13.372344,37.284375 0.26662011,50.294077 v 17.793518 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23000000;fill-opacity:1;fill-rule:evenodd;stroke:none;stroke-width:0.94566709' id='path124' d='m 75.903661,9.8260743 2.095458,1.2627307 c 0.635918,0.51602 1.25364,1.43269 1.25364,1.43269 L 78.89544,11.076646 80.37922,10.955262 C 79.979507,10.840021 79.761498,10.754862 79.386004,10.578866 78.750107,10.281402 78.374613,10.099283 77.865896,9.607541 77.405615,9.1582936 77.28448,9.1643734 76.927161,8.25982 l -0.224075,1.1109695 -0.799425,0.455306 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23ffffff;fill-opacity:1;stroke:%23000000;stroke-width:0.37669724;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:3.86400008;stroke-dasharray:none;stroke-opacity:1' id='path132' d='M 70.796368,0.44625824 C 69.212036,0.47866964 67.440211,1.1445014 65.975366,1.042056 57.981115,0.46532377 50.760844,4.2672443 47.417804,11.965015 c -3.851776,8.845155 1.564706,17.819807 8.057024,23.368511 0.635896,0.540296 1.350218,1.048499 2.113316,1.588794 l 0.198017,-9.069365 c -2.440668,-2.537601 -4.088505,-5.485852 -4.094549,-9.267965 -0.01268,-2.489026 0.802553,-5.521716 3.103932,-7.94397 2.19842,-2.306908 6.300408,-3.5814385 7.396606,-3.508587 2.561783,0.1578202 1.997358,1.3572326 2.905809,2.383191 0.399713,0.455306 1.855767,0.649287 3.169974,0.728197 2.870665,0.163964 7.220968,-0.125832 7.396606,-0.198493 C 77.834027,9.9785986 75.315905,3.2672573 73.371866,1.3731607 72.633748,0.65833046 71.746853,0.42691732 70.796262,0.44636416 Z M 65.909325,34.208131 V 42.4169 c 3.851772,2.798629 5.955535,5.97181 5.28329,9.929962 C 69.406017,62.897916 46.995319,65.771367 46.55927,79.885958 46.371523,86.151018 49.078518,90.97216 57.654185,97.82609 L 57.852202,87.631329 C 54.484929,85.008739 52.437084,82.061632 54.484105,78.230964 57.796876,72.026618 79.297118,63.240778 78.655156,50.625669 78.243334,42.502949 72.365167,38.585173 65.909219,34.208131 Z m 0.198018,60.043173 0.198017,9.996166 c 2.101503,1.54805 3.28589,2.42998 3.764344,5.36218 0.932649,5.75512 -5.558655,8.54262 -9.113675,10.98915 -3.936579,2.71364 -13.737191,7.79037 -9.509921,16.48374 2.416429,4.972 6.598617,5.61658 6.538071,5.42838 -0.399713,-1.26879 -3.687927,-3.35646 -3.50018,-8.40737 0.254359,-6.82357 15.76185,-10.61816 19.680255,-18.00633 1.071958,-2.02158 6.55671,-11.6834 -8.057017,-21.845916 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path272' d='m 1018.8932,97.637109 v -22.26769 c 0,-3.569617 -0.9811,-5.74296 -4.2636,-5.74296 -4.5422,0 -8.1093,5.111595 -8.1093,11.206675 v 16.803975 h -8.87242 V 69.69323 c -1.6715,-0.279246 -4.05163,-0.558514 -6.06834,-0.698137 v -5.597279 c 4.39684,-0.564572 9.77476,-0.777068 14.46226,-0.637424 0,1.687681 -0.1392,4.413458 -0.4966,6.301475 h 0.073 c 2.1016,-4.06136 6.1471,-6.93284 11.8037,-6.93284 7.8246,0 10.338,4.978051 10.338,11.067051 v 24.441033 h -8.8664 z M 971.25174,68.646024 c -3.70641,0 -6.28639,2.731858 -6.77694,7.072464 h 12.7787 c 0.14539,-4.48023 -2.22871,-7.072464 -6.00176,-7.072464 m 14.59556,13.094714 h -21.51178 c -0.0727,6.441098 3.14924,9.591845 9.56886,9.591845 3.28248,0 6.77694,-0.698137 10.05338,-2.100491 l 0.98111,6.793217 c -3.76699,1.541978 -8.30918,2.312966 -12.57278,2.312966 -10.96181,0 -17.10889,-5.536565 -17.10889,-17.787417 0,-10.642124 5.86848,-18.485576 16.27312,-18.485576 10.12606,0 14.60162,6.926782 14.60162,15.541223 0,1.195959 -0.0786,2.592234 -0.28464,4.134233 M 933.11129,98.33586 c -2.72531,0 -5.51723,-0.279267 -7.752,-0.910633 l 0.62379,-7.491354 c 2.10152,0.910612 4.82079,1.396274 7.40679,1.396274 3.35517,0 5.58993,-1.469126 5.58993,-3.70924 0,-6.234725 -14.11107,-2.592234 -14.11107,-14.424216 0,-6.161873 4.67543,-11.127765 14.66218,-11.127765 2.03492,0 4.40291,0.273166 6.36512,0.692057 l -0.41786,7.005713 c -1.89563,-0.558514 -4.12431,-0.910633 -6.29247,-0.910633 -3.49445,0 -5.09934,1.402354 -5.09934,3.569639 0,5.882605 14.31697,2.944332 14.31697,14.351364 0,6.932841 -6.07443,11.558794 -15.29204,11.558794 m -30.87717,0 c -2.73137,0 -5.51723,-0.279267 -7.75804,-0.910633 l 0.62983,-7.491354 c 2.10152,0.910612 4.82683,1.396274 7.40679,1.396274 3.35516,0 5.58387,-1.469126 5.58387,-3.70924 0,-6.234725 -14.10501,-2.592234 -14.10501,-14.424216 0,-6.161873 4.67543,-11.127765 14.66825,-11.127765 2.02884,0 4.39683,0.273166 6.35905,0.692057 l -0.43,7.005713 c -1.87743,-0.558514 -4.11217,-0.910633 -6.28033,-0.910633 -3.48839,0 -5.09935,1.402354 -5.09935,3.569639 0,5.882605 14.32302,2.944332 14.32302,14.351364 0,6.932841 -6.07441,11.558794 -15.29808,11.558794 M 860.24976,97.637109 V 50.369894 h 25.63611 v 7.630978 h -16.55172 v 11.625587 h 15.71594 v 7.424582 h -15.71594 v 12.809366 h 16.55172 v 7.776702 z m -37.37371,0 V 75.648687 c 0,-3.715342 -0.83576,-6.022228 -4.11824,-6.022228 -3.9063,0 -7.95793,4.553081 -7.95793,11.346319 v 16.664331 h -8.81185 V 75.575836 c 0,-3.429995 -0.69646,-5.949377 -4.11217,-5.949377 -4.13037,0 -7.96397,4.832348 -7.96397,11.346319 v 16.664331 h -8.87846 V 69.69323 c -1.67153,-0.279246 -4.05165,-0.558514 -6.07443,-0.698137 v -5.597279 c 4.4029,-0.564572 9.84746,-0.777068 14.45626,-0.637424 0,1.687681 -0.13314,4.273835 -0.41182,6.301475 l 0.0727,0.07287 c 2.08941,-4.267777 6.28033,-7.005692 11.45237,-7.005692 6.14102,0 8.72705,3.575697 9.56886,6.93284 1.60492,-3.357143 5.45063,-6.93284 11.31309,-6.93284 6.71033,0 10.41068,3.71532 10.41068,11.558794 v 23.94929 h -8.94507 z m -65.80236,0.06737 c 0,-1.608771 0.0788,-4.267776 0.41788,-6.301496 h -0.13926 c -2.02279,4.061381 -6.08048,6.932861 -11.73096,6.932861 -7.89733,0 -10.41068,-4.971993 -10.41068,-11.067073 V 69.693824 c -1.67153,-0.279268 -3.97895,-0.558515 -6.01386,-0.698138 v -5.597279 c 4.62091,-0.564593 10.13817,-0.777068 14.95287,-0.637445 v 22.340541 c 0,3.569618 0.98113,5.742961 4.26362,5.742961 4.54219,0 8.10325,-5.111596 8.10325,-11.200617 V 62.833814 h 8.8724 v 27.937819 c 1.67153,0.279247 4.05165,0.558493 6.07443,0.704196 v 5.597279 c -4.4029,0.564594 -9.78085,0.770989 -14.38965,0.631366 m -41.56288,-0.06737 -10.75589,-17.228925 v 17.228925 h -8.9451 V 53.380998 c -1.67757,-0.279247 -4.04556,-0.558514 -6.07441,-0.704217 v -5.597258 c 4.47556,-0.558514 9.84746,-0.77101 15.01951,-0.631366 V 77.330288 L 714.881,62.833242 h 10.8952 l -12.22152,15.893341 13.12995,18.910526 z M 678.55684,57.652417 c -3.00391,0 -5.5233,-2.385817 -5.5233,-5.396942 0,-3.005045 2.51939,-5.451576 5.5233,-5.451576 3.08263,0 5.58991,2.373679 5.58991,5.451576 0,2.944332 -2.50728,5.396942 -5.58991,5.396942 m -4.40291,39.982256 V 69.696874 c -1.66546,-0.279268 -4.05163,-0.564594 -6.07441,-0.704217 v -5.597279 c 4.40895,-0.564573 9.78085,-0.770989 14.94683,-0.631365 v 34.87066 z m -22.63038,0.0021 v -22.26765 c 0,-3.569639 -0.97506,-5.742982 -4.25755,-5.742982 -4.54823,0 -8.10325,5.111616 -8.10325,11.206696 v 16.803954 h -8.87847 V 69.692913 c -1.67152,-0.279247 -4.05162,-0.558514 -6.07441,-0.698138 v -5.597279 c 4.40289,-0.564572 9.7869,-0.777047 14.46231,-0.637424 0,1.687681 -0.14539,4.413458 -0.49054,6.301475 h 0.0727 c 2.09546,-4.06136 6.14102,-6.93284 11.80363,-6.93284 7.81861,0 10.33196,4.978051 10.33196,11.067073 v 24.441011 h -8.86636 z M 612.12703,57.652417 c -3.00391,0 -5.51723,-2.385817 -5.51723,-5.396942 0,-3.005045 2.51332,-5.451576 5.51723,-5.451576 3.07052,0 5.58991,2.373679 5.58991,5.451576 0,2.944332 -2.51939,5.396942 -5.58991,5.396942 m -4.4029,39.982256 V 69.696874 c -1.67757,-0.279268 -4.05163,-0.564594 -6.07441,-0.704217 v -5.597279 c 4.39684,-0.564573 9.77478,-0.770989 14.94683,-0.631365 v 34.87066 z m -22.62795,0.0021 V 53.38068 c -1.68364,-0.279247 -4.05163,-0.558493 -6.08047,-0.704196 v -5.597279 c 4.46951,-0.558514 9.84746,-0.770989 15.0195,-0.631365 v 51.188951 z m -18.87066,0 -10.74985,-17.228925 v 17.228925 h -8.94508 V 53.38068 c -1.67152,-0.279247 -4.05162,-0.558493 -6.07443,-0.704196 v -5.597279 c 4.46952,-0.558514 9.84749,-0.770989 15.01951,-0.631365 v 30.882151 l 10.12606,-14.497067 h 10.89518 l -12.2215,15.893342 13.13599,18.910525 z m -44.63275,0.698752 c -2.72531,0 -5.51723,-0.279247 -7.752,-0.910612 l 0.62985,-7.491376 c 2.09546,0.910633 4.81473,1.396296 7.40073,1.396296 3.35516,0 5.58993,-1.469147 5.58993,-3.709262 0,-6.234703 -14.10501,-2.592234 -14.10501,-14.424216 0,-6.161852 4.67541,-11.127765 14.66823,-11.127765 2.0228,0 4.39684,0.273188 6.35301,0.692079 l -0.41787,7.005691 c -1.88349,-0.558514 -4.1243,-0.910633 -6.2864,-0.910633 -3.49445,0 -5.09936,1.402355 -5.09936,3.569639 0,5.882605 14.31697,2.944332 14.31697,14.351364 0,6.932841 -6.07441,11.558795 -15.29808,11.558795 m -22.55893,0 c -7.69143,0 -10.05942,-2.798629 -10.05942,-10.92137 V 69.766378 h -6.0078 v -6.932861 h 6.0078 V 52.124643 l 8.87239,-2.385818 v 13.094692 h 8.44847 v 6.932861 h -8.44847 v 15.261977 c 0,4.48633 1.04774,5.742982 4.11826,5.742982 1.47167,0 2.79799,-0.139602 3.985,-0.491743 l 0.62381,7.078544 c -2.23477,0.558514 -5.09331,0.977405 -7.54004,0.977405 M 467.6661,57.092378 c -2.65263,0 -4.82076,-2.173343 -4.82076,-4.832349 0,-2.731857 2.16813,-4.83237 4.82076,-4.83237 2.65264,0 4.82077,2.100513 4.82077,4.83237 0,2.659006 -2.16813,4.832349 -4.82077,4.832349 m -2.51334,24.926695 c -9.07832,0 -11.52503,2.45259 -11.52503,5.463714 0,2.306908 1.53221,3.988509 4.11824,3.988509 4.39684,0 7.40679,-4.200983 7.40679,-8.401988 z M 453.55505,57.092378 c -2.65265,0 -4.82079,-2.173343 -4.82079,-4.832349 0,-2.731857 2.16814,-4.83237 4.82079,-4.83237 2.65263,0 4.82077,2.100513 4.82077,4.83237 0,2.659006 -2.16814,4.832349 -4.82077,4.832349 m 12.29417,40.613621 c 0,-2.03372 0.0727,-4.134212 0.35128,-5.955457 l -0.0727,-0.06673 c -1.67153,3.921737 -5.93512,6.653594 -11.10716,6.653594 -6.2864,0 -9.91408,-3.854944 -9.91408,-9.458303 0,-8.329115 8.24255,-12.669742 20.04618,-12.669742 V 74.52776 c 0,-3.64249 -1.75025,-5.536566 -6.77694,-5.536566 -3.1432,0 -7.33412,1.056315 -10.12606,2.525441 l -1.05378,-7.351731 c 3.56713,-1.189879 8.17593,-2.100491 12.43954,-2.100491 11.31914,0 14.45627,4.480229 14.45627,12.117308 V 90.7732 c 1.67757,0.279268 4.05163,0.558514 6.08047,0.698137 v 5.603359 c -4.4029,0.564572 -9.78085,0.770989 -14.32304,0.631365 m -34.8549,0.629523 c -7.68539,0 -10.05942,-2.798629 -10.05942,-10.92137 V 69.766421 h -6.0078 v -6.932862 h 6.0078 V 52.124685 l 8.8724,-2.385818 v 13.094692 h 8.45453 v 6.932862 h -8.45453 v 15.261976 c 0,4.486331 1.04774,5.742982 4.1243,5.742982 1.45956,0 2.78588,-0.139602 3.97895,-0.491742 l 0.62985,7.078543 c -2.23474,0.558514 -5.10541,0.977405 -7.54608,0.977405 M 403.46925,57.652142 c -3.00391,0 -5.5233,-2.385818 -5.5233,-5.396943 0,-3.005045 2.51939,-5.451575 5.5233,-5.451575 3.07657,0 5.58991,2.373679 5.58991,5.451575 0,2.944332 -2.51334,5.396943 -5.58991,5.396943 m -4.39684,39.982276 V 69.696599 c -1.67757,-0.279247 -4.05162,-0.564573 -6.08047,-0.704217 v -5.597258 c 4.4029,-0.564594 9.78085,-0.770989 14.94683,-0.631366 v 34.87066 z m -26.90003,0.701167 c -2.72532,0 -5.51724,-0.279246 -7.75201,-0.910612 l 0.62986,-7.491376 c 2.09545,0.910634 4.82077,1.396296 7.40679,1.396296 3.3491,0 5.58386,-1.469147 5.58386,-3.709262 0,-6.234703 -14.11107,-2.592234 -14.11107,-14.424216 0,-6.161851 4.68148,-11.127765 14.67429,-11.127765 2.02281,0 4.39684,0.273188 6.35301,0.692079 l -0.41786,7.005692 c -1.8835,-0.558514 -4.12431,-0.910633 -6.2864,-0.910633 -3.49445,0 -5.09937,1.402354 -5.09937,3.569638 0,5.882606 14.31698,2.944332 14.31698,14.351365 0,6.93284 -6.07441,11.558794 -15.29808,11.558794 M 357.92685,70.958122 c -6.63765,-1.402354 -9.85352,2.798629 -9.85352,12.463348 v 14.217799 h -8.8724 V 69.695391 c -1.67153,-0.279267 -4.05163,-0.558514 -6.07441,-0.698137 v -5.603359 c 4.40288,-0.558514 9.78083,-0.770989 14.46231,-0.631365 0,1.821245 -0.21197,4.625953 -0.62986,7.072484 h 0.13927 c 1.67153,-4.407399 5.02667,-8.262343 11.24646,-7.703829 l -0.41789,8.826937 z m -45.12874,-2.312373 c -3.70037,0 -6.2864,2.731857 -6.77694,7.072485 h 12.78474 c 0.13927,-4.480251 -2.23475,-7.072485 -6.0078,-7.072485 m 14.59555,13.094713 h -21.51178 c -0.0727,6.441119 3.14924,9.591846 9.57493,9.591846 3.27642,0 6.77087,-0.698137 10.05943,-2.100491 l 0.97506,6.793217 c -3.77305,1.541998 -8.31524,2.312987 -12.57884,2.312987 -10.96181,0 -17.1089,-5.536566 -17.1089,-17.787438 0,-10.642103 5.86245,-18.485576 16.27313,-18.485576 10.12606,0 14.60162,6.926781 14.60162,15.541244 0,1.195938 -0.0727,2.592234 -0.28465,4.134211 m -47.21693,15.896371 h -9.08438 L 258.241,62.832966 h 9.78085 l 5.51723,15.686926 c 0.84182,2.452611 1.6776,5.25124 2.30743,7.70385 h 0.13926 c 0.55718,-2.379759 1.25364,-5.044823 2.09546,-7.424582 l 5.58387,-15.966194 h 9.50225 L 280.17669,97.636833 Z M 246.99575,57.652417 c -3.00391,0 -5.5233,-2.385817 -5.5233,-5.396942 0,-3.005045 2.51939,-5.451576 5.5233,-5.451576 3.07052,0 5.58387,2.373679 5.58387,5.451576 0,2.944332 -2.51335,5.396942 -5.58387,5.396942 m -4.4029,39.982256 V 69.696874 c -1.67151,-0.279268 -4.05163,-0.564594 -6.07441,-0.704217 v -5.597279 c 4.4029,-0.564573 9.78085,-0.770989 14.94683,-0.631365 v 34.87066 z m -22.6322,0.0021 v -22.26765 c 0,-3.569639 -0.97506,-5.742982 -4.26359,-5.742982 -4.53613,0 -8.09722,5.111616 -8.09722,11.206696 v 16.803954 h -8.87241 V 69.692913 c -1.67758,-0.279247 -4.05768,-0.558514 -6.08048,-0.698138 v -5.597279 c 4.4029,-0.564572 9.78085,-0.777047 14.46233,-0.637424 0,1.687681 -0.13927,4.413458 -0.49056,6.301475 h 0.0727 c 2.08942,-4.06136 6.14105,-6.93284 11.80366,-6.93284 7.82465,0 10.338,4.978051 10.338,11.067073 v 24.441011 h -8.8724 z m -54.55772,0.769273 c -14.88022,0 -18.22932,-8.12272 -18.22932,-17.156073 V 50.36786 h 9.08437 v 30.323617 c 0,5.949377 1.95012,10.150381 9.77478,10.150381 6.98892,0 9.92014,-2.944332 9.92014,-10.994221 V 50.36786 h 9.0117 v 28.921262 c 0,12.882239 -7.12821,19.116942 -19.56167,19.116942' /%3E%3C/svg%3E");
}



================================================
FILE: tests/test_module_report/Snakefile
================================================

module test:
    snakefile:
        "https://raw.githubusercontent.com/snakemake/snakemake/v6.3.0/tests/test_report/Snakefile"
    config:
        config
    replace_prefix: 
        {"results/": "results/testmodule/"}


use rule * from test



================================================
FILE: tests/test_module_report/testdir/1.txt
================================================
1



================================================
FILE: tests/test_module_report/testdir/2.txt
================================================
2



================================================
FILE: tests/test_module_report/testdir/3.txt
================================================
3



================================================
FILE: tests/test_module_report/testdir/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC40Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.4.out", "incomplete": false, "starttime": 1585317519.3560429, "endtime": 1585317520.3800488, "job_hash": 3471465265311225051, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC41Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.5.out", "incomplete": false, "starttime": 1585317518.3440373, "endtime": 1585317521.3600543, "job_hash": -4453652366712017362, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC42Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.6.out", "incomplete": false, "starttime": 1585317518.3320372, "endtime": 1585317519.348043, "job_hash": -5792833771891348507, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC43Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.7.out", "incomplete": false, "starttime": 1585317518.340037, "endtime": 1585317520.3520486, "job_hash": 169637128606654253, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC44Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.8.out", "incomplete": false, "starttime": 1585317519.4240434, "endtime": 1585317522.4480605, "job_hash": 4196649577361415263, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC45Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.9.out", "incomplete": false, "starttime": 1585317518.3760374, "endtime": 1585317519.392043, "job_hash": 4841685047946511577, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC4wLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.0.out", "incomplete": false, "starttime": 1585317519.4040432, "endtime": 1585317522.4320605, "job_hash": 1584247219081097085, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC4xLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.1.out", "incomplete": false, "starttime": 1585317518.3680375, "endtime": 1585317521.3880546, "job_hash": 4581141678275462678, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC4yLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.2.out", "incomplete": false, "starttime": 1585317518.3840375, "endtime": 1585317520.400049, "job_hash": 7969642367240267386, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC4zLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.3.out", "incomplete": false, "starttime": 1585317518.3520372, "endtime": 1585317521.3680544, "job_hash": -6992618918864179503, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdC5jc3Y=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YVwAAAGN1cmwgaHR0cDovL3NhbXBsZWNzdnMuczMuYW1hem9uYXdzLmNvbS9TYWNyYW1lbnRvcmVhbGVzdGF0ZXRyYW5zYWN0aW9ucy5jc3YgPiB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "d", "input": [], "log": [], "params": [], "shellcmd": "curl http://samplecsvs.s3.amazonaws.com/Sacramentorealestatetransactions.csv > test.csv", "incomplete": false, "starttime": 1585317518.3600373, "endtime": 1585317519.4120433, "job_hash": 5729990202001040214, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_module_report/.snakemake/metadata/dGVzdGRpcg==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YdQAAAAogICAgICAgIG1rZGlyIHtvdXRwdXR9CiAgICAgICAgZm9yIHggaW4gMSAyIDMKICAgICAgICBkbyAKICAgICAgICAgICAgZWNobyAkeCA+IHRlc3RkaXIvJHgudHh0CiAgICAgICAgZG9uZQogICAgICAgIHEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "e", "input": [], "log": [], "params": [], "shellcmd": "\n        mkdir testdir\n        for x in 1 2 3\n        do \n            echo $x > testdir/$x.txt\n        done\n        ", "incomplete": false, "starttime": 1585317518.3560374, "endtime": 1585317518.3680375, "job_hash": 2045751853232800755, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_module_report/.snakemake/metadata/ZmlnMi5wbmc=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YMwAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgY3AgZGF0YS9maWcyLnBuZyB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "b", "input": ["test.0.out", "test.1.out", "test.2.out", "test.3.out", "test.4.out", "test.5.out", "test.6.out", "test.7.out", "test.8.out", "test.9.out"], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png fig2.png", "incomplete": false, "starttime": 1585317522.4760606, "endtime": 1585317523.5040665, "job_hash": 7638699196749507097, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_module_report/.snakemake/metadata/ZmlnMS5zdmc=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YMwAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgY3AgZGF0YS9maWcxLnN2ZyB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "a", "input": ["test.0.out", "test.1.out", "test.2.out", "test.3.out", "test.4.out", "test.5.out", "test.6.out", "test.7.out", "test.8.out", "test.9.out"], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; cp data/fig1.svg fig1.svg", "incomplete": false, "starttime": 1585317522.4600606, "endtime": 1585317524.4880722, "job_hash": 2107624497111535029, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_module_wildcard_constraints/Snakefile
================================================
rule all:
    input:
        "result/a.txt",
        "result/b.txt"


module testmodule:
    snakefile:
        "testmodule/Snakefile"


use rule * from testmodule

rule b:
    output:
        "result/{sample}.txt"
    shell:
        "echo rule-b-{wildcards.sample} > {output}"

wildcard_constraints:
    sample="b"


================================================
FILE: tests/test_module_wildcard_constraints/expected-results/result/a.txt
================================================
rule-a-a



================================================
FILE: tests/test_module_wildcard_constraints/expected-results/result/b.txt
================================================
rule-b-b



================================================
FILE: tests/test_module_wildcard_constraints/testmodule/Snakefile
================================================
rule a:
    output:
        "result/{sample}.txt"
    shell:
        "echo rule-a-{wildcards.sample} > {output}"

wildcard_constraints:
    sample = "a"



================================================
FILE: tests/test_module_with_script/config.yaml
================================================
test: true



================================================
FILE: tests/test_module_with_script/Snakefile
================================================
from snakemake.utils import min_version

min_version("6.1.1")

configfile: "config.yaml"

module with_script:
    snakefile:
        github("snakemake/snakemake", tag="v6.1.1", path="tests/test_script_py/Snakefile")
    config:
        config

use rule * from with_script as with_script_*

rule all:
    input:
        rules.with_script_all.input



================================================
FILE: tests/test_module_workflow_snakefile_usage/Snakefile
================================================
configfile: "config/config.yaml"


module test:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix: 
        {"results/": "results/testmodule/"}


use rule * from test



================================================
FILE: tests/test_module_workflow_snakefile_usage/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_module_workflow_snakefile_usage/module-test/Snakefile
================================================
configfile: "config.yaml" # does not exist, but this statement should be ignored on module import


def params_func(filename):
    return str((Path(workflow.snakefile).parent / filename).as_posix())


rule a:
    output:
        "results/test.out"
    params:
        some_resource=params_func("some_file.txt"),
        other_resource=lambda w: params_func("some_file.txt")
    shell:
        "cat {params.some_resource} {params.other_resource} > {output}"



================================================
FILE: tests/test_module_workflow_snakefile_usage/module-test/some_file.txt
================================================
[Empty file]


================================================
FILE: tests/test_modules_all/Snakefile
================================================
shell.executable("bash")


configfile: "config/config.yaml"


module test:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix:
        {"results/": "results/testmodule/"}


use rule * from test as test_*


rule all:
    input:
        multiext(expand("results/testmodule/c/{name}.", name="test")[0], "tsv", "txt"),


assert test.some_func() == 15



================================================
FILE: tests/test_modules_all/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_all/module-test/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    output:
        temp("results/a/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_all_exclude/Snakefile
================================================
shell.executable("bash")

configfile: "config/config.yaml"


module test:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix:
        {"results/": "results/testmodule/"}


use rule * from test as test_*

assert test.some_func() == 15



================================================
FILE: tests/test_modules_all_exclude/Snakefile_exclude
================================================
shell.executable("bash")

configfile: "config/config.yaml"


module test:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix:
        {"results/": "results/testmodule/"}


use rule * from test exclude b, d

assert test.some_func() == 15



================================================
FILE: tests/test_modules_all_exclude/config/config.yaml
================================================
test: 1
testb: 2
testc: 3
testd: 4



================================================
FILE: tests/test_modules_all_exclude/module-test/Snakefile
================================================
configfile: "config.yaml" # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule all:
    input:
        "results/test.out", "results/test2.out"


rule a:
    output:
        "results/test.out"
    shell:
        "echo {config[test]} > {output}"

rule b:
    output:
        "results/test2.out"
    shell:
        "echo {config[testb]} > {output}"

rule c:
    output:
        "results/test2.out"
    shell:
        "echo {config[testc]} > {output}"

rule d:
    output:
        "results/test2.out"
    shell:
        "echo {config[testc]} > {output}"



================================================
FILE: tests/test_modules_dynamic/Snakefile
================================================
shell.executable("bash")


configfile: "config/config.yaml"

for module_name, use_as in zip(["module1", "module2"], ['use_as_1', 'use_as_2']):
    module:
        name: module_name
        snakefile:
            f"{module_name}/Snakefile"
        config:
            config
        replace_prefix:
            {"results/": "results/testmodule/"}

    use rule * from module_name as use_as*

rule all:
    input:
        multiext(expand("results/testmodule/c1/{name}.", name="test")[0], "tsv", "txt"),
        multiext(expand("results/testmodule/c2/{name}.", name="test")[0], "tsv", "txt"),


assert module1.some_func() == 15
assert module2.some_func() == 25



================================================
FILE: tests/test_modules_dynamic/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_dynamic/module1/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    output:
        temp("results/a1/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b1/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c1/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c1/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_dynamic/module2/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 25


rule a:
    output:
        temp("results/a2/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b2/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c2/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c2/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_dynamic_import_rules/Snakefile
================================================
shell.executable("bash")

configfile: "config/config.yaml"

for module_name in ["module1", "module2", "module3"]:
    module:
        name: module_name
        snakefile:
            f"{module_name}/Snakefile"
        config:
            config
        replace_prefix:
            {"results/": "results/testmodule/"}


use rule a from module1 as rule_a with:
    output:
        "results/testmodule/a2/{name}.out"

use rule b,c from module2

use rule * from module3 exclude a,b,c


rule all:
    input:
        expand("results/testmodule/b2/{name}.txt", name="test"),
        expand("results/testmodule/b2/{name}.tsv", name="test"),



================================================
FILE: tests/test_modules_dynamic_import_rules/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_dynamic_import_rules/expected-results/results/testmodule/b2/test.txt
================================================
5



================================================
FILE: tests/test_modules_dynamic_import_rules/module1/Snakefile
================================================
rule a:
    output:
        temp("results/a1/{name}.out"),
    shell:
        "echo 5 > {output}"


rule b:
    input:
        expand("results/a1/{name}.out", name="test"),
    output:
        "results/b1/{name}.out",
    shell:
        "cat {input} > {output}"


rule c:
    input:
        expand("results/b1/{name}.out", name="test"),
    output:
        "results/c1/{name}.txt",
    shell:
        "cat {input} > {output}"


================================================
FILE: tests/test_modules_dynamic_import_rules/module2/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


rule a:
    output:
        temp("results/a2/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand("results/a2/{name}.out", name="test"),
    output:
        "results/b2/{name}.out",
    shell:
        "cat {input} > {output}"


rule c:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/b2/{name}.txt",
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_modules_dynamic_import_rules/module3/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


rule a:
    output:
        temp("results/a2/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand("results/a2/{name}.out", name="test"),
    output:
        "results/b2/{name}.out",
    shell:
        "cat {input} > {output}"


rule c:
    input:
        expand("results/b2/{name}.out", name="test"),
    output:
        "results/b2/{name}.txt",
    shell:
        "cat {input} > {output}"


rule d:
    input:
        expand("results/b2/{name}.txt", name="test"),
    output:
        "results/b2/{name}.tsv",
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_modules_dynamic_module_as_alias/Snakefile
================================================
shell.executable("bash")


configfile: "config/config.yaml"

for module_name in ["module1", "module2"]:
    module:
        name: module_name
        snakefile:
            f"{module_name}/Snakefile"
        config:
            config
        replace_prefix:
            {"results/": "results/testmodule/"}

    use rule * from module_name as module_name*

rule all:
    input:
        multiext(expand("results/testmodule/c1/{name}.", name="test")[0], "tsv", "txt"),
        multiext(expand("results/testmodule/c2/{name}.", name="test")[0], "tsv", "txt"),


assert module1.some_func() == 15
assert module2.some_func() == 25



================================================
FILE: tests/test_modules_dynamic_module_as_alias/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_dynamic_module_as_alias/module1/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    output:
        temp("results/a1/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b1/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c1/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c1/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_dynamic_module_as_alias/module2/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 25


rule a:
    output:
        temp("results/a2/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b2/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c2/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c2/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_dynamic_no_as/Snakefile
================================================
shell.executable("bash")


configfile: "config/config.yaml"

for module_name in ["module1", "module2"]:
    module:
        name: module_name
        snakefile:
            f"{module_name}/Snakefile"
        config:
            config
        replace_prefix:
            {"results/": "results/testmodule/"}

    use rule * from module_name

rule all:
    input:
        multiext(expand("results/testmodule/c1/{name}.", name="test")[0], "tsv", "txt"),
        multiext(expand("results/testmodule/c2/{name}.", name="test")[0], "tsv", "txt"),


assert module1.some_func() == 15
assert module2.some_func() == 25



================================================
FILE: tests/test_modules_dynamic_no_as/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_dynamic_no_as/module1/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule module1_a:
    output:
        temp("results/a1/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule module1_b:
    input:
        expand(rules.module1_a.output, name="test"),
    output:
        "results/b1/{name}.out",
    shell:
        "cat {input} > {output}"


rule module1_c_tsv:
    input:
        expand(rules.module1_b.output, name="test"),
    output:
        "results/c1/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule module1_c_tsv as module1_c_txt with:
    output:
        "results/c1/{name}.txt",


rule module1_all:
    input:
        expand(rules.module1_c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_dynamic_no_as/module2/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 25


rule module2_a:
    output:
        temp("results/a2/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule module2_b:
    input:
        expand(rules.module2_a.output, name="test"),
    output:
        "results/b2/{name}.out",
    shell:
        "cat {input} > {output}"


rule module2_c_tsv:
    input:
        expand(rules.module2_b.output, name="test"),
    output:
        "results/c2/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule module2_c_tsv as module2_c_txt with:
    output:
        "results/c2/{name}.txt",


rule module2_all:
    input:
        expand(rules.module2_c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_meta_wrapper/config.yaml
================================================
samples:
  a:
    - "test.1.fastq"
    - "test.2.fastq"


================================================
FILE: tests/test_modules_meta_wrapper/Snakefile
================================================
configfile: "config.yaml"


module bwa_mapping:
    meta_wrapper: "0.72.0/meta/bio/bwa_mapping"


use rule * from bwa_mapping


def get_input(wildcards):
    return config["samples"][wildcards.sample]


use rule bwa_mem from bwa_mapping with:
    input:
        get_input


================================================
FILE: tests/test_modules_meta_wrapper/test.1.fastq
================================================
[Empty file]


================================================
FILE: tests/test_modules_meta_wrapper/test.2.fastq
================================================
[Empty file]


================================================
FILE: tests/test_modules_meta_wrapper/expected-results/.gitempty
================================================
[Empty file]


================================================
FILE: tests/test_modules_name/Snakefile
================================================
shell.executable("bash")


configfile: "config/config.yaml"


module:
    name: 'test'
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix:
        {"results/": "results/testmodule/"}


use rule * from test as test_*


rule all:
    input:
        multiext(expand("results/testmodule/c/{name}.", name="test")[0], "tsv", "txt"),


assert test.some_func() == 15



================================================
FILE: tests/test_modules_name/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_name/module-test/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    output:
        temp("results/a/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_no_name/Snakefile
================================================
shell.executable("bash")


configfile: "config/config.yaml"


module:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix:
        {"results/": "results/testmodule/"}


use rule * from test as test_*


rule all:
    input:
        multiext(expand("results/testmodule/c/{name}.", name="test")[0], "tsv", "txt"),


assert test.some_func() == 15



================================================
FILE: tests/test_modules_no_name/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_no_name/module-test/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    output:
        temp("results/a/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_peppy/Snakefile
================================================
shell.executable("bash")

configfile: "config/config.yaml"


module test:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    prefix:
        "foo"


use rule * from test



================================================
FILE: tests/test_modules_peppy/config/config.yaml
================================================
test: 1
pepfile: "pep/config.yaml"


================================================
FILE: tests/test_modules_peppy/module-test/Snakefile
================================================
configfile: "config.yaml" # does not exist, but this statement should be ignored on module import
pepfile: config["pepfile"]

def some_func():
    return 15

print(pep)

rule a:
    output:
        "results/test.out",
        "/tmp/foo.txt"
    shell:
        "echo {config[test]} > {output[0]}; touch {output[1]}"


================================================
FILE: tests/test_modules_peppy/pep/config.yaml
================================================
pep_version: "2.0.0"
sample_table: sample_table.csv



================================================
FILE: tests/test_modules_peppy/pep/sample_table.csv
================================================
sample_name,protocol
a,test
b,test2



================================================
FILE: tests/test_modules_prefix/Snakefile
================================================
shell.executable("bash")

configfile: "config/config.yaml"


module test:
    snakefile:
        "module-test/Snakefile"
    config:
        config
    prefix:
        "foo"


use rule * from test

assert test.some_func() == 15



================================================
FILE: tests/test_modules_prefix/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_prefix/module-test/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    input:
        storage.http("https://raw.githubusercontent.com/snakemake/snakemake/main/LICENSE.md"),
    output:
        "results/test.out",
        "/tmp/foo.txt",
    shell:
        "echo {config[test]} > {output[0]}; touch {output[1]}"



================================================
FILE: tests/test_modules_prefix_local/input.txt
================================================
test_a



================================================
FILE: tests/test_modules_prefix_local/Snakefile
================================================

module module1:
    snakefile:
        "module1/Snakefile"
    config:
        config
    prefix:
        "out_1"


use rule * from module1 as module1_*


rule joint_all:
    input:
        "out_1/test_final.txt",
    default_target: True



================================================
FILE: tests/test_modules_prefix_local/expected-results/out_1/test_final.txt
================================================
test_a



================================================
FILE: tests/test_modules_prefix_local/module1/Snakefile
================================================
rule a:
    input:
        local("input.txt"),
    output:
        "test_final.txt",
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_modules_ruledeps_inheritance/Snakefile
================================================
rule all:
    input:
        "test2.txt",
        "test3.txt",
        "test4.txt",


module somemodule:
    snakefile:
        "module/Snakefile"
    config:
        config


use rule * from somemodule as somemodule_*


module someothermodule:
    snakefile:
        "module2/Snakefile"
    config:
        config


use rule * from someothermodule as someothermodule_*


================================================
FILE: tests/test_modules_ruledeps_inheritance/expected-results/test2.txt
================================================
test



================================================
FILE: tests/test_modules_ruledeps_inheritance/module/Snakefile
================================================
rule a:
    output:
        "test.txt"
    shell:
        "echo test > {output}"


rule b:
    input:
        rules.a.output[0]
    output:
        "test2.txt"
    shell:
        "cp {input} {output}"



use rule b as c with:
    output:
        "test4.txt"


================================================
FILE: tests/test_modules_ruledeps_inheritance/module2/Snakefile
================================================
rule a:
    output:
        "test3.txt"
    shell:
        "echo foo > {output}"


================================================
FILE: tests/test_modules_semi_dynamic/Snakefile
================================================
shell.executable("bash")


configfile: "config/config.yaml"

for module_name, use_as in zip(["module1", "module2"], ['use_as_1', 'use_as_2']):
    module:
        name: module_name
        snakefile:
            f"{module_name}/Snakefile"
        config:
            config
        replace_prefix:
            {"results/": "results/testmodule/"}

use rule * from module1 as test1*
use rule * from module2 as test2*

rule all:
    input:
        multiext(expand("results/testmodule/c1/{name}.", name="test")[0], "tsv", "txt"),
        multiext(expand("results/testmodule/c2/{name}.", name="test")[0], "tsv", "txt"),


assert module1.some_func() == 15
assert module2.some_func() == 25



================================================
FILE: tests/test_modules_semi_dynamic/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_semi_dynamic/module1/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    output:
        temp("results/a1/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b1/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c1/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c1/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_semi_dynamic/module2/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 25


rule a:
    output:
        temp("results/a2/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b2/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c2/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c2/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_modules_specific/Snakefile
================================================
shell.executable("bash")

module test:
    snakefile: "module-test/Snakefile"


use rule * from test as test_*


use rule a from test as test_* with:
    output:
        "test2.out"
    params:
        test=2


================================================
FILE: tests/test_modules_specific/module-test/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.out"
    params:
        test=1
    shell:
        "echo {params.test} > {output}"


================================================
FILE: tests/test_modules_two_names/Snakefile
================================================
shell.executable("bash")


configfile: "config/config.yaml"


module test:
    name: "test"
    snakefile:
        "module-test/Snakefile"
    config:
        config
    replace_prefix:
        {"results/": "results/testmodule/"}


use rule * from test as test_*


rule all:
    input:
        multiext(expand("results/testmodule/c/{name}.", name="test")[0], "tsv", "txt"),


assert test.some_func() == 15



================================================
FILE: tests/test_modules_two_names/config/config.yaml
================================================
test: 1


================================================
FILE: tests/test_modules_two_names/module-test/Snakefile
================================================
configfile: "config.yaml"  # does not exist, but this statement should be ignored on module import


def some_func():
    return 15


rule a:
    output:
        temp("results/a/{name}.out"),
    shell:
        "echo {config[test]} > {output}"


rule b:
    input:
        expand(rules.a.output, name="test"),
    output:
        "results/b/{name}.out",
    shell:
        "cat {input} > {output}"


rule c_tsv:
    input:
        expand(rules.b.output, name="test"),
    output:
        "results/c/{name}.tsv",
    shell:
        "cat {input} > {output}"


use rule c_tsv as c_txt with:
    output:
        "results/c/{name}.txt",


rule all:
    input:
        expand(rules.c_tsv.output, name="test"),



================================================
FILE: tests/test_multicomp_group_jobs/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_multicomp_group_jobs/Snakefile
================================================
samples = list(range(10))


rule all:
    input:
        "test.out"


rule a:
    output:
        "a/{sample}.out"
    shell:
        "touch {output}"


rule b:
    input:
        "a/{sample}.out"
    output:
        "b/{sample}.out"
    shell:
        "touch {output}"


rule c:
    input:
        expand("b/{sample}.out", sample=samples)
    output:
        "test.out"
    group: 1
    shell:
        "touch {output}"



================================================
FILE: tests/test_multiext/Snakefile
================================================
rule a:
    output:
        multiext("ref/genome", ".ann", ".bwt", ".sa", "_test.fai")
    shell:
        "touch {output}"



================================================
FILE: tests/test_multiext/expected-results/ref/genome.ann
================================================
[Empty file]


================================================
FILE: tests/test_multiext/expected-results/ref/genome.bwt
================================================
[Empty file]


================================================
FILE: tests/test_multiext/expected-results/ref/genome.sa
================================================
[Empty file]


================================================
FILE: tests/test_multiext_named/Snakefile
================================================
rule all:
  input:
    multiext("a", out1 = ".1", out2 = ".2")

rule a:
    output:
        multiext("a", out1 = ".1", out2 = ".2"),
    shell: "touch {output.out1} && touch {output.out2}"


================================================
FILE: tests/test_multiext_named/expected-results/a.1
================================================
[Empty file]


================================================
FILE: tests/test_multiext_named/expected-results/a.2
================================================
[Empty file]


================================================
FILE: tests/test_multiple_includes/Snakefile
================================================
from pathlib import Path


include: 'test_rule.smk'
include: 'test_second_rule.smk'
include: Path('test_third_rule.smk')

rule all:
    input: rules.test_third_rule.output


================================================
FILE: tests/test_multiple_includes/test_rule.smk
================================================
rule test_rule: 
    output: 'test1.txt'
    shell: 'touch {output}'


================================================
FILE: tests/test_multiple_includes/test_second_rule.smk
================================================
rule test_second_rule: 
    input: rules.test_rule.output
    output: 'test2.txt'
    shell: 'touch {output}'


================================================
FILE: tests/test_multiple_includes/test_third_rule.smk
================================================
rule test_third_rule:
    input: rules.test_second_rule.output
    output: 'test3.txt'
    shell: 'touch {output}'


================================================
FILE: tests/test_multiple_includes/expected-results/test1.txt
================================================
[Empty file]


================================================
FILE: tests/test_multiple_includes/expected-results/test2.txt
================================================
[Empty file]


================================================
FILE: tests/test_multiple_includes/expected-results/test3.txt
================================================
[Empty file]


================================================
FILE: tests/test_name_override/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        txt = 'out2.txt'

some_var: str = 'generate_out'

rule:
    name: some_var
    output: 'out1.txt'
    shell: '''echo {rule} > {output}'''


rule name1:
    name: "name2"
    input: 'out1.txt'
    output: 'out2.txt'
    shell: '''echo {rule} > {output}'''



================================================
FILE: tests/test_name_override/expected-results/out1.txt
================================================
generate_out



================================================
FILE: tests/test_name_override/expected-results/out2.txt
================================================
name2



================================================
FILE: tests/test_no_temp/Snakefile
================================================
rule all:
    input:
        "a",


rule create_b:
    output:
        temp("b"),
    shell:
        "touch {output}"


rule create_a:
    input:
        rules.create_b.output,
    output:
        "a",
    shell:
        "touch {output}"



================================================
FILE: tests/test_no_temp/expected-results/a
================================================
[Empty file]


================================================
FILE: tests/test_no_temp/expected-results/b
================================================
[Empty file]


================================================
FILE: tests/test_no_workflow_profile/dummy-general-profile/config.yaml
================================================
set-resources:
  a:
    foo: test-general



================================================
FILE: tests/test_no_workflow_profile/workflow/Snakefile
================================================
rule a:
    output:
        "test.out"
    shell:
        "echo {resources.foo} > {output}"




================================================
FILE: tests/test_no_workflow_profile/workflow/profiles/default/config.yaml
================================================
set-resources:
  a:
    foo: test-workflow-specific



================================================
FILE: tests/test_nodelocal/qsub
================================================
#!/bin/sh

# Simulates node-local storage (a.k.a. "local scratch"):
# - files and paths only accessible within a single compute node
# - NOT accessible on other nodes nor on the login node

mkdir -p scratch local

# Unshare the mount namespace:
# - mounts within qsub_stage2 NOT shared with other processes
# - outer process will keep seeing "scratch" unaffected
unshare --mount --map-root-user ./qsub_stage2 "$@"



================================================
FILE: tests/test_nodelocal/qsub_stage2
================================================
#!/bin/bash

# Simulates node-local storage (a.k.a. "local scratch"):
# - files and paths only accessible within a single compute node
# - NOT accessible on other nodes nor on the login node
#
# Make the mount namespace private and bind 'local' to appear as 'scratch' within the namespace
mount --make-rprivate --bind local scratch
# Create a marker file to verify the binding is limited
# - current script ^will see it into "scratch/" too
# - outer calling processes will only see it in "local/", will never see content appearing in "scratch/"
echo "local" > scratch/local

# normal qsub like other cluster-simulated tests

echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_nodelocal/Snakefile
================================================
rule all:
    input:
        "results/result.txt",


rule create_nodelocal_temp:
    output:
        temp("scratch/temp.txt", group_jobs=True),
    shell:
        """
        sleep 4
        echo "test" > "{output}"
        """


rule create_nodelocal_persist:
    output:
        flag("scratch/persist.txt", "nodelocal"),
    shell:
        """
        sleep 4
        echo "test" > "{output}"
        """


rule consume_nodelocal:
    input:
        tmp="scratch/temp.txt",
        persist="scratch/persist.txt",
    output:
        "results/result.txt",
    shell:
        """
        ls "{input.tmp}" > "{output}"
        ls "{input.persist}" >> "{output}"
        """



================================================
FILE: tests/test_nodelocal/expected-results/local/local
================================================
local



================================================
FILE: tests/test_nodelocal/expected-results/local/persist.txt
================================================
test



================================================
FILE: tests/test_nodelocal/expected-results/results/result.txt
================================================
scratch/temp.txt
scratch/persist.txt



================================================
FILE: tests/test_nonstr_params/Snakefile
================================================
import numpy as np
from pathlib import Path

rule:
    output:
        "test.out"
    benchmark:
        "test.jsonl"
    params:
        test=True,
        path=Path("/tmp"),
        np=np.int64(1),
    run:
        assert params.test is True
        shell("echo {params.path} {params.np} > {output}")



================================================
FILE: tests/test_omitfrom/Snakefile
================================================

rule all: 
    input:
        "levelthree.txt",
        "independent.txt",
        expand("test{num}.final", num=[1, 2])

rule levelone:
    output: "levelone.txt"
    shell: "touch {output}"

rule leveltwo_first:
    input: rules.levelone.output
    output: "leveltwo_first.txt"   
    shell: "cp -f {input} {output}"

rule leveltwo_second:
    input: rules.levelone.output
    output: "leveltwo_second.txt"
    shell: "cp -f {input} {output}"

rule levelthree: # should not be created
    input: 
        rules.leveltwo_first.output, 
        rules.leveltwo_second.output
    output: "levelthree.txt"
    shell: "cat {input} > {output}"

rule independent: # should be created in --omit-from but not --until
    output: "independent.txt"
    shell: "touch {output}"

###### Wildcard Rules #######

rule zeroth_wildcard:
    output: "test{num}.txt"
    shell: "touch {output}"

rule first_wildcard:
    input: 'test{num}.txt'
    output: 'test{num}.first'
    shell: 'cp -f {input} {output}'

rule second_wildcard:
    input: 'test{num}.first'
    output: 'test{num}.second'
    shell: 'cp -f {input} {output}'

rule final_wildcard:
    input: 'test{num}.second'
    output: 'test{num}.final'
    shell: 'cp -f {input} {output}'





================================================
FILE: tests/test_omitfrom/expected-results/independent.txt
================================================
[Empty file]


================================================
FILE: tests/test_omitfrom/expected-results/levelone.txt
================================================
[Empty file]


================================================
FILE: tests/test_output_file_cache/Snakefile
================================================
"""This test verifies that the cache directive generates the right files.

There should be a total of four cached files. One for the rules a (cached via
cli/ api parameter) and c (cached via directive) each.
For rule b there should be two files, one for each extention generated by the
multiext helper.
"""
shell.executable("bash")


rule all:
    input:
        "test3.x.out",
        "test3.y.out"


rule a:
    output:
        "test.out"
    shell:
        "echo test > {output}"


rule b:
    input:
        "test.out"
    output:
        multiext("test2", ".out", "_out2")
    shell:
        "echo test2 > {output[0]}; echo test22 > {output[1]}"


rule c:
    input:
        "test2.out",
        "test.out"
    output:
        "test3.{w}.out"
    params:
        a=1.5
    cache: True
    shell:
        "echo test3 {params.a} > {output}"


rule invalid_multi:
    output:
        "invalid1.txt",
        "invalid2.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_output_file_cache/cache/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_output_file_cache/expected-results/cache/695af872f8dc47c9acd2dd3ebec1caad623f7348dca2c38ef81df8acef58b4eb_out2
================================================
test22



================================================
FILE: tests/test_output_file_cache_storage/Snakefile
================================================
"""This test verifies that the cache directive generates the right files.

There should be a total of four cached files. One for the rules a (cached via
cli/ api parameter) and c (cached via directive) each.
For rule b there should be two files, one for each extention generated by the
multiext helper.
"""
shell.executable("bash")


rule all:
    input:
        "test3.x.out",
        "test3.y.out"


rule a:
    output:
        "test.out"
    shell:
        "echo test > {output}"


rule b:
    input:
        "test.out"
    output:
        multiext("test2", ".out", ".out2")
    shell:
        "echo test2 > {output[0]}; echo test22 > {output[1]}"


rule c:
    input:
        "test2.out",
        "test.out"
    output:
        "test3.{w}.out"
    params:
        a=1.5
    cache: True
    shell:
        "echo test3 {params.a} > {output}"


rule invalid_multi:
    output:
        "invalid1.txt",
        "invalid2.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_output_file_cache_storage/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_params/Snakefile
================================================

"""
This is a test for the params syntax.
"""
shell.executable("bash")

rule:
	input: "somedir/test.out"

rule:
	params: lambda wildcards: "-f", dir="{dir}"
	output: "{dir}/test.out"
	shell: "rm -r {params.dir}; mkdir -p {params.dir}; touch {params[0]} {output}"



================================================
FILE: tests/test_params_empty_inherit/Snakefile
================================================
rule all:
    input:
        "test.txt"

rule parent:
    run:
        with open(output[0], "w") as f:
            f.write(params.a)

use rule parent as child with:
    output:
        "test.txt"
    params:
        a="a"



================================================
FILE: tests/test_params_empty_inherit/expected-results/test.txt
================================================
a



================================================
FILE: tests/test_params_outdated_code/Snakefile
================================================

"""
This is a test for the params syntax.
"""
shell.executable("bash")

rule:
	params: lambda wildcards: "-f", dir="{dir}"
	output: "{dir}/test.out"
	shell: "rm -r {params.dir}; mkdir -p {params.dir}; touch {params[0]} {output}"



================================================
FILE: tests/test_params_outdated_code/.snakemake/metadata/c29tZWRpci90ZXN0Lm91dA==
================================================
{"code": "rm  -r {params.dir}; mkdir -p {params.dir}; touch {params[0]} {output}", "rule": "2", "input": [], "log": [], "params": ["'-f'", "'somedir'"], "shellcmd": "rm -r somedir; mkdir -p somedir; touch -f somedir/test.out", "incomplete": false, "starttime": 1728902332.536915, "endtime": 1728902332.541915, "job_hash": 8738033125687, "conda_env": null, "container_img_url": null, "input_checksums": {}}



================================================
FILE: tests/test_params_pickling/Snakefile
================================================
import pandas as pd
import numpy as np
import polars as pl

testpd = pd.DataFrame({"a": [1, 2, 3]})
testnp = np.array([1, 2, np.float64(3)])
testpl = pl.DataFrame({"a": [1, 2, 3]})


rule a:
    output:
        pd="testpd.tsv",
        np="testnp.tsv",
        pl="testpl.tsv",
    params:
        testpd=testpd,
        testnp=testnp,
        testpl=testpl,
    script:
        "test.py"


================================================
FILE: tests/test_params_pickling/test.py
================================================
import numpy as np
import polars as pl
import pandas as pd

assert isinstance(snakemake.params.testnp, np.ndarray)
assert isinstance(snakemake.params.testpl, pl.DataFrame)
assert isinstance(snakemake.params.testpd, pd.DataFrame)

snakemake.params.testpd.to_csv(snakemake.output.pd, sep="\t")
snakemake.params.testpl.write_csv(snakemake.output.pl, separator="\t")
np.savetxt(snakemake.output.np, snakemake.params.testnp, delimiter="\t")


================================================
FILE: tests/test_params_pickling/expected-results/testnp.tsv
================================================
1.000000000000000000e+00
2.000000000000000000e+00
3.000000000000000000e+00



================================================
FILE: tests/test_params_pickling/expected-results/testpd.tsv
================================================
	a
0	1
1	2
2	3



================================================
FILE: tests/test_params_pickling/expected-results/testpl.tsv
================================================
a
1
2
3



================================================
FILE: tests/test_paramspace/params.tsv
================================================
alpha	beta	gamma
1	0.1	0.99
2	0.0	3.9



================================================
FILE: tests/test_paramspace/Snakefile
================================================
from snakemake.utils import Paramspace
import pandas as pd


# should result in alpha~{alpha}/beta~{beta}/gamma~{gamma}
paramspace_default = Paramspace(pd.read_csv("params.tsv", sep="\t"))

# should result in alpha~{alpha}/beta~{beta}/gamma~{gamma}
paramspace_empty = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=[])

# should result in alpha~{alpha}/gamma~{gamma}/beta~{beta}
paramspace_one = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=["beta"])

# should result in alpha~{alpha}/beta~{beta}_gamma~{gamma}
paramspace_two = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=["beta", "gamma"])

# should result in alpha~{alpha}_beta~{beta}_gamma~{gamma}
paramspace_full = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=["alpha", "beta", "gamma"])

# should result in beta~{beta}_gamma~{gamma}_alpha~{alpha}
paramspace_full_reorder = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=["beta", "gamma", "alpha"])

# should result in alpha:{alpha}/beta:{beta}/gamma:{gamma}
paramspace_sep = Paramspace(pd.read_csv("params.tsv", sep="\t"), param_sep="_is_")

# should result in beta={beta}_gamma={gamma}_alpha={alpha}
paramspace_sep_and_pattern = Paramspace(
    pd.read_csv("params.tsv", sep="\t"),
    filename_params=["beta", "gamma", "alpha"],
    param_sep="=",
)

# should result in alpha~{alpha}::beta~{beta}::gamma~{gamma}
paramspace_filenamesep = Paramspace(
    pd.read_csv("params.tsv", sep="\t"),
    filename_params="*",
    filename_sep="__",
)


rule all:
    input:
        expand("results/default/plots/{params}.pdf", params=paramspace_default.instance_patterns),
        expand("results/empty/plots/{params}.pdf", params=paramspace_empty.instance_patterns),
        expand("results/one/plots/{params}.pdf", params=paramspace_one.instance_patterns),
        expand("results/two/plots/{params}.pdf", params=paramspace_two.instance_patterns),
        expand("results/full/plots/{params}.pdf", params=paramspace_full.instance_patterns),
        expand("results/full_reorder/plots/{params}.pdf", params=paramspace_full_reorder.instance_patterns),
        expand("results/sep/plots/{params}.pdf", params=paramspace_sep.instance_patterns),
        expand("results/sep_and_pattern/plots/{params}.pdf", params=paramspace_sep_and_pattern.instance_patterns),
        expand("results/filenamesep/plots/{params}.pdf", params=paramspace_filenamesep.instance_patterns),


rule simulate_default:
    output:
        f"results/default/simulations/{paramspace_default.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_default.instance
    script:
        "scripts/simulate.py"


rule plot_default:
    input:
        f"results/default/simulations/{paramspace_default.wildcard_pattern}.tsv"
    output:
        f"results/default/plots/{paramspace_default.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_empty:
    output:
        f"results/empty/simulations/{paramspace_empty.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_empty.instance
    script:
        "scripts/simulate.py"


rule plot_empty:
    input:
        f"results/empty/simulations/{paramspace_empty.wildcard_pattern}.tsv"
    output:
        f"results/empty/plots/{paramspace_empty.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_one:
    output:
        f"results/one/simulations/{paramspace_one.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_one.instance
    script:
        "scripts/simulate.py"


rule plot_one:
    input:
        f"results/one/simulations/{paramspace_one.wildcard_pattern}.tsv"
    output:
        f"results/one/plots/{paramspace_one.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_two:
    output:
        f"results/two/simulations/{paramspace_two.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_two.instance
    script:
        "scripts/simulate.py"


rule plot_two:
    input:
        f"results/two/simulations/{paramspace_two.wildcard_pattern}.tsv"
    output:
        f"results/two/plots/{paramspace_two.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_full:
    output:
        f"results/full/simulations/{paramspace_full.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_full.instance
    script:
        "scripts/simulate.py"


rule plot_full:
    input:
        f"results/full/simulations/{paramspace_full.wildcard_pattern}.tsv"
    output:
        f"results/full/plots/{paramspace_full.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_full_reorder:
    output:
        f"results/full_reorder/simulations/{paramspace_full_reorder.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_full_reorder.instance
    script:
        "scripts/simulate.py"


rule plot_full_reorder:
    input:
        f"results/full_reorder/simulations/{paramspace_full_reorder.wildcard_pattern}.tsv"
    output:
        f"results/full_reorder/plots/{paramspace_full_reorder.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_sep:
    output:
        f"results/sep/simulations/{paramspace_sep.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_sep.instance
    script:
        "scripts/simulate.py"


rule plot_sep:
    input:
        f"results/sep/simulations/{paramspace_sep.wildcard_pattern}.tsv"
    output:
        f"results/sep/plots/{paramspace_sep.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_sep_and_pattern:
    output:
        f"results/sep_and_pattern/simulations/{paramspace_sep_and_pattern.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_sep_and_pattern.instance
    script:
        "scripts/simulate.py"


rule plot_sep_and_pattern:
    input:
        f"results/sep_and_pattern/simulations/{paramspace_sep_and_pattern.wildcard_pattern}.tsv"
    output:
        f"results/sep_and_pattern/plots/{paramspace_sep_and_pattern.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_filenamesep:
    output:
        f"results/filenamesep/simulations/{paramspace_filenamesep.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_filenamesep.instance
    script:
        "scripts/simulate.py"


rule plot_filenamesep:
    input:
        f"results/filenamesep/simulations/{paramspace_filenamesep.wildcard_pattern}.tsv"
    output:
        f"results/filenamesep/plots/{paramspace_filenamesep.wildcard_pattern}.pdf"
    shell:
        "touch {output}"



================================================
FILE: tests/test_paramspace/expected-results/results/default/simulations/alpha~1/beta~0.1/gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace/expected-results/results/default/simulations/alpha~2/beta~0.0/gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace/expected-results/results/empty/simulations/alpha~1/beta~0.1/gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace/expected-results/results/empty/simulations/alpha~2/beta~0.0/gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace/expected-results/results/filenamesep/simulations/alpha~1__beta~0.1__gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace/expected-results/results/filenamesep/simulations/alpha~2__beta~0.0__gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace/expected-results/results/full/simulations/alpha~1_beta~0.1_gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace/expected-results/results/full/simulations/alpha~2_beta~0.0_gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace/expected-results/results/full_reorder/simulations/beta~0.0_gamma~3.9_alpha~2.tsv
================================================
{"beta": 0.0, "gamma": 3.9, "alpha": 2}


================================================
FILE: tests/test_paramspace/expected-results/results/full_reorder/simulations/beta~0.1_gamma~0.99_alpha~1.tsv
================================================
{"beta": 0.1, "gamma": 0.99, "alpha": 1}


================================================
FILE: tests/test_paramspace/expected-results/results/one/simulations/alpha~1/gamma~0.99/beta~0.1.tsv
================================================
{"alpha": 1, "gamma": 0.99, "beta": 0.1}


================================================
FILE: tests/test_paramspace/expected-results/results/one/simulations/alpha~2/gamma~3.9/beta~0.0.tsv
================================================
{"alpha": 2, "gamma": 3.9, "beta": 0.0}


================================================
FILE: tests/test_paramspace/expected-results/results/sep/simulations/alpha_is_1/beta_is_0.1/gamma_is_0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace/expected-results/results/sep/simulations/alpha_is_2/beta_is_0.0/gamma_is_3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace/expected-results/results/sep_and_pattern/simulations/beta=0.0_gamma=3.9_alpha=2.tsv
================================================
{"beta": 0.0, "gamma": 3.9, "alpha": 2}


================================================
FILE: tests/test_paramspace/expected-results/results/sep_and_pattern/simulations/beta=0.1_gamma=0.99_alpha=1.tsv
================================================
{"beta": 0.1, "gamma": 0.99, "alpha": 1}


================================================
FILE: tests/test_paramspace/expected-results/results/two/simulations/alpha~1/beta~0.1_gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace/expected-results/results/two/simulations/alpha~2/beta~0.0_gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace/scripts/simulate.py
================================================
import json

with open(snakemake.output[0], "w") as out:
    # convert numpy types to native python types for serialization
    json.dump({k: v.item() for k, v in snakemake.params.simulation.items()}, out)


================================================
FILE: tests/test_paramspace_single_wildcard/params.tsv
================================================
alpha	beta	gamma
1	0.1	0.99
2	0.0	3.9



================================================
FILE: tests/test_paramspace_single_wildcard/Snakefile
================================================
from snakemake.utils import Paramspace
import pandas as pd


# should result in alpha~{alpha}/beta~{beta}/gamma~{gamma}
paramspace_default = Paramspace(pd.read_csv("params.tsv", sep="\t"), single_wildcard="space")

# should result in alpha~{alpha}/beta~{beta}/gamma~{gamma}
paramspace_empty = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=[], single_wildcard="space")

# should result in alpha~{alpha}/gamma~{gamma}/beta~{beta}
paramspace_one = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=["beta"], single_wildcard="space")

# should result in alpha~{alpha}/beta~{beta}_gamma~{gamma}
paramspace_two = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=["beta", "gamma"], single_wildcard="space")

# should result in alpha~{alpha}_beta~{beta}_gamma~{gamma}
paramspace_full = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=["alpha", "beta", "gamma"], single_wildcard="space")

# should result in beta~{beta}_gamma~{gamma}_alpha~{alpha}
paramspace_full_reorder = Paramspace(pd.read_csv("params.tsv", sep="\t"), filename_params=["beta", "gamma", "alpha"], single_wildcard="space")

# should result in alpha:{alpha}/beta:{beta}/gamma:{gamma}
paramspace_sep = Paramspace(pd.read_csv("params.tsv", sep="\t"), param_sep="_is_", single_wildcard="space")

# should result in beta={beta}_gamma={gamma}_alpha={alpha}
paramspace_sep_and_pattern = Paramspace(
    pd.read_csv("params.tsv", sep="\t"),
    filename_params=["beta", "gamma", "alpha"],
    param_sep="=", single_wildcard="space"
)

# should result in alpha~{alpha}::beta~{beta}::gamma~{gamma}
paramspace_filenamesep = Paramspace(
    pd.read_csv("params.tsv", sep="\t"),
    filename_params="*",
    filename_sep="__", single_wildcard="space"
)


rule all:
    input:
        expand("results/default/plots/{params}.pdf", params=paramspace_default.instance_patterns),
        expand("results/empty/plots/{params}.pdf", params=paramspace_empty.instance_patterns),
        expand("results/one/plots/{params}.pdf", params=paramspace_one.instance_patterns),
        expand("results/two/plots/{params}.pdf", params=paramspace_two.instance_patterns),
        expand("results/full/plots/{params}.pdf", params=paramspace_full.instance_patterns),
        expand("results/full_reorder/plots/{params}.pdf", params=paramspace_full_reorder.instance_patterns),
        expand("results/sep/plots/{params}.pdf", params=paramspace_sep.instance_patterns),
        expand("results/sep_and_pattern/plots/{params}.pdf", params=paramspace_sep_and_pattern.instance_patterns),
        expand("results/filenamesep/plots/{params}.pdf", params=paramspace_filenamesep.instance_patterns),


rule simulate_default:
    output:
        f"results/default/simulations/{paramspace_default.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_default.instance
    script:
        "scripts/simulate.py"


rule plot_default:
    input:
        f"results/default/simulations/{paramspace_default.wildcard_pattern}.tsv"
    output:
        f"results/default/plots/{paramspace_default.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_empty:
    output:
        f"results/empty/simulations/{paramspace_empty.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_empty.instance
    script:
        "scripts/simulate.py"


rule plot_empty:
    input:
        f"results/empty/simulations/{paramspace_empty.wildcard_pattern}.tsv"
    output:
        f"results/empty/plots/{paramspace_empty.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_one:
    output:
        f"results/one/simulations/{paramspace_one.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_one.instance
    script:
        "scripts/simulate.py"


rule plot_one:
    input:
        f"results/one/simulations/{paramspace_one.wildcard_pattern}.tsv"
    output:
        f"results/one/plots/{paramspace_one.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_two:
    output:
        f"results/two/simulations/{paramspace_two.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_two.instance
    script:
        "scripts/simulate.py"


rule plot_two:
    input:
        f"results/two/simulations/{paramspace_two.wildcard_pattern}.tsv"
    output:
        f"results/two/plots/{paramspace_two.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_full:
    output:
        f"results/full/simulations/{paramspace_full.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_full.instance
    script:
        "scripts/simulate.py"


rule plot_full:
    input:
        f"results/full/simulations/{paramspace_full.wildcard_pattern}.tsv"
    output:
        f"results/full/plots/{paramspace_full.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_full_reorder:
    output:
        f"results/full_reorder/simulations/{paramspace_full_reorder.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_full_reorder.instance
    script:
        "scripts/simulate.py"


rule plot_full_reorder:
    input:
        f"results/full_reorder/simulations/{paramspace_full_reorder.wildcard_pattern}.tsv"
    output:
        f"results/full_reorder/plots/{paramspace_full_reorder.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_sep:
    output:
        f"results/sep/simulations/{paramspace_sep.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_sep.instance
    script:
        "scripts/simulate.py"


rule plot_sep:
    input:
        f"results/sep/simulations/{paramspace_sep.wildcard_pattern}.tsv"
    output:
        f"results/sep/plots/{paramspace_sep.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_sep_and_pattern:
    output:
        f"results/sep_and_pattern/simulations/{paramspace_sep_and_pattern.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_sep_and_pattern.instance
    script:
        "scripts/simulate.py"


rule plot_sep_and_pattern:
    input:
        f"results/sep_and_pattern/simulations/{paramspace_sep_and_pattern.wildcard_pattern}.tsv"
    output:
        f"results/sep_and_pattern/plots/{paramspace_sep_and_pattern.wildcard_pattern}.pdf"
    shell:
        "touch {output}"


rule simulate_filenamesep:
    output:
        f"results/filenamesep/simulations/{paramspace_filenamesep.wildcard_pattern}.tsv"
    params:
        simulation=paramspace_filenamesep.instance
    script:
        "scripts/simulate.py"


rule plot_filenamesep:
    input:
        f"results/filenamesep/simulations/{paramspace_filenamesep.wildcard_pattern}.tsv"
    output:
        f"results/filenamesep/plots/{paramspace_filenamesep.wildcard_pattern}.pdf"
    shell:
        "touch {output}"



================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/default/simulations/alpha~1/beta~0.1/gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/default/simulations/alpha~2/beta~0.0/gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/empty/simulations/alpha~1/beta~0.1/gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/empty/simulations/alpha~2/beta~0.0/gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/filenamesep/simulations/alpha~1__beta~0.1__gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/filenamesep/simulations/alpha~2__beta~0.0__gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/full/simulations/alpha~1_beta~0.1_gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/full/simulations/alpha~2_beta~0.0_gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/full_reorder/simulations/beta~0.0_gamma~3.9_alpha~2.tsv
================================================
{"beta": 0.0, "gamma": 3.9, "alpha": 2}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/full_reorder/simulations/beta~0.1_gamma~0.99_alpha~1.tsv
================================================
{"beta": 0.1, "gamma": 0.99, "alpha": 1}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/one/simulations/alpha~1/gamma~0.99/beta~0.1.tsv
================================================
{"alpha": 1, "gamma": 0.99, "beta": 0.1}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/one/simulations/alpha~2/gamma~3.9/beta~0.0.tsv
================================================
{"alpha": 2, "gamma": 3.9, "beta": 0.0}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/sep/simulations/alpha_is_1/beta_is_0.1/gamma_is_0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/sep/simulations/alpha_is_2/beta_is_0.0/gamma_is_3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/sep_and_pattern/simulations/beta=0.0_gamma=3.9_alpha=2.tsv
================================================
{"beta": 0.0, "gamma": 3.9, "alpha": 2}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/sep_and_pattern/simulations/beta=0.1_gamma=0.99_alpha=1.tsv
================================================
{"beta": 0.1, "gamma": 0.99, "alpha": 1}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/two/simulations/alpha~1/beta~0.1_gamma~0.99.tsv
================================================
{"alpha": 1, "beta": 0.1, "gamma": 0.99}


================================================
FILE: tests/test_paramspace_single_wildcard/expected-results/results/two/simulations/alpha~2/beta~0.0_gamma~3.9.tsv
================================================
{"alpha": 2, "beta": 0.0, "gamma": 3.9}


================================================
FILE: tests/test_paramspace_single_wildcard/scripts/simulate.py
================================================
import json

with open(snakemake.output[0], "w") as out:
    # convert numpy types to native python types for serialization
    json.dump({k: v.item() for k, v in snakemake.params.simulation.items()}, out)


================================================
FILE: tests/test_parser/Snakefile
================================================


class Test:
	def __init__(self):
		self.include = "test.out"


rule:
	output: Test().include
	shell: "touch {output}"





================================================
FILE: tests/test_parsing_terminal_comment_following_statement/Snakefile
================================================
rule all:
    output: touch('done')
#include: "tmp/t2.smk" $ snakemake -s merge.smk -j 1 -p


================================================
FILE: tests/test_parsing_terminal_comment_following_statement/expected-results/done
================================================
[Empty file]


================================================
FILE: tests/test_path with spaces/Snakefile
================================================
rule default:
    output:
        "test.out",
    run:
        with open(output[0], "w") as f:
            f.write("hello there")



================================================
FILE: tests/test_pathlib/existing_file.txt
================================================
I exist!



================================================
FILE: tests/test_pathlib/Snakefile
================================================
from pathlib import Path

# Untested:
# ancient
# subworkflows
# directory

OUTDIR = Path("outdir")
existing_file = Path("existing_file.txt")

rule all:
	input:
		OUTDIR / "output_existing",
		OUTDIR / "output_temp",
		OUTDIR / "output_protected",
		OUTDIR / "output_other_rule_output_existing",
		OUTDIR / "function_unpacking/output_function_unpacking",


rule input_Path:
	input: Path("{sample}_file.txt")
	output: OUTDIR / "output_{sample}"
	log: OUTDIR / "log_{sample}"
	shell:
		"""
		cat {input} > {output}
		echo log {wildcards.sample} > {log}
		"""

rule input_Path_output_temp:
	input: existing_file
	output: temp(OUTDIR / "output_temp")
	log: OUTDIR / "log_temp"
	shell:
		"""
		cat {input} > {output}
		echo log temp > {log}
		"""

rule input_Path_output_protected:
	input: existing_file
	output: protected(OUTDIR / "output_protected")
	log: OUTDIR / "log_protected"
	shell:
		"""
		cat {input} > {output}
		echo log protected > {log}
		"""

rule input_other_rule_output:
	input: rules.input_Path.output
	output: OUTDIR / "output_other_rule_output_{sample}"
	log: OUTDIR / "log_other_rule_output_{sample}"
	shell:
		"""
		cat {input} > {output}
		echo log protected > {log}
		"""

def myfunc1():
    return [existing_file]

def myfunc2():
    return {'existing_file2': existing_file}

rule input_function_unpacking:
	input:
		*myfunc1(),
		**myfunc2(),
	output: OUTDIR / "function_unpacking/output_function_unpacking"
	log: OUTDIR / "log_function_unpacking"
	shell:
		"""
		echo {input[0]} > {output}
		echo {input.existing_file2} >> {output}
		echo log function unpacking > {log}
		"""



================================================
FILE: tests/test_pathlib/expected-results/outdir/log_existing
================================================
log existing



================================================
FILE: tests/test_pathlib/expected-results/outdir/log_function_unpacking
================================================
log function unpacking



================================================
FILE: tests/test_pathlib/expected-results/outdir/log_other_rule_output_existing
================================================
log protected



================================================
FILE: tests/test_pathlib/expected-results/outdir/log_protected
================================================
log protected



================================================
FILE: tests/test_pathlib/expected-results/outdir/log_temp
================================================
log temp



================================================
FILE: tests/test_pathlib/expected-results/outdir/output_existing
================================================
I exist!



================================================
FILE: tests/test_pathlib/expected-results/outdir/output_other_rule_output_existing
================================================
I exist!



================================================
FILE: tests/test_pathlib/expected-results/outdir/output_protected
================================================
I exist!



================================================
FILE: tests/test_pathlib/expected-results/outdir/function_unpacking/output_function_unpacking
================================================
existing_file.txt
existing_file.txt



================================================
FILE: tests/test_pathlib_missing_file/Snakefile
================================================
from pathlib import Path

# Things to test:
# ancient
# subworkflows

output = Path("output")
nonexistent_file = Path("nonexistent_file.txt")

rule all:
	input:
		output


rule input_nonexistent_Path:
	input: nonexistent_file
	output: output
	shell: 
		"""
		echo "This should fail" > {output}
		"""



================================================
FILE: tests/test_pathlib_missing_file/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_pep_pathlib/Snakefile
================================================
from pathlib import Path

pepfile: Path("pep/config.yaml")


pepschema: Path("workflow/schemas/pep.yaml")

rule all:
    input:
        expand("{sample}.txt", sample=pep.sample_table["sample_name"])

rule a:
    output:
        "{sample}.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_pep_pathlib/expected-results/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_pep_pathlib/expected-results/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_pep_pathlib/pep/config.yaml
================================================
pep_version: "2.0.0"
sample_table: sample_table.csv



================================================
FILE: tests/test_pep_pathlib/pep/sample_table.csv
================================================
sample_name,protocol
a,test
b,test2



================================================
FILE: tests/test_pep_pathlib/workflow/schemas/pep.yaml
================================================
description: "Schema for a minimal PEP"
version: "2.0.0"
properties:
  name: 
    type: string
    pattern: "^\\S*$"
    description: "Project name with no whitespace"
  config:
    pep_version:
      description: "Version of the PEP Schema this PEP follows"
      type: string
    sample_table:
      type: string
      description: "Path to the sample annotation table with one row per sample"
    subsample_table:
      type: string
      description: "Path to the subsample annotation table with one row per subsample and sample_name attribute matching an entry in the sample table"
    sample_modifiers:
      type: object
      properties:
        append:
          type: object
        duplicate:
          type: object
        imply:
          type: array
          items:
            type: object
            properties:
              if:
                type: object
              then:
                type: object
        derive:
          type: object
          properties:
            attributes:
              type: array
              items:
                type: string
            sources:
              type: object
    project_modifiers:
      type: object
      properties:
        amend:
          description: "Object overwriting original project attributes"
          type: object
        import:
          description: "List of external PEP project config files to import"
          type: array
          items:
            type: string
    required:
      - pep_version
  samples:
    type: array
    items:
      type: object
      properties:
        sample_name: 
          type: string
          pattern: "^\\S*$"
          description: "Unique name of the sample with no whitespace"
      required:
        - sample_name
required:
  - samples



================================================
FILE: tests/test_peppy/Snakefile
================================================
pepfile: "pep/config.yaml"
pepfile: "pep/config.yaml" # test overwriting

pepschema: "workflow/schemas/pep.yaml"

rule all:
    input:
        expand("{sample}.txt", sample=pep.sample_table["sample_name"])

rule a:
    output:
        "{sample}.txt"
    shell:
        "touch {output}"



================================================
FILE: tests/test_peppy/expected-results/a.txt
================================================
[Empty file]


================================================
FILE: tests/test_peppy/expected-results/b.txt
================================================
[Empty file]


================================================
FILE: tests/test_peppy/pep/config.yaml
================================================
pep_version: "2.0.0"
sample_table: sample_table.csv



================================================
FILE: tests/test_peppy/pep/sample_table.csv
================================================
sample_name,protocol
a,test
b,test2



================================================
FILE: tests/test_peppy/workflow/schemas/pep.yaml
================================================
description: "Schema for a minimal PEP"
version: "2.0.0"
properties:
  name: 
    type: string
    pattern: "^\\S*$"
    description: "Project name with no whitespace"
  config:
    pep_version:
      description: "Version of the PEP Schema this PEP follows"
      type: string
    sample_table:
      type: string
      description: "Path to the sample annotation table with one row per sample"
    subsample_table:
      type: string
      description: "Path to the subsample annotation table with one row per subsample and sample_name attribute matching an entry in the sample table"
    sample_modifiers:
      type: object
      properties:
        append:
          type: object
        duplicate:
          type: object
        imply:
          type: array
          items:
            type: object
            properties:
              if:
                type: object
              then:
                type: object
        derive:
          type: object
          properties:
            attributes:
              type: array
              items:
                type: string
            sources:
              type: object
    project_modifiers:
      type: object
      properties:
        amend:
          description: "Object overwriting original project attributes"
          type: object
        import:
          description: "List of external PEP project config files to import"
          type: array
          items:
            type: string
    required:
      - pep_version
  samples:
    type: array
    items:
      type: object
      properties:
        sample_name: 
          type: string
          pattern: "^\\S*$"
          description: "Unique name of the sample with no whitespace"
      required:
        - sample_name
required:
  - samples



================================================
FILE: tests/test_pipe_depend/Snakefile
================================================
rule all:
    input:
        "test.txt"


rule a:
    output:
        pipe("test.txt")
    shell:
        "echo test > {output}"




================================================
FILE: tests/test_pipe_depend/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_pipes/producer.py
================================================
[Empty file]


================================================
FILE: tests/test_pipes/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        expand("test.{i}.out", i=range(2))


rule a:
    output:
        pipe("test.{i}.txt")
    shell:
        r"echo -e '{wildcards.i}\n{wildcards.i}\n{wildcards.i}' > {output}"


rule b:
    input:
        "test.{i}.txt"
    output:
        "test.{i}.out"
    shell:
        "grep {wildcards.i} < {input} > {output}"




================================================
FILE: tests/test_pipes2/Snakefile
================================================
shell.executable("bash")

rule all:
   input:
       "foo.txt"

rule one:
   output:
       pipe("pipe.txt")
   shell:
       "echo test > {output}"

rule two:
   input:
       pipe("pipe.txt")
   output:
       "foo.txt"
   shell:
       """
       cat {input} > /dev/null
       sleep 10
       touch {output}
       """



================================================
FILE: tests/test_pipes2/expected-results/foo.txt
================================================
[Empty file]


================================================
FILE: tests/test_pipes_fail/Snakefile
================================================
rule all:
    input:
        expand("test.{i}.out", i=range(2))


rule a:
    output:
        pipe("test.{i}.txt")
    shell:
        # this job fails because of a syntax error
        "for i in {{0..2}}; ddo echo {wildcards.i} >> {output}; done"


rule b:
    input:
        "test.{i}.txt"
    output:
        "test.{i}.out"
    shell:
        "grep {wildcards.i} < {input} > {output}"




================================================
FILE: tests/test_pipes_multiple/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        "test.out"

rule a:
    output:
        pipe("testa.{i}.txt")
    shell:
        "echo {wildcards.i} > {output}"

rule b:
    input:
        rules.a.output
    output:
        pipe("testb.{i}.txt")
    shell:
        "cat {input} > {output}"

rule c:
    input:
        expand(rules.b.output, i=range(2))
    output:
        "test.out"
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_prebuilt_conda_script/env.yaml
================================================
name: test_prebuilt_conda_script
dependencies:
  - pip:
    - "-e dummy_package"



================================================
FILE: tests/test_prebuilt_conda_script/Snakefile
================================================
rule all:
    input:
        o='output.txt'

rule test_prebuilt_conda_script:
    output:
        o='output.txt'
    conda:
        'test_prebuilt_conda_script'
    script:
        'test.py'



================================================
FILE: tests/test_prebuilt_conda_script/test.py
================================================
import dummy

with open('output.txt', 'w') as file:
	file.write("something\n")



================================================
FILE: tests/test_prebuilt_conda_script/dummy_package/pyproject.toml
================================================
[build-system]
requires = [
    "setuptools>=42",
    "wheel"
]
build-backend = "setuptools.build_meta"



================================================
FILE: tests/test_prebuilt_conda_script/dummy_package/setup.cfg
================================================
[metadata]
name = dummy
version = 0.0.0
author = Egor Kosaretskiy
classifiers = 
	Programming Language :: Python :: 3
	License :: OSI Approved :: MIT License
	Operating System :: OS Independent

[options]
package_dir = 
	= src

[options.packages.find]
where = src

[egg_info]
tag_build = 
tag_date = 0




================================================
FILE: tests/test_prebuilt_conda_script/dummy_package/src/dummy/__init__.py
================================================
[Empty file]


================================================
FILE: tests/test_prebuilt_conda_script/dummy_package/src/dummy/dummy.py
================================================
def dummy():
	pass



================================================
FILE: tests/test_prebuilt_conda_script/expected-results/output.txt
================================================
something



================================================
FILE: tests/test_profile/config.yaml
================================================
configfile: "workflow-config.yaml"
cores: all
set-threads:
  - a=max(1024*24*input.size_mb, 2)
  - b=4
default-resources:
  - mem_mb=max(1024*32*input.size_mb, 5)
  - eggs_factor=twitter
set-resources:
  a:
    mem_mb: max(1024*24*input.size_mb, 1)
    spam_factor: X
    double_jeopardy: attempt

groups:
  a: grp1
  b: grp1
  c: grp1

group-components:
  grp1: 5



================================================
FILE: tests/test_profile/input.txt
================================================
[Empty file]


================================================
FILE: tests/test_profile/Snakefile
================================================
rule:
    shell:
        "python -m snakemake --cores 3 --profile . -s Snakefile.internal"



================================================
FILE: tests/test_profile/Snakefile.internal
================================================
shell.executable("bash")

rule a:
    input:
        "input.txt",
    output:
        config["out"],
    shell:
        'echo "'
        'threads: {threads}\n'
        'mem_mb: {resources.mem_mb}\n'
        'eggs_factor: {resources.eggs_factor}\n'
        'spam_factor: {resources.spam_factor}\n'
        'double_jeopardy: {resources.double_jeopardy}'
        '"> {output}'


================================================
FILE: tests/test_profile/workflow-config.yaml
================================================
out: "test.out"



================================================
FILE: tests/test_protected_symlink_output/Snakefile
================================================
# vim: ft=python
"""Snakemake will not delete write-protected files. But a symlink to a
   write-protected file is another matter. We should be able to overwrite
   those.
"""

"""Description of the test:

    protected1 and protected2 are non-writeable files
    outlink is a symlink to protected1
    after the main rule runs, outlink should point to
    protected2, and outfile should contain "contents2".
"""

import os
import time

# Set up the files just once
if not os.path.exists("outlink"):

    shell("echo contents1 > protected1")
    shell("echo contents2 > protected2")
    shell("ln -s protected1 outlink")

    shell("chmod a-w protected1 protected2")

    # Print the result for debugging
    shell("ls -lR > /dev/stderr")

rule main:
    output: "outfile", "outlink"
    run:
        # outlink should be gone but not protected1 or protected2
        shell("test ! -e outlink")
        shell("test -e protected1 && test -e protected2")
        # Re-point the symlink
        shell("ln -s protected2 outlink")
        shell("cat outlink > outfile")



================================================
FILE: tests/test_protected_symlink_output/expected-results/outfile
================================================
contents2



================================================
FILE: tests/test_protected_symlink_output/expected-results/outlink
================================================
contents2



================================================
FILE: tests/test_queue_input/Snakefile
================================================
import threading, queue, time


finish_sentinel = object()
all_results = queue.Queue()


def update_results():
    try:
        for i in range(20):
            all_results.put(f"test{i}.txt")
            time.sleep(1)
        all_results.put(finish_sentinel)
        all_results.join()
    except (KeyboardInterrupt, SystemExit):
        return

update_thread = threading.Thread(target=update_results)
update_thread.start()


rule all:
    input:
        from_queue(all_results, finish_sentinel=finish_sentinel)


rule generate:
    output:
        "test{i}.txt"
    shell:
        "echo {wildcards.i} > {output}"



================================================
FILE: tests/test_queue_input/expected-results/test0.txt
================================================
0



================================================
FILE: tests/test_queue_input/expected-results/test1.txt
================================================
1



================================================
FILE: tests/test_queue_input/expected-results/test10.txt
================================================
10



================================================
FILE: tests/test_queue_input/expected-results/test11.txt
================================================
11



================================================
FILE: tests/test_queue_input/expected-results/test12.txt
================================================
12



================================================
FILE: tests/test_queue_input/expected-results/test13.txt
================================================
13



================================================
FILE: tests/test_queue_input/expected-results/test14.txt
================================================
14



================================================
FILE: tests/test_queue_input/expected-results/test15.txt
================================================
15



================================================
FILE: tests/test_queue_input/expected-results/test16.txt
================================================
16



================================================
FILE: tests/test_queue_input/expected-results/test17.txt
================================================
17



================================================
FILE: tests/test_queue_input/expected-results/test18.txt
================================================
18



================================================
FILE: tests/test_queue_input/expected-results/test19.txt
================================================
19



================================================
FILE: tests/test_queue_input/expected-results/test2.txt
================================================
2



================================================
FILE: tests/test_queue_input/expected-results/test3.txt
================================================
3



================================================
FILE: tests/test_queue_input/expected-results/test4.txt
================================================
4



================================================
FILE: tests/test_queue_input/expected-results/test5.txt
================================================
5



================================================
FILE: tests/test_queue_input/expected-results/test6.txt
================================================
6



================================================
FILE: tests/test_queue_input/expected-results/test7.txt
================================================
7



================================================
FILE: tests/test_queue_input/expected-results/test8.txt
================================================
8



================================================
FILE: tests/test_queue_input/expected-results/test9.txt
================================================
9



================================================
FILE: tests/test_r_wrapper/environment.yaml
================================================
channels:
  - conda-forge
dependencies: []



================================================
FILE: tests/test_r_wrapper/Snakefile
================================================
rule r_wrapper:
    output:
        "deleteme"
    wrapper:
        "file://tests/test_r_wrapper"



================================================
FILE: tests/test_r_wrapper/wrapper.R
================================================
outfile = snakemake@output[[1]]
x <- data.frame()
write.table(x, file=outfile, col.names=FALSE)



================================================
FILE: tests/test_remote_auto/Snakefile
================================================
#import re, os, sys
shell.executable("bash")

from snakemake.remote import AUTO

rule all:
    input:
        AUTO.remote("https://github.com/snakemake/snakemake/raw/main/images/logo.png")
    run:
        shell("cp {input} ./")



================================================
FILE: tests/test_remote_azure/README.md
================================================
# Instruction for testing of Azure Storage integration

In order to perform this test, you need an Azure Storage account
with read/write access.
Both the storage account and associated key or SAS token (without
leading questionmark) need to be
passed to snakemake at runtime, by exporting
environment variables `AZURE_ACCOUNT` and either `AZURE_KEY` or
`SAS_TOKEN`.

Furthermore, in the storage account, a container "snakemake-test"
needs to be created prior to running the test.

And lastly, a local file called `test.txt.gz` needs to be created.




================================================
FILE: tests/test_remote_azure/Snakefile
================================================
import os
import fnmatch
import snakemake
from snakemake.exceptions import MissingInputException
from snakemake.remote.AzBlob import RemoteProvider as AzureRemoteProvider

# setup Azure Storage for remote access
# for testing, set AZ_AZURE_ACCOUNT and AZ_ACCOUNT_KEY or AZ_SAS_TOKEN as env vars to CircleCI
AS = AzureRemoteProvider()


rule upload_to_azure_storage:
    input:
        "test.txt.gz"
    output:
        AS.remote("snakemake-test/data_upload/test.txt.gz")
    run:
        shell("cp {input} {output}")

rule download_from_azure_storage:
    input:
        AS.remote("snakemake-test/data_upload/test.txt.gz")
    output:
        "test.txt.gz"
    run:
        shell("cp {input} {output}")

rule test_globbing:
    input:
        AS.remote("snakemake-test/data_upload/test.txt.gz")
    output:
        touch("globbing.done")
    run:
        basenames, = AS.glob_wildcards("snakemake-test/data_upload/{base}.txt.gz")
        assert "test" in basenames

rule test_download:
    input:
        "globbing.done",
        "test.txt.gz"

rule test_upload:
    input:
        AS.remote("snakemake-test/data_upload/test.txt.gz")



================================================
FILE: tests/test_remote_gs/landsat-data.txt
================================================
GROUP = L1_METADATA_FILE
  GROUP = METADATA_FILE_INFO
    ORIGIN = "Image courtesy of the U.S. Geological Survey"
    REQUEST_ID = "0501705013406_00001"
    LANDSAT_SCENE_ID = "LC80010032017120LGN00"
    LANDSAT_PRODUCT_ID = "LC08_L1GT_001003_20170430_20170501_01_RT"
    COLLECTION_NUMBER = 01
    FILE_DATE = 2017-05-01T16:00:24Z
    STATION_ID = "LGN"
    PROCESSING_SOFTWARE_VERSION = "LPGS_2.7.0"
  END_GROUP = METADATA_FILE_INFO
  GROUP = PRODUCT_METADATA
    DATA_TYPE = "L1GT"
    COLLECTION_CATEGORY = "RT"
    ELEVATION_SOURCE = "GLS2000"
    OUTPUT_FORMAT = "GEOTIFF"
    SPACECRAFT_ID = "LANDSAT_8"
    SENSOR_ID = "OLI_TIRS"
    WRS_PATH = 1
    WRS_ROW = 3
    NADIR_OFFNADIR = "NADIR"
    TARGET_WRS_PATH = 1
    TARGET_WRS_ROW = 3
    DATE_ACQUIRED = 2017-04-30
    SCENE_CENTER_TIME = "14:07:16.5180850Z"
    CORNER_UL_LAT_PRODUCT = 80.21528
    CORNER_UL_LON_PRODUCT = -17.96312
    CORNER_UR_LAT_PRODUCT = 80.28798
    CORNER_UR_LON_PRODUCT = -3.45901
    CORNER_LL_LAT_PRODUCT = 77.79053
    CORNER_LL_LON_PRODUCT = -16.19251
    CORNER_LR_LAT_PRODUCT = 77.84841
    CORNER_LR_LON_PRODUCT = -4.56164
    CORNER_UL_PROJECTION_X_PRODUCT = 330600.000
    CORNER_UL_PROJECTION_Y_PRODUCT = 8918700.000
    CORNER_UR_PROJECTION_X_PRODUCT = 604200.000
    CORNER_UR_PROJECTION_Y_PRODUCT = 8918700.000
    CORNER_LL_PROJECTION_X_PRODUCT = 330600.000
    CORNER_LL_PROJECTION_Y_PRODUCT = 8645400.000
    CORNER_LR_PROJECTION_X_PRODUCT = 604200.000
    CORNER_LR_PROJECTION_Y_PRODUCT = 8645400.000
    PANCHROMATIC_LINES = 18221
    PANCHROMATIC_SAMPLES = 18241
    REFLECTIVE_LINES = 9111
    REFLECTIVE_SAMPLES = 9121
    THERMAL_LINES = 9111
    THERMAL_SAMPLES = 9121
    FILE_NAME_BAND_1 = "LC08_L1GT_001003_20170430_20170501_01_RT_B1.TIF"
    FILE_NAME_BAND_2 = "LC08_L1GT_001003_20170430_20170501_01_RT_B2.TIF"
    FILE_NAME_BAND_3 = "LC08_L1GT_001003_20170430_20170501_01_RT_B3.TIF"
    FILE_NAME_BAND_4 = "LC08_L1GT_001003_20170430_20170501_01_RT_B4.TIF"
    FILE_NAME_BAND_5 = "LC08_L1GT_001003_20170430_20170501_01_RT_B5.TIF"
    FILE_NAME_BAND_6 = "LC08_L1GT_001003_20170430_20170501_01_RT_B6.TIF"
    FILE_NAME_BAND_7 = "LC08_L1GT_001003_20170430_20170501_01_RT_B7.TIF"
    FILE_NAME_BAND_8 = "LC08_L1GT_001003_20170430_20170501_01_RT_B8.TIF"
    FILE_NAME_BAND_9 = "LC08_L1GT_001003_20170430_20170501_01_RT_B9.TIF"
    FILE_NAME_BAND_10 = "LC08_L1GT_001003_20170430_20170501_01_RT_B10.TIF"
    FILE_NAME_BAND_11 = "LC08_L1GT_001003_20170430_20170501_01_RT_B11.TIF"
    FILE_NAME_BAND_QUALITY = "LC08_L1GT_001003_20170430_20170501_01_RT_BQA.TIF"
    ANGLE_COEFFICIENT_FILE_NAME = "LC08_L1GT_001003_20170430_20170501_01_RT_ANG.txt"
    METADATA_FILE_NAME = "LC08_L1GT_001003_20170430_20170501_01_RT_MTL.txt"
    CPF_NAME = "LC08CPF_20170401_20170630_01.02"
    BPF_NAME_OLI = "LO8BPF20170430140614_20170430144404.01"
    BPF_NAME_TIRS = "LT8BPF20170426235522_20170427000359.01"
    RLUT_FILE_NAME = "LC08RLUT_20150303_20431231_01_12.h5"
  END_GROUP = PRODUCT_METADATA
  GROUP = IMAGE_ATTRIBUTES
    CLOUD_COVER = 24.32
    CLOUD_COVER_LAND = -1
    IMAGE_QUALITY_OLI = 9
    IMAGE_QUALITY_TIRS = 7
    TIRS_SSM_MODEL = "PRELIMINARY"
    TIRS_SSM_POSITION_STATUS = "ESTIMATED"
    TIRS_STRAY_LIGHT_CORRECTION_SOURCE = "TIRS"
    ROLL_ANGLE = -0.001
    SUN_AZIMUTH = -156.47580997
    SUN_ELEVATION = 24.99099132
    EARTH_SUN_DISTANCE = 1.0074392
    SATURATION_BAND_1 = "N"
    SATURATION_BAND_2 = "N"
    SATURATION_BAND_3 = "N"
    SATURATION_BAND_4 = "N"
    SATURATION_BAND_5 = "N"
    SATURATION_BAND_6 = "N"
    SATURATION_BAND_7 = "N"
    SATURATION_BAND_8 = "N"
    SATURATION_BAND_9 = "N"
    TRUNCATION_OLI = "UPPER"
  END_GROUP = IMAGE_ATTRIBUTES
  GROUP = MIN_MAX_RADIANCE
    RADIANCE_MAXIMUM_BAND_1 = 748.87909
    RADIANCE_MINIMUM_BAND_1 = -61.84268
    RADIANCE_MAXIMUM_BAND_2 = 766.86133
    RADIANCE_MINIMUM_BAND_2 = -63.32766
    RADIANCE_MAXIMUM_BAND_3 = 706.65613
    RADIANCE_MINIMUM_BAND_3 = -58.35589
    RADIANCE_MAXIMUM_BAND_4 = 595.89227
    RADIANCE_MINIMUM_BAND_4 = -49.20898
    RADIANCE_MAXIMUM_BAND_5 = 364.65634
    RADIANCE_MINIMUM_BAND_5 = -30.11344
    RADIANCE_MAXIMUM_BAND_6 = 90.68671
    RADIANCE_MINIMUM_BAND_6 = -7.48894
    RADIANCE_MAXIMUM_BAND_7 = 30.56628
    RADIANCE_MINIMUM_BAND_7 = -2.52417
    RADIANCE_MAXIMUM_BAND_8 = 674.38605
    RADIANCE_MINIMUM_BAND_8 = -55.69102
    RADIANCE_MAXIMUM_BAND_9 = 142.51598
    RADIANCE_MINIMUM_BAND_9 = -11.76902
    RADIANCE_MAXIMUM_BAND_10 = 22.00180
    RADIANCE_MINIMUM_BAND_10 = 0.10033
    RADIANCE_MAXIMUM_BAND_11 = 22.00180
    RADIANCE_MINIMUM_BAND_11 = 0.10033
  END_GROUP = MIN_MAX_RADIANCE
  GROUP = MIN_MAX_REFLECTANCE
    REFLECTANCE_MAXIMUM_BAND_1 = 1.210700
    REFLECTANCE_MINIMUM_BAND_1 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_2 = 1.210700
    REFLECTANCE_MINIMUM_BAND_2 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_3 = 1.210700
    REFLECTANCE_MINIMUM_BAND_3 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_4 = 1.210700
    REFLECTANCE_MINIMUM_BAND_4 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_5 = 1.210700
    REFLECTANCE_MINIMUM_BAND_5 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_6 = 1.210700
    REFLECTANCE_MINIMUM_BAND_6 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_7 = 1.210700
    REFLECTANCE_MINIMUM_BAND_7 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_8 = 1.210700
    REFLECTANCE_MINIMUM_BAND_8 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_9 = 1.210700
    REFLECTANCE_MINIMUM_BAND_9 = -0.099980
  END_GROUP = MIN_MAX_REFLECTANCE
  GROUP = MIN_MAX_PIXEL_VALUE
    QUANTIZE_CAL_MAX_BAND_1 = 65535
    QUANTIZE_CAL_MIN_BAND_1 = 1
    QUANTIZE_CAL_MAX_BAND_2 = 65535
    QUANTIZE_CAL_MIN_BAND_2 = 1
    QUANTIZE_CAL_MAX_BAND_3 = 65535
    QUANTIZE_CAL_MIN_BAND_3 = 1
    QUANTIZE_CAL_MAX_BAND_4 = 65535
    QUANTIZE_CAL_MIN_BAND_4 = 1
    QUANTIZE_CAL_MAX_BAND_5 = 65535
    QUANTIZE_CAL_MIN_BAND_5 = 1
    QUANTIZE_CAL_MAX_BAND_6 = 65535
    QUANTIZE_CAL_MIN_BAND_6 = 1
    QUANTIZE_CAL_MAX_BAND_7 = 65535
    QUANTIZE_CAL_MIN_BAND_7 = 1
    QUANTIZE_CAL_MAX_BAND_8 = 65535
    QUANTIZE_CAL_MIN_BAND_8 = 1
    QUANTIZE_CAL_MAX_BAND_9 = 65535
    QUANTIZE_CAL_MIN_BAND_9 = 1
    QUANTIZE_CAL_MAX_BAND_10 = 65535
    QUANTIZE_CAL_MIN_BAND_10 = 1
    QUANTIZE_CAL_MAX_BAND_11 = 65535
    QUANTIZE_CAL_MIN_BAND_11 = 1
  END_GROUP = MIN_MAX_PIXEL_VALUE
  GROUP = RADIOMETRIC_RESCALING
    RADIANCE_MULT_BAND_1 = 1.2371E-02
    RADIANCE_MULT_BAND_2 = 1.2668E-02
    RADIANCE_MULT_BAND_3 = 1.1674E-02
    RADIANCE_MULT_BAND_4 = 9.8438E-03
    RADIANCE_MULT_BAND_5 = 6.0239E-03
    RADIANCE_MULT_BAND_6 = 1.4981E-03
    RADIANCE_MULT_BAND_7 = 5.0494E-04
    RADIANCE_MULT_BAND_8 = 1.1140E-02
    RADIANCE_MULT_BAND_9 = 2.3543E-03
    RADIANCE_MULT_BAND_10 = 3.3420E-04
    RADIANCE_MULT_BAND_11 = 3.3420E-04
    RADIANCE_ADD_BAND_1 = -61.85505
    RADIANCE_ADD_BAND_2 = -63.34032
    RADIANCE_ADD_BAND_3 = -58.36757
    RADIANCE_ADD_BAND_4 = -49.21882
    RADIANCE_ADD_BAND_5 = -30.11946
    RADIANCE_ADD_BAND_6 = -7.49044
    RADIANCE_ADD_BAND_7 = -2.52468
    RADIANCE_ADD_BAND_8 = -55.70216
    RADIANCE_ADD_BAND_9 = -11.77137
    RADIANCE_ADD_BAND_10 = 0.10000
    RADIANCE_ADD_BAND_11 = 0.10000
    REFLECTANCE_MULT_BAND_1 = 2.0000E-05
    REFLECTANCE_MULT_BAND_2 = 2.0000E-05
    REFLECTANCE_MULT_BAND_3 = 2.0000E-05
    REFLECTANCE_MULT_BAND_4 = 2.0000E-05
    REFLECTANCE_MULT_BAND_5 = 2.0000E-05
    REFLECTANCE_MULT_BAND_6 = 2.0000E-05
    REFLECTANCE_MULT_BAND_7 = 2.0000E-05
    REFLECTANCE_MULT_BAND_8 = 2.0000E-05
    REFLECTANCE_MULT_BAND_9 = 2.0000E-05
    REFLECTANCE_ADD_BAND_1 = -0.100000
    REFLECTANCE_ADD_BAND_2 = -0.100000
    REFLECTANCE_ADD_BAND_3 = -0.100000
    REFLECTANCE_ADD_BAND_4 = -0.100000
    REFLECTANCE_ADD_BAND_5 = -0.100000
    REFLECTANCE_ADD_BAND_6 = -0.100000
    REFLECTANCE_ADD_BAND_7 = -0.100000
    REFLECTANCE_ADD_BAND_8 = -0.100000
    REFLECTANCE_ADD_BAND_9 = -0.100000
  END_GROUP = RADIOMETRIC_RESCALING
  GROUP = TIRS_THERMAL_CONSTANTS
    K1_CONSTANT_BAND_10 = 774.8853
    K2_CONSTANT_BAND_10 = 1321.0789
    K1_CONSTANT_BAND_11 = 480.8883
    K2_CONSTANT_BAND_11 = 1201.1442
  END_GROUP = TIRS_THERMAL_CONSTANTS
  GROUP = PROJECTION_PARAMETERS
    MAP_PROJECTION = "UTM"
    DATUM = "WGS84"
    ELLIPSOID = "WGS84"
    UTM_ZONE = 29
    GRID_CELL_SIZE_PANCHROMATIC = 15.00
    GRID_CELL_SIZE_REFLECTIVE = 30.00
    GRID_CELL_SIZE_THERMAL = 30.00
    ORIENTATION = "NORTH_UP"
    RESAMPLING_OPTION = "CUBIC_CONVOLUTION"
  END_GROUP = PROJECTION_PARAMETERS
END_GROUP = L1_METADATA_FILE
END



================================================
FILE: tests/test_remote_gs/Snakefile
================================================
from snakemake.remote import GS
import google.auth
try:
    GS = GS.RemoteProvider()

    rule copy:
        input:
            GS.remote("gcp-public-data-landsat/LC08/01/001/003/LC08_L1GT_001003_20170430_20170501_01_RT/LC08_L1GT_001003_20170430_20170501_01_RT_MTL.txt")
        output:
            "landsat-data.txt"
        shell:
            "cp {input} {output}"
except google.auth.exceptions.DefaultCredentialsError:
    # ignore the test if not authenticated
    print("skipping test_remote_gs because we are not authenticated with gcloud")



================================================
FILE: tests/test_remote_gs/expected-results/landsat-data.txt
================================================
GROUP = L1_METADATA_FILE
  GROUP = METADATA_FILE_INFO
    ORIGIN = "Image courtesy of the U.S. Geological Survey"
    REQUEST_ID = "0501705013406_00001"
    LANDSAT_SCENE_ID = "LC80010032017120LGN00"
    LANDSAT_PRODUCT_ID = "LC08_L1GT_001003_20170430_20170501_01_RT"
    COLLECTION_NUMBER = 01
    FILE_DATE = 2017-05-01T16:00:24Z
    STATION_ID = "LGN"
    PROCESSING_SOFTWARE_VERSION = "LPGS_2.7.0"
  END_GROUP = METADATA_FILE_INFO
  GROUP = PRODUCT_METADATA
    DATA_TYPE = "L1GT"
    COLLECTION_CATEGORY = "RT"
    ELEVATION_SOURCE = "GLS2000"
    OUTPUT_FORMAT = "GEOTIFF"
    SPACECRAFT_ID = "LANDSAT_8"
    SENSOR_ID = "OLI_TIRS"
    WRS_PATH = 1
    WRS_ROW = 3
    NADIR_OFFNADIR = "NADIR"
    TARGET_WRS_PATH = 1
    TARGET_WRS_ROW = 3
    DATE_ACQUIRED = 2017-04-30
    SCENE_CENTER_TIME = "14:07:16.5180850Z"
    CORNER_UL_LAT_PRODUCT = 80.21528
    CORNER_UL_LON_PRODUCT = -17.96312
    CORNER_UR_LAT_PRODUCT = 80.28798
    CORNER_UR_LON_PRODUCT = -3.45901
    CORNER_LL_LAT_PRODUCT = 77.79053
    CORNER_LL_LON_PRODUCT = -16.19251
    CORNER_LR_LAT_PRODUCT = 77.84841
    CORNER_LR_LON_PRODUCT = -4.56164
    CORNER_UL_PROJECTION_X_PRODUCT = 330600.000
    CORNER_UL_PROJECTION_Y_PRODUCT = 8918700.000
    CORNER_UR_PROJECTION_X_PRODUCT = 604200.000
    CORNER_UR_PROJECTION_Y_PRODUCT = 8918700.000
    CORNER_LL_PROJECTION_X_PRODUCT = 330600.000
    CORNER_LL_PROJECTION_Y_PRODUCT = 8645400.000
    CORNER_LR_PROJECTION_X_PRODUCT = 604200.000
    CORNER_LR_PROJECTION_Y_PRODUCT = 8645400.000
    PANCHROMATIC_LINES = 18221
    PANCHROMATIC_SAMPLES = 18241
    REFLECTIVE_LINES = 9111
    REFLECTIVE_SAMPLES = 9121
    THERMAL_LINES = 9111
    THERMAL_SAMPLES = 9121
    FILE_NAME_BAND_1 = "LC08_L1GT_001003_20170430_20170501_01_RT_B1.TIF"
    FILE_NAME_BAND_2 = "LC08_L1GT_001003_20170430_20170501_01_RT_B2.TIF"
    FILE_NAME_BAND_3 = "LC08_L1GT_001003_20170430_20170501_01_RT_B3.TIF"
    FILE_NAME_BAND_4 = "LC08_L1GT_001003_20170430_20170501_01_RT_B4.TIF"
    FILE_NAME_BAND_5 = "LC08_L1GT_001003_20170430_20170501_01_RT_B5.TIF"
    FILE_NAME_BAND_6 = "LC08_L1GT_001003_20170430_20170501_01_RT_B6.TIF"
    FILE_NAME_BAND_7 = "LC08_L1GT_001003_20170430_20170501_01_RT_B7.TIF"
    FILE_NAME_BAND_8 = "LC08_L1GT_001003_20170430_20170501_01_RT_B8.TIF"
    FILE_NAME_BAND_9 = "LC08_L1GT_001003_20170430_20170501_01_RT_B9.TIF"
    FILE_NAME_BAND_10 = "LC08_L1GT_001003_20170430_20170501_01_RT_B10.TIF"
    FILE_NAME_BAND_11 = "LC08_L1GT_001003_20170430_20170501_01_RT_B11.TIF"
    FILE_NAME_BAND_QUALITY = "LC08_L1GT_001003_20170430_20170501_01_RT_BQA.TIF"
    ANGLE_COEFFICIENT_FILE_NAME = "LC08_L1GT_001003_20170430_20170501_01_RT_ANG.txt"
    METADATA_FILE_NAME = "LC08_L1GT_001003_20170430_20170501_01_RT_MTL.txt"
    CPF_NAME = "LC08CPF_20170401_20170630_01.02"
    BPF_NAME_OLI = "LO8BPF20170430140614_20170430144404.01"
    BPF_NAME_TIRS = "LT8BPF20170426235522_20170427000359.01"
    RLUT_FILE_NAME = "LC08RLUT_20150303_20431231_01_12.h5"
  END_GROUP = PRODUCT_METADATA
  GROUP = IMAGE_ATTRIBUTES
    CLOUD_COVER = 24.32
    CLOUD_COVER_LAND = -1
    IMAGE_QUALITY_OLI = 9
    IMAGE_QUALITY_TIRS = 7
    TIRS_SSM_MODEL = "PRELIMINARY"
    TIRS_SSM_POSITION_STATUS = "ESTIMATED"
    TIRS_STRAY_LIGHT_CORRECTION_SOURCE = "TIRS"
    ROLL_ANGLE = -0.001
    SUN_AZIMUTH = -156.47580997
    SUN_ELEVATION = 24.99099132
    EARTH_SUN_DISTANCE = 1.0074392
    SATURATION_BAND_1 = "N"
    SATURATION_BAND_2 = "N"
    SATURATION_BAND_3 = "N"
    SATURATION_BAND_4 = "N"
    SATURATION_BAND_5 = "N"
    SATURATION_BAND_6 = "N"
    SATURATION_BAND_7 = "N"
    SATURATION_BAND_8 = "N"
    SATURATION_BAND_9 = "N"
    TRUNCATION_OLI = "UPPER"
  END_GROUP = IMAGE_ATTRIBUTES
  GROUP = MIN_MAX_RADIANCE
    RADIANCE_MAXIMUM_BAND_1 = 748.87909
    RADIANCE_MINIMUM_BAND_1 = -61.84268
    RADIANCE_MAXIMUM_BAND_2 = 766.86133
    RADIANCE_MINIMUM_BAND_2 = -63.32766
    RADIANCE_MAXIMUM_BAND_3 = 706.65613
    RADIANCE_MINIMUM_BAND_3 = -58.35589
    RADIANCE_MAXIMUM_BAND_4 = 595.89227
    RADIANCE_MINIMUM_BAND_4 = -49.20898
    RADIANCE_MAXIMUM_BAND_5 = 364.65634
    RADIANCE_MINIMUM_BAND_5 = -30.11344
    RADIANCE_MAXIMUM_BAND_6 = 90.68671
    RADIANCE_MINIMUM_BAND_6 = -7.48894
    RADIANCE_MAXIMUM_BAND_7 = 30.56628
    RADIANCE_MINIMUM_BAND_7 = -2.52417
    RADIANCE_MAXIMUM_BAND_8 = 674.38605
    RADIANCE_MINIMUM_BAND_8 = -55.69102
    RADIANCE_MAXIMUM_BAND_9 = 142.51598
    RADIANCE_MINIMUM_BAND_9 = -11.76902
    RADIANCE_MAXIMUM_BAND_10 = 22.00180
    RADIANCE_MINIMUM_BAND_10 = 0.10033
    RADIANCE_MAXIMUM_BAND_11 = 22.00180
    RADIANCE_MINIMUM_BAND_11 = 0.10033
  END_GROUP = MIN_MAX_RADIANCE
  GROUP = MIN_MAX_REFLECTANCE
    REFLECTANCE_MAXIMUM_BAND_1 = 1.210700
    REFLECTANCE_MINIMUM_BAND_1 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_2 = 1.210700
    REFLECTANCE_MINIMUM_BAND_2 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_3 = 1.210700
    REFLECTANCE_MINIMUM_BAND_3 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_4 = 1.210700
    REFLECTANCE_MINIMUM_BAND_4 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_5 = 1.210700
    REFLECTANCE_MINIMUM_BAND_5 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_6 = 1.210700
    REFLECTANCE_MINIMUM_BAND_6 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_7 = 1.210700
    REFLECTANCE_MINIMUM_BAND_7 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_8 = 1.210700
    REFLECTANCE_MINIMUM_BAND_8 = -0.099980
    REFLECTANCE_MAXIMUM_BAND_9 = 1.210700
    REFLECTANCE_MINIMUM_BAND_9 = -0.099980
  END_GROUP = MIN_MAX_REFLECTANCE
  GROUP = MIN_MAX_PIXEL_VALUE
    QUANTIZE_CAL_MAX_BAND_1 = 65535
    QUANTIZE_CAL_MIN_BAND_1 = 1
    QUANTIZE_CAL_MAX_BAND_2 = 65535
    QUANTIZE_CAL_MIN_BAND_2 = 1
    QUANTIZE_CAL_MAX_BAND_3 = 65535
    QUANTIZE_CAL_MIN_BAND_3 = 1
    QUANTIZE_CAL_MAX_BAND_4 = 65535
    QUANTIZE_CAL_MIN_BAND_4 = 1
    QUANTIZE_CAL_MAX_BAND_5 = 65535
    QUANTIZE_CAL_MIN_BAND_5 = 1
    QUANTIZE_CAL_MAX_BAND_6 = 65535
    QUANTIZE_CAL_MIN_BAND_6 = 1
    QUANTIZE_CAL_MAX_BAND_7 = 65535
    QUANTIZE_CAL_MIN_BAND_7 = 1
    QUANTIZE_CAL_MAX_BAND_8 = 65535
    QUANTIZE_CAL_MIN_BAND_8 = 1
    QUANTIZE_CAL_MAX_BAND_9 = 65535
    QUANTIZE_CAL_MIN_BAND_9 = 1
    QUANTIZE_CAL_MAX_BAND_10 = 65535
    QUANTIZE_CAL_MIN_BAND_10 = 1
    QUANTIZE_CAL_MAX_BAND_11 = 65535
    QUANTIZE_CAL_MIN_BAND_11 = 1
  END_GROUP = MIN_MAX_PIXEL_VALUE
  GROUP = RADIOMETRIC_RESCALING
    RADIANCE_MULT_BAND_1 = 1.2371E-02
    RADIANCE_MULT_BAND_2 = 1.2668E-02
    RADIANCE_MULT_BAND_3 = 1.1674E-02
    RADIANCE_MULT_BAND_4 = 9.8438E-03
    RADIANCE_MULT_BAND_5 = 6.0239E-03
    RADIANCE_MULT_BAND_6 = 1.4981E-03
    RADIANCE_MULT_BAND_7 = 5.0494E-04
    RADIANCE_MULT_BAND_8 = 1.1140E-02
    RADIANCE_MULT_BAND_9 = 2.3543E-03
    RADIANCE_MULT_BAND_10 = 3.3420E-04
    RADIANCE_MULT_BAND_11 = 3.3420E-04
    RADIANCE_ADD_BAND_1 = -61.85505
    RADIANCE_ADD_BAND_2 = -63.34032
    RADIANCE_ADD_BAND_3 = -58.36757
    RADIANCE_ADD_BAND_4 = -49.21882
    RADIANCE_ADD_BAND_5 = -30.11946
    RADIANCE_ADD_BAND_6 = -7.49044
    RADIANCE_ADD_BAND_7 = -2.52468
    RADIANCE_ADD_BAND_8 = -55.70216
    RADIANCE_ADD_BAND_9 = -11.77137
    RADIANCE_ADD_BAND_10 = 0.10000
    RADIANCE_ADD_BAND_11 = 0.10000
    REFLECTANCE_MULT_BAND_1 = 2.0000E-05
    REFLECTANCE_MULT_BAND_2 = 2.0000E-05
    REFLECTANCE_MULT_BAND_3 = 2.0000E-05
    REFLECTANCE_MULT_BAND_4 = 2.0000E-05
    REFLECTANCE_MULT_BAND_5 = 2.0000E-05
    REFLECTANCE_MULT_BAND_6 = 2.0000E-05
    REFLECTANCE_MULT_BAND_7 = 2.0000E-05
    REFLECTANCE_MULT_BAND_8 = 2.0000E-05
    REFLECTANCE_MULT_BAND_9 = 2.0000E-05
    REFLECTANCE_ADD_BAND_1 = -0.100000
    REFLECTANCE_ADD_BAND_2 = -0.100000
    REFLECTANCE_ADD_BAND_3 = -0.100000
    REFLECTANCE_ADD_BAND_4 = -0.100000
    REFLECTANCE_ADD_BAND_5 = -0.100000
    REFLECTANCE_ADD_BAND_6 = -0.100000
    REFLECTANCE_ADD_BAND_7 = -0.100000
    REFLECTANCE_ADD_BAND_8 = -0.100000
    REFLECTANCE_ADD_BAND_9 = -0.100000
  END_GROUP = RADIOMETRIC_RESCALING
  GROUP = TIRS_THERMAL_CONSTANTS
    K1_CONSTANT_BAND_10 = 774.8853
    K2_CONSTANT_BAND_10 = 1321.0789
    K1_CONSTANT_BAND_11 = 480.8883
    K2_CONSTANT_BAND_11 = 1201.1442
  END_GROUP = TIRS_THERMAL_CONSTANTS
  GROUP = PROJECTION_PARAMETERS
    MAP_PROJECTION = "UTM"
    DATUM = "WGS84"
    ELLIPSOID = "WGS84"
    UTM_ZONE = 29
    GRID_CELL_SIZE_PANCHROMATIC = 15.00
    GRID_CELL_SIZE_REFLECTIVE = 30.00
    GRID_CELL_SIZE_THERMAL = 30.00
    ORIENTATION = "NORTH_UP"
    RESAMPLING_OPTION = "CUBIC_CONVOLUTION"
  END_GROUP = PROJECTION_PARAMETERS
END_GROUP = L1_METADATA_FILE
END



================================================
FILE: tests/test_remote_irods/README.md
================================================
# Test Remote iRODS

These are the instructions for testing Snakemake remote iRODS support locally
on your computer.

## Prerequisites

1. The latest Docker installation (a.k.a. not the one from the Ubuntu 16.04
repository). Follow the instructions on
https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/
2. The irods icommand tools as described in https://packages.irods.org/ to
setup the repository and `apt-get install irods-icommands` for installation.
The password file has to be generated with `iinit`. A valid environment file
is located in `setup-data/irods_environment.json` (in this case the
authentication file is expected in `~/.irods/.irodsA`). This is necessary,
because the obfuscation of the password uses the uid, so the `.irodsA` file
can't be shipped.
3. Snakemake

## Build and run the Docker container

```
make run
```

## Stop and delete the Docker container

```
make stop
```

## Run the test

```
snakemake -s Snakefile.local
```

## Touch the input file (for a new test)

```
make touch
```

## Show iRODS content

```
make ls
```

## Example

```
make run
make ls
snakemake -s Snakefile.local
make ls
snakemake -s Snakefile.local
# nothing to do here
make touch
snakemake -s Snakefile.local
```



================================================
FILE: tests/test_remote_irods/Dockerfile
================================================
FROM "mjstealey/irods-provider-postgres:4.2.2"

COPY test-data/infile /incoming/infile



================================================
FILE: tests/test_remote_irods/Makefile
================================================
.PHONY: run build touch stop test

build:
	docker build -t irods-server .

run: build
	docker run \
		-d \
		-p 1247:1247 \
		--name provider \
		irods-server \
			-i run_irods
	sleep 10
	docker exec -u irods provider \
		iput /incoming/infile

touch:
	docker exec -u irods provider \
		iput /incoming/infile -f

stop:
	docker ps -a \
	| grep provider \
	| cut -d ' ' -f 1 \
	| xargs -I{} \
		bash -c "docker stop {}; docker rm {}"

ls:
	docker exec -u irods provider ils -l



================================================
FILE: tests/test_remote_irods/Snakefile
================================================
from snakemake.remote.iRODS import RemoteProvider

irods = RemoteProvider(timezone="Europe/Berlin")

rule all:
    input:
        'outfile',
        irods.remote("home/rods/out/subfolder/outfile")

rule step_one:
    input:
        irods.remote('home/rods/infile')
    output:
        irods.remote('home/rods/tmp/intermediate')
    shell:
        r"""
        cp {input} {output}
        """

rule step_two:
    input:
        irods.remote('home/rods/tmp/intermediate')
    output:
        local='outfile',
        remote=irods.remote('home/rods/out/subfolder/outfile')
    shell:
        r"""
        cp {input} {output.local}
        cp {input} {output.remote}
        """




================================================
FILE: tests/test_remote_irods/Snakefile.local
================================================
from snakemake.remote.iRODS import RemoteProvider

irods = RemoteProvider(timezone="Europe/Berlin",
                       irods_env_file="setup-data/irods_environment.json")

rule all:
    input:
        'outfile',
        irods.remote("home/rods/out/subfolder/outfile")

rule step_one:
    input:
        irods.remote('home/rods/infile')
    output:
        irods.remote('home/rods/tmp/intermediate')
    shell:
        r"""
        cp {input} {output}
        """

rule step_two:
    input:
        irods.remote('home/rods/tmp/intermediate')
    output:
        local='outfile',
        remote=irods.remote('home/rods/out/subfolder/outfile')
    shell:
        r"""
        cp {input} {output.local}
        cp {input} {output.remote}
        """




================================================
FILE: tests/test_remote_irods/expected-results/outfile
================================================
test



================================================
FILE: tests/test_remote_irods/setup-data/irods_environment.json
================================================
{
    "irods_host": "localhost",
    "irods_port": 1247,
    "irods_user_name": "rods",
    "irods_zone_name": "tempZone"
}



================================================
FILE: tests/test_remote_irods/setup-data/.irodsA
================================================
.Ijgvvo1*a' 


================================================
FILE: tests/test_remote_irods/test-data/infile
================================================
test



================================================
FILE: tests/test_remote_log/Snakefile
================================================
#import re, os, sys

# clean up moto state
shell("rm -f motoState.p")

from snakemake.remote.S3Mocked import RemoteProvider as S3RemoteProvider

S3 = S3RemoteProvider()


rule test:
    input:
        S3.remote('test-remote-bucket/test.txt')
    log:
        S3.remote("test-remote-bucket/testlog.txt")
    shell:
        "exit 1"



================================================
FILE: tests/test_remote_log/test.txt
================================================
[Empty file]


================================================
FILE: tests/test_remote_log/expected-results/motoState.p
================================================
[Binary file]


================================================
FILE: tests/test_remote_ncbi/Snakefile
================================================
from snakemake.remote.NCBI import RemoteProvider as NCBIRemoteProvider
NCBI = NCBIRemoteProvider(email="someone@example.com") # email required by NCBI to prevent abuse

# get accessions for the first 3 results in a search for full-length Zika virus genomes
# the query parameter accepts standard NCBI search syntax
query = '"Zika virus"[Organism] AND (("9000"[SLEN] : "20000"[SLEN]) AND ("2017/03/20"[PDAT] : "2017/03/24"[PDAT])) '
accessions = NCBI.search(query, retmax=3, return_all=False)

# give the accessions a file extension to help the RemoteProvider determine the 
# proper output type. 
input_files = expand("{acc}.fasta", acc=accessions)

rule all:
    input:
        "sizes.txt"

rule download_and_count:
    input:
        # Since *.fasta files could come from several different databases, specify the database here.
        # if the input files are ambiguous, the provider will alert the user with possible options
        NCBI.remote(input_files, db="nuccore", seq_start=5000)
    output:
        "sizes.txt"
    shell:
        r"wc -c {input} | sed 's/^[ \t]*//' > sizes.txt"



================================================
FILE: tests/test_remote_ncbi/expected-results/sizes.txt
================================================
5801 KY785484.1.fasta
5255 KY785481.1.fasta
5318 KY785480.1.fasta
16374 total



================================================
FILE: tests/test_remote_ncbi_simple/Snakefile
================================================
from snakemake.remote.NCBI import RemoteProvider as NCBIRemoteProvider
NCBI = NCBIRemoteProvider(email="someone@example.com") # email required by NCBI to prevent abuse

rule all:
    input:
        "sizes.txt"

rule download_and_count:
    input:
        NCBI.remote("KY785484.1.fasta", db="nuccore")
    output:
        "sizes.txt"
    shell:
        r"wc -c {input} | sed 's/^[ \t]*//' > sizes.txt"



================================================
FILE: tests/test_remote_ncbi_simple/expected-results/sizes.txt
================================================
10861 KY785484.1.fasta



================================================
FILE: tests/test_remote_sftp/Snakefile
================================================
from snakemake.remote.SFTP import RemoteProvider
SFTP = RemoteProvider(username="demo", password="password")

rule a:
    input:
        SFTP.remote("test.rebex.net/readme.txt")
    output:
        "readme.txt"
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_remote_sftp/expected-results/readme.txt
================================================
Welcome,

you are connected to an FTP or SFTP server used for testing purposes by Rebex FTP/SSL or Rebex SFTP sample code.
Only read access is allowed and the FTP download speed is limited to 16KBps.

For infomation about Rebex FTP/SSL, Rebex SFTP and other Rebex .NET components, please visit our website at http://www.rebex.net/

For feedback and support, contact support@rebex.net

Thanks!



================================================
FILE: tests/test_remote_zenodo/Snakefile
================================================
import os
from snakemake.remote.zenodo import RemoteProvider

access_token_sandbox=os.environ["ZENODO_SANDBOX_PAT"]
zen_sandbox = RemoteProvider(access_token=access_token_sandbox, sandbox=True)

rule all:
    input: "download.txt", zen_sandbox.remote("large_upload.txt")

rule download:
    input:
        zen_sandbox.remote("uploaded.txt")
    output:
        "download.txt"
    shell:
        "cp {input} {output}"

rule upload:
    input: "test.txt"
    output:
        zen_sandbox.remote("uploaded.txt")
    shell:
        "cp {input} {output}"

try:
    rule too_large_upload:
        output: zen_sandbox.remote("large_upload.txt")
        shell: "head -c 101000000 /dev/urandom > {output}"
except ZenodoFileException:
    print("Current Zenodo stable API supports <=100MB per file.")



================================================
FILE: tests/test_remote_zenodo/test.txt
================================================
Freedom of self-doubt
6 p.m.
Rising


================================================
FILE: tests/test_remote_zenodo/expected-results/download.txt
================================================
Freedom of self-doubt
6 p.m.
Rising


================================================
FILE: tests/test_report/Snakefile
================================================

report: "report/workflow.rst"


shell.executable("bash")


rule all:
    input:
        ["fig1.svg", "testmodel.fig2.png", "test.csv", "testdir"],


rule c:
    input:
        report("lorem-ipsum.pdf", caption="report/fig1.rst"),
    output:
        "test.{i}.out",
    singularity:
        "docker://continuumio/miniconda3:4.4.10"
    conda:
        "envs/test.yaml"
    shell:
        "sleep `shuf -i 1-3 -n 1`; touch {output}"


rule a:
    input:
        expand("test.{i}.out", i=range(10)),
    output:
        report(
            "fig1.svg",
            caption="report/fig1.rst",
            category="Step 1",
            labels=lambda w, params: {"type": "figure", "num": "1"},
        ),
    shell:
        "sleep `shuf -i 1-3 -n 1`; cp data/fig1.svg {output}"


rule b:
    input:
        expand("test.{i}.out", i=range(10)),
    output:
        report(
            "{model}.fig2.png",
            caption="report/fig2.rst",
            category=lambda w: "Step 2",
            subcategory="{model}",
            labels={"type": "figure", "model": "{model}"},
        ),
    shell:
        "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png {output}"


rule d:
    output:
        report("test.csv", caption="report/table.rst"),
    run:
        shell(
            "curl http://samplecsvs.s3.amazonaws.com/Sacramentorealestatetransactions.csv > {output}"
        )


rule e:
    output:
        report(
            directory("testdir"), caption="report/testdir.rst", patterns=["{name}.txt"]
        ),
    shell:
        """
        mkdir {output}
        for x in 1 2 3
        do 
            echo $x > testdir/$x.txt
        done
        """



================================================
FILE: tests/test_report/envs/test.yaml
================================================
channels:
  - conda-forge
dependencies:
  - make



================================================
FILE: tests/test_report/report/fig1.rst
================================================
This is a caption with a `link <https://www.google.com>`_.



================================================
FILE: tests/test_report/report/fig2.rst
================================================
Some math :math:`\sum_i i^2`.



================================================
FILE: tests/test_report/report/table.rst
================================================
An example table.



================================================
FILE: tests/test_report/report/testdir.rst
================================================
Files obtained from a directory. This file starts with {{ snakemake.wildcards.name }}. This value has been dynamically inferred from the given pattern.


================================================
FILE: tests/test_report/report/workflow.rst
================================================
This is the workflow description. Test reference fig1.svg_. And a reference to a category `Step 2`_.



================================================
FILE: tests/test_report/testdir/1.txt
================================================
1



================================================
FILE: tests/test_report/testdir/2.txt
================================================
2



================================================
FILE: tests/test_report/testdir/3.txt
================================================
3



================================================
FILE: tests/test_report/testdir/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC40Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.4.out", "incomplete": false, "starttime": 1585317519.3560429, "endtime": 1585317520.3800488, "job_hash": 3471465265311225051, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC41Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.5.out", "incomplete": false, "starttime": 1585317518.3440373, "endtime": 1585317521.3600543, "job_hash": -4453652366712017362, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC42Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.6.out", "incomplete": false, "starttime": 1585317518.3320372, "endtime": 1585317519.348043, "job_hash": -5792833771891348507, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC43Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.7.out", "incomplete": false, "starttime": 1585317518.340037, "endtime": 1585317520.3520486, "job_hash": 169637128606654253, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC44Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.8.out", "incomplete": false, "starttime": 1585317519.4240434, "endtime": 1585317522.4480605, "job_hash": 4196649577361415263, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC45Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.9.out", "incomplete": false, "starttime": 1585317518.3760374, "endtime": 1585317519.392043, "job_hash": 4841685047946511577, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC4wLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.0.out", "incomplete": false, "starttime": 1585317519.4040432, "endtime": 1585317522.4320605, "job_hash": 1584247219081097085, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC4xLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.1.out", "incomplete": false, "starttime": 1585317518.3680375, "endtime": 1585317521.3880546, "job_hash": 4581141678275462678, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC4yLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.2.out", "incomplete": false, "starttime": 1585317518.3840375, "endtime": 1585317520.400049, "job_hash": 7969642367240267386, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC4zLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.3.out", "incomplete": false, "starttime": 1585317518.3520372, "endtime": 1585317521.3680544, "job_hash": -6992618918864179503, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdC5jc3Y=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YVwAAAGN1cmwgaHR0cDovL3NhbXBsZWNzdnMuczMuYW1hem9uYXdzLmNvbS9TYWNyYW1lbnRvcmVhbGVzdGF0ZXRyYW5zYWN0aW9ucy5jc3YgPiB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "d", "input": [], "log": [], "params": [], "shellcmd": "curl http://samplecsvs.s3.amazonaws.com/Sacramentorealestatetransactions.csv > test.csv", "incomplete": false, "starttime": 1585317518.3600373, "endtime": 1585317519.4120433, "job_hash": 5729990202001040214, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_report/.snakemake/metadata/dGVzdGRpcg==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YdQAAAAogICAgICAgIG1rZGlyIHtvdXRwdXR9CiAgICAgICAgZm9yIHggaW4gMSAyIDMKICAgICAgICBkbyAKICAgICAgICAgICAgZWNobyAkeCA+IHRlc3RkaXIvJHgudHh0CiAgICAgICAgZG9uZQogICAgICAgIHEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "e", "input": [], "log": [], "params": [], "shellcmd": "\n        mkdir testdir\n        for x in 1 2 3\n        do \n            echo $x > testdir/$x.txt\n        done\n        ", "incomplete": false, "starttime": 1585317518.3560374, "endtime": 1585317518.3680375, "job_hash": 2045751853232800755, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_report/.snakemake/metadata/ZmlnMi5wbmc=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YMwAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgY3AgZGF0YS9maWcyLnBuZyB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "b", "input": ["test.0.out", "test.1.out", "test.2.out", "test.3.out", "test.4.out", "test.5.out", "test.6.out", "test.7.out", "test.8.out", "test.9.out"], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png fig2.png", "incomplete": false, "starttime": 1585317522.4760606, "endtime": 1585317523.5040665, "job_hash": 7638699196749507097, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_report/.snakemake/metadata/ZmlnMS5zdmc=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YMwAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgY3AgZGF0YS9maWcxLnN2ZyB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "a", "input": ["test.0.out", "test.1.out", "test.2.out", "test.3.out", "test.4.out", "test.5.out", "test.6.out", "test.7.out", "test.8.out", "test.9.out"], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; cp data/fig1.svg fig1.svg", "incomplete": false, "starttime": 1585317522.4600606, "endtime": 1585317524.4880722, "job_hash": 2107624497111535029, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_report_after_run/Snakefile
================================================

report: "report/workflow.rst"


shell.executable("bash")


rule all:
    input:
        ["fig1.svg", "testmodel.fig2.png", "test.csv", "testdir"],


rule c:
    input:
        report("lorem-ipsum.pdf", caption="report/fig1.rst"),
    output:
        "test.{i}.out",
    singularity:
        "docker://continuumio/miniconda3:4.4.10"
    conda:
        "envs/test.yaml"
    shell:
        "sleep `shuf -i 1-3 -n 1`; touch {output}"


rule a:
    input:
        expand("test.{i}.out", i=range(10)),
    output:
        report(
            "fig1.svg",
            caption="report/fig1.rst",
            category="Step 1",
            labels=lambda w, params: {"type": "figure", "num": "1"},
        ),
    shell:
        "sleep `shuf -i 1-3 -n 1`; cp data/fig1.svg {output}"


rule b:
    input:
        expand("test.{i}.out", i=range(10)),
    output:
        report(
            "{model}.fig2.png",
            caption="report/fig2.rst",
            category=lambda w: "Step 2",
            subcategory="{model}",
            labels={"type": "figure", "model": "{model}"},
        ),
    shell:
        "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png {output}"


rule d:
    output:
        report("test.csv", caption="report/table.rst"),
    run:
        shell(
            "curl http://samplecsvs.s3.amazonaws.com/Sacramentorealestatetransactions.csv > {output}"
        )


rule e:
    output:
        report(
            directory("testdir"), caption="report/testdir.rst", patterns=["{name}.txt"]
        ),
    shell:
        """
        mkdir {output}
        for x in 1 2 3
        do 
            echo $x > testdir/$x.txt
        done
        """



================================================
FILE: tests/test_report_after_run/envs/test.yaml
================================================
channels:
  - conda-forge
dependencies:
  - make



================================================
FILE: tests/test_report_after_run/report/fig1.rst
================================================
This is a caption with a `link <https://www.google.com>`_.



================================================
FILE: tests/test_report_after_run/report/fig2.rst
================================================
Some math :math:`\sum_i i^2`.



================================================
FILE: tests/test_report_after_run/report/table.rst
================================================
An example table.



================================================
FILE: tests/test_report_after_run/report/testdir.rst
================================================
Files obtained from a directory. This file starts with {{ snakemake.wildcards.name }}. This value has been dynamically inferred from the given pattern.


================================================
FILE: tests/test_report_after_run/report/workflow.rst
================================================
This is the workflow description. Test reference fig1.svg_. And a reference to a category `Step 2`_.



================================================
FILE: tests/test_report_dir/caption.rst
================================================
the caption



================================================
FILE: tests/test_report_dir/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        report(directory("test"), caption="caption.rst", htmlindex="test.html")
    shell:
    	"mkdir test; cp template.html test/test.html; mkdir test/js; echo \"alert('test')\" > test/js/test.js"



================================================
FILE: tests/test_report_dir/template.html
================================================
<html>
<head>
	<script src="js/test.js"></script>
</head>
<body>Test HTML from directory</body>
</html>



================================================
FILE: tests/test_report_dir/test/test.html
================================================
<html>
<head>
	<script src="js/test.js"></script>
</head>
<body>Test HTML from directory</body>
</html>



================================================
FILE: tests/test_report_dir/test/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_report_dir/test/js/test.js
================================================
alert('test')



================================================
FILE: tests/test_report_display_code/Snakefile
================================================
rule all:
    input:
        "test.vcf.gz",


rule a:
    output:
        "{prefix}.txt",
    shell:
        "touch {output}"


rule b:
    input:
        "{prefix}.txt",
    output:
        "{prefix}.vcf",
    script:
        "test.py"


rule bgzip:
    input:
        "{prefix}.vcf",
    output:
        "{prefix}.vcf.gz",
    log:
        "logs/bgzip/{prefix}.log",
    wrapper:
        "v1.1.0/bio/bgzip"



================================================
FILE: tests/test_report_display_code/test.py
================================================
with open(snakemake.output[0], "w") as f:
    f.write("bar")



================================================
FILE: tests/test_report_href/Snakefile
================================================
rule a:
    input:
        report("test.html"),
        report(
            "subdir",
            patterns=["subdir/{name}.html"],
        )
    output:
        report(
            "test2.html",
        )
    script:
        "test_script.py"



================================================
FILE: tests/test_report_href/test.html
================================================
<!DOCTYPE html>
<html>
    <head>
        <title>Test</title>
    </head>
    <body>
        Test
        <h1 id="bar">Section</h1>
        <script>
            const urlParams = new URLSearchParams(window.location.search);
            console.log(urlParams.get("foo"));
        </script>
    </body>
</html>


================================================
FILE: tests/test_report_href/test_script.py
================================================
import textwrap

with open(snakemake.output[0], "w") as f:
    print(
        textwrap.dedent(f"""
        <html>
        <head>
        <title>Report</title>
        </head>
        <body>
            <a href={snakemake.report_href("test.html").url_args(foo=4).anchor("bar")}>Link to test.html</a>
            <a href={snakemake.report_href("subdir").child_path("subdir/test3.html")}>Link to subdir/test3.html</a>
        </body>
        </html>
          """
        ),
        file=f,
    )


================================================
FILE: tests/test_report_href/expected-results/test2.html
================================================

<html>
<head>
<title>Report</title>
</head>
<body>
    <a href=../f34a7fb61a9cc01eb48c32a902d2ef73398b12d8baccaa64ae41317c1d2304cb/test.html?foo=4#bar>Link to test.html</a>
    <a href=../bdf6c15545b679e2500a451b7ff7c30b8784658e2a553d4913adb4651c0a78d3/subdir/test3.html>Link to subdir/test3.html</a>
</body>
</html>




================================================
FILE: tests/test_report_href/subdir/subdir/test3.html
================================================
<!DOCTYPE html>
<html>
    <head>
        <title>Test</title>
    </head>
    <body>
        <a id=foo>Test</a>
    </body>
</html>


================================================
FILE: tests/test_report_metadata/report.rst
================================================

Name of the workflow
====================

This is the workflow description. And this not.

More text
^^^^^^^^^

And more description.



================================================
FILE: tests/test_report_metadata/Snakefile
================================================
shell.executable("bash")

report: 'report.rst'

rule a:
    output:
        "test.out"
    shell:
        "echo test > {output}"


use rule a as b with:
    output:
        "test2.out"



================================================
FILE: tests/test_report_metadata/yte_template.yaml
================================================
__definitions__:
- import os

Workflow name: Test Workflow
Workdir: /test/workdir
Contributors:
  - Test Contributor
  - Another Contributor




================================================
FILE: tests/test_report_zip/custom-stylesheet.css
================================================
#brand {
  margin: auto;
  height: 30px;
  width: 311px;
  background-repeat: no-repeat;
  background-size: 311px 30px;
  background-image: url("data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8' standalone='no'%3F%3E%3C!-- Creator: CorelDRAW --%3E%3Csvg xmlns:dc='http://purl.org/dc/elements/1.1/' xmlns:cc='http://creativecommons.org/ns%23' xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns%23' xmlns:svg='http://www.w3.org/2000/svg' xmlns='http://www.w3.org/2000/svg' xmlns:sodipodi='http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd' xmlns:inkscape='http://www.inkscape.org/namespaces/inkscape' xml:space='preserve' width='1568.9631' height='150.92992' viewBox='0 0 1483.7169 142.72835' version='1.1' id='svg12' sodipodi:docname='uk-uni-white.svg' style='clip-rule:evenodd;fill-rule:evenodd;image-rendering:optimizeQuality;shape-rendering:geometricPrecision;text-rendering:geometricPrecision' inkscape:version='0.92.4 (5da689c313, 2019-01-14)'%3E%3Cmetadata id='metadata18'%3E%3Crdf:RDF%3E%3Ccc:Work rdf:about=''%3E%3Cdc:format%3Eimage/svg+xml%3C/dc:format%3E%3Cdc:type rdf:resource='http://purl.org/dc/dcmitype/StillImage' /%3E%3Cdc:title%3E%3C/dc:title%3E%3C/cc:Work%3E%3C/rdf:RDF%3E%3C/metadata%3E%3Cdefs id='defs16'%3E%3C/defs%3E%3Csodipodi:namedview pagecolor='%23ffffff' bordercolor='%23666666' borderopacity='1' objecttolerance='10' gridtolerance='10' guidetolerance='10' inkscape:pageopacity='0' inkscape:pageshadow='2' inkscape:window-width='1850' inkscape:window-height='1025' id='namedview14' showgrid='false' inkscape:pagecheckerboard='true' fit-margin-top='0' fit-margin-left='0' fit-margin-right='0' fit-margin-bottom='0' inkscape:zoom='1.2128168' inkscape:cx='136.25611' inkscape:cy='10.038152' inkscape:window-x='70' inkscape:window-y='27' inkscape:window-maximized='1' inkscape:current-layer='svg12' showguides='true' inkscape:guide-bbox='true'%3E%3Csodipodi:guide position='989.81019,142.98824' orientation='0,1' id='guide840' inkscape:locked='false' /%3E%3C/sodipodi:namedview%3E%3Cg id='g845' transform='matrix(0.77078623,0,0,0.77078623,1422.0842,-34.102505)' style='stroke-width:1.29737616'%3E%3Cpath d='m -427.2909,73.871082 c 0,2.94841 0.13153,6.70348 3.30574,9.69765 2.01076,1.875846 5.54175,2.732196 8.98239,2.732196 2.99413,0 5.80821,-0.66893 8.08768,-2.509216 3.35373,-2.72769 3.39663,-7.46386 3.39663,-9.4312 v -21.71651 h -7.28323 v 21.6736 c 0,5.98828 -3.21768,5.98828 -4.6041,5.98828 -4.42403,0 -4.42403,-4.06669 -4.42403,-6.03397 v -21.62786 h -7.46108 v 21.22708 z m 32.19369,11.573456 h 6.61433 V 70.070802 c 0,-0.84902 -0.18008,-5.00434 -0.22298,-5.94255 h 0.22298 c 1.69859,3.97807 1.7415,4.11242 2.45672,5.40737 l 8.71595,15.908866 h 5.89964 V 52.643952 h -6.61433 v 12.1995 c 0,1.24981 0,1.51852 0.13435,3.75452 l 0.17726,3.30574 h -0.22298 c -0.13435,-0.22298 -0.58088,-1.42989 -0.71523,-1.69859 -0.31161,-0.66951 -0.76038,-1.56424 -1.11772,-2.23318 l -8.445,-15.328 h -6.883 v 32.800526 z m 39.5255,0.007 V 52.643682 h -7.24032 v 32.807856 z m 23.25703,-0.007 9.78625,-32.800536 h -7.14945 l -4.87281,17.3382 c -0.53515,1.9673 -1.07256,3.93234 -1.56198,5.89963 h -0.17951 c -0.26871,-1.02965 -0.80386,-2.90551 -1.56424,-5.58805 l -5.09352,-17.64983 h -7.19519 l 10.00923,32.800536 h 7.82123 z m 14.67543,0 h 23.06058 v -6.122616 h -15.81965 v -7.72976 h 10.143 v -5.98828 h -10.23448 v -6.9722 h 14.88203 v -5.98773 h -22.03148 v 32.800536 z m 36.70918,-27.707576 c 4.02096,0 4.91571,0 6.03119,0.40306 1.42989,0.53515 2.14512,1.65286 2.14512,3.93233 0,1.78722 -0.17782,3.57445 -2.45673,4.3783 -1.02909,0.35733 -1.65287,0.35733 -5.71956,0.35733 v -9.07104 z m 0,14.30117 h 2.411 l 6.21405,13.406406 h 7.8184 l -7.14945,-14.569846 c 4.6041,-1.38416 6.25695,-4.55612 6.25695,-8.71367 0,-8.80233 -6.74866,-9.51697 -12.06516,-9.51697 h -10.59181 v 32.800536 h 7.10599 V 72.038182 Z m 43.14115,-14.16683 c -2.6797,-3.93234 -6.07688,-5.89908 -10.90399,-5.89908 -6.74639,0 -10.67874,3.88887 -10.67874,9.29625 0,6.47994 4.28967,8.66799 9.24885,10.58954 4.91571,1.92213 6.25695,2.45954 6.25695,4.91627 0,2.99413 -2.81462,3.70879 -4.37886,3.70879 -2.81631,0 -5.00659,-1.69859 -6.21349,-4.46692 l -5.58522,3.7088 c 2.94841,5.273586 6.3456,6.664556 12.1109,6.664556 1.65287,0 4.3783,-0.49168 6.29986,-1.659636 4.64983,-2.81688 4.64983,-7.19519 4.64983,-8.67082 0,-3.08276 -1.42989,-5.2279 -1.83295,-5.7653 -1.29553,-1.78722 -3.62017,-3.12792 -5.58522,-3.84314 l -4.15758,-1.51852 c -2.01076,-0.71523 -4.15531,-1.65287 -4.15531,-3.97807 0,-0.35733 0.0886,-3.17139 3.84315,-3.17139 2.54817,0 4.647,1.6986 5.67382,3.66364 l 5.40797,-3.57501 z m 14.45472,27.580556 V 52.644002 h -7.24093 v 32.807856 z m 5.96175,-32.807856 v 5.98773 h 8.35635 v 26.812856 h 7.50904 V 58.631732 h 8.17856 v -5.98773 z m 33.26626,-3.21767 h 5.67609 v -5.18439 h -5.67609 z m 8.71595,0 h 5.67382 v -5.18439 h -5.67382 z m 2.09939,23.90734 h -7.37243 l 3.57445,-14.8363 h 0.17782 l 3.62017,14.8363 z m -8.08768,-20.68969 -9.25112,32.800536 h 7.01511 l 1.6986,-6.525676 h 10.05496 l 1.65287,6.525676 h 7.01734 l -9.47411,-32.800536 h -8.71367 z m 20.88669,0 v 5.98773 h 8.35636 v 26.812856 h 7.50904 V 58.631712 h 8.17855 v -5.98773 z' id='path5' inkscape:connector-curvature='0' style='fill:%23ffffff;fill-rule:nonzero;stroke-width:1.29737616' /%3E%3Cpath d='m -425.94284,170.75254 c 8.35636,0 15.60855,0 17.05765,-0.13661 4.07629,-0.34548 7.18331,-0.69096 10.84415,-2.55495 5.59256,-2.83325 6.90504,-8.91073 6.97448,-14.64212 l 0.0694,-8.63412 c 0.0694,-11.39621 -0.82756,-14.91817 -3.03986,-18.16408 -2.90042,-4.2129 -7.24992,-6.55899 -19.61256,-6.55899 h -12.2932 v 50.69104 z m 11.32621,-41.71349 c 5.93805,0 8.35636,0 10.49639,3.10703 1.72739,2.48777 1.86625,6.70065 1.86625,12.77758 0,1.72739 -0.13887,4.48894 -0.13887,6.21632 0,9.73773 -4.55838,10.56583 -9.04734,10.56583 h -3.17647 v -32.66675 z m 59.49367,23.82546 c 0,4.55838 0.20604,10.35978 5.10989,14.98761 3.10703,2.90043 8.56523,4.22476 13.88118,4.22476 4.62837,0 8.98012,-1.03643 12.50208,-3.87928 5.17989,-4.21289 5.24706,-11.53282 5.24706,-14.57212 v -33.56374 h -11.25677 v 33.49375 c 0,9.25617 -4.97103,9.25617 -7.11331,9.25617 -6.83787,0 -6.83787,-6.28576 -6.83787,-9.32562 v -33.4243 h -11.53226 v 32.80281 z m 85.11614,17.89991 v -50.70267 h -11.18738 v 50.70267 z m 68.33672,-42.62236 c -4.14572,-6.07971 -9.39279,-9.11674 -16.85159,-9.11674 -10.42867,0 -16.5061,6.00744 -16.5061,14.36324 0,10.01655 6.62898,13.39907 14.2944,16.36949 7.59769,2.96986 9.6705,3.79742 9.6705,7.59769 0,4.62555 -4.35176,5.73199 -6.76786,5.73199 -4.35177,0 -7.73709,-2.62495 -9.60112,-6.90726 l -8.6324,5.73144 c 4.55837,8.14974 9.80772,10.30448 18.71557,10.30448 2.55551,0 6.76842,-0.76095 9.73828,-2.56737 7.18276,-4.35176 7.18276,-11.1196 7.18276,-13.39907 0,-4.76442 -2.20947,-8.0803 -2.831,-8.91013 -2.00286,-2.76156 -5.59421,-4.83442 -8.63412,-5.93804 l -6.42237,-2.34891 c -3.10704,-1.10586 -6.42238,-2.55495 -6.42238,-6.14632 0,-0.55152 0.13661,-4.90385 5.93972,-4.90385 3.93459,0 7.18108,2.62494 8.76901,5.66421 l 8.35863,-5.52482 z m 34.90954,42.61053 h 16.3667 c 6.42293,0 10.77472,-0.27604 14.43551,-4.00403 3.38308,-3.38534 3.9346,-7.18276 3.9346,-10.91355 0,-7.45881 -4.69499,-11.11738 -8.07975,-11.60226 v -0.34548 c 3.93686,-0.897 6.97615,-4.83442 6.97615,-10.98077 0,-4.21289 -1.72739,-8.49524 -5.45538,-10.77471 -2.90043,-1.79456 -7.11332,-2.07004 -11.46566,-2.07004 h -16.71215 v 50.69104 z m 10.84188,-22.23754 c 3.93685,0 7.87369,0 9.46223,0.62152 2.62438,1.10361 3.2459,3.93686 3.2459,6.35294 0,5.17988 -2.76381,7.25275 -7.18331,7.25275 h -5.52483 v -14.22723 z m 0,-20.65127 h 5.04272 c 2.27947,0 6.76787,0 6.76787,6.1486 0,6.42237 -2.90043,6.69842 -11.81059,6.69842 z m 58.943778,25.00078 c 0,4.55838 0.20661,10.35978 5.11045,14.98761 3.10646,2.90043 8.56468,4.22476 13.88118,4.22476 4.62781,0 8.97957,-1.03643 12.50153,-3.87928 5.17988,-4.21289 5.24705,-11.53282 5.24705,-14.57211 v -33.56375 h -11.25677 v 33.49375 c 0,9.25617 -4.97103,9.25617 -7.11331,9.25617 -6.83726,0 -6.83726,-6.28576 -6.83726,-9.32562 v -33.4243 h -11.53282 v 32.80281 z m 84.01234,-24.93134 c 6.2163199,0 7.5959699,0 9.3233399,0.62152 2.20947,0.82983 3.31534003,2.55495 3.31534003,6.07916 0,2.76156 -0.27604,5.52538 -3.79799003,6.76787 -1.59021,0.55208 -2.5572,0.55208 -8.8407299,0.55208 v -14.02062 z m 0,22.10093 h 3.7285499 l 9.60112003,20.71844 H 12.451478 L 1.4035379,148.23956 c 7.11331,-2.14286 9.6682801,-7.0467 9.6682801,-13.46908 0,-13.60512 -10.42866007,-14.70873 -18.6478501,-14.70873 H -23.943292 v 50.69104 h 10.98077 v -20.71844 z m 57.90821,2.48495 c 0,6.28576 0.69094,10.49866 3.45196,14.02062 3.31591,4.14347 8.01091,5.60665 13.12363,5.60665 7.80203,0 10.08095,-2.63624 11.53227,-4.36362 h 0.48266 l 1.45132,3.5338 h 4.97327 v -27.0838 h -16.99047 v 8.4258 h 5.94026 v 3.45251 c 0,1.86625 0,6.63126 -5.80143,6.63126 -5.45761,0 -6.70065,-4.07629 -6.70065,-10.70528 V 136.083 c 0,-2.96986 0.41494,-7.94091 6.21632,-7.94091 3.79746,0 6.07688,2.07004 6.28576,6.55898 h 11.05016 c 0,-9.80545 -6.83726,-15.67572 -16.43893,-15.67572 -4.90383,0 -9.4594,0.48209 -13.32963,4.00403 -5.2465,4.83442 -5.2465,11.25904 -5.2465,15.74799 v 13.74174 z' id='path7' inkscape:connector-curvature='0' style='fill:%23ffffff;fill-rule:nonzero;stroke-width:1.29737616' /%3E%3Cpath d='m -397.02371,227.62821 h 35.63661 v -9.46223 h -24.44701 v -11.94776 h 15.67572 v -9.25334 h -15.81516 v -10.77416 h 22.99847 v -9.25335 h -34.04868 v 50.69104 z m 99.01173,-42.61054 c -4.14572,-6.07971 -9.39223,-9.11673 -16.85159,-9.11673 -10.42866,0 -16.50555,6.00744 -16.50555,14.36324 0,10.01655 6.62843,13.39907 14.29385,16.36948 7.59825,2.96986 9.67111,3.79743 9.67111,7.59769 0,4.62556 -4.35233,5.73144 -6.76842,5.73144 -4.35177,0 -7.73708,-2.62438 -9.60112,-6.9067 l -8.63185,5.73143 c 4.55838,8.14975 9.80773,10.30449 18.71558,10.30449 2.55495,0 6.76787,-0.76095 9.73772,-2.56737 7.18331,-4.35177 7.18331,-11.1196 7.18331,-13.39907 0,-4.76442 -2.21003,-8.08031 -2.83099,-8.91013 -2.00342,-2.76156 -5.59482,-4.83442 -8.63468,-5.93804 l -6.42237,-2.34891 c -3.10704,-1.10587 -6.42238,-2.55495 -6.42238,-6.14632 0,-0.55209 0.13661,-4.90386 5.94027,-4.90386 3.9346,0 7.18048,2.62439 8.76846,5.66422 l 8.35863,-5.52483 z m 63.51163,0 c -4.14516,-6.07971 -9.39223,-9.11673 -16.85104,-9.11673 -10.42922,0 -16.5061,6.00744 -16.5061,14.36324 0,10.01655 6.62899,13.39907 14.29385,16.36948 7.59824,2.96986 9.67111,3.79743 9.67111,7.59769 0,4.62556 -4.35233,5.73144 -6.76787,5.73144 -4.35233,0 -7.73708,-2.62438 -9.60167,-6.9067 l -8.63185,5.73143 c 4.55838,8.14975 9.80772,10.30449 18.71557,10.30449 2.55495,0 6.76787,-0.76095 9.73829,-2.56737 7.18275,-4.35177 7.18275,-11.1196 7.18275,-13.39907 0,-4.76442 -2.20947,-8.08031 -2.83099,-8.91013 -2.00343,-2.76156 -5.59483,-4.83442 -8.63468,-5.93804 l -6.42238,-2.34891 c -3.10703,-1.10587 -6.42237,-2.55495 -6.42237,-6.14632 0,-0.55209 0.13661,-4.90386 5.94027,-4.90386 3.9346,0 7.18048,2.62439 8.76902,5.66422 l 8.35807,-5.52483 z m 31.87984,42.61054 h 35.63661 v -9.46223 h -24.44701 v -11.94776 h 15.67572 v -9.25334 h -15.81516 v -10.77416 h 22.99791 v -9.25335 h -34.04812 v 50.69104 z m 67.93341,0 h 10.22033 v -23.7583 c 0,-1.31021 -0.27604,-7.73486 -0.34548,-9.1839 h 0.34548 c 2.62438,6.14632 2.69382,6.35293 3.79743,8.3558 l 13.46907,24.58645 h 9.116738 v -50.69104 h -10.222608 v 18.85219 c 0,1.93343 0,2.3489 0.2083,5.80365 l 0.27604,5.10989 h -0.34547 c -0.20887,-0.34548 -0.897,-2.20947 -1.10587,-2.62494 -0.48209,-1.03644 -1.17304,-2.41778 -1.72738,-3.45421 l -13.05136,-23.68663 h -10.63527 v 50.69104 z' id='path9' inkscape:connector-curvature='0' style='fill:%23ffffff;fill-rule:nonzero;stroke-width:1.29737616' /%3E%3C/g%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path44' d='m 57.849497,14.92373 v 118.42303 l 8.333397,-0.0127 V 14.923159 h -8.333397 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:none;stroke:%23000000;stroke-width:0.53388023;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:3.86400008;stroke-dasharray:none;stroke-opacity:1' id='path48' d='m 57.849497,14.92373 v 118.42303 l 8.333397,-0.0127 V 14.923159 h -8.333397 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path52' d='M 13.340586,37.253319 0.2644434,50.29467 V 68.102403 H 26.614851 V 50.29467 Z M 0.2644434,80.614155 V 98.421888 L 13.340586,111.39704 26.614851,98.355688 V 80.614155 Z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23fffcfd;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path68' d='M 109.56251,37.253319 96.420323,50.22847 V 68.102403 H 122.77073 V 50.36087 Z M 96.420323,80.614155 V 98.421888 L 109.56251,111.39704 122.77073,98.355688 V 80.614155 Z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23fefefe;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path84' d='m 35.002074,106.82926 -13.010102,13.17375 13.010102,13.23995 h 17.699027 v -26.4137 z m 36.190541,0 v 26.4137 H 89.023718 L 101.96778,120.06921 88.957676,106.82926 Z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23fffffa;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path100' d='m 35.002074,15.076403 -13.010102,13.10755 13.010102,13.30615 h 17.699027 v -26.4137 z m 36.190541,0 v 26.3475 H 89.023718 L 101.96778,28.316353 88.957676,15.076403 Z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:none;stroke:%23000000;stroke-width:0.53388023;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:3.86400008;stroke-dasharray:none;stroke-opacity:1' id='path112' d='M 52.720437,15.060303 V 41.474342 H 35.011957 L 21.997076,28.215729 35.011957,15.060303 Z M 71.221652,41.436105 V 15.064561 H 88.930119 L 101.945,28.323195 88.99673,41.442185 71.221652,41.43583 Z m 0,91.817145 v -26.4201 h 17.708467 l 13.014881,13.2647 -12.94827,13.14932 -17.775078,0.006 z M 96.442915,80.605851 H 122.78153 V 98.344715 L 109.53047,111.39085 96.442915,98.423647 Z m 0,-12.51798 H 122.78153 V 50.330789 L 109.53047,37.28465 96.442915,50.257937 Z M 52.720437,106.83364 V 133.2416 H 35.011957 L 21.997076,119.98299 35.011957,106.83364 Z M 0.26662011,80.605851 26.599193,80.593141 V 98.344143 L 13.372344,111.3903 0.26662011,98.423075 V 80.605279 Z m 0,-12.51798 26.33257289,-0.0064 V 50.294077 L 13.372344,37.284375 0.26662011,50.294077 v 17.793518 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23000000;fill-opacity:1;fill-rule:evenodd;stroke:none;stroke-width:0.94566709' id='path124' d='m 75.903661,9.8260743 2.095458,1.2627307 c 0.635918,0.51602 1.25364,1.43269 1.25364,1.43269 L 78.89544,11.076646 80.37922,10.955262 C 79.979507,10.840021 79.761498,10.754862 79.386004,10.578866 78.750107,10.281402 78.374613,10.099283 77.865896,9.607541 77.405615,9.1582936 77.28448,9.1643734 76.927161,8.25982 l -0.224075,1.1109695 -0.799425,0.455306 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23ffffff;fill-opacity:1;stroke:%23000000;stroke-width:0.37669724;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:3.86400008;stroke-dasharray:none;stroke-opacity:1' id='path132' d='M 70.796368,0.44625824 C 69.212036,0.47866964 67.440211,1.1445014 65.975366,1.042056 57.981115,0.46532377 50.760844,4.2672443 47.417804,11.965015 c -3.851776,8.845155 1.564706,17.819807 8.057024,23.368511 0.635896,0.540296 1.350218,1.048499 2.113316,1.588794 l 0.198017,-9.069365 c -2.440668,-2.537601 -4.088505,-5.485852 -4.094549,-9.267965 -0.01268,-2.489026 0.802553,-5.521716 3.103932,-7.94397 2.19842,-2.306908 6.300408,-3.5814385 7.396606,-3.508587 2.561783,0.1578202 1.997358,1.3572326 2.905809,2.383191 0.399713,0.455306 1.855767,0.649287 3.169974,0.728197 2.870665,0.163964 7.220968,-0.125832 7.396606,-0.198493 C 77.834027,9.9785986 75.315905,3.2672573 73.371866,1.3731607 72.633748,0.65833046 71.746853,0.42691732 70.796262,0.44636416 Z M 65.909325,34.208131 V 42.4169 c 3.851772,2.798629 5.955535,5.97181 5.28329,9.929962 C 69.406017,62.897916 46.995319,65.771367 46.55927,79.885958 46.371523,86.151018 49.078518,90.97216 57.654185,97.82609 L 57.852202,87.631329 C 54.484929,85.008739 52.437084,82.061632 54.484105,78.230964 57.796876,72.026618 79.297118,63.240778 78.655156,50.625669 78.243334,42.502949 72.365167,38.585173 65.909219,34.208131 Z m 0.198018,60.043173 0.198017,9.996166 c 2.101503,1.54805 3.28589,2.42998 3.764344,5.36218 0.932649,5.75512 -5.558655,8.54262 -9.113675,10.98915 -3.936579,2.71364 -13.737191,7.79037 -9.509921,16.48374 2.416429,4.972 6.598617,5.61658 6.538071,5.42838 -0.399713,-1.26879 -3.687927,-3.35646 -3.50018,-8.40737 0.254359,-6.82357 15.76185,-10.61816 19.680255,-18.00633 1.071958,-2.02158 6.55671,-11.6834 -8.057017,-21.845916 z' /%3E%3Cpath inkscape:connector-curvature='0' style='fill:%23ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.94566709' id='path272' d='m 1018.8932,97.637109 v -22.26769 c 0,-3.569617 -0.9811,-5.74296 -4.2636,-5.74296 -4.5422,0 -8.1093,5.111595 -8.1093,11.206675 v 16.803975 h -8.87242 V 69.69323 c -1.6715,-0.279246 -4.05163,-0.558514 -6.06834,-0.698137 v -5.597279 c 4.39684,-0.564572 9.77476,-0.777068 14.46226,-0.637424 0,1.687681 -0.1392,4.413458 -0.4966,6.301475 h 0.073 c 2.1016,-4.06136 6.1471,-6.93284 11.8037,-6.93284 7.8246,0 10.338,4.978051 10.338,11.067051 v 24.441033 h -8.8664 z M 971.25174,68.646024 c -3.70641,0 -6.28639,2.731858 -6.77694,7.072464 h 12.7787 c 0.14539,-4.48023 -2.22871,-7.072464 -6.00176,-7.072464 m 14.59556,13.094714 h -21.51178 c -0.0727,6.441098 3.14924,9.591845 9.56886,9.591845 3.28248,0 6.77694,-0.698137 10.05338,-2.100491 l 0.98111,6.793217 c -3.76699,1.541978 -8.30918,2.312966 -12.57278,2.312966 -10.96181,0 -17.10889,-5.536565 -17.10889,-17.787417 0,-10.642124 5.86848,-18.485576 16.27312,-18.485576 10.12606,0 14.60162,6.926782 14.60162,15.541223 0,1.195959 -0.0786,2.592234 -0.28464,4.134233 M 933.11129,98.33586 c -2.72531,0 -5.51723,-0.279267 -7.752,-0.910633 l 0.62379,-7.491354 c 2.10152,0.910612 4.82079,1.396274 7.40679,1.396274 3.35517,0 5.58993,-1.469126 5.58993,-3.70924 0,-6.234725 -14.11107,-2.592234 -14.11107,-14.424216 0,-6.161873 4.67543,-11.127765 14.66218,-11.127765 2.03492,0 4.40291,0.273166 6.36512,0.692057 l -0.41786,7.005713 c -1.89563,-0.558514 -4.12431,-0.910633 -6.29247,-0.910633 -3.49445,0 -5.09934,1.402354 -5.09934,3.569639 0,5.882605 14.31697,2.944332 14.31697,14.351364 0,6.932841 -6.07443,11.558794 -15.29204,11.558794 m -30.87717,0 c -2.73137,0 -5.51723,-0.279267 -7.75804,-0.910633 l 0.62983,-7.491354 c 2.10152,0.910612 4.82683,1.396274 7.40679,1.396274 3.35516,0 5.58387,-1.469126 5.58387,-3.70924 0,-6.234725 -14.10501,-2.592234 -14.10501,-14.424216 0,-6.161873 4.67543,-11.127765 14.66825,-11.127765 2.02884,0 4.39683,0.273166 6.35905,0.692057 l -0.43,7.005713 c -1.87743,-0.558514 -4.11217,-0.910633 -6.28033,-0.910633 -3.48839,0 -5.09935,1.402354 -5.09935,3.569639 0,5.882605 14.32302,2.944332 14.32302,14.351364 0,6.932841 -6.07441,11.558794 -15.29808,11.558794 M 860.24976,97.637109 V 50.369894 h 25.63611 v 7.630978 h -16.55172 v 11.625587 h 15.71594 v 7.424582 h -15.71594 v 12.809366 h 16.55172 v 7.776702 z m -37.37371,0 V 75.648687 c 0,-3.715342 -0.83576,-6.022228 -4.11824,-6.022228 -3.9063,0 -7.95793,4.553081 -7.95793,11.346319 v 16.664331 h -8.81185 V 75.575836 c 0,-3.429995 -0.69646,-5.949377 -4.11217,-5.949377 -4.13037,0 -7.96397,4.832348 -7.96397,11.346319 v 16.664331 h -8.87846 V 69.69323 c -1.67153,-0.279246 -4.05165,-0.558514 -6.07443,-0.698137 v -5.597279 c 4.4029,-0.564572 9.84746,-0.777068 14.45626,-0.637424 0,1.687681 -0.13314,4.273835 -0.41182,6.301475 l 0.0727,0.07287 c 2.08941,-4.267777 6.28033,-7.005692 11.45237,-7.005692 6.14102,0 8.72705,3.575697 9.56886,6.93284 1.60492,-3.357143 5.45063,-6.93284 11.31309,-6.93284 6.71033,0 10.41068,3.71532 10.41068,11.558794 v 23.94929 h -8.94507 z m -65.80236,0.06737 c 0,-1.608771 0.0788,-4.267776 0.41788,-6.301496 h -0.13926 c -2.02279,4.061381 -6.08048,6.932861 -11.73096,6.932861 -7.89733,0 -10.41068,-4.971993 -10.41068,-11.067073 V 69.693824 c -1.67153,-0.279268 -3.97895,-0.558515 -6.01386,-0.698138 v -5.597279 c 4.62091,-0.564593 10.13817,-0.777068 14.95287,-0.637445 v 22.340541 c 0,3.569618 0.98113,5.742961 4.26362,5.742961 4.54219,0 8.10325,-5.111596 8.10325,-11.200617 V 62.833814 h 8.8724 v 27.937819 c 1.67153,0.279247 4.05165,0.558493 6.07443,0.704196 v 5.597279 c -4.4029,0.564594 -9.78085,0.770989 -14.38965,0.631366 m -41.56288,-0.06737 -10.75589,-17.228925 v 17.228925 h -8.9451 V 53.380998 c -1.67757,-0.279247 -4.04556,-0.558514 -6.07441,-0.704217 v -5.597258 c 4.47556,-0.558514 9.84746,-0.77101 15.01951,-0.631366 V 77.330288 L 714.881,62.833242 h 10.8952 l -12.22152,15.893341 13.12995,18.910526 z M 678.55684,57.652417 c -3.00391,0 -5.5233,-2.385817 -5.5233,-5.396942 0,-3.005045 2.51939,-5.451576 5.5233,-5.451576 3.08263,0 5.58991,2.373679 5.58991,5.451576 0,2.944332 -2.50728,5.396942 -5.58991,5.396942 m -4.40291,39.982256 V 69.696874 c -1.66546,-0.279268 -4.05163,-0.564594 -6.07441,-0.704217 v -5.597279 c 4.40895,-0.564573 9.78085,-0.770989 14.94683,-0.631365 v 34.87066 z m -22.63038,0.0021 v -22.26765 c 0,-3.569639 -0.97506,-5.742982 -4.25755,-5.742982 -4.54823,0 -8.10325,5.111616 -8.10325,11.206696 v 16.803954 h -8.87847 V 69.692913 c -1.67152,-0.279247 -4.05162,-0.558514 -6.07441,-0.698138 v -5.597279 c 4.40289,-0.564572 9.7869,-0.777047 14.46231,-0.637424 0,1.687681 -0.14539,4.413458 -0.49054,6.301475 h 0.0727 c 2.09546,-4.06136 6.14102,-6.93284 11.80363,-6.93284 7.81861,0 10.33196,4.978051 10.33196,11.067073 v 24.441011 h -8.86636 z M 612.12703,57.652417 c -3.00391,0 -5.51723,-2.385817 -5.51723,-5.396942 0,-3.005045 2.51332,-5.451576 5.51723,-5.451576 3.07052,0 5.58991,2.373679 5.58991,5.451576 0,2.944332 -2.51939,5.396942 -5.58991,5.396942 m -4.4029,39.982256 V 69.696874 c -1.67757,-0.279268 -4.05163,-0.564594 -6.07441,-0.704217 v -5.597279 c 4.39684,-0.564573 9.77478,-0.770989 14.94683,-0.631365 v 34.87066 z m -22.62795,0.0021 V 53.38068 c -1.68364,-0.279247 -4.05163,-0.558493 -6.08047,-0.704196 v -5.597279 c 4.46951,-0.558514 9.84746,-0.770989 15.0195,-0.631365 v 51.188951 z m -18.87066,0 -10.74985,-17.228925 v 17.228925 h -8.94508 V 53.38068 c -1.67152,-0.279247 -4.05162,-0.558493 -6.07443,-0.704196 v -5.597279 c 4.46952,-0.558514 9.84749,-0.770989 15.01951,-0.631365 v 30.882151 l 10.12606,-14.497067 h 10.89518 l -12.2215,15.893342 13.13599,18.910525 z m -44.63275,0.698752 c -2.72531,0 -5.51723,-0.279247 -7.752,-0.910612 l 0.62985,-7.491376 c 2.09546,0.910633 4.81473,1.396296 7.40073,1.396296 3.35516,0 5.58993,-1.469147 5.58993,-3.709262 0,-6.234703 -14.10501,-2.592234 -14.10501,-14.424216 0,-6.161852 4.67541,-11.127765 14.66823,-11.127765 2.0228,0 4.39684,0.273188 6.35301,0.692079 l -0.41787,7.005691 c -1.88349,-0.558514 -4.1243,-0.910633 -6.2864,-0.910633 -3.49445,0 -5.09936,1.402355 -5.09936,3.569639 0,5.882605 14.31697,2.944332 14.31697,14.351364 0,6.932841 -6.07441,11.558795 -15.29808,11.558795 m -22.55893,0 c -7.69143,0 -10.05942,-2.798629 -10.05942,-10.92137 V 69.766378 h -6.0078 v -6.932861 h 6.0078 V 52.124643 l 8.87239,-2.385818 v 13.094692 h 8.44847 v 6.932861 h -8.44847 v 15.261977 c 0,4.48633 1.04774,5.742982 4.11826,5.742982 1.47167,0 2.79799,-0.139602 3.985,-0.491743 l 0.62381,7.078544 c -2.23477,0.558514 -5.09331,0.977405 -7.54004,0.977405 M 467.6661,57.092378 c -2.65263,0 -4.82076,-2.173343 -4.82076,-4.832349 0,-2.731857 2.16813,-4.83237 4.82076,-4.83237 2.65264,0 4.82077,2.100513 4.82077,4.83237 0,2.659006 -2.16813,4.832349 -4.82077,4.832349 m -2.51334,24.926695 c -9.07832,0 -11.52503,2.45259 -11.52503,5.463714 0,2.306908 1.53221,3.988509 4.11824,3.988509 4.39684,0 7.40679,-4.200983 7.40679,-8.401988 z M 453.55505,57.092378 c -2.65265,0 -4.82079,-2.173343 -4.82079,-4.832349 0,-2.731857 2.16814,-4.83237 4.82079,-4.83237 2.65263,0 4.82077,2.100513 4.82077,4.83237 0,2.659006 -2.16814,4.832349 -4.82077,4.832349 m 12.29417,40.613621 c 0,-2.03372 0.0727,-4.134212 0.35128,-5.955457 l -0.0727,-0.06673 c -1.67153,3.921737 -5.93512,6.653594 -11.10716,6.653594 -6.2864,0 -9.91408,-3.854944 -9.91408,-9.458303 0,-8.329115 8.24255,-12.669742 20.04618,-12.669742 V 74.52776 c 0,-3.64249 -1.75025,-5.536566 -6.77694,-5.536566 -3.1432,0 -7.33412,1.056315 -10.12606,2.525441 l -1.05378,-7.351731 c 3.56713,-1.189879 8.17593,-2.100491 12.43954,-2.100491 11.31914,0 14.45627,4.480229 14.45627,12.117308 V 90.7732 c 1.67757,0.279268 4.05163,0.558514 6.08047,0.698137 v 5.603359 c -4.4029,0.564572 -9.78085,0.770989 -14.32304,0.631365 m -34.8549,0.629523 c -7.68539,0 -10.05942,-2.798629 -10.05942,-10.92137 V 69.766421 h -6.0078 v -6.932862 h 6.0078 V 52.124685 l 8.8724,-2.385818 v 13.094692 h 8.45453 v 6.932862 h -8.45453 v 15.261976 c 0,4.486331 1.04774,5.742982 4.1243,5.742982 1.45956,0 2.78588,-0.139602 3.97895,-0.491742 l 0.62985,7.078543 c -2.23474,0.558514 -5.10541,0.977405 -7.54608,0.977405 M 403.46925,57.652142 c -3.00391,0 -5.5233,-2.385818 -5.5233,-5.396943 0,-3.005045 2.51939,-5.451575 5.5233,-5.451575 3.07657,0 5.58991,2.373679 5.58991,5.451575 0,2.944332 -2.51334,5.396943 -5.58991,5.396943 m -4.39684,39.982276 V 69.696599 c -1.67757,-0.279247 -4.05162,-0.564573 -6.08047,-0.704217 v -5.597258 c 4.4029,-0.564594 9.78085,-0.770989 14.94683,-0.631366 v 34.87066 z m -26.90003,0.701167 c -2.72532,0 -5.51724,-0.279246 -7.75201,-0.910612 l 0.62986,-7.491376 c 2.09545,0.910634 4.82077,1.396296 7.40679,1.396296 3.3491,0 5.58386,-1.469147 5.58386,-3.709262 0,-6.234703 -14.11107,-2.592234 -14.11107,-14.424216 0,-6.161851 4.68148,-11.127765 14.67429,-11.127765 2.02281,0 4.39684,0.273188 6.35301,0.692079 l -0.41786,7.005692 c -1.8835,-0.558514 -4.12431,-0.910633 -6.2864,-0.910633 -3.49445,0 -5.09937,1.402354 -5.09937,3.569638 0,5.882606 14.31698,2.944332 14.31698,14.351365 0,6.93284 -6.07441,11.558794 -15.29808,11.558794 M 357.92685,70.958122 c -6.63765,-1.402354 -9.85352,2.798629 -9.85352,12.463348 v 14.217799 h -8.8724 V 69.695391 c -1.67153,-0.279267 -4.05163,-0.558514 -6.07441,-0.698137 v -5.603359 c 4.40288,-0.558514 9.78083,-0.770989 14.46231,-0.631365 0,1.821245 -0.21197,4.625953 -0.62986,7.072484 h 0.13927 c 1.67153,-4.407399 5.02667,-8.262343 11.24646,-7.703829 l -0.41789,8.826937 z m -45.12874,-2.312373 c -3.70037,0 -6.2864,2.731857 -6.77694,7.072485 h 12.78474 c 0.13927,-4.480251 -2.23475,-7.072485 -6.0078,-7.072485 m 14.59555,13.094713 h -21.51178 c -0.0727,6.441119 3.14924,9.591846 9.57493,9.591846 3.27642,0 6.77087,-0.698137 10.05943,-2.100491 l 0.97506,6.793217 c -3.77305,1.541998 -8.31524,2.312987 -12.57884,2.312987 -10.96181,0 -17.1089,-5.536566 -17.1089,-17.787438 0,-10.642103 5.86245,-18.485576 16.27313,-18.485576 10.12606,0 14.60162,6.926781 14.60162,15.541244 0,1.195938 -0.0727,2.592234 -0.28465,4.134211 m -47.21693,15.896371 h -9.08438 L 258.241,62.832966 h 9.78085 l 5.51723,15.686926 c 0.84182,2.452611 1.6776,5.25124 2.30743,7.70385 h 0.13926 c 0.55718,-2.379759 1.25364,-5.044823 2.09546,-7.424582 l 5.58387,-15.966194 h 9.50225 L 280.17669,97.636833 Z M 246.99575,57.652417 c -3.00391,0 -5.5233,-2.385817 -5.5233,-5.396942 0,-3.005045 2.51939,-5.451576 5.5233,-5.451576 3.07052,0 5.58387,2.373679 5.58387,5.451576 0,2.944332 -2.51335,5.396942 -5.58387,5.396942 m -4.4029,39.982256 V 69.696874 c -1.67151,-0.279268 -4.05163,-0.564594 -6.07441,-0.704217 v -5.597279 c 4.4029,-0.564573 9.78085,-0.770989 14.94683,-0.631365 v 34.87066 z m -22.6322,0.0021 v -22.26765 c 0,-3.569639 -0.97506,-5.742982 -4.26359,-5.742982 -4.53613,0 -8.09722,5.111616 -8.09722,11.206696 v 16.803954 h -8.87241 V 69.692913 c -1.67758,-0.279247 -4.05768,-0.558514 -6.08048,-0.698138 v -5.597279 c 4.4029,-0.564572 9.78085,-0.777047 14.46233,-0.637424 0,1.687681 -0.13927,4.413458 -0.49056,6.301475 h 0.0727 c 2.08942,-4.06136 6.14105,-6.93284 11.80366,-6.93284 7.82465,0 10.338,4.978051 10.338,11.067073 v 24.441011 h -8.8724 z m -54.55772,0.769273 c -14.88022,0 -18.22932,-8.12272 -18.22932,-17.156073 V 50.36786 h 9.08437 v 30.323617 c 0,5.949377 1.95012,10.150381 9.77478,10.150381 6.98892,0 9.92014,-2.944332 9.92014,-10.994221 V 50.36786 h 9.0117 v 28.921262 c 0,12.882239 -7.12821,19.116942 -19.56167,19.116942' /%3E%3C/svg%3E");
}



================================================
FILE: tests/test_report_zip/Snakefile
================================================

report: "report/workflow.rst"
shell.executable("bash")

rule all:
    input:
        ["fig1.svg", "testmodel.fig2.png", "test.csv", "testdir"]


rule c:
    output:
        "test.{i}.out"
    singularity:
        "docker://continuumio/miniconda3:4.4.10"
    conda:
        "envs/test.yaml"
    shell:
        "sleep `shuf -i 1-3 -n 1`; touch {output}"


rule a:
    input:
        expand("test.{i}.out", i=range(10))
    output:
        report("fig1.svg", caption="report/fig1.rst", category="Step 1")
    shell:
        "sleep `shuf -i 1-3 -n 1`; cp data/fig1.svg {output}"


rule b:
    input:
        expand("test.{i}.out", i=range(10))
    output:
        report("{model}.fig2.png", caption="report/fig2.rst", category="Step 2", subcategory="{model}")
    shell:
        "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png {output}"


rule d:
    output:
        report("test.csv", caption="report/table.rst")
    shell:
        "curl http://samplecsvs.s3.amazonaws.com/Sacramentorealestatetransactions.csv > {output}"

rule e:
    output:
        report(directory("testdir"), caption="report/testdir.rst", patterns=["{name}.txt"])
    shell:
        """
        mkdir {output}
        for x in 1 2 3
        do 
            echo $x > testdir/$x.txt
        done
        """



================================================
FILE: tests/test_report_zip/envs/test.yaml
================================================
channels:
  - conda-forge
dependencies:
  - make



================================================
FILE: tests/test_report_zip/report/fig1.rst
================================================
This is a caption with a `link <https://www.google.com>`_.



================================================
FILE: tests/test_report_zip/report/fig2.rst
================================================
Some math :math:`\sum_i i^2`.



================================================
FILE: tests/test_report_zip/report/table.rst
================================================
An example table.



================================================
FILE: tests/test_report_zip/report/testdir.rst
================================================
Files obtained from a directory. This file starts with {{ snakemake.wildcards.name }}. This value has been dynamically inferred from the given pattern.


================================================
FILE: tests/test_report_zip/report/workflow.rst
================================================
This is the workflow description. Test reference fig1.svg_.



================================================
FILE: tests/test_report_zip/testdir/1.txt
================================================
1



================================================
FILE: tests/test_report_zip/testdir/2.txt
================================================
2



================================================
FILE: tests/test_report_zip/testdir/3.txt
================================================
3



================================================
FILE: tests/test_report_zip/testdir/.snakemake_timestamp
================================================
[Empty file]


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC40Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.4.out", "incomplete": false, "starttime": 1585317519.3560429, "endtime": 1585317520.3800488, "job_hash": 3471465265311225051, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC41Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.5.out", "incomplete": false, "starttime": 1585317518.3440373, "endtime": 1585317521.3600543, "job_hash": -4453652366712017362, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC42Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.6.out", "incomplete": false, "starttime": 1585317518.3320372, "endtime": 1585317519.348043, "job_hash": -5792833771891348507, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC43Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.7.out", "incomplete": false, "starttime": 1585317518.340037, "endtime": 1585317520.3520486, "job_hash": 169637128606654253, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC44Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.8.out", "incomplete": false, "starttime": 1585317519.4240434, "endtime": 1585317522.4480605, "job_hash": 4196649577361415263, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC45Lm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.9.out", "incomplete": false, "starttime": 1585317518.3760374, "endtime": 1585317519.392043, "job_hash": 4841685047946511577, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC4wLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.0.out", "incomplete": false, "starttime": 1585317519.4040432, "endtime": 1585317522.4320605, "job_hash": 1584247219081097085, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC4xLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.1.out", "incomplete": false, "starttime": 1585317518.3680375, "endtime": 1585317521.3880546, "job_hash": 4581141678275462678, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC4yLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.2.out", "incomplete": false, "starttime": 1585317518.3840375, "endtime": 1585317520.400049, "job_hash": 7969642367240267386, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC4zLm91dA==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YKAAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgdG91Y2gge291dHB1dH1xF2gPaBKGcRhlWAUAAABzaGVsbHEZhXEadHEbLg==", "rule": "c", "input": [], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; touch test.3.out", "incomplete": false, "starttime": 1585317518.3520372, "endtime": 1585317521.3680544, "job_hash": -6992618918864179503, "conda_env": null, "container_img_url": "docker://continuumio/miniconda3:4.4.10"}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdC5jc3Y=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YVwAAAGN1cmwgaHR0cDovL3NhbXBsZWNzdnMuczMuYW1hem9uYXdzLmNvbS9TYWNyYW1lbnRvcmVhbGVzdGF0ZXRyYW5zYWN0aW9ucy5jc3YgPiB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "d", "input": [], "log": [], "params": [], "shellcmd": "curl http://samplecsvs.s3.amazonaws.com/Sacramentorealestatetransactions.csv > test.csv", "incomplete": false, "starttime": 1585317518.3600373, "endtime": 1585317519.4120433, "job_hash": 5729990202001040214, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdG1vZGVsLmZpZzIucG5n
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YMwAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgY3AgZGF0YS9maWcyLnBuZyB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "b", "input": ["test.0.out", "test.1.out", "test.2.out", "test.3.out", "test.4.out", "test.5.out", "test.6.out", "test.7.out", "test.8.out", "test.9.out"], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png testmodel.fig2.png", "incomplete": false, "starttime": 1585665922.897908, "endtime": 1585665923.9139128, "job_hash": 7401160745559470288, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/dGVzdGRpcg==
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YdQAAAAogICAgICAgIG1rZGlyIHtvdXRwdXR9CiAgICAgICAgZm9yIHggaW4gMSAyIDMKICAgICAgICBkbyAKICAgICAgICAgICAgZWNobyAkeCA+IHRlc3RkaXIvJHgudHh0CiAgICAgICAgZG9uZQogICAgICAgIHEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "e", "input": [], "log": [], "params": [], "shellcmd": "\n        mkdir testdir\n        for x in 1 2 3\n        do \n            echo $x > testdir/$x.txt\n        done\n        ", "incomplete": false, "starttime": 1585317518.3560374, "endtime": 1585317518.3680375, "job_hash": 2045751853232800755, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/ZmlnMi5wbmc=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YMwAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgY3AgZGF0YS9maWcyLnBuZyB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "b", "input": ["test.0.out", "test.1.out", "test.2.out", "test.3.out", "test.4.out", "test.5.out", "test.6.out", "test.7.out", "test.8.out", "test.9.out"], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; cp data/fig2.png fig2.png", "incomplete": false, "starttime": 1585317522.4760606, "endtime": 1585317523.5040665, "job_hash": 7638699196749507097, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_report_zip/.snakemake/metadata/ZmlnMS5zdmc=
================================================
{"version": null, "code": "gAMoQxJ0AGQBfA58EWQCjQMBAGQAUwBxAChYBQAAAGlucHV0cQFYBgAAAG91dHB1dHECWAYAAABwYXJhbXNxA1gJAAAAd2lsZGNhcmRzcQRYBwAAAHRocmVhZHNxBVgJAAAAcmVzb3VyY2VzcQZYAwAAAGxvZ3EHWAcAAAB2ZXJzaW9ucQhYBAAAAHJ1bGVxCVgJAAAAY29uZGFfZW52cQpYDQAAAGNvbnRhaW5lcl9pbWdxC1gQAAAAc2luZ3VsYXJpdHlfYXJnc3EMWA8AAAB1c2Vfc2luZ3VsYXJpdHlxDVgLAAAAZW52X21vZHVsZXNxDlgMAAAAYmVuY2hfcmVjb3JkcQ9YBQAAAGpvYmlkcRBYCAAAAGlzX3NoZWxscRFYDwAAAGJlbmNoX2l0ZXJhdGlvbnESWA8AAABjbGVhbnVwX3NjcmlwdHNxE1gKAAAAc2hhZG93X2RpcnEUdHEVXXEWKE5YMwAAAHNsZWVwIGBzaHVmIC1pIDEtMyAtbiAxYDsgY3AgZGF0YS9maWcxLnN2ZyB7b3V0cHV0fXEXaA9oEoZxGGVYBQAAAHNoZWxscRmFcRp0cRsu", "rule": "a", "input": ["test.0.out", "test.1.out", "test.2.out", "test.3.out", "test.4.out", "test.5.out", "test.6.out", "test.7.out", "test.8.out", "test.9.out"], "log": [], "params": [], "shellcmd": "sleep `shuf -i 1-3 -n 1`; cp data/fig1.svg fig1.svg", "incomplete": false, "starttime": 1585317522.4600606, "endtime": 1585317524.4880722, "job_hash": 2107624497111535029, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_resource_quoting/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
cat $1 >> qsub.log
sh $1



================================================
FILE: tests/test_resource_quoting/Snakefile
================================================
rule a:
    output:
        "test.txt"
    shell:
        "echo {resources.test} > {output}"



================================================
FILE: tests/test_resource_quoting/expected-results/test.txt
================================================
foo



================================================
FILE: tests/test_resource_quoting/test-profile/config.yaml
================================================
set-resources:
  a:
    test: "'foo'"



================================================
FILE: tests/test_resource_quoting/test-profile-default/config.yaml
================================================
default-resources:
  test: "'foo'"



================================================
FILE: tests/test_resource_string_in_cli_or_profile/Snakefile
================================================
# fails when submitted as
# $ snakemake --executor slurm -j2 --workflow-profile ./profiles/ --default-resources slurm_account=m2_zdvhpc

rule all:
     input: "a.out"

rule test1:
     output: "a.out"
     shell: "touch {output}"



================================================
FILE: tests/test_resource_string_in_cli_or_profile/profiles/config.yaml
================================================
set-resources:
    test1:
        slurm_partition: "smp"
        slurm_extra: "'--nice=150'"



================================================
FILE: tests/test_resource_tbdstring/Snakefile
================================================
rule all:
     input: "a.out"

rule test1:
     output: "a.out"
     resources:
        mem = lambda wildcards, attempt: f"{attempt}MB",
        runtime = lambda wildcards, attempt: f"{attempt}m"
     shell: "touch {output}"



================================================
FILE: tests/test_restartable_job_cmd_exit_1/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1
exit 0



================================================
FILE: tests/test_restartable_job_cmd_exit_1/qsub.py
================================================
#!/usr/bin/env python3
import sys
import os
import random

from snakemake.utils import read_job_properties

jobscript = sys.argv[1]
job_properties = read_job_properties(jobscript)
with open("qsub.log", "a") as log:
    print(job_properties, file=log)

print(random.randint(1, 100))
os.system("sh {}".format(jobscript))



================================================
FILE: tests/test_restartable_job_cmd_exit_1/Snakefile
================================================
localrules: all

rule all:
    input: '.done'

rule fails_sometimes:
    output:
        '.done'
    resources:
        mem=lambda wildcards, attempt: 100 * attempt
    shell:
        r"""
        echo {resources.mem}
        if [[ ! -f ".first" ]]; then
            touch .first
            exit 1
        else
            echo {resources.mem} > {output}
        fi
        """



================================================
FILE: tests/test_restartable_job_cmd_exit_1/expected-results/.done
================================================
200



================================================
FILE: tests/test_restartable_job_qsub_exit_1/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# ensure to fail on the first submission
if [[ ! -f ".first" ]]; then
    touch .first
    exit 1
fi
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_restartable_job_qsub_exit_1/qsub.py
================================================
#!/usr/bin/env python3
import sys
import os
import random

from snakemake.utils import read_job_properties

jobscript = sys.argv[1]
job_properties = read_job_properties(jobscript)
with open("qsub.log", "a") as log:
    print(job_properties, file=log)

print(random.randint(1, 100))
os.system("sh {}".format(jobscript))



================================================
FILE: tests/test_restartable_job_qsub_exit_1/Snakefile
================================================
localrules: all


rule all:
	input: '.done'

rule fails_sometimes:
	output:
		'.done'
	shell:
		r"""
		touch .done
		"""



================================================
FILE: tests/test_restartable_job_qsub_exit_1/expected-results/.done
================================================
[Empty file]


================================================
FILE: tests/test_retries/Snakefile
================================================
rule a:
    output:
        "test.txt"
    resources:
        shouldfail=lambda w, attempt: attempt < 3
    retries: 3
    run:
        if resources.shouldfail:
            raise ValueError("not enough attempts")
        with open(output[0], "w") as out:
            print("test", file=out)



================================================
FILE: tests/test_retries/expected-results/test.txt
================================================
test



================================================
FILE: tests/test_retries_not_overriden/Snakefile
================================================
rule a:
    output:
        "test.txt"
    resources:
        shouldfail=lambda w, attempt: attempt < 3
    retries: 0
    run:
        if resources.shouldfail:
            raise ValueError("not enough attempts")
        with open(output[0], "w") as out:
            print("test", file=out)



================================================
FILE: tests/test_retries_not_overriden/expected-results/test.txt
================================================
test



================================================
FILE: tests/test_rule_defined_in_for_loop/iteration-01.txt
================================================
[Empty file]


================================================
FILE: tests/test_rule_defined_in_for_loop/Snakefile
================================================
rule all:
  input: 'iteration-02.txt'


for i in range(2, 4):
  rule:
    output:
      fasta='iteration-{nr:02d}.txt'.format(nr=i)
    input:
      fasta='iteration-{nr:02d}.txt'.format(nr=i-1)
    run:
      shell("cp -p {input.fasta} {output.fasta}")



================================================
FILE: tests/test_rule_defined_in_for_loop/expected-results/iteration-01.txt
================================================
[Empty file]


================================================
FILE: tests/test_rule_defined_in_for_loop/expected-results/iteration-02.txt
================================================
[Empty file]


================================================
FILE: tests/test_rule_inheritance_globals/caption.rst
================================================
[Empty file]


================================================
FILE: tests/test_rule_inheritance_globals/foo.txt
================================================
test {}



================================================
FILE: tests/test_rule_inheritance_globals/Snakefile
================================================
rule a:
    output:
        "test.out"
    shell:
        "echo test '{config}' > {output}"


use rule a as b with:
    output:
        report("foo.txt", caption="caption.rst")



================================================
FILE: tests/test_rule_inheritance_globals/expected-results/report.html
================================================
[Empty file]


================================================
FILE: tests/test_rule_inheritance_globals/.snakemake/metadata/Zm9vLnR4dA==
================================================
{"version": null, "code": "gASVqgEAAAAAAAAoQxJ0AGQBfA58EWQCjQMBAGQAUwCUKIwFaW5wdXSUjAZvdXRwdXSUjAZwYXJhbXOUjAl3aWxkY2FyZHOUjAd0aHJlYWRzlIwJcmVzb3VyY2VzlIwDbG9nlIwHdmVyc2lvbpSMBHJ1bGWUjAljb25kYV9lbnaUjA1jb250YWluZXJfaW1nlIwQc2luZ3VsYXJpdHlfYXJnc5SMD3VzZV9zaW5ndWxhcml0eZSMC2Vudl9tb2R1bGVzlIwMYmVuY2hfcmVjb3JklIwFam9iaWSUjAhpc19zaGVsbJSMD2JlbmNoX2l0ZXJhdGlvbpSMD2NsZWFudXBfc2NyaXB0c5SMCnNoYWRvd19kaXKUjA1lZGl0X25vdGVib29rlIwPY29uZGFfYmFzZV9wYXRolIwHYmFzZWRpcpSMGHJ1bnRpbWVfc291cmNlY2FjaGVfcGF0aJSMGF9faXNfc25ha2VtYWtlX3J1bGVfZnVuY5R0lF2UKE6MH2VjaG8gdGVzdCAne2NvbmZpZ30nID4ge291dHB1dH2UaA9oEoaUZYwFc2hlbGyUhZR0lC4=", "rule": "b", "input": [], "log": [], "params": [], "shellcmd": "echo test '{}' > foo.txt", "incomplete": false, "starttime": 1651478820.4933672, "endtime": 1651478820.5000339, "job_hash": 8740185748842, "conda_env": null, "container_img_url": null}


================================================
FILE: tests/test_ruledag/Snakefile
================================================

rule all:
	input: expand("{id}.d", id="1 2 3 4 5".split())

rule rule1:
	input: "{id}.c"
	output: "{id}.d"
	shell: "touch {output}"

rule rule2:
	input: lambda wildcards: (expand("{id}.b", id="1 2 3".split()) if wildcards.id == "1" else expand("{id}.b", id=wildcards.id))
	output: "{id}.c"
	shell: "touch {output}"

rule rule3:
	input: "{id}.a"
	output: "{id}.b"
	shell: "touch {output}"



================================================
FILE: tests/test_ruledeps/Snakefile
================================================


rule all:
    input: "test.out"


rule a:
    output: "{sample,.+}.in"
    shell:  "touch {output}"


rule b:
    input:  rules.a.output
    output: "{sample}.inter"
    shell:  "touch {output}"


rule c:
    input:  rules.a.output
    output: "{sample}.inter"
    shell:  "exit 1"


rule d:
    input:  rules.b.output
    output: "{sample}.out"
    shell:  "touch {output}"



================================================
FILE: tests/test_run_namedlist/Snakefile
================================================
rule:
    output: txt='file.txt'
    run:
        shell('touch {output.txt}')
        os.stat(output.txt)



================================================
FILE: tests/test_run_namedlist/expected-results/file.txt
================================================
[Empty file]


================================================
FILE: tests/test_runtime_conversion_from_workflow_profile/workflow/Snakefile
================================================
rule a:
     output: "a.out"
     #resources:
     #   runtime="5m"
     shell: """
          if [ {resources.runtime} -eq 5 ]; then
             touch {output}
          else
             echo "Runtime is not 300 seconds"
             exit 1
          fi
          """



================================================
FILE: tests/test_runtime_conversion_from_workflow_profile/workflow/profiles/default/config.yaml
================================================
set-resources:
    a:
        runtime: 5m



================================================
FILE: tests/test_same_wildcard/Snakefile
================================================
shell.executable("bash")

rule:
	input:  "test_test.out"

rule:
	input: "{name}_{name}.in"
	output: "{name}_{name}.out"
	shell: "echo {wildcards.name} > {output}"



================================================
FILE: tests/test_same_wildcard/test_test.in
================================================
test



================================================
FILE: tests/test_scatter_gather/Snakefile
================================================
scattergather:
    split=8


rule all:
    input:
        "gathered/all.txt"


rule split:
    output:
        scatter.split("splitted/{scatteritem}.txt")
    shell:
        "touch {output}"


rule intermediate:
    input:
        "splitted/{scatteritem}.txt"
    output:
        "splitted/{scatteritem}.post.txt"
    shell:
        "cp {input} {output}"


rule gather:
    input:
        gather.split("splitted/{scatteritem}.post.txt")
    output:
        "gathered/all.txt"
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_scatter_gather/expected-results/gathered/all.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather/expected-results/splitted/1-of-2.post.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather/expected-results/splitted/1-of-2.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather/expected-results/splitted/2-of-2.post.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather/expected-results/splitted/2-of-2.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/Snakefile
================================================
scattergather:
    rule_a=4,
    rule_b=3,


rule all:
    input:
        "gathered/all_a.txt",
        "gathered/all_b.txt",


rule split_a:
    output:
        scatter.rule_a("split_a/{scatteritem}.txt"),
    shell:
        "touch {output}"


rule split_b:
    output:
        scatter.rule_b("split_b/{scatteritem}.txt"),
    shell:
        "touch {output}"


rule intermediate_a:
    input:
        "split_a/{scatteritem}.txt",
    output:
        "split_a/{scatteritem}.post.txt",
    shell:
        "cp {input} {output}"


rule intermediate_b:
    input:
        "split_b/{scatteritem}.txt",
    output:
        "split_b/{scatteritem}.post.txt",
    shell:
        "cp {input} {output}"


rule gather_a:
    input:
        gather.rule_a("split_a/{scatteritem}.post.txt"),
    output:
        "gathered/all_a.txt",
    shell:
        "cat {input} > {output}"


rule gather_b:
    input:
        gather.rule_b("split_b/{scatteritem}.post.txt"),
    output:
        "gathered/all_b.txt",
    shell:
        "cat {input} > {output}"



================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/gathered/all_a.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/gathered/all_b.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_a/1-of-4.post.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_a/1-of-4.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_a/2-of-4.post.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_a/2-of-4.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_a/3-of-4.post.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_a/3-of-4.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_a/4-of-4.post.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_a/4-of-4.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_b/1-of-2.post.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_b/1-of-2.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_b/2-of-2.post.txt
================================================
[Empty file]


================================================
FILE: tests/test_scatter_gather_multiple_processes/expected-results/split_b/2-of-2.txt
================================================
[Empty file]


================================================
FILE: tests/test_scheduler_sequential_all_cores/Snakefile
================================================
from pathlib import Path


running_marker = Path("running")
if workflow.is_main_process and running_marker.exists():
    running_marker.unlink()


rule all:
    input:
        expand("test.{i}.out", i=range(2))


rule a:
    output:
        "test.{i}.out"
    threads:
        max(workflow.cores - 1, 1)
    run:
        import time
        if running_marker.exists():
            raise ValueError("running file exists")

        running_marker.touch()

        time.sleep(5)
        
        with open(output[0], "w") as f:
            f.write("Hello, world!\n")
        running_marker.unlink()


================================================
FILE: tests/test_script/config.yaml
================================================
test: true
testint: 123
testfloat: 7.65432
foodict:
  key0: "val0"
  key1: "val1"
"foo' bar": "let's go" # test quote escaping in key and value



================================================
FILE: tests/test_script/Snakefile
================================================
# vim: ft=python
from pathlib import Path


configfile: "config.yaml"

rule all:
    input:
        "test.out",
        "test.html",
        "rel_source.out",
        "julia.out",
        "bash.out",


rule:
    input:
        "test.in"
    output:
        txt="test.out"
    params:
        null_param=None,
        logical_param=True,
        integer_param=123,
        double_param=123.0,
        nan_param=float("NaN"),
        inf_param=float("Inf"),
        neginf_param=float("-Inf"),
        complex_param=complex(1,2),
        character_param="abc",
        vector_param=[1, 2, 3],
        list_param=[True, 123, "abc"],
    conda:
        "envs/r.yaml"
    script:
        "scripts/test.R"


rule:
    output:
        "test.in"
    script:
        Path("scripts/test.py")


rule:
    output:
        "test.html"
    params:
        test="testparam"
    conda:
        "envs/r.yaml"
    script:
        "scripts/test.Rmd"


rule:
    output:
        "rel_source.out"
    conda:
        "envs/r.yaml"
    script:
        "scripts/rel_source.R"


rule julia:
    input:
        "test.in",
        named_input="test.in"
    output:
        "julia.out"
    params:
        integer=123,
        astring="foo\n'\\\" ", # nasty string
    conda:
        "envs/julia.yaml"
    script:
        "scripts/test.jl"


rule bash:
    input:
        "test2.in",
        named="test.in",
    output:
        "{bash}{empty}.out"
    wildcard_constraints:
        bash = "bash",
        empty = "()",
    params:
        integer=123,
        astring="foo\n'\\\" ", # nasty string
        alist=["a", "b"],
    resources:
        mem_mb=1024
    log:
        "{bash}{empty}.log"
    conda:
        "envs/bash.yaml"
    script:
        "scripts/test.sh"



================================================
FILE: tests/test_script/test2.in
================================================
1 2 3



================================================
FILE: tests/test_script/envs/bash.yaml
================================================
channels:
  - conda-forge
dependencies:
  - bash >=4.0



================================================
FILE: tests/test_script/envs/julia.yaml
================================================
channels:
  - conda-forge
  - bioconda
dependencies:
  - julia



================================================
FILE: tests/test_script/envs/r.yaml
================================================
channels:
  - conda-forge
  - bioconda
dependencies:
  - r-base =3.6.1
  - r-rmarkdown =1.17
  - xorg-libxrender
  - xorg-libxext
  - xorg-libxau
  - xorg-libxdmcp
  - xorg-libsm



================================================
FILE: tests/test_script/expected-results/test.html
================================================
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Mattias" />

<meta name="date" content="2017-03-22" />

<title>Test Report</title>










</head>

<body>




<h1 class="title toc-ignore">Test Report</h1>
<h4 class="author">Mattias</h4>
<h4 class="date">March 22, 2017</h4>



<div id="r-markdown" class="section level2">
<h2>R Markdown</h2>
<p>This is an R Markdown document.</p>
<p>Test include from snakemake testparam.</p>
</div>



<!-- code folding -->



</body>
</html>



================================================
FILE: tests/test_script/expected-results/test.in
================================================
1 2 3



================================================
FILE: tests/test_script/scripts/rel_source.R
================================================
snakemake@source("source_me.R")
cat(hi(),file=snakemake@output[[1]])



================================================
FILE: tests/test_script/scripts/source_me.R
================================================
hi <- function(){return("Hi")}



================================================
FILE: tests/test_script/scripts/test.jl
================================================
println("Julia script executing!")

@assert snakemake.config["test"] == true
@assert snakemake.config["foo' bar"] == "let's go"
@assert snakemake.params["integer"] == 123
@assert snakemake.params["astring"] == "foo\n'\\\" "
@assert snakemake.output[1] == "julia.out"
@assert snakemake.input[1] == "test.in"
@assert snakemake.input["named_input"] == "test.in"

f = open(snakemake.output[1], "w")
println(f, "Julia test succeded!")
close(f)



================================================
FILE: tests/test_script/scripts/test.py
================================================
assert snakemake.input.get("sort", "missing") == "missing"

with open(snakemake.output[0], "w") as out:
    print(1, 2, 3, file=out)



================================================
FILE: tests/test_script/scripts/test.R
================================================
print(snakemake@wildcards)
print(snakemake@threads)
print(snakemake@log)
print(snakemake@config)
print(snakemake@params)

if (!is.null(snakemake@params[["null_param"]])) {
    stop("Error evaluating null value.")
}
if (snakemake@params[["logical_param"]] != TRUE) {
    stop("Error evaluating logical value.")
}
if (snakemake@params[["integer_param"]] != 123L || typeof(snakemake@params[["integer_param"]]) != "integer") {
    stop("Error evaluating integer.")
}
if (snakemake@params[["double_param"]] != 123.0 || typeof(snakemake@params[["double_param"]]) != "double") {
    stop("Error evaluating double.")
}
if (!is.nan(snakemake@params[["nan_param"]])) {
    stop("Error evaluating NaN.")
}
if (!is.infinite(snakemake@params[["inf_param"]]) || snakemake@params[["inf_param"]] < 0) {
    stop("Error evaluating infinity.")
}
if (!is.infinite(snakemake@params[["neginf_param"]]) || snakemake@params[["neginf_param"]] > 0) {
    stop("Error evaluating negative infinity.")
}
if (snakemake@params[["complex_param"]] != 1+2i) {
    stop("Error evaluating complex.")
}
if (snakemake@params[["character_param"]] != "abc") {
    stop("Error evaluating character.")
}
if (!all.equal(snakemake@params[["vector_param"]], c(1, 2, 3))) {
    stop("Error evaluating vector.")
}
if (!identical(snakemake@params[["list_param"]], list(TRUE, 123L, "abc"))) {
    stop("Error evaluating list.")
}

if (snakemake@config[["test"]] != TRUE) {
    stop("Error evaluating config.")
}

if (snakemake@config[["foo\' bar"]] != "let\'s go") {
    stop("Error with the key/value containing single quotes.")
}

values <- scan(snakemake@input[[1]])
write(values, file = snakemake@output[["txt"]])



================================================
FILE: tests/test_script/scripts/test.Rmd
================================================
---
title: "Test Report"
author: "Mattias"
date: "March 22, 2017"
output:
  html_document:
    highlight: null
    number_sections: no
    theme: null
    mathjax: null
---


## R Markdown

This is an R Markdown document.

Test include from snakemake `r snakemake@params[["test"]]`.



================================================
FILE: tests/test_script/scripts/test.sh
================================================
#!/usr/bin/env bash
set -euo pipefail
exec > "${snakemake_output[0]}" 2> "${snakemake_log[0]}"
set -x

# Awkward characters are robustly quoted?
test "${snakemake_params[astring]}" = $'foo\n\'\\\" ' || echo MISMATCH

echo "The first input file is ${snakemake_input[0]}"
echo "The named input file is ${snakemake_input[named]}"
echo "The requested number of threads is ${snakemake[threads]}"

echo "snakemake_config is type ${snakemake_config@a}"

echo "The list passed as a parameter is *${snakemake_params[alist]}*"
echo "The wildcards are *${snakemake_wildcards[bash]}* and *${snakemake_wildcards[empty]}*"
echo "The config items are *${snakemake_config[test]}* *${snakemake_config[testint]}* *${snakemake_config[testfloat]}*"
echo "The config item with quotes in is *${snakemake_config['foo'"'"' bar']}*"



================================================
FILE: tests/test_script_pre_py39/env.yaml
================================================
channels:
  - conda-forge
  - nodefaults
dependencies:
  - python =3.7


================================================
FILE: tests/test_script_pre_py39/script.py
================================================
from snakemake.shell import shell
print("test", file=open(snakemake.output[0], "w"))
shell("echo test")


================================================
FILE: tests/test_script_pre_py39/Snakefile
================================================
rule a:
    output:
        "test.out"
    conda:
        "env.yaml"
    script:
        "script.py"


================================================
FILE: tests/test_script_py/config.yaml
================================================
test: true



================================================
FILE: tests/test_script_py/Snakefile
================================================
shell.executable("bash")


configfile: "config.yaml"


rule all:
    input:
        "test.out",
        "explicit_import.py.out",


rule:
    output:
        "test.out",
    script:
        "scripts/test.py"


rule:
    output:
        "explicit_import.py.out",
    script:
        "scripts/test_explicit_import.py"



================================================
FILE: tests/test_script_py/scripts/test.py
================================================
with open(snakemake.output[0], "w") as out:
    print(1, 2, 3, file=out)



================================================
FILE: tests/test_script_py/scripts/test_explicit_import.py
================================================
from snakemake.script import snakemake

with open(snakemake.output[0], "w") as out:
    print(1, 2, 3, file=out)



================================================
FILE: tests/test_script_rs/config.yaml
================================================
test: true



================================================
FILE: tests/test_script_rs/Snakefile
================================================
from pathlib import Path


configfile: "config.yaml"


rule all:
    input:
        "rust.out",
        "rust-manifest.out",
        "rust-outer-line-doc.out",


rule python:
    output:
        "test.in"
    script:
        Path("scripts/test.py")


rule simple:
    input:
        "test.in",
        "test2.in",
        ["test.in", "test2.in"],
        named_input="test.in",
    output:
        "rust.out",
    params:
        integer=123
    conda:
        "envs/rust.yaml"
    script:
        "scripts/test.rs"

rule manifest_doc:
    output:
        "rust-manifest.out",
    params:
        keep="-"
    conda:
        "envs/rust.yaml"
    log:
        "rust-manifest.log"
    script:
        "scripts/test-manifest.rs"

rule manifest_outer_line_doc:
    output:
        "rust-outer-line-doc.out",
    params:
        keep="-",
    conda:
        "envs/rust.yaml"
    script:
        "scripts/test-outer-line-doc.rs"



================================================
FILE: tests/test_script_rs/test2.in
================================================
1 2 3



================================================
FILE: tests/test_script_rs/envs/rust.yaml
================================================
channels:
  - conda-forge
  - bioconda
dependencies:
  - rust-script>=0.35.0
  - openssl
  - c-compiler
  - pkg-config



================================================
FILE: tests/test_script_rs/scripts/test-manifest.rs
================================================
//! This is a regular crate doc comment, but it also contains a partial
//! Cargo manifest.  Note the use of a *fenced* code block, and the
//! `cargo` "language".
//!
//! ```cargo
//! [dependencies]
//! csv = "1.1"
//! serde = { version = "1.0", features = ["derive"] }
//! ```


use std::error::Error;
use std::io::{BufWriter, Write};
use std::fs::File;
use serde_derive::Deserialize;


const BED: &[u8] = b"chrom1	1	15	foo	454	-
chrom1	40	45	bar	2	+
chrom2	4	45	baz	2	-
";

#[allow(dead_code)]
#[derive(Deserialize)]
struct BedRecord {
    chrom: String,
    start: u64,
    end: u64,
    name: Option<String>,
    score: Option<u16>,
    strand: Option<char>,
}

fn main() -> Result<(), Box<dyn Error>> {
    snakemake.redirect_stderr(&snakemake.log[0])?;
    let f_out = File::create(&snakemake.output[0])?;

    let mut ostream = BufWriter::new(f_out);
    println!("Loaded");

    let keep_strand = match &snakemake.params.keep {
        s if s.len() == 1 => Some(s.chars().next().unwrap() as char),
        _ => None,
    };

    println!("Reading BED file...");
    let mut rdr = csv::ReaderBuilder::new().has_headers(false).delimiter(b'\t').from_reader(BED);
    for result in rdr.deserialize() {
        // Notice that we need to provide a type hint for automatic
        // deserialization.
        let record: BedRecord = result?;
        let l = record.end - record.start;
        if record.strand == keep_strand {
            write!(&mut ostream, "{}\t{}\n", record.chrom, l)?;
        }
    }
    println!("Output written to {}", &snakemake.output[0]);
    Ok(())
}



================================================
FILE: tests/test_script_rs/scripts/test-outer-line-doc.rs
================================================
/// This is a regular outer line doc comment, but it also contains a partial
/// Cargo manifest.  Note the use of a *fenced* code block, and the
/// `cargo` "language".
///
/// ```cargo
/// [dependencies]
/// csv = "1.1"
/// serde = { version = "1.0", features = ["derive"] }
/// ```


use std::error::Error;
use std::io::{BufWriter, Write};
use std::fs::File;
use serde_derive::Deserialize;


static BED: &[u8] = b"chrom1	1	15	foo	454	-
chrom1	40	45	bar	2	+
chrom2	4	45	baz	2	-
";

#[allow(dead_code)]
#[derive(Debug, Deserialize)]
struct BedRecord {
    chrom: String,
    start: u64,
    end: u64,
    name: Option<String>,
    score: Option<u16>,
    strand: Option<char>,
}

fn main() -> Result<(), Box<dyn Error>> {
    let f_out = File::create(&snakemake.output[0])?;

    let mut ostream = BufWriter::new(f_out);
    println!("Loaded");

    let keep_strand = match &snakemake.params.keep {
        s if s.len() == 1 => Some(s.chars().next().unwrap() as char),
        _ => None,
    };

    println!("Reading BED file...");
    let mut rdr = csv::ReaderBuilder::new().has_headers(false).delimiter(b'\t').from_reader(BED);
    for result in rdr.deserialize() {
        // Notice that we need to provide a type hint for automatic
        // deserialization.
        let record: BedRecord = result?;
        let l = record.end - record.start;
        if record.strand == keep_strand {
            write!(&mut ostream, "{}\t{}\n", record.chrom, l)?;
        }
    }
    println!("Output written to {}", &snakemake.output[0]);
    Ok(())
}



================================================
FILE: tests/test_script_rs/scripts/test.py
================================================
assert snakemake.input.get("sort", "missing") == "missing"

with open(snakemake.output[0], "w") as out:
    print(1, 2, 3, file=out)



================================================
FILE: tests/test_script_rs/scripts/test.rs
================================================
use std::io::Write;
println!("Rust script executing!");

assert_eq!(snakemake.config.test, true);
assert_eq!(snakemake.params.integer, 123);
assert_eq!(snakemake.output[0], "rust.out");
assert_eq!(snakemake.input[0], "test.in");
assert_eq!(snakemake.input.named_input, "test.in");
for (idx, val) in (&snakemake.input).into_iter().enumerate() {
    dbg!(idx, &val);
}

let input = &snakemake.input;
for value in input {
    dbg!(value);
}

let mut f = std::fs::File::create(&snakemake.output[0])?;
write!(&mut f, "Rust test succeded!")?;



================================================
FILE: tests/test_script_xsh/Snakefile
================================================
rule all:
	input:
		"test.out"


rule test_xonsh:
	output:
		"test.out"
	conda:
		"envs/xonsh.yaml"
	script:
		"scripts/test.xsh"



================================================
FILE: tests/test_script_xsh/envs/xonsh.yaml
================================================
channels:
  - conda-forge
  - bioconda
dependencies:
  - xonsh
  - bcftools



================================================
FILE: tests/test_script_xsh/scripts/test.xsh
================================================
echo @($(bcftools --help).splitlines()[1]) > @(snakemake.output[0])



================================================
FILE: tests/test_service_jobs/Snakefile
================================================
rule all:
    input:
        "test.txt",
        "test2.txt",


rule a:
    output:
        service("foo.socket")
    shell:
        "ln -s /dev/random {output}; sleep 10000"


rule b:
    input:
        "foo.socket"
    output:
        "test.txt"
    shell:
        "head -n1 {input} > {output}"


rule c:
    input:
        "foo.socket"
    output:
        "test2.txt"
    shell:
        "head -n1 {input} > {output}"


================================================
FILE: tests/test_service_jobs/expected-results/test.txt
================================================
test



================================================
FILE: tests/test_service_jobs/expected-results/test2.txt
================================================
test



================================================
FILE: tests/test_set_resources/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.txt"
    resources:
        a=2, b="bar"
    shell:
        "echo {resources.a} {resources.b} > {output}"



================================================
FILE: tests/test_set_resources/expected-results/test.txt
================================================
1 foo



================================================
FILE: tests/test_set_resources_complex/Snakefile
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    run:
        with open(input[0]) as f:
            with open(output[0], "w") as out:
                print(resources.slurm_extra, file=out)



================================================
FILE: tests/test_set_resources_complex/test.in
================================================
[Empty file]


================================================
FILE: tests/test_set_resources_complex/test-profile/config.yaml
================================================
set-resources:
    a:
        slurm_partition: "m2_gpu"
        runtime: 5
        slurm_extra: "'--nice=150 --gres=gpu:1'"


================================================
FILE: tests/test_set_threads/Snakefile
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    run:
        with open(output[0], "w") as f:
            print(threads, file=f)


================================================
FILE: tests/test_set_threads/test.in
================================================
[Empty file]


================================================
FILE: tests/test_shadow/Snakefile
================================================
rule all:
    input: "simple_shallow.out", "simple_full.out",
           "absolute_exists.out", "touched.out", "log_exists.out",
           "protected.out", "benchmark_exists.out", "minimal_ok.out"

rule shallow:
    input: "test.in"
    output: "simple_shallow.out"
    shadow: "shallow"
    shell:
        """
        echo 1 > junk.out
        cat {input} >> {output}
        echo simple_shallow >> {output}
        test ! -f more_junk.out
        """

rule full:
    input: "test.in"
    output: "simple_full.out"
    shadow: "full"
    shell:
        """
        echo 1 > more_junk.out
        cat {input} > {output}
        echo simple_full >> {output}
        test ! -f junk.out
        """

rule absolute_link:
    input: "test.in"
    output: "absolute.out", "absolute_link.out"
    shadow: "shallow"
    shell:
        """
        echo 1 > absolute.out
        DIRNAME=$(perl -e 'use Cwd "abs_path"; print abs_path(shift)' .)
        ln -s $DIRNAME/absolute.out absolute_link.out
        """

# Snakemake has check_broken_symlink so we just need to test existence
rule absolute_stat:
    input: "absolute_link.out"
    output: touch("absolute_exists.out")
    shell:
        """
        test -f {input}
        """

rule log_stat:
    input: "touched.out"
    output: touch("log_exists.out")
    shell:
        """
        test -f shadow.log
        """

rule touch:
    output: touch("touched.out")
    log: "shadow.log"
    shadow: "shallow"
    shell:
        """
        echo 1 > {log}
        """

rule protected:
    output: protected("protected.out")
    shadow: "shallow"
    shell:
        """
        touch {output}
        """

rule benchmark:
    output: touch("benchmark.out")
    benchmark: "benchmark.txt"
    shadow: "shallow"
    shell:
        """
        touch {output}
        """

rule benchmark_stat:
    input: "benchmark.out"
    output: touch("benchmark_exists.out")
    shell:
        """
        test -f benchmark.txt
        """

# Setup files for testing of shadow: "minimal"
rule minimal_setup:
    input: "test.in"
    output:
        "subdir1/subdir2/test.in",
        "subdir1/subdir2/test.symbolic.in"
    shell:
        """
        cp -P {input} {output[0]}
        cd subdir1/subdir2
        ln -s test.in test.symbolic.in
        """

# Tests relative inputs/outputs and in the current dir
rule minimal_rel_curdir:
    input: "test.in"
    output: protected("simple_minimal.out")
    benchmark: "benchmark_minimal.txt"
    log: "minimal.log"
    shadow: "minimal"
    shell:
        """
        touch minimal_junk.out
        cat {input} >> {output}
        echo simple_minimal >> {output}
        echo minimal_log > {log}
        """

# Tests relative inputs/outputs in subdirectories
rule minimal_rel_subdir:
    input: "subdir1/subdir2/test.in"
    output: "outdir/minimal.out"
    shadow: "minimal"
    shell:
        """
        touch outdir/minimal_junk.out
        touch {output}
        """

# Tests symbolic input/output
rule minimal_symbolic:
    input: "subdir1/subdir2/test.symbolic.in"
    output: "outdir/minimal_real.out",
            "outdir/minimal_symbolic.out"
    shadow: "minimal"
    shell:
        """
        touch outdir/minimal_real.out
        cd outdir
        ln -s minimal_real.out minimal_symbolic.out
        """

# Tests absolute input/output
rule minimal_absolute:
    input:
        os.path.join(os.getcwd(),"test.in")
    output: os.path.join(os.getcwd(),"outdir/minimal_absolute.out")
    shadow: "minimal"
    shell:
        """
        touch {output}
        """

# Aggregates tests for shadow: "minimal"
rule minimal_ok:
    input:  "simple_minimal.out",
            "outdir/minimal.out",
            "outdir/minimal_symbolic.out",
            os.path.join(os.getcwd(),"outdir/minimal_absolute.out")
    output: "minimal_ok.out"
    shell:
        """
        #test ! -w {input[0]}
        test -f benchmark_minimal.txt
        test -f minimal.log
        test ! -f minimal_junk.out
        test ! -f outdir/minimal_junk.out
        touch {output}
        """



================================================
FILE: tests/test_shadow/test.in
================================================
in



================================================
FILE: tests/test_shadow_copy/Snakefile
================================================
rule all:
    input: "minimal_ok.out"

# Setup files for testing of shadow: "minimal"
rule minimal_setup:
    input: "test.in"
    output:
        "subdir1/subdir2/test.in",
        "subdir1/subdir2/test.symbolic.in"
    shell:
        """
        cp -P {input} {output[0]}
        cd subdir1/subdir2
        ln -s test.in test.symbolic.in
        """

# Tests relative inputs/outputs and in the current dir
rule minimal_rel_curdir:
    input: "test.in"
    output: protected("simple_minimal.out")
    benchmark: "benchmark_minimal.txt"
    log: "minimal.log"
    shadow: "copy-minimal"
    shell:
        """
        if [ ! -f "{input}" -o -L "{input}" ]; then
            echo "Input file is symbolic link and not a copy"
            exit 1
        fi
        
        touch minimal_junk.out
        cat {input} >> {output}
        echo simple_minimal >> {output}
        echo minimal_log > {log}
        """

# Tests relative inputs/outputs in subdirectories
rule minimal_rel_subdir:
    input: "subdir1/subdir2/test.in"
    output: "outdir/minimal.out"
    shadow: "copy-minimal"
    shell:
        """
        if [ ! -f "{input}" -o -L "{input}" ]; then
            echo "Input file is symbolic link and not a copy"
            exit 1
        fi
        
        touch outdir/minimal_junk.out
        touch {output}
        """

# Tests symbolic input/output
rule minimal_symbolic:
    input: "subdir1/subdir2/test.symbolic.in"
    output: "outdir/minimal_real.out",
            "outdir/minimal_symbolic.out"
    shadow: "copy-minimal"
    shell:
        """
        if [ ! -f "{input}" -o -L "{input}" ]; then
            echo "Input file is symbolic link and not a copy"
            exit 1
        fi
        
        touch outdir/minimal_real.out
        cd outdir
        ln -s minimal_real.out minimal_symbolic.out
        """

# Tests absolute input/output
rule minimal_absolute:
    input:
        os.path.join(os.getcwd(),"test.in")
    output: os.path.join(os.getcwd(),"outdir/minimal_absolute.out")
    shadow: "copy-minimal"
    shell:
        """
        if [ ! -f "{input}" -o -L "{input}" ]; then
            echo "Input file is symbolic link and not a copy"
            exit 1
        fi
        
        touch {output}
        """

# Aggregates tests for shadow: "minimal"
rule minimal_ok:
    input:  "simple_minimal.out",
            "outdir/minimal.out",
            "outdir/minimal_symbolic.out",
            os.path.join(os.getcwd(),"outdir/minimal_absolute.out")
    output: "minimal_ok.out"
    shell:
        """
        #test ! -w {input[0]}
        test -f benchmark_minimal.txt
        test -f minimal.log
        test ! -f minimal_junk.out
        test ! -f outdir/minimal_junk.out
        touch {output}
        """



================================================
FILE: tests/test_shadow_copy/test.in
================================================
in



================================================
FILE: tests/test_shadow_prefix/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
$1



================================================
FILE: tests/test_shadow_prefix/Snakefile
================================================
shell.executable("bash")

rule all:
    input:  "test.in"
    output: "shadow_prefix"
    shadow: "shallow"
    shell: "test -h shadowdir && cp -L {input} {output}"



================================================
FILE: tests/test_shadow_prefix/test.in
================================================
in



================================================
FILE: tests/test_shadow_prefix/expected-results/shadow_prefix
================================================
in



================================================
FILE: tests/test_shadowed_log/Snakefile
================================================
rule all:
    shadow: "minimal"
    output: touch("all.out")
    log: "all.log"



================================================
FILE: tests/test_shell/Snakefile
================================================
shell.executable("bash")

rule:
	input: "test.in"
	output: "test.out"
	run:
		test = shell("echo 42;", read=True)
		assert int(test) == 42
		with open(output[0], "w") as f:
			for l in shell("cat {input}", iterable=True):
				print(l, file=f)



================================================
FILE: tests/test_shell/test.in
================================================
foo
bar



================================================
FILE: tests/test_shell_exec/Snakefile
================================================
rule a:
    output:
        "test.out"
    resources:
        shell_exec="sh"
    # image does not have bash, hence this would fail if shell_exec is not set to sh
    container: "docker://busybox:1.33"
    shell:
        "echo 'hello world' > {output}"


================================================
FILE: tests/test_singularity/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_singularity/script.py
================================================
open(snakemake.output[0], "w").close()




================================================
FILE: tests/test_singularity/Snakefile
================================================


rule a:
    input:
        "test.txt",
    output:
        "test.out",
    singularity:
        "docker://bash"
    shell:
        'ls {input}; echo "test" > {output}'


rule b:
    output:
        "test.txt",
    singularity:
        "docker://python:3.9"
    script:
        "script.py"


rule c:
    output:
        "invalid.txt",
    singularity:
        "docker://invalid/image url even with whitespace"
    shell:
        "touch {output}"



================================================
FILE: tests/test_singularity_conda/Snakefile
================================================


singularity: "docker://continuumio/miniconda3"


rule a:
    output:
        "test.out"
    conda:
        "test-env.yaml"
    shell:
        "ls /singularity; " # this fails if not in singularity container
        "bwa 2> {output} || true"



================================================
FILE: tests/test_singularity_conda/test-env.yaml
================================================
channels:
  - bioconda
  - conda-forge
dependencies:
  - bwa ==0.7.17



================================================
FILE: tests/test_singularity_global/Snakefile
================================================
singularity: "docker://bash"

rule a:
    output:
        "test.out"
    shell:
        'if [ -d "/.singularity.d/" ]; then echo "In Singularity" > {output}; else echo "Not in Singularity" > {output};  fi'



================================================
FILE: tests/test_singularity_module/module.smk
================================================


rule a:
    input:
        "test.txt"
    output:
        "test.out"
    singularity:
        "docker://bash"
    shell:
        'ls {input}; echo "test" > {output}'


rule b:
    output:
        "test.txt"
    singularity:
        "docker://python:3.8.3"
    script:
        "script.py"


rule c:
    output:
        "invalid.txt"
    singularity:
        "docker://invalid/image url even with whitespace"
    shell:
        "touch {output}"



================================================
FILE: tests/test_singularity_module/script.py
================================================
open(snakemake.output[0], "w").close()




================================================
FILE: tests/test_singularity_module/Snakefile
================================================
module test:
    snakefile: "module.smk"
    config: config


use rule * from test



================================================
FILE: tests/test_singularity_module/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_singularity_none/Snakefile
================================================
singularity: "docker://bash"

rule a:
    singularity:
        None
    output:
        "test.out"
    shell:
        'if [ -d "/.singularity.d/" ]; then echo "In Singularity" > {output}; else echo "Not in Singularity" > {output};  fi'



================================================
FILE: tests/test_slurm_mpi/pi_MPI.c
================================================
#include <stdio.h>
#include <math.h>
#include "stdlib.h"
#include "mpi.h"


double f(double x){
  return ( 4.0/(1.0 + x*x) );
}


int main(int argc,char *argv[])
{

  int myrank, size;

  MPI_Init(&argc, &argv);
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

  long long int i, n;
  double PI25DT = 3.141592653589793238462643;
  double mypi, pi, h, mysum, x;
  
  

  /* Argument handling from command line */
  if( 2 == argc ){
    n = atoll(argv[1]);
  }else{
    if( 0 == myrank ){
      n = 10000000;
      printf("Too many or no argument given; using n = %d instead.\n", n);
    }
  }
  

  MPI_Bcast(&n, 1, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);

  h = 1.0/( (double)n );
  mysum = 0.0;
  
  for(i = myrank+1 ; i <= n ; i += size){
    x = h*((double)i - 0.5);
    mysum = mysum + f(x);
  }
  
  mypi = h*mysum;

  MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
  
  /* Output result if myrank==0 */
  if( 0 == myrank ){
    char* filename = argv[1];   
    FILE *fp;
    char buf[0x100];    
    snprintf(buf, sizeof(buf), "%s", filename);
    fp = fopen(buf, "w+");
    fprintf(fp, "\nUsing %d processes and the value n = %d.\n",size,n);
    fprintf(fp, "Calculated pi: %.16f, with error %.16f\n\n", pi, fabs(pi - PI25DT));
    fclose(fp);    
  }

  MPI_Finalize();
  return 0;
}




================================================
FILE: tests/test_slurm_mpi/Snakefile
================================================
# Test case for slurm executor.
# Note that in reality, the mpi, account, and partition resources should be specified
# via --default-resources, in order to keep such infrastructure specific details out of the
# workflow definition.


localrules:
    all,
    clean,


rule all:
    input:
        "pi.calc",


rule clean:
    shell:
        "rm -f pi.calc"


rule compile:
    input:
        "pi_MPI.c",
    output:
        temp("pi_MPI"),
    log:
        "logs/compile.log",
    conda:
        "envs/mpicc.yaml"
    resources:
        mem_mb=0,
    shell:
        "mpicc -o {output} {input} &> {log}"


rule calc_pi:
    input:
        "pi_MPI",
    output:
        "pi.calc",
    log:
        "logs/calc_pi.log",
    resources:
        mem_mb=0,
        tasks=1,
        mpi="mpiexec",
    shell:
        "{resources.mpi} -n {resources.tasks} {input} 10 > {output} 2> {log}"



================================================
FILE: tests/test_slurm_mpi/envs/mpicc.yaml
================================================
channels:
  - conda-forge
  - nodefaults
dependencies:
  - openmpi-mpicc =4.1.4


================================================
FILE: tests/test_slurm_mpi/expected-results/pi.calc
================================================
[Empty file]


================================================
FILE: tests/test_solver/Snakefile
================================================
shell.executable("bash") 

rule all:
    input:
        "test.out"

rule a:
    output:
        temp("long-path-with-stange-symbols-%,._long-path-with-stange-symbols-%,._long-path-with-stange-symbols-%,._input/stange-symbols-%,._long-path/_ebio_abt3_projects_software_dev_llmga_find_refs_scratch_TEST_nyoungblut_LLMGA_find_refs_2405@30379736_2_T3_R2.fq.gz.txt")
    shell:
        "echo test > {output}"

rule b:
    input:
        "long-path-with-stange-symbols-%,._long-path-with-stange-symbols-%,._long-path-with-stange-symbols-%,._input/stange-symbols-%,._long-path/_ebio_abt3_projects_software_dev_llmga_find_refs_scratch_TEST_nyoungblut_LLMGA_find_refs_2405@30379736_2_T3_R2.fq.gz.txt"
    output:
        "test.out"
    shell:
        "echo test1 > {output}"



================================================
FILE: tests/test_source_path/workflow/Snakefile
================================================
rule a:
    output:
        "test.out"
    params:
        b=workflow.source_path("resources/test.txt")
    shell:
        "cat {params.b} > {output}"



================================================
FILE: tests/test_source_path/workflow/resources/test.txt
================================================
foo



================================================
FILE: tests/test_spaces_in_fnames/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
$1



================================================
FILE: tests/test_spaces_in_fnames/Snakefile
================================================
rule indelrealigner:
    input: bam="{base}.bam",intervals="{base}.intervals",bai='{base}.bam.bai'
    output: bam=temp("{base} realigned.bam")
    params: batch="-q ccr -l nodes=1:gpfs"
    threads: 1
    shell: "touch {output:q}"

rule realignertargetcreator:
    input: "{base}.bam", "{base}.bam.bai"
    output: temp("{base}.intervals")
    params: batch="-q ccr -l nodes=1:gpfs"
    threads: 32
    shell: "touch {output:q}"


rule indexbam:
    threads: 1
    input: '{base}.bam'
    output: temp('{base}.bam.bai')
    params: batch="-q ccr -l nodes=1:gpfs"
    shell: 'touch {output:q}'



================================================
FILE: tests/test_spaces_in_fnames/test bam file.bam
================================================
[Empty file]


================================================
FILE: tests/test_spaces_in_fnames/expected-results/test bam file realigned.bam
================================================
[Empty file]


================================================
FILE: tests/test_speed/Snakefile
================================================

rule all:
    input:
        expand("step3/{sample}.txt", sample=range(100))


rule a:
    output:
        "step1/{sample}.txt"
    shell:
        "touch {output}"


rule b:
    input:
        "step1/{sample}.txt"
    output:
        "step2/{sample}.txt"
    shell:
        "cp {input} {output}"


rule c:
    input:
        "step2/{sample}.txt"
    output:
        "step3/{sample}.txt"
    shell:
        "cp {input} {output}"



================================================
FILE: tests/test_storage/Snakefile
================================================
storage:
    provider="s3",
    retries=5


storage web:
    provider="http"


rule all:
    input:
        storage.s3(f"{config['s3_prefix']}/test2.out")


rule a:
    input:
        gg=storage("https://www.google.com"),
        gg2=storage.web("https://www.google.com")
    output:
        storage.s3(f"{config['s3_prefix']}/test.out")
    shell:
        "cp {input.gg} {output}"


rule b:
    input:
        rules.a.output[0]
    output:
        storage.s3(f"{config['s3_prefix']}/test2.out")
    shell:
        "cp {input} {output}"




================================================
FILE: tests/test_storage/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_storage_call/Snakefile
================================================
rule all:
    input:
        "test.txt"
    
rule a:
    output:
        storage(f"{config['s3_prefix']}/test.txt", retries=5)
    shell:
        "echo 'call' > {output}"

rule b:
    input:
        rules.a.output
    output:
        "test.txt"
    shell:
        "cat {input} > {output}"


================================================
FILE: tests/test_storage_call/expected-results/test.txt
================================================
call


================================================
FILE: tests/test_storage_call/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_storage_cleanup_local/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
sh $1



================================================
FILE: tests/test_storage_cleanup_local/Snakefile
================================================
rule a:
    input:
        "test.in"
    output:
        "test.out"
    shell:
        "cp {input} {output}"


rule b:
    output:
        "test.in"
    shell:
        "touch {output}"


================================================
FILE: tests/test_storage_localrule/qsub
================================================
#!/bin/bash
echo `date` >> qsub.log
tail -n1 $1 >> qsub.log
# simulate printing of job id by a random number
echo $RANDOM
export JOBID=$RANDOM

sh $1



================================================
FILE: tests/test_storage_localrule/Snakefile
================================================
localrules: a


rule a:
    input:
        "test.txt"
    output:
        "test.out"
    shell:
        "cat {input} > {output}"


rule b:
    output:
        "test.txt"
    shell:
        "echo hello > {output}"



================================================
FILE: tests/test_storage_localrule/expected-results/fs-storage/test.txt
================================================
hello



================================================
FILE: tests/test_storage_noretrieve_dryrun/Snakefile
================================================

storage fs:
    provider="fs"


rule all:
    input:
        storage("fs/test2.txt")


checkpoint foo:
    output:
        storage("fs/test.txt")
    shell:
        "echo 'test' > {output}"


rule bar:
    input:
        storage("fs/test.txt")
    output:
        storage("fs/test2.txt")
    shell:
        "cp {input} {output}"


================================================
FILE: tests/test_storage_noretrieve_dryrun/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_storage_noretrieve_dryrun/fs/test.txt
================================================
test


================================================
FILE: tests/test_strict_mode/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.out"
    shell:
        """
        echo 'prints 1'
        $(exit 1)  # Should fail with set -e
        echo 'prints 2'
        echo 'hello' > {output}
        """



================================================
FILE: tests/test_string_resources/qsub.py
================================================
#!/usr/bin/env python3
import sys
import os
import random

from snakemake.utils import read_job_properties

jobscript = sys.argv[1]
job_properties = read_job_properties(jobscript)

expected_model = "nvidia-tesla-p100" if job_properties["rule"] == "a" else "nvidia-tesla-1000"
print(job_properties)

assert job_properties["resources"]["gpu_model"] == expected_model
with open("qsub.log", "a") as log:
    print(job_properties, file=log)

print(random.randint(1, 100))
os.system("sh {}".format(jobscript))



================================================
FILE: tests/test_string_resources/Snakefile
================================================
rule b:
    input:
        "test.txt"
    output:
        "test2.txt"
    shell:
        "echo {resources.gpu_model} > {output}"

rule a:
    output:
        "test.txt"
    resources:
        gpu_model="nvidia-tesla-p100"
    shell:
        "echo {resources.gpu_model} > {output}"



================================================
FILE: tests/test_string_resources/expected-results/test.txt
================================================
nvidia-tesla-p100



================================================
FILE: tests/test_string_resources/expected-results/test2.txt
================================================
nvidia-tesla-1000



================================================
FILE: tests/test_subpath/Snakefile
================================================
rule all:
    input:
        "results/foo/bar/b.txt"

rule a:
    output:
        test="results/foo/bar/{sample}.txt",
    params:
        prefix=subpath(output.test, ancestor=2)
    shell:
        "echo {params.prefix} > {output}"


================================================
FILE: tests/test_subpath/expected-results/results/foo/bar/b.txt
================================================
results/foo



================================================
FILE: tests/test_subworkflow_config/environment.yml
================================================
test: test




================================================
FILE: tests/test_subworkflow_config/Snakefile
================================================
configfile: "environment.yml"

subworkflow test:
    snakefile: "sub.snake"

rule all:
    input: test('test.txt')




================================================
FILE: tests/test_subworkflow_config/sub.snake
================================================
rule all:
    output: touch('test.txt')



================================================
FILE: tests/test_subworkflow_config/expected-results/test.txt
================================================
[Empty file]


================================================
FILE: tests/test_subworkflows/Snakefile
================================================
import os.path

subworkflow test02:
    workdir: config["subworkdir"]
    configfile: "subconfig.yaml" # test optional config files

rule:
    input: "test.out"


def test_in(wildcards):
    return test02("test.out")


rule:
    input: test_in
    output: "test.out"
    shell: "cp {input} {output}"



================================================
FILE: tests/test_subworkflows/subconfig.yaml
================================================
test:
    "xy"



================================================
FILE: tests/test_symlink_temp/a
================================================
[Empty file]


================================================
FILE: tests/test_symlink_temp/b
================================================
[Empty file]


================================================
FILE: tests/test_symlink_temp/Snakefile
================================================
rule all:
    input: 'a.out', 'b.out'

rule one: #the first rule in the work-flow
    input: '{sample}'
    output: temp('1.{sample}')
    shell: 'touch {output}'

rule two: #this rule simply creates symbol link, following rule one
    input: '1.{sample}'
    output: temp('2.{sample}')
    shell: 'ln -s {input} {output}'

rule three: #this rule simply creates symbol link, following rule two
    input: '2.{sample}'
    output: temp('3.{sample}')
    shell: 'ln -s {input} {output}'

rule four: #this rule creates the final output, following rule three
    input: '3.{sample}'
    output: '{sample}.out'
    shell: 'touch {output}'



================================================
FILE: tests/test_symlink_time_handling/Snakefile
================================================
# vim: ft=python
"""Snakemake should work where the input/output of a rule is a symlink, no problem.
   Here we test that updates to symlinks are recognised and that the timestamps
   on symlinks are updated properly.

   Unfortunately this does not work on some Python builds where
   os.supports_follow_symlinks does not include utime().  This includes the Miniconda
   build used on continuum.io.  Therefore this test is an expected failure on those
   systems.

   In practise, the impact of the bug is low - lutime() on a link will just be a no-op
   and users will see a warning message.  Anyone for whom this is a real problem should
   install a fully-working version of Python.
"""

"""Description of the test:

   input_file is a file that is 1 hour old
   input_link is a link to input file that is 4 hours old
   output_link is a symlink to input_link that is 2 hours old

   So output_link needs to be re-evaluated since the contents of input_file changed.

   rule main outputs the time difference between inout and output in hours which
   can be checked by Nose to ensure output_link gets touched by Snakemake but
   neither input_file nor input_link are changed.
"""

import os
import time

#Slightly crude way to stop this running twice
if not os.path.exists("input_file"):
    def timestr(hr_delta=0):
        return time.strftime("%Y%m%d%H%M",
                             time.localtime(time.time() - (hr_delta * 3600) - 5))

    shell("touch -t {} input_file".format(timestr(1)))
    shell("ln -s input_file input_link")
    shell("touch -h -t {} input_link".format(timestr(4)))

    shell("ln -s input_link output_link")
    shell("touch -h -t {} output_link".format(timestr(2)))

    shell("ls -lR > /dev/stderr")

rule main:
    output: "time_diff.txt"
    input: "output_link"
    run:
        time_diff1 = int( (
            os.stat("output_link", follow_symlinks=False).st_mtime -
            os.stat("input_link", follow_symlinks=False).st_mtime
        ) / (60*60) )
        time_diff2 = int( (
            os.stat("output_link", follow_symlinks=False).st_mtime -
            os.stat("input_file", follow_symlinks=False).st_mtime
        ) / (60*60) )
        # I expect the result "4 1"
        shell("ls -lR > /dev/stderr")
        shell("echo {time_diff1} {time_diff2} | tee time_diff.txt 2>&1")

rule make_output:
    output: "output_link"
    input: "input_link"
    run:
        #shell("rm -f {output}") - no longer necessary
        shell("ln -s {input} {output}")
        #This next command should be undone by Snakemake which now touches all outpout
        shell("touch -h -t 201604011600 {output}")



================================================
FILE: tests/test_symlink_time_handling/expected-results/time_diff.txt
================================================
4 1



================================================
FILE: tests/test_temp/Snakefile
================================================
rule indelrealigner:
    input: bam="{base}.bam",intervals="{base}.intervals",bai='{base}.bam.bai'
    output: bam=temp("{base}.realigned.bam")
    params: batch="-q ccr -l nodes=1:gpfs"
    threads: 1
    shell: "touch {output}"

rule realignertargetcreator:
    input: "{base}.bam", "{base}.bam.bai"
    output: temp("{base}.intervals")
    params: batch="-q ccr -l nodes=1:gpfs"
    threads: 32
    shell: "touch {output}"


rule indexbam:
    threads: 1
    input: '{base}.bam'
    output: temp('{base}.bam.bai')
    params: batch="-q ccr -l nodes=1:gpfs"
    shell: 'touch {output}'



================================================
FILE: tests/test_temp/test.bam
================================================
[Empty file]


================================================
FILE: tests/test_temp/expected-results/test.realigned.bam
================================================
[Empty file]


================================================
FILE: tests/test_temp_and_all_input/Snakefile
================================================
rule all:
    input:
        "foo.txt"

rule a:
    output:
        temp("foo.txt")
    shell:
        "touch {output}"



================================================
FILE: tests/test_temp_and_all_input/expected-results/foo.txt
================================================
[Empty file]


================================================
FILE: tests/test_temp_expand/Snakefile
================================================


rule:
    input:
        "a.txt"
    output:
        "test.txt"
    shell:
        "touch {output}"


rule:
    output:
        temp(expand("{ds}.txt", ds="a b c".split()))
    shell:
        "touch {output}"



================================================
FILE: tests/test_temp_expand/expected-results/test.txt
================================================
[Empty file]


================================================
FILE: tests/test_template_engine/Snakefile
================================================
rule all:
    input:
        expand("rendered.{engine}", engine=["yte", "jinja2"])


rule render_yte_template:
    input:
        "template.yte.yaml"
    output:
        "rendered.yte"
    template_engine:
        "yte"


rule render_jinja2_template:
    input:
        "template.jinja2.txt"
    output:
        "rendered.jinja2"
    template_engine:
        "jinja2"


================================================
FILE: tests/test_template_engine/template.jinja2.txt
================================================
{% set foo = 1 %}

foo: {{ foo }}


================================================
FILE: tests/test_template_engine/template.yte.yaml
================================================
?if True:
  foo: 1
?else:
  bar: 2


================================================
FILE: tests/test_template_engine/expected-results/rendered.jinja2
================================================


foo: 1


================================================
FILE: tests/test_template_engine/expected-results/rendered.yte
================================================
foo: 1



================================================
FILE: tests/test_tes/Snakefile
================================================

rule all:
    input:
        "test_input.txt"
    output:
        "test_output.txt"
    log:
        "test_log.txt"
    shell:
        """
        echo "task submitted to tes"
        """



================================================
FILE: tests/test_tes/test_input.txt
================================================
test


================================================
FILE: tests/test_tes/expected-results/.gitkeep
================================================
[Empty file]


================================================
FILE: tests/test_threads/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.out"
    threads: 20
    run:
        print(threads)
        shell("echo {threads} > {output}")



================================================
FILE: tests/test_threads0/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.out"
    threads: 0
    shell:
        "echo {threads} > {output}"



================================================
FILE: tests/test_tibanna/README.md
================================================
```
# the following has been performed by aws admin already
tibanna deploy_unicorn --usergroup=johannes --buckets=snakemake-tibanna-test,snakemake-tibanna-test2
```
```
# run the following to do a test run
pip install -U tibanna
python cleanup.py  # first delete output files that are already on s3
export TIBANNA_DEFAULT_STEP_FUNCTION_NAME=tibanna_unicorn_johannes
snakemake --tibanna --use-conda --configfile=config.json --default-remote-prefix=snakemake-tibanna-test/1
```




================================================
FILE: tests/test_tibanna/cleanup.py
================================================
import subprocess
import boto3

# clean up local
subprocess.call(["rm", "-rf", ".snakemake"])
subprocess.call(["rm", "-rf", "snakemake-tibanna-test"])

# clean up s3
s3 = boto3.client("s3")
s3.delete_objects(
    Bucket="snakemake-tibanna-test",
    Delete={
        "Objects": [
            {"Key": "1/message1"},
            {"Key": "1/message2"},
            {"Key": "1/next_message"},
            {"Key": "1/final_message"},
        ]
    },
)
s3.delete_objects(
    Bucket="snakemake-tibanna-test2", Delete={"Objects": [{"Key": "1/final_message"}]}
)



================================================
FILE: tests/test_tibanna/config.json
================================================
{"message": "hahaha"}




================================================
FILE: tests/test_tibanna/env.yml
================================================
name: pairix
channels:
  - bioconda
dependencies:
  - pairix



================================================
FILE: tests/test_tibanna/Snakefile
================================================
# vim: syntax=python
#

# step1 and step1a are grouped. step1b must run concurrently with step1+step1a.
# step2 waits for step1, step1a and step1b to finish.
# all is the final rule.
# step1, step1a and step1b use a conda environment.
# Input of step1, step1b and all are set remote.
# Output of step2 is set remote.

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider

S3 = S3RemoteProvider()


configfile: "config.json"


# envvars:
#    "TEST_ENVVAR1",
#    "TEST_ENVVAR2"


localrules:
    all,


rule all:
    conda:
        "env.yml"
    input:
        final=S3.remote("snakemake-tibanna-test/1/final_message"),
        # final = "snakemake-tibanna-test/1/final_message"
        final2=S3.remote("snakemake-tibanna-test2/1/final_message"),
    shell:
        "echo test"


rule step1:
    conda:
        "env.yml"
    group:
        "mygroup"
    input:
        S3.remote("snakemake-tibanna-test/1/somefile"),
    output:
        message="message1",
    shell:
        """
        echo {config[message]} > message1
        cat snakemake-tibanna-test/1/somefile >> message1
        """


rule step1a:
    conda:
        "env.yml"
    group:
        "mygroup"
    input:
        message="message1",
    output:
        next_message="next_message",
    shell:
        """
        cat message1 message1 > next_message
        """


rule step1b:
    conda:
        "env.yml"
    input:
        S3.remote("snakemake-tibanna-test/1/somefile2"),
    output:
        message="message2",
    log:
        "log.txt",
    benchmark:
        "benchmark.txt"
    threads: 1
    resources:
        mem_mb=1024,
    shell:
        """
        echo "abcdefg" > message2
        cat snakemake-tibanna-test/1/somefile2 >> message2
        """


rule step2:
    input:
        message1="next_message",
        message2="message2",
    output:
        final=S3.remote("snakemake-tibanna-test/1/final_message"),
        # final = "snakemake-tibanna-test/1/final_message"
        final2=S3.remote("snakemake-tibanna-test2/1/final_message"),
    script:
        "scripts/step2.py"



================================================
FILE: tests/test_tibanna/expected-results/.gitkeep
================================================




================================================
FILE: tests/test_tibanna/scripts/step2.py
================================================
from shutil import copyfile

copyfile("message2", "snakemake-tibanna-test/1/final_message")
copyfile("message2", "snakemake-tibanna-test2/1/final_message")



================================================
FILE: tests/test_tmpdir/Snakefile
================================================
rule a:
    input:
        "test2.txt"
    output:
        "test.txt"
    shell:
        "echo {resources.tmpdir} > {output}"


rule b:
    output:
        "test2.txt"
    resources:
        tmpdir="foo"
    shell:
        "echo {resources.tmpdir} $TMPDIR $TEMP $TMP $TEMPDIR > {output}"



================================================
FILE: tests/test_tmpdir/expected-results/test.txt
================================================
/tmp



================================================
FILE: tests/test_tmpdir/expected-results/test2.txt
================================================
foo foo foo foo foo



================================================
FILE: tests/test_toposort/Snakefile
================================================
from snakemake.dag import toposort

graph1 = {"job4": ["job2", "job3"], "job3": ["job1"], "job2": ["job1"]}
assert toposort(graph1) == [{'job1'}, {'job2', 'job3'}, {'job4'}], "Basic graph toposort failed"

graph2 = {}
assert toposort(graph2) == [], "Empty graph toposort failed"

graph3 = {"job1": [], "job2": []}
assert toposort(graph3) == [{'job1', 'job2'}], "Disconnected graph toposort failed"



================================================
FILE: tests/test_touch/Snakefile
================================================
rule:
    output:
        touch("test.out")
    shell:
        "exit 0"



================================================
FILE: tests/test_touch_pipeline_with_temp_dir/out.txt
================================================
tmp




================================================
FILE: tests/test_touch_pipeline_with_temp_dir/Snakefile
================================================
rule all:
    input: "out.txt"

rule input:
    output: temp("tmp.txt")
    run:
        with open(f"{output}", "w") as output:
            print("tmp", file=output)

rule intemediate:
    input: "tmp.txt"
    output: "out.txt"
    run:
        with open(f"{input}") as input:
          with open(f"{output}", "w") as output:
            print(*input.readlines(), file=output)


================================================
FILE: tests/test_touch_pipeline_with_temp_dir/expected-results/out.txt
================================================
tmp




================================================
FILE: tests/test_touch_remote_prefix/Snakefile.touch
================================================
rule all:
    input: "fileJob2.txt"

rule job1:
    output: "fileJob1"
    shell: "echo Hello World -- job1 > {output}"

rule job2:
    input: "fileJob1"
    output: touch("fileJob2.txt")
    shell: "echo Hello World! -- job2"



================================================
FILE: tests/test_touch_remote_prefix/expected-results/fileJob1
================================================
Hello World -- job1



================================================
FILE: tests/test_touch_remote_prefix/expected-results/fileJob2.txt
================================================
[Empty file]


================================================
FILE: tests/test_touch_with_directories/Snakefile
================================================
# need a dummy file in expected-results in order to commit it to git
with open("out", 'w'):
    pass

rule a:
    output: directory('output/'),


================================================
FILE: tests/test_unpack_dict/prefix.in1.txt
================================================
1



================================================
FILE: tests/test_unpack_dict/prefix.in2.txt
================================================
2



================================================
FILE: tests/test_unpack_dict/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        'prefix.out1.txt',
        'prefix.out2.txt',


def get_in(wildcards):
    result = {'one': '{wildcards.prefix}.in1.txt', 'two': '{wildcards.prefix}.in2.txt'}
    return {key: value.format(wildcards=wildcards) for key, value in result.items()}


def get_out():
    return {'one': '{prefix}.out1.txt', 'two': '{prefix}.out2.txt'}


rule copy_files:
    input:
        unpack(get_in)
    output:
        **get_out()
    shell:
        """
        cp {input.one} {output.one}
        cp {input.two} {output.two}
        """






================================================
FILE: tests/test_unpack_dict/expected-results/prefix.out1.txt
================================================
1



================================================
FILE: tests/test_unpack_dict/expected-results/prefix.out2.txt
================================================
2



================================================
FILE: tests/test_unpack_list/prefix.in1.txt
================================================
1



================================================
FILE: tests/test_unpack_list/prefix.in2.txt
================================================
2



================================================
FILE: tests/test_unpack_list/Snakefile
================================================
shell.executable("bash")

rule all:
    input:
        'prefix.out1.txt',
        'prefix.out2.txt',


def get_in(wildcards):
    result = ['{wildcards.prefix}.in1.txt', '{wildcards.prefix}.in2.txt']
    return [x.format(wildcards=wildcards) for x in result]


def get_out():
    return ['{prefix}.out1.txt', '{prefix}.out2.txt']


rule copy_files:
    input:
        unpack(get_in)
    output:
        *get_out()
    shell:
        r"""
        cp {input[0]} {output[0]}
        cp {input[1]} {output[1]}
        """





================================================
FILE: tests/test_unpack_list/expected-results/prefix.out1.txt
================================================
1



================================================
FILE: tests/test_unpack_list/expected-results/prefix.out2.txt
================================================
2



================================================
FILE: tests/test_until/Snakefile
================================================

rule all: 
    input:
        "levelthree.txt",
        "independent.txt",
        expand("test{num}.final", num=[1, 2])

rule levelone:
    output: "levelone.txt"
    shell: "touch {output}"

rule leveltwo_first:
    input: rules.levelone.output
    output: "leveltwo_first.txt"   
    shell: "cp -f {input} {output}"

rule leveltwo_second:
    input: rules.levelone.output
    output: "leveltwo_second.txt"
    shell: "cp -f {input} {output}"

rule levelthree: # should not be created
    input: 
        rules.leveltwo_first.output, 
        rules.leveltwo_second.output
    output: "levelthree.txt"
    shell: "cat {input} > {output}"

rule independent: # should be created in --omit-from but not --until
    output: "independent.txt"
    shell: "touch {output}"

###### Wildcard Rules #######

rule zeroth_wildcard:
    output: "test{num}.txt"
    shell: "touch {output}"

rule first_wildcard:
    input: 'test{num}.txt'
    output: 'test{num}.first'
    shell: 'cp -f {input} {output}'

rule second_wildcard:
    input: 'test{num}.first'
    output: 'test{num}.second'
    shell: 'cp -f {input} {output}'

rule final_wildcard:
    input: 'test{num}.second'
    output: 'test{num}.final'
    shell: 'cp -f {input} {output}'





================================================
FILE: tests/test_until/expected-results/levelone.txt
================================================
[Empty file]


================================================
FILE: tests/test_until/expected-results/leveltwo_first.txt
================================================
[Empty file]


================================================
FILE: tests/test_until/expected-results/leveltwo_second.txt
================================================
[Empty file]


================================================
FILE: tests/test_update_config/cp.rule
================================================
# -*- snakemake -*-
from snakemake.utils import update_config

c = {
    'cp': {
        'options': '-f',
        },
    }

update_config(config, c)

rule cp:
    params: options = config['cp']['options']
    input: '{prefix}.out'
    output: temporary('{prefix}.out.copy')
    shell: 'cp {params.options} {input} {output}'



================================================
FILE: tests/test_update_config/echo.rule
================================================
# -*- snakemake -*-
configfile: 'echo.yaml'

rule echo:
    params: options = config['echo']['options']
    output: temporary('{prefix}.out')
    shell: 'echo "{params.options}" > {output}'



================================================
FILE: tests/test_update_config/echo.yaml
================================================
echo:
  options: ''



================================================
FILE: tests/test_update_config/Snakefile
================================================
import yaml

include: "echo.rule"
include: "cp.rule"

configfile: "test.yaml"

rule all:
    input: "test.out.copy"
    output: yaml = "config.yaml"
    run:
        with open(str(output.yaml), "w") as fh:
            fh.write(yaml.dump(config, default_flow_style=False))




================================================
FILE: tests/test_update_config/test.yaml
================================================
cp:
  options: -u
echo:
  options: echo



================================================
FILE: tests/test_update_config/expected-results/config.yaml
================================================
cp:
  options: -u
echo:
  options: echo



================================================
FILE: tests/test_update_flag/in.txt
================================================
Mon Mar 18 10:22:50 AM CET 2024



================================================
FILE: tests/test_update_flag/Snakefile
================================================
shell("date > in.txt")


rule all:
    input:
        "out.txt" if exists("test.txt") else [],
        "test.txt"


rule b:
    input:
        before_update("test.txt")
    output:
        "out.txt"
    shell:
        "cp {input} {output}"


rule a:
    input:
        "in.txt"
    output:
        update("test.txt")
    shell:
        "echo bar >> test.txt"


================================================
FILE: tests/test_update_flag/test.txt
================================================
foo



================================================
FILE: tests/test_update_flag/expected-results/out.txt
================================================
foo



================================================
FILE: tests/test_update_flag/expected-results/test.txt
================================================
foo
bar



================================================
FILE: tests/test_url_include/Snakefile
================================================


include: "https://github.com/snakemake/snakemake/raw/main/tests/test05/Snakefile"

rule:
	input: "test.predictions"



================================================
FILE: tests/test_url_include/test.in
================================================
testz0r



================================================
FILE: tests/test_url_include/expected-results/test.1.inter
================================================
testz0r
Part test.1.inter



================================================
FILE: tests/test_url_include/expected-results/test.1.inter2
================================================
testz0r
Part test.1.inter



================================================
FILE: tests/test_url_include/expected-results/test.2.inter
================================================
testz0r
Part test.2.inter



================================================
FILE: tests/test_url_include/expected-results/test.2.inter2
================================================
testz0r
Part test.2.inter



================================================
FILE: tests/test_url_include/expected-results/test.3.inter
================================================
testz0r
Part test.3.inter



================================================
FILE: tests/test_url_include/expected-results/test.3.inter2
================================================
testz0r
Part test.3.inter



================================================
FILE: tests/test_url_include/expected-results/test.predictions
================================================
testz0r
Part test.1.inter
testz0r
Part test.2.inter
testz0r
Part test.3.inter



================================================
FILE: tests/test_use_rule_same_module/Snakefile
================================================
shell.executable("bash")

rule a:
    output:
        "test.out"
    shell:
        "echo test > {output}"


use rule a as b with:
    output:
        "test2.out"


================================================
FILE: tests/test_validate/config.fail.yaml
================================================
samples: samples.tsv
adapter: ACGTX



================================================
FILE: tests/test_validate/config.schema.yaml
================================================
$schema: "https://json-schema.org/draft/2020-12/schema"

description: snakemake configuration file

type: object

properties:
  samples:
    type: string
  adapter:
    type: string
    pattern: "^[ACGT]+$"

required:
  - samples



================================================
FILE: tests/test_validate/config.yaml
================================================
samples: samples.tsv
adapter: ACGT



================================================
FILE: tests/test_validate/samples.schema.yaml
================================================
$schema: "https://json-schema.org/draft/2020-12/schema"
description: an entry in the sample sheet
properties:
  sample:
    type: string
    description: sample name/identifier
  condition:
    type: string
    description: sample condition
  n:
    type: integer
    default: 0
    description: replicate count
  tissue:
    type: string
    default: blood
    description: sample tissue of origin

required:
  - sample
  - condition



================================================
FILE: tests/test_validate/samples.tsv
================================================
sample	condition	n
A	case	1
B	control	NA



================================================
FILE: tests/test_validate/Snakefile
================================================
shell.executable("bash")

import pandas as pd
import polars as pl
from snakemake.utils import validate


configfile: "config.yaml"


# Dict
df = pd.read_table(config["samples"])
samples = df.iloc[0].to_dict()
validate(samples, "samples.schema.yaml")
assert samples["tissue"] == "blood"
assert samples["n"] == 1
samples = {k: v for k, v in df.iloc[1].to_dict().items() if pd.notnull(v)}
validate(samples, "samples.schema.yaml")
assert samples["tissue"] == "blood"
assert samples["n"] == 0

# Pandas DataFrame without index
samples = pd.read_table(config["samples"])
validate(samples, "samples.schema.yaml")
assert samples.iloc[0]["tissue"] == "blood"
assert samples.iloc[0]["n"] == 1
assert samples.iloc[1]["n"] == 0

# Polars DataFrame
samples = pl.read_csv(
    config["samples"],
    separator="\t",
    schema={"sample": pl.String, "condition": pl.String, "n": pl.UInt8},
    null_values="NA",
)
validate(samples, "samples.schema.yaml")
assert samples[0, "tissue"] == "blood"
assert samples[0, "n"] == 1
assert samples[1, "n"] == 0

# Polars LazyFrame
samples = pl.scan_csv(
    config["samples"],
    separator="\t",
    schema={"sample": pl.String, "condition": pl.String, "n": pl.UInt8},
    null_values="NA",
)
validate(samples, "samples.schema.yaml", set_default=False)
assert samples.collect()[0, "n"] == 1

# Pandas DataFrame with index
validate(config, "config.schema.yaml")
samples = pd.read_table(config["samples"]).set_index("sample", drop=False)
validate(samples, "samples.schema.yaml")
assert samples.iloc[0]["tissue"] == "blood"
assert samples.iloc[0]["n"] == 1
assert samples.iloc[1]["n"] == 0


rule all:
    input:
        expand("test.{sample}.txt", sample=samples.index),


module test:
    snakefile:
        "module-test/Snakefile"
    config:
        config


use rule * from test as test_*



================================================
FILE: tests/test_validate/expected-results/test.A.txt
================================================
[Empty file]


================================================
FILE: tests/test_validate/expected-results/test.B.txt
================================================
[Empty file]


================================================
FILE: tests/test_validate/module-test/config.schema.yaml
================================================
$schema: "https://json-schema.org/draft/2020-12/schema"

description: snakemake configuration file

type: object

properties:
  samples:
    type: string
  adapter:
    type: string
    pattern: "^[ACTG]+$"

required:
  - samples



================================================
FILE: tests/test_validate/module-test/Snakefile
================================================
shell.executable("bash")

import pandas as pd
from snakemake.utils import validate


validate(config, "config.schema.yaml")


rule a:
    output:
        "test.{sample}.txt",
    shell:
        "touch {output}"



================================================
FILE: tests/test_wildcard_count_ambiguity/Snakefile
================================================


rule all:
    input: "test.out"


rule a:
    output: "{prefix}.out"
    shell: "touch {output}"


rule b:
    output: "test.out"
    shell: "touch {output}"



================================================
FILE: tests/test_wildcard_keyword/Snakefile
================================================
shell.executable("bash")

rule all:
    input: "stringbar.txt", "localbar.txt", "globalbar.txt", "ambig.u.ous.log"


##: NB: setting output: "{foo}.in" only works for globalwildcard rule
## since constraints will be set to "globalbar"
rule infile:
    output: temp("{foo,(globalbar|localbar|stringbar)}.in")
    shell: "touch {output}"


rule stringwildcard:
    input: "{foo}.in"
    output: "{foo,stringbar}.txt"
    log: "{foo}.log"
    shell: "echo {input} {output} {log} > {output}; touch {log}"


rule localwildcard:
    input: "{foo}.in"
    output: "{foo}.txt"
    wildcard_constraints: foo="localbar"
    log: "{foo}.log"
    shell: "echo {input} {output} {log} > {output}; touch {log}"


rule globalwildcard:
    input: "{foo}.in"
    output: "{foo}.txt"
    log: "{foo}.log"
    shell: "echo {input} {output} {log} > {output}; touch {log}"


rule ambigouslog:
    output: "{foo}.{bar}.txt"
    log: "{foo}.{bar}.log"
    wildcard_constraints: foo="ambig", bar=".+"
    shell: "echo a={wildcards.foo} b={wildcards.bar} > {output}; touch {log}"


wildcard_constraints:
    foo='globalbar',
    bar='out/{SM,[a-z]+}_{PU,[0-9]+}'



================================================
FILE: tests/test_wildcard_keyword/expected-results/ambig.u.ous.txt
================================================
a=ambig b=u.ous



================================================
FILE: tests/test_wildcard_keyword/expected-results/globalbar.txt
================================================
globalbar.in globalbar.txt globalbar.log



================================================
FILE: tests/test_wildcard_keyword/expected-results/localbar.txt
================================================
localbar.in localbar.txt localbar.log



================================================
FILE: tests/test_wildcard_keyword/expected-results/stringbar.txt
================================================
stringbar.in stringbar.txt stringbar.log



================================================
FILE: tests/test_workflow_profile/dummy-general-profile/config.yaml
================================================
set-resources:
  a:
    foo: test-general



================================================
FILE: tests/test_workflow_profile/workflow/Snakefile
================================================
rule a:
    output:
        "test.out"
    shell:
        "echo {resources.foo} > {output}"




================================================
FILE: tests/test_workflow_profile/workflow/profiles/default/config.yaml
================================================
set-resources:
  a:
    foo: test-workflow-specific



================================================
FILE: tests/test_wrapper/Snakefile
================================================
rule compress_vcf:
    input:
        "test.vcf"
    output:
        "test.vcf.gz"
    wrapper:
        "0.42.0/bio/vcf/compress"



================================================
FILE: tests/test_wrapper/test.vcf
================================================
##fileformat=VCFv4.1
##INFO=<ID=S1,Number=1,Type=String,Description="Single INFO string">
##INFO=<ID=N1,Number=1,Type=Integer,Description="Single INFO integer">
##INFO=<ID=F1,Number=1,Type=Float,Description="Single INFO float">
##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">
##FORMAT=<ID=FS1,Number=1,Type=String,Description="Single FORMAT string">
##FORMAT=<ID=FN1,Number=1,Type=Integer,Description="Single FORMAT integer">
##FORMAT=<ID=FF1,Number=1,Type=Float,Description="Single FORMAT float">
#CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	one	two
19	3111939	rs1234	A	AG	.	PASS	S1=string1;N1=1;F1=1.0	GT:FS1:FN1	./1:LongString1:1	1/1:ss1:2
19	3113255	rs2345	G	GC	.	PASS	S1=string2;N1=2;F1=2.0	GT:FS1:FN1	1|1:LongString2:1	1/1:ss2:2
19	3113259	rs2345	G	GC	.	PASS	S1=string3;N1=3;F1=3.0	GT:FS1:FN1	0/1:.:1	1/1:ss3:2
19	3113262	rs2345	G	GC	.	PASS	S1=string4;N1=4;F1=4.0	GT:FS1:FN1	0|1:LongString4:1	1/1:.:2
19	3113268	rs2345	G	GC	.	PASS	S1=string5;N1=5;F1=5.0	GT:FS1:FN1	1|.:evenlength:1	1/1:veenlength:2
19	3113272	rs2345	G	GC	.	PASS	S1=string6;N1=6;F1=6.0	GT:FS1:FN1	1/1:ss6:1	1/1:longstring6:2



================================================
FILE: tests/test_wrapper/expected-results/test.vcf.gz
================================================
[Binary file]


================================================
FILE: tests/test_xrootd/Snakefile
================================================
from snakemake.remote.XRootD import RemoteProvider as XRootDRemoteProvider

XRootD = XRootDRemoteProvider(stay_on_remote=True)

remote_path = 'root://eospublic.cern.ch//eos/opendata/lhcb/MasterclassDatasets/'
my_files, = XRootD.glob_wildcards(remote_path+'D0lifetime/2014/mclasseventv2_D0_{n}.root')

rule all:
    input:
        expand('access_remotely/mclasseventv2_D0_{n}.root', n=my_files),
        expand('access_locally/mclasseventv2_D0_{n}.root', n=my_files)

rule access_remotely:
    input:
        XRootD.remote(remote_path+'D0lifetime/2014/mclasseventv2_D0_{n}.root')
    output:
        'access_remotely/mclasseventv2_D0_{n}.root'
    shell:
        'xrdcp {input[0]} {output[0]}'

rule access_locally:
    input:
        XRootD.remote(remote_path+'D0lifetime/2014/mclasseventv2_D0_{n}.root', stay_on_remote=False)
    output:
        'access_locally/mclasseventv2_D0_{n}.root'
    shell:
        'cp {input[0]} {output[0]}'



================================================
FILE: tests/test_yaml_config/Snakefile
================================================


configfile: "test.yaml"

rule:
    output:
        config["outfile"]
    shell:
        "touch {output}"



================================================
FILE: tests/test_yaml_config/test.yaml
================================================
---
outfile: test.out


================================================
FILE: tests/testHighWorkload/Snakefile
================================================
import os.path
import sys

instances = [os.path.basename(s[:len(s)-4]) for s in os.listdir('mfa') if s.endswith('.mfa')]

rule all:
	input: ['fisher/%s.pairs.gz'%s for s in instances] + ['ph/%s.ph'%s for s in instances]

rule extract_mismatch_counts:
	input: 'mfa/{instance}.mfa'
	output: 'fisher/{instance}.pairs.gz'
	message: 'Extracting number mismatches for each pair of sequences from {input}'
	shell: 'sleep 2; touch {output}'

rule create_tree:
	input: 'mfa/{instance}.mfa'
	output: 'ph/{instance}.ph', 'ph/{instance}.ph.log'
	message: 'Running CLUSTALW to compute NJ tree from {input}'
	shell: "sleep 2; touch {output[0]}; touch {output[1]}"



================================================
FILE: tests/testHighWorkload/mfa/00.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/01.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/02.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/03.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/04.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/05.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/06.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/07.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/08.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/09.mfa
================================================
[Empty file]


================================================
FILE: tests/testHighWorkload/mfa/10.mfa
================================================
[Empty file]


================================================
FILE: .github/CODEOWNERS
================================================
*	@johanneskoester



================================================
FILE: .github/dependabot.yml
================================================
version: 2
updates:
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"



================================================
FILE: .github/pull_request_template.md
================================================
<!--Add a description of your PR here-->

### QC
<!-- Make sure that you can tick the boxes below. -->

* [ ] The PR contains a test case for the changes or the changes are already covered by an existing test case.
* [ ] The documentation (`docs/`) is updated to reflect the changes or this is not necessary (e.g. if the change does neither modify the language nor the behavior or functionalities of Snakemake).



================================================
FILE: .github/ISSUE_TEMPLATE/bug_report.md
================================================
---
name: Bug report
about: Create a report to help us improve
title: ''
labels: bug
assignees: ''

---

<!-- Please do not post usage questions here. Ask them on Stack Overflow: https://stackoverflow.com/questions/tagged/snakemake -->

**Snakemake version**
<!--Note the Snakemake version for which you experience the bug.
Please only report bugs of the **latest stable release of Snakemake**.
If possible please check whether the bug has been already fixed in the main branch.-->

**Describe the bug**
<!--A clear and concise description of what the bug is.-->

**Logs**
<!--If applicable, any terminal output to help explain your problem.-->

**Minimal example**
<!--Add a minimal example for reproducing the bug.-->

**Additional context**
<!--Add any other context about the problem here.-->



================================================
FILE: .github/ISSUE_TEMPLATE/feature_request.md
================================================
---
name: Feature request
about: Suggest an idea for this project
title: ''
labels: enhancement
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



================================================
FILE: .github/workflows/codespell.yml
================================================
# Codespell configuration is within pyproject.toml
---
name: Codespell

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read

jobs:
  codespell:
    if: github.event.pull_request.merged != true || github.ref != 'refs/heads/main'
    name: Check for spelling errors
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Codespell
        uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_list: Crate,crate



================================================
FILE: .github/workflows/conventional-prs.yml
================================================
name: PR
on:
  pull_request_target:
    types:
      - opened
      - reopened
      - edited
      - synchronize

permissions:
  contents: read

jobs:
  title-format:
    permissions:
      pull-requests: read  # for amannn/action-semantic-pull-request to analyze PRs
      statuses: write  # for amannn/action-semantic-pull-request to mark status of analyzed PR
    runs-on: ubuntu-latest
    steps:
      - uses: amannn/action-semantic-pull-request@v5
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}



================================================
FILE: .github/workflows/docker-publish.yml
================================================
name: Publish to Docker Hub

on:
  push:
    branches:
      - main

jobs:
  update:
    if: github.repository == 'snakemake/snakemake'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Publish to Registry
        uses: elgohr/Publish-Docker-Github-Action@v5
        with:
          name: snakemake/snakemake
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_TOKEN }}
          platforms: linux/amd64,linux/arm64



================================================
FILE: .github/workflows/docs.yml
================================================
name: docs

on:
  push:
    branches:
      - main
  pull_request:

concurrency:
  # Cancel concurrent flows on PRs
  group: ci-docs-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  docs:
    if: github.event.pull_request.merged != true || github.ref != 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.14
        with:
          environments: "docs"
          pixi-version: v0.42.1
          cache: false # no pixi.lock
          locked: false
      
      - name: Build Docs
        run: |
          pixi run --environment docs build-docs

      - name: Upload documentation
        uses: actions/upload-artifact@v4
        with:
          name: docs-html
          path: docs/_build/html/

  apidocs:
    if: github.event.pull_request.merged != true || github.ref != 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.14
        with:
          environments: "docs"
          pixi-version: v0.42.1
          cache: false # no pixi.lock
          locked: false

      - name: Build API Docs
        run: |
          pixi run --environment docs build-apidocs

      - name: Upload documentation
        uses: actions/upload-artifact@v4
        with:
          name: apidocs-html
          path: apidocs/_build/html/



================================================
FILE: .github/workflows/main.yml
================================================
name: CI

on:
  push:
    branches:
      - main
  pull_request:

concurrency:
  # Cancel concurrent flows on PRs
  group: ci-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  formatting:
    if: github.event.pull_request.merged != true || github.ref != 'refs/heads/main'
    permissions:
      contents: read # for actions/checkout to fetch code
      pull-requests: write # for marocchino/sticky-pull-request-comment to create or update PR comment
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.14
        with:
          environments: "quality"
          pixi-version: v0.42.1
          cache: false # no pixi.lock
          locked: false

      - name: Run black
        run: pixi run -e quality format --check --diff .

      - name: Comment PR
        if: github.event_name == 'pull_request' && failure()
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          message: "Please format your code with [black](https://black.readthedocs.io): `black snakemake tests/*.py`."
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  ################################################################################################
  # Testing: Run tests using Pixi
  ################################################################################################
  tests:
    if: github.event.pull_request.merged != true || github.ref != 'refs/heads/main'
    strategy:
      fail-fast: false
      matrix:
        test_group: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        # see pyprojec.toml: [tool.pixi.feature.test] for available test types
        os: [ubuntu-latest, windows-latest, macos-latest] #  , macos-13 not supported yet
        env: ["py311", "py312", "py313"]
        exclude:
          - os: windows-latest
            env: "py311"
          - os: macos-latest
            env: "py311"

    runs-on: ${{ matrix.os }}

    env:
      AWS_AVAILABLE: "${{ secrets.AWS_ACCESS_KEY_ID }}"
      GCP_AVAILABLE: "${{ secrets.GCP_SA_KEY }}"
      ZENODO_SANDBOX_PAT: "${{ secrets.ZENODO_SANDBOX_PAT }}"
      CI: true
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.14
        with:
          environments: ${{ matrix.env }}
          pixi-version: v0.42.1
          cache: false # no pixi.lock
          locked: false

      # See https://github.com/apptainer/apptainer/pull/2262
      - name: Disable apparmor namespace restrictions for apptainer (os='Linux')
        if: runner.os == 'Linux'
        run: |
          sudo sh -c 'echo kernel.apparmor_restrict_unprivileged_userns=0 \
              >/etc/sysctl.d/90-disable-userns-restrictions.conf'
          sudo sysctl -p /etc/sysctl.d/90-disable-userns-restrictions.conf

      - name: Setup MinIO for AWS S3 testing (os='Linux', for AWS S3 testing)
        if: runner.os == 'Linux'
        uses: comfuture/minio-action@v1
        with:
          access_key: minio
          secret_key: minio123
          port: 9000

      - name: Test MinIO (os='Linux')
        if: runner.os == 'Linux'
        run: |
          export AWS_ACCESS_KEY_ID=minio
          export AWS_SECRET_ACCESS_KEY=minio123
          export AWS_EC2_METADATA_DISABLED=true
          aws --endpoint-url http://127.0.0.1:9000/ s3 mb s3://test

      - name: Run Tests for Linux (os= 'Linux')
        if: runner.os == 'Linux'
        run: |
          pixi run --environment ${{matrix.env}} test-all \
            --splits 10 \
            --group ${{ matrix.test_group }} \
            --splitting-algorithm=least_duration \
            --showlocals \
            --show-capture=all

          cd tests/test_report
          pixi run -e ${{ matrix.env }} snakemake \
            --use-conda \
            --cores 1 \
            --report report.zip

      - name: Run Tests for MacOS (os='macOS')
        if: runner.os == 'macOS'
        run: |
          pixi run --environment ${{matrix.env}} test-simple \
          --splits 10 \
          --group ${{ matrix.test_group }} \
          --splitting-algorithm=least_duration \
          --showlocals \
          --show-capture=all

      - name: Run Tests for Windows (os='Windows')
        if: runner.os == 'Windows'
        run: |
          pixi run --environment ${{matrix.env}} test-simple --splits 10 --group ${{ matrix.test_group }} --splitting-algorithm=least_duration --showlocals --show-capture=all

  build-container-image:
    if: github.event.pull_request.merged != true || github.ref != 'refs/heads/main'
    runs-on: ubuntu-latest
    needs: tests
    steps:
      - uses: actions/checkout@v4

      - name: Build container image
        run: docker build .

  testing-done:
    if: github.event.pull_request.merged != true || github.ref != 'refs/heads/main'
    needs:
      - tests
    runs-on: ubuntu-latest
    steps:
      - run: echo "All tests passed."



================================================
FILE: .github/workflows/release-please.yml
================================================
on:
  push:
    branches:
      - main

name: release-please

jobs:
  release-please:
    runs-on: ubuntu-latest
    steps:

      - uses: googleapis/release-please-action@v4
        id: release
        with:
          release-type: python
          token: ${{ secrets.RELEASE_PLEASE_PR_CI_TOKEN }}

      - uses: actions/checkout@v4
        if: ${{ steps.release.outputs.release_created }}
        with:
          fetch-depth: 0

      - name: Set up Pixi
        if: ${{ steps.release.outputs.release_created }}
        uses: prefix-dev/setup-pixi@v0.8.14
        with:
          environments: quality
          pixi-version: v0.47.0
          cache: false
          locked: false

      - name: Build and check package
        if: ${{ steps.release.outputs.release_created }}
        run: |
          pixi run --environment publish build-check

      - name: Publish to PyPI
        if: ${{ steps.release.outputs.release_created }}
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          user: __token__
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Publish to Registry
        if: ${{ steps.release.outputs.release_created }}
        uses: elgohr/Publish-Docker-Github-Action@v5
        with:
          name: snakemake/snakemake
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_TOKEN }}
          tags: "v${{ steps.release.outputs.major }}.${{ steps.release.outputs.minor }}.${{ steps.release.outputs.patch }},stable"



================================================
FILE: .github/workflows/stale.yml
================================================
name: Close inactive issues
on:
  schedule:
    - cron: "1 1 * * 1"

jobs:
  close-issues:
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/stale@v9
        with:
          days-before-issue-stale: 180
          days-before-issue-close: 365
          stale-issue-label: "stale"
          stale-issue-message: "This issue was marked as stale because it has been open for 6 months with no activity."
          close-issue-message: "This issue was closed because it has been inactive for 1 year since being marked as stale. Feel free to re-open it if you have any further comments."
          days-before-pr-stale: 180
          days-before-pr-close: -1
          stale-pr-message: "This PR was marked as stale because it has been open for 6 months with no activity."
          repo-token: ${{ secrets.GITHUB_TOKEN }}



================================================
FILE: .github/workflows/scripts/sacct-proxy.py
================================================
#!python

import argparse
import subprocess as sp

parser = argparse.ArgumentParser()
parser.add_argument("--name")
parser.add_argument("--format")
parser.add_argument("-X", action="store_true")
parser.add_argument("--parsable2", action="store_true")
parser.add_argument("--noheader", action="store_true")
parser.add_argument("-n", action="store_true")
parser.add_argument("-u")
parser.add_argument("-o")


args = parser.parse_args()

if args.n and args.u and args.o:
    # mimic account query from the executor
    print("runner")
elif args.name:
    sp.call(["squeue", "--noheader", "--format", "%F|%T", "--name", args.name])
else:
    raise ValueError("Unsupported arguments")


